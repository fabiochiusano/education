{"content": "By Matthew Mayo , KDnuggets. comments Many methods for feature selection exist, some of which treat the process strictly as an artform, others as a science, while, in reality, some form of domain knowledge along with a disciplined approach are likely your best bet. When it comes to disciplined approaches to feature selection, wrapper methods are those which marry the feature selection process to the type of model being built, evaluating feature subsets in order to detect the model performance between features, and subsequently select the best performing subset. In other words, instead of existing as an independent process taking place prior to model building, wrapper methods attempt to optimize feature selection process for a given machine learning algorithm in tandem with this algorithm. 2 prominent wrapper methods for feature selection are step forward feature selection and step backward features selection. Image source Step forward feature selection starts with the evaluation of each individual feature, and selects that which results in the best performing selected algorithm model. What's the \"best?\" That depends entirely on the defined evaluation criteria (AUC, prediction accuracy, RMSE, etc.). Next, all possible combinations of the that selected feature and a subsequent feature are evaluated, and a second feature is selected, and so on, until the required predefined number of features is selected. Step backward feature selection is closely related, and as you may have guessed starts with the entire set of features and works backward from there, removing features to find the optimal subset of a predefined size. These are both potentially very computationally expensive. Do you have a large, multidimensional dataset? These methods may take too long to be at all useful, or may be totally infeasible. That said, with a dataset of accommodating size and dimensionality, such an approach may well be your best possible approach. To see how they work, let's take a look at step forward feature selection, specifically. Note that, as discussed, a machine learning algorithm must be defined prior to beginning our symbiotic feature selection process. Keep in mind that an optimized set of selected features using a given algorithm may or may not perform equally well with a different algorithm. If we select features using logistic regression, for example, there is no guarantee that these same features will perform optimally if we then tried them out using K-nearest neighbors, or an SVM. \u00a0 Implementing Feature Selection and Building a Model \u00a0 So, how do we perform step forward feature selection in Python? Sebastian Raschka's mlxtend library includes an implementation ( Sequential Feature Selector ), and so we will use it to demonstrate. It goes without saying that you should have mlxtend installed before moving forward (check the Github repo). We will use a Random Forest classifier for feature selection and model building (which, again, are intimately related in the case of step forward feature selection). We need data to use for demonstration, so let's use the wine quality dataset . Specifically, I have used the untouched winequality-white.csv file as input in the code below. Arbitrarily, we will set the desired number of features to 5 (there are 12 in the dataset). What we are able to do is compare the evaluation scores for each iteration of the feature selection process, and so keep in mind that if we find that a lower number of features has a better score we can alternatively choose that best-performing subset to run with in our \"live\" model moving forward. Also keep in mind that setting our desired number of features too low could lead to a sub-optimal number and combination of features being decided upon (say, if some combination of 11 features in our case is better than the best combination of <= 10 features we find during the selection process). Since we are more interested in demonstrating how to implement step forward feature selection than we are with the actual results on this particular dataset, we won't be overly concerned with the actual performance of our models, but we will compare the performances anyhow, as to show how it would be done in a meaningful project. First, we will make our imports, load the dataset, and split it into training and testing sets. HTML generated using hilite.me import numpy as np \r import pandas as pd \r from sklearn.ensemble import RandomForestClassifier\r from sklearn.model_selection import train_test_split\r from sklearn.metrics import accuracy_score as acc\r from mlxtend.feature_selection import SequentialFeatureSelector as sfs\r \r # Read data \r df = pd . \r \r # Train/test split \r X_train, X_test, y_train, y_test = \r \r y_train = y_train . \r y_test = y_test . \r \r print ( 'Training dataset shape:' , X_train . shape, y_train . shape)\r print ( 'Testing dataset shape:' , X_test . shape, y_test . shape)\r \u00a0 \r Training dataset shape: (3673, 11) (3673,)\r Testing dataset shape: (1225, 11) (1225,) Next, we will define a classifier, as well as a step forward feature selector, and then perform our feature selection. The feature feature selector in mlxtend has some parameters we can define, so here's how we will proceed: First, we pass our classifier, the Random Forest classifier defined above the feature selector Next, we define the subset of features we are looking to select (k_features=5) We then set floating to False; see the documentation for more info on floating: The floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded), so that a larger number of feature subset combinations can be sampled. We set he desired level of verbosity for mlxtend to report Importantly, we set our scoring to accuracy; this is but one metric which could be used to score our resulting models built on the selected features mlxtend feature selector uses cross validation internally, and we set our desired folds to 5 for our demonstration The dataset we chose isn't very large, and so the following code should not take long to execute. HTML generated using hilite.me # Build RF classifier to use in feature selection \r clf = \r \r # Build step forward feature selection \r sfs1 = \r \r # Perform SFFS \r sfs1 = sfs1 . \r \u00a0 \r []: Done 1 out of 1 | elapsed: 2.2s remaining: 0.0s\r []: Done 11 out of 11 | elapsed: 24.3s finished\r \r [2018-06-12 14:47:47] Features: 1/5 -- score: 0.49768148939247264[]: Done 1 out of 1 | elapsed: 2.2s remaining: 0.0s\r []: Done 10 out of 10 | elapsed: 22.7s finished\r \r [2018-06-12 14:48:09] Features: 2/5 -- score: 0.5442629071398873[]: Done 1 out of 1 | elapsed: 2.7s remaining: 0.0s\r []: Done 9 out of 9 | elapsed: 21.2s finished\r \r [2018-06-12 14:48:31] Features: 3/5 -- score: 0.6052194438136681[]: Done 1 out of 1 | elapsed: 2.5s remaining: 0.0s\r []: Done 8 out of 8 | elapsed: 20.3s finished\r \r [2018-06-12 14:48:51] Features: 4/5 -- score: 0.6261526236769334[]: Done 1 out of 1 | elapsed: 2.4s remaining: 0.0s\r []: Done 7 out of 7 | elapsed: 17.3s finished\r \r [2018-06-12 14:49:08] Features: 5/5 -- score: 0.6444222989869156 Our best performing model, given our scoring metric, is some subset of 5 features, with a score of 0.644 (remember that this is using cross validation, and so will be different than that which is reported on our full models below, using train and test sets). But which subset of 5 features were selected? HTML generated using hilite.me # Which features? \r feat_cols = list (sfs1 . k_feature_idx_)\r print (feat_cols)\r \u00a0 \r [1, 2, 3, 7, 10]\r The columns at these indexes are those which were selected. Great! So what now...? We can now use those features to build a full model using our training and test sets. If we had a much larger set (i.e. many more instances as opposed to many more features), this would be especially beneficial as we could have used the feature selector above on a smaller subset of instances, determined our best performing subset of features, and then applied them to the full dataset for classification. The code below builds a classifier on only the subset of selected features. HTML generated using hilite.me # Build full model with selected features \r clf = \r clf . \r \r y_train_pred = clf . \r print ( 'Training accuracy on selected features: %.3f ' % )\r \r y_test_pred = clf . \r print ( 'Testing accuracy on selected features: %.3f ' % )\r \u00a0 \r Training accuracy on selected features: 0.558\r Testing accuracy on selected features: 0.512\r Don't worry about the actual accuracies; we're concerned here with the process, not the end result. But what if we were concerned with the end result, and wanted to know if our feature selection troubles had been worth it? Well, we could compare the resultant accuracies of the full model built using the selected features (immediately above) with the resultant accuracies of another full model using all of the features, just as we do below: HTML generated using hilite.me # Build full model on ALL features, for comparison \r clf = \r clf . \r \r y_train_pred = clf . \r print ( 'Training accuracy on all features: %.3f ' % )\r \r y_test_pred = clf . \r print ( 'Testing accuracy on all features: %.3f ' % )\r \u00a0 \r Training accuracy on all features: 0.566\r Testing accuracy on all features: 0.509\r And there you have them for comparison. They are both poor and comparable to our model built with the selected features (though I promise this is not always the case!). With very little work, you could see how these selected features perform with a different algorithm, to help scratch that itch as to wondering whether these features selected with one algorithm are equally well performing with another. Such a feature selection method can be an effective part of a disciplined machine learning pipeline. Keep in mind that step forward (or step backward) methods, specifically, can provide problems when dealing with especially large or highly-dimensional datasets. There are ways of getting around (or trying to get around) these sticking points, such as sampling from the data to find the feature subset which works best, and then using these features for the modeling process on the full dataset. Of course, these are not the only disciplined approaches to feature selection either, and so checking out alternatives may be warranted when dealing with these larger datasets. \u00a0 Related : Quick Feature Engineering with Dates Using fast.ai Generating Text with RNNs in 4 Lines of Code Multi-objective Optimization for Feature Selection", "title_html": "<h1 id=\"title\">Step Forward Feature Selection: A Practical Example in Python</h1> ", "url": "https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html", "tfidf": {"tfidf": {"matthew": 6.908616187989999, "onc": 1.4974533106999999, "too": 3.6317053643, "troubl": 4.99088337001, "comparison": 9.919400187439999, "form": 1.12755681818, "immedi": 2.02862254025, "fastai": 1587.6, "addit": 1.24634950542, "featcol": 3175.2, "number": 6.60857499654, "etc": 4.2066772655, "dataset": 2904.1463414699997, "intim": 14.459016393399999, "iter": 37.4433962264, "suboptim": 588.0, "python": 56.2978723404, "well": 5.3278743539999995, "whether": 2.20683903253, "approach": 10.37782716695, "know": 2.59327017315, "concern": 5.655860349120001, "chose": 4.42105263158, "equal": 5.08438751, "particular": 1.3814827706200001, "instead": 1.59461631177, "traintest": 1587.6, "how": 9.61501968306, "test": 23.91364016739, "place": 1.1004366812200002, "elaps": 872.3076923079999, "had": 2.0951501154799996, "especi": 3.33424341068, "would": 2.1657458563599996, "end": 2.21360847742, "given": 4.06278256419, "word": 1.7965372864099998, "interest": 1.60331246213, "guarante": 6.57119205298, "will": 11.02329887364, "full": 13.33837429112, "sklearnmetr": 1587.6, "second": 1.1130898128, "repo": 369.209302326, "next": 4.485168094920001, "below": 9.02430012788, "perform": 21.439567859500002, "but": 4.06529671596, "demonstr": 10.59989984976, "need": 1.4372623574099999, "our": 44.79417879421, "classif": 8.067073170730001, "xtrain": 3175.2, "point": 1.25990000794, "mlxtendfeatureselect": 1587.6, "level": 1.6544393497299998, "has": 2.0872995004, "generat": 12.31652443752, "paramet": 17.256521739100002, "pass": 1.61818367139, "panda": 111.802816901, "subsequ": 3.5069582505000003, "sequenti": 39.5910224439, "out": 12.72200333892, "model": 37.6307611272, "treat": 3.59023066486, "clf": 14288.4, "disciplin": 27.71890004364, "comment": 3.05954904606, "cours": 2.15092805853, "optim": 57.688953488500005, "anyhow": 360.818181818, "forward": 40.32232740712, "print": 23.08037383176, "not": 5.07836990595, "are": 16.47849497248, "altern": 4.278092158440001, "logist": 14.0994671403, "specif": 5.615847187829999, "quick": 2.205, "poor": 2.42196796339, "metric": 44.470588235200005, "said": 1.54751925139, "though": 1.36076112111, "then": 5.432893025799999, "index": 6.9969149405, "fold": 12.3452566096, "they": 3.09051975861, "anoth": 2.27287043664, "instal": 3.78721374046, "effect": 1.3963060686000002, "let": 6.97233201582, "scienc": 2.31969608416, "kdnugget": 1587.6, "larger": 6.722371206779998, "github": 1587.6, "hilitem": 7938.0, "featur": 126.75144286246, "until": 1.14852058164, "provid": 1.21552714187, "either": 1.5830092731099998, "such": 3.18454132122, "accuraci": 165.9067524114, "those": 3.58644578313, "without": 1.29547123623, "some": 5.2018348624, "sourc": 1.69760479042, "should": 3.3286508019800003, "both": 2.10431440122, "result": 8.022812590240001, "document": 2.5409731114, "subset": 327.9036144576, "use": 26.770607691879995, "set": 14.24495289372, "info": 37.7102137767, "from": 6.00340328982, "done": 25.632760898249998, "num": 80.02520320080001, "instanc": 6.514567090680001, "exclud": 5.31859296482, "for": 15.004725600150001, "depend": 2.2411067193700003, "predict": 5.18484650555, "finish": 16.14399023795, "inclus": 8.756756756760002, "warrant": 15.703264095, "sequentialfeatureselector": 1587.6, "with": 23.027558806769996, "verbos": 378.0, "determin": 2.1658935879900003, "split": 6.9418452120600005, "better": 4.0131445905000005, "meaning": 21.8076923077, "look": 3.8172637653199994, "numpi": 1587.6, "veri": 3.77640342531, "intern": 1.30355530011, "accuracyscor": 1587.6, "work": 4.46080359652, "combin": 8.4880239521, "forest": 9.79093432008, "promis": 3.5030891438699996, "column": 7.078020508250001, "again": 1.50883862384, "compar": 7.464911249560001, "remov": 4.011623499680001, "littl": 1.5499365420299998, "tandem": 45.7521613833, "valid": 13.22448979592, "xtest": 3175.2, "defin": 16.369822993619998, "train": 17.4291290559, "great": 1.26592775696, "step": 36.763092269279994, "scratch": 25.8146341463, "sklearnensembl": 1587.6, "stick": 11.5377906977, "type": 2.0281042411900003, "randomforestclassifi": 1587.6, "note": 1.42449528937, "fals": 6.21613155834, "oppos": 2.51282051282, "machin": 12.073003802279999, "into": 1.01502461479, "prior": 4.35615310742, "realiti": 4.563380281690001, "abov": 5.7114761961900005, "requir": 1.52844902282, "traintestsplit": 1587.6, "close": 1.2848818387799998, "what": 5.01373756512, "rememb": 4.88793103448, "choos": 4.17899447223, "guess": 25.0410094637, "help": 1.39962972759, "larg": 3.55724848755, "find": 6.917647058839999, "line": 1.4182597820299998, "librari": 2.68266306185, "relat": 3.71252630757, "deal": 4.36693714758, "decid": 1.9257641921400002, "sebastian": 26.8175675676, "here": 4.84615384616, "lower": 2.10055570257, "report": 2.7268979732, "were": 4.09835430784, "wrapper": 756.0, "long": 2.5314518057799997, "much": 1.1942229577299999, "symbiot": 93.3882352941, "evalu": 34.754816112099995, "about": 1.06486015159, "problem": 1.76674827509, "selector": 685.294964028, "mind": 14.367420814479999, "just": 1.33580143037, "entir": 3.18731178478, "sklearnmodelselect": 1587.6, "dure": 1.0503473370799998, "classifi": 31.7625875292, "check": 13.0131147541, "expens": 3.5453327378300004, "multidimension": 160.363636364, "marri": 2.89233011478, "live": 1.30591428806, "start": 2.53347163488, "run": 1.55692850838, "criteria": 11.7426035503, "them": 3.29628347982, "load": 6.80497213888, "keep": 8.16981860284, "execut": 2.2363713199, "order": 1.24625166811, "raschka": 1587.6, "abl": 1.8208510150200001, "mani": 3.13280273631, "over": 1.02525024217, "date": 1.63081664099, "build": 14.7075656202, "algorithm": 251.55633802859998, "total": 1.5460122699399999, "select": 95.10221769034001, "acc": 115.043478261, "same": 1.11857958148, "rnns": 1587.6, "there": 5.20456333595, "exist": 2.9294215333599998, "highlydimension": 1587.6, "move": 2.58251321676, "sfs": 1058.4, "possibl": 2.8347468976, "say": 3.5088960106, "best": 14.245663010940003, "exampl": 1.50483412322, "knowledg": 3.3981164383599998, "take": 4.55846672888, "follow": 1.04640126549, "now": 2.321561746, "tri": 3.7089125102199993, "code": 15.522855047679998, "strict": 4.7235941684, "desir": 12.00680657968, "than": 3.0983606557499996, "regress": 51.2129032258, "goe": 4.251740760580001, "accommod": 5.70668583753, "also": 1.01476510067, "around": 2.42789417342, "bestperform": 1587.6, "method": 18.00000000001, "artform": 721.636363636, "the": 58.0, "may": 7.3641243125099995, "mayo": 49.7680250784, "bet": 29.1838235294, "promin": 2.39746300211, "want": 1.99698113208, "ytest": 6350.4, "individu": 1.8004082558400003, "actual": 5.62446858762, "ytrainpr": 3175.2, "backward": 58.421343146400005, "which": 10.051918449999999, "like": 1.14918566775, "infeas": 233.470588235, "upon": 1.60331246213, "other": 2.01984732824, "appli": 2.2972073506, "benefici": 18.269275028800003, "one": 2.01254991444, "show": 1.26703910615, "begin": 1.3305397251100002, "see": 3.81726376533, "imag": 2.70137825421, "get": 3.5712518277, "random": 14.3804347826, "implement": 10.729443568379999, "wonder": 7.265903890160001, "arbitrarili": 50.8846153846, "numf": 185.14285714279998, "file": 3.7710213776699995, "detect": 5.41288782816, "process": 15.25723438338, "knearest": 1587.6, "independ": 1.58950740889, "includ": 2.0381282495599997, "this": 6.02276176026, "sampl": 14.46560364464, "wine": 11.0326615705, "part": 1.04330682789, "worri": 10.302401038300001, "smaller": 2.59369384088, "project": 1.7534791252500002, "differ": 3.7096347067499997, "sfsnum": 6350.4, "been": 1.0239277652399998, "attempt": 1.4721810088999998, "proceed": 3.4333910034599997, "predefin": 417.78947368400003, "multiobject": 1587.6, "between": 1.03453668708, "low": 2.13072070863, "kfeaturesnum": 1587.6, "domain": 9.39408284024, "alway": 2.06745670009, "along": 1.2973768080399999, "all": 7.080275229370001, "itch": 193.609756098, "input": 12.2029208301, "ytrain": 6350.4, "case": 4.45496211768, "that": 17.06772908375, "built": 7.97788944724, "kfeatureidx": 1587.6, "more": 4.0686827268, "and": 28.001763779639997, "list": 1.36321483771, "discuss": 2.19676214197, "these": 10.741542625200001, "float": 32.7115384614, "befor": 1.10036041031, "lead": 1.2664326739, "untouch": 43.495890411000005, "mlxtend": 7938.0, "can": 7.05756834852, "ytestpr": 3175.2, "comput": 3.9277585353800006, "make": 1.0762660158600001, "way": 1.2190739461, "onli": 2.0512953033200003, "each": 2.37949640288, "size": 4.9877474081, "read": 2.3149606299200003, "neighbor": 5.781500364169999, "remain": 5.82990599295, "while": 1.0441988950299999, "data": 10.12930667802, "qualiti": 2.9329392204, "shape": 25.62711864408, "sinc": 1.08368600683, "come": 1.32831325301, "must": 1.9220338983099996, "cross": 4.66255506608, "worth": 5.210370856580001, "text": 3.12827586207, "dimension": 54.1843003413, "potenti": 2.52080025405, "import": 10.721593786960002, "first": 2.01523229246, "engin": 2.47135740971, "could": 6.0218479745, "when": 3.0623030926499997, "have": 7.104263887979998, "score": 47.17341977315, "exclus": 3.40906162766, "learn": 6.968251645950001, "pipelin": 32.1376518219, "winequalitywhitecsv": 1587.6}, "logtfidf": {"matthew": 1.9327693554900003, "onc": 0.403765872355, "too": 1.1931103094439999, "troubl": 1.60761292215, "comparison": 3.20269054786, "form": 0.120053184191, "immedi": 0.707357011133, "fastai": 7.369978720910001, "addit": 0.220218882972, "featcol": 14.739957441820001, "number": 0.5796514705116, "etc": 1.4366730879700003, "dataset": 78.9876684996, "intim": 2.67131819183, "iter": 3.62283035867, "suboptim": 6.3767269479, "python": 4.03065674296, "well": 0.317572191578, "whether": 0.791561189647, "approach": 3.6511680729050005, "know": 0.952919694398, "concern": 1.902239846619, "chose": 1.48637781968, "equal": 1.866054782686, "particular": 0.323157393804, "instead": 0.46663315041500003, "traintest": 7.369978720910001, "how": 2.8294017415800004, "test": 8.795019933927, "place": 0.0957070839572, "elaps": 44.6855712683, "had": 0.0929560488222, "especi": 1.022197219418, "would": 0.1592352559294, "end": 0.202953597236, "given": 0.9097674324930001, "word": 0.585861082385, "interest": 0.47207177798199995, "guarante": 1.8826952548500002, "will": 1.825078814235, "full": 4.089628993184, "sklearnmetr": 7.369978720910001, "second": 0.10713976337999999, "repo": 5.91136369821, "next": 1.206491056497, "below": 3.254506367744, "perform": 5.96653190812, "but": 0.0647694882876, "demonstr": 3.8982007672759997, "need": 0.362740163442, "our": 16.295145069458002, "classif": 2.08779073629, "xtrain": 14.739957441820001, "point": 0.23103235903299998, "mlxtendfeatureselect": 7.369978720910001, "level": 0.503462189943, "has": 0.0854478897096, "generat": 4.3150940504160005, "paramet": 2.8481901438599997, "pass": 0.48130432974, "panda": 4.7167367562999996, "subsequ": 1.123203771814, "sequenti": 3.6786023866, "out": 0.7011166910316, "model": 13.274101315998001, "treat": 1.27821645249, "clf": 66.32980848819001, "disciplin": 7.7432805258, "comment": 1.11826753454, "cours": 0.765899404133, "optim": 12.228138977049998, "anyhow": 5.8883741799800005, "forward": 14.28911079959, "print": 8.35150742769, "not": 0.0777620650375, "are": 0.4714795773232, "altern": 1.520719944564, "logist": 2.6461370052, "specif": 1.8809405026230002, "quick": 0.790727508899, "poor": 0.8845804177050001, "metric": 6.203361703119999, "said": 0.436653165815, "though": 0.308044191079, "then": 0.4151693261545, "index": 1.94546932912, "fold": 2.5132719091099998, "they": 0.0891809843028, "anoth": 0.255792723304, "instal": 1.3316305879, "effect": 0.333830227158, "let": 2.4976051345599997, "scienc": 0.841436178891, "kdnugget": 7.369978720910001, "larger": 2.420485985334, "github": 7.369978720910001, "hilitem": 36.849893604550005, "featur": 35.141155705786, "until": 0.138474663439, "provid": 0.19517784432500002, "either": 0.459327638815, "such": 0.179087933418, "accuraci": 33.1041950278, "those": 0.5356481726189999, "without": 0.258874517941, "some": 0.1978675453225, "sourc": 0.529218310751, "should": 1.018839753516, "both": 0.10168506677860001, "result": 0.954652358667, "document": 0.932547122383, "subset": 39.6937566846, "use": 0.7594085130216001, "set": 2.0579521354679997, "info": 3.6299309802199997, "from": 0.0034023250131959997, "done": 9.305735814419, "num": 0.025199231631760004, "instanc": 2.36178715944, "exclud": 1.67120878808, "for": 0.004724855930955001, "depend": 0.806969815, "predict": 1.6457402376899999, "finish": 5.86054973245, "inclus": 2.16982560315, "warrant": 2.7538685948799997, "sequentialfeatureselector": 7.369978720910001, "with": 0.02754230940797, "verbos": 5.934894195619999, "determin": 0.772833019022, "split": 2.48884087864, "better": 1.3928558812, "meaning": 3.08226276571, "look": 1.2927733872, "numpi": 7.369978720910001, "veri": 0.6904793797140001, "intern": 0.265095377816, "accuracyscor": 7.369978720910001, "work": 0.436138269092, "combin": 2.6460915537550003, "forest": 3.1766194152, "promis": 1.25364519176, "column": 1.95699427938, "again": 0.411340231612, "compar": 2.4956767237079998, "remov": 1.3920976831760001, "littl": 0.438213989466, "tandem": 3.8232390339599998, "valid": 3.7778464353600003, "xtest": 14.739957441820001, "defin": 6.0220806555, "train": 5.948264815551, "great": 0.235805258079, "step": 13.51408574074, "scratch": 3.2509415461, "sklearnensembl": 7.369978720910001, "stick": 2.4456277954099996, "type": 0.707101485387, "randomforestclassifi": 7.369978720910001, "note": 0.353817568083, "fals": 1.8271477773099998, "oppos": 0.921405832541, "machin": 4.17707874186, "into": 0.0149128632287, "prior": 1.556884345042, "realiti": 1.51806363875, "abov": 1.9315956894480002, "requir": 0.424253510675, "traintestsplit": 7.369978720910001, "close": 0.250666759864, "what": 0.903549187308, "rememb": 1.5867691126199999, "choos": 1.43007066072, "guess": 3.22051485947, "help": 0.336207721344, "larg": 0.511125181818, "find": 2.191125321152, "line": 0.349430614452, "librari": 0.986809980943, "relat": 0.639300904962, "deal": 1.561829402506, "decid": 0.655322871893, "sebastian": 3.2890571790200003, "here": 1.7700763767400003, "lower": 0.742201929994, "report": 0.6200350180740001, "were": 0.09716457472439999, "wrapper": 16.58828726253, "long": 0.471291587756, "much": 0.17749572930100002, "symbiot": 4.53676537685, "evalu": 9.69440121565, "about": 0.0628434774746, "problem": 0.569140724273, "selector": 28.428539284619998, "mind": 5.114675355319999, "just": 0.289531434109, "entir": 0.9320613605399999, "sklearnmodelselect": 7.369978720910001, "dure": 0.0491209066894, "classifi": 9.9991778109, "check": 3.74562099124, "expens": 1.26563201674, "multidimension": 5.0774439637699995, "marri": 1.06206244535, "live": 0.266903399347, "start": 0.472886738582, "run": 0.442714975539, "criteria": 2.4632235573, "them": 0.2825499807279, "load": 1.91765354188, "keep": 2.8566093786919997, "execut": 0.804854605864, "order": 0.22014038079300002, "raschka": 7.369978720910001, "abl": 0.599303982475, "mani": 0.1299472743663, "over": 0.0249367214957, "date": 0.489080896097, "build": 4.4202370688190005, "algorithm": 29.97398155662, "total": 0.43567888670500005, "select": 33.125820295251, "acc": 4.74531012875, "same": 0.112059649604, "rnns": 7.369978720910001, "there": 0.2004894646275, "exist": 0.7633155881739999, "highlydimension": 7.369978720910001, "move": 0.511231718506, "sfs": 6.964513612799999, "possibl": 0.697610949782, "say": 1.124308561104, "best": 4.133051396523, "exampl": 0.40868267499899996, "knowledg": 1.2232212893899999, "take": 0.522767848788, "follow": 0.045356911094199995, "now": 0.298185890042, "tri": 1.23518305832, "code": 5.42407638388, "strict": 1.55256998618, "desir": 4.396717371359999, "than": 0.0967825866546, "regress": 3.9359915164199997, "goe": 1.4473284897999998, "accommod": 1.7416384414200001, "also": 0.0146571578, "around": 0.38775421156400003, "bestperform": 7.369978720910001, "method": 6.611231261887, "artform": 6.58152136054, "the": 0.0, "may": 0.3549699669908, "mayo": 3.90737271112, "bet": 3.3736145670499997, "promin": 0.8744110957960001, "want": 0.6916366062549999, "ytest": 29.479914883640003, "individu": 0.588013447985, "actual": 1.885542544944, "ytrainpr": 14.739957441820001, "backward": 10.72554770712, "which": 0.0517841384543, "like": 0.139053576545, "infeas": 5.45305610873, "upon": 0.47207177798199995, "other": 0.01974949583952, "appli": 0.8316941898119999, "benefici": 2.90522068864, "one": 0.012510703291, "show": 0.236682766013, "begin": 0.285584668268, "see": 0.722764756476, "imag": 0.99376210729, "get": 1.159538011564, "random": 3.9454428130199997, "implement": 3.8231382272100003, "wonder": 1.98319270637, "arbitrarili": 3.9295606260900002, "numf": 15.339333466960001, "file": 1.32734588723, "detect": 1.68878274493, "process": 4.750462791225, "knearest": 7.369978720910001, "independ": 0.463424162503, "includ": 0.037769362781, "this": 0.022718694315, "sampl": 3.9572529767800004, "wine": 2.40086010702, "part": 0.04239531098280001, "worri": 2.3323769785799997, "smaller": 0.9530830530519999, "project": 0.561601885907, "differ": 0.6369633639360001, "sfsnum": 29.479914883640003, "been": 0.023645982368400004, "attempt": 0.38674498075099994, "proceed": 1.23354840355, "predefin": 10.68366094724, "multiobject": 7.369978720910001, "between": 0.033953681165299995, "low": 0.7564602833490001, "kfeaturesnum": 7.369978720910001, "domain": 2.24008000599, "alway": 0.726319204572, "along": 0.260344385917, "all": 0.07981842468459999, "itch": 5.26584456664, "input": 2.50167533539, "ytrain": 29.479914883640003, "case": 1.186218806667, "that": 0.06759452245388, "built": 2.76151814026, "kfeatureidx": 7.369978720910001, "more": 0.06809972639999999, "and": 0.0017637239777808, "list": 0.309845761506, "discuss": 0.78698452262, "these": 0.715336194008, "float": 7.16734675545, "befor": 0.0956377718795, "lead": 0.23620402986699998, "untouch": 3.7726664603199995, "mlxtend": 36.849893604550005, "can": 0.974046578364, "ytestpr": 14.739957441820001, "comput": 1.36806891594, "make": 0.07349765782289999, "way": 0.19809150993500002, "onli": 0.050648536658199995, "each": 0.347483378608, "size": 1.8276744121219999, "read": 0.83939268088, "neighbor": 1.7546632275799998, "remain": 0.76781481545, "while": 0.04324998379380001, "data": 3.6504617544, "qualiti": 1.07600506711, "shape": 9.3136765692, "sinc": 0.0803681994577, "come": 0.28390990653000003, "must": 0.653383947388, "cross": 1.692832829518, "worth": 1.65065103492, "text": 1.14048200999, "dimension": 3.99239120489, "potenti": 0.9245764122419999, "import": 2.342546216528, "first": 0.015174579624319999, "engin": 0.904767558276, "could": 0.9297813614500001, "when": 0.0616649665752, "have": 0.1034950163884, "score": 16.015288528470002, "exclus": 1.22643707092, "learn": 2.528256194235, "pipelin": 3.47002829672, "winequalitywhitecsv": 7.369978720910001}, "logidf": {"matthew": 1.9327693554900003, "onc": 0.403765872355, "too": 0.5965551547219999, "troubl": 1.60761292215, "comparison": 1.60134527393, "form": 0.120053184191, "immedi": 0.707357011133, "fastai": 7.369978720910001, "addit": 0.220218882972, "featcol": 7.369978720910001, "number": 0.0966085784186, "etc": 1.4366730879700003, "dataset": 5.26584456664, "intim": 2.67131819183, "iter": 3.62283035867, "suboptim": 6.3767269479, "python": 4.03065674296, "well": 0.0635144383156, "whether": 0.791561189647, "approach": 0.7302336145810001, "know": 0.952919694398, "concern": 0.634079948873, "chose": 1.48637781968, "equal": 0.933027391343, "particular": 0.323157393804, "instead": 0.46663315041500003, "traintest": 7.369978720910001, "how": 0.47156695693000006, "test": 0.977224437103, "place": 0.0957070839572, "elaps": 4.46855712683, "had": 0.0464780244111, "especi": 0.511098609709, "would": 0.0796176279647, "end": 0.101476798618, "given": 0.303255810831, "word": 0.585861082385, "interest": 0.47207177798199995, "guarante": 1.8826952548500002, "will": 0.202786534915, "full": 0.511203624148, "sklearnmetr": 7.369978720910001, "second": 0.10713976337999999, "repo": 5.91136369821, "next": 0.402163685499, "below": 0.813626591936, "perform": 0.42618085058, "but": 0.0161923720719, "demonstr": 0.9745501918189999, "need": 0.362740163442, "our": 0.8576392141820001, "classif": 2.08779073629, "xtrain": 7.369978720910001, "point": 0.23103235903299998, "mlxtendfeatureselect": 7.369978720910001, "level": 0.503462189943, "has": 0.0427239448548, "generat": 0.719182341736, "paramet": 2.8481901438599997, "pass": 0.48130432974, "panda": 4.7167367562999996, "subsequ": 0.561601885907, "sequenti": 3.6786023866, "out": 0.0584263909193, "model": 0.7374500731110001, "treat": 1.27821645249, "clf": 7.369978720910001, "disciplin": 1.93582013145, "comment": 1.11826753454, "cours": 0.765899404133, "optim": 2.4456277954099996, "anyhow": 5.8883741799800005, "forward": 1.29901007269, "print": 1.19307248967, "not": 0.0155524130075, "are": 0.0294674735827, "altern": 0.760359972282, "logist": 2.6461370052, "specif": 0.626980167541, "quick": 0.790727508899, "poor": 0.8845804177050001, "metric": 3.1016808515599994, "said": 0.436653165815, "though": 0.308044191079, "then": 0.08303386523089999, "index": 1.94546932912, "fold": 2.5132719091099998, "they": 0.0297269947676, "anoth": 0.127896361652, "instal": 1.3316305879, "effect": 0.333830227158, "let": 1.2488025672799998, "scienc": 0.841436178891, "kdnugget": 7.369978720910001, "larger": 0.806828661778, "github": 7.369978720910001, "hilitem": 7.369978720910001, "featur": 0.423387418142, "until": 0.138474663439, "provid": 0.19517784432500002, "either": 0.459327638815, "such": 0.059695977806, "accuraci": 2.5464765406, "those": 0.17854939087299998, "without": 0.258874517941, "some": 0.0395735090645, "sourc": 0.529218310751, "should": 0.509419876758, "both": 0.050842533389300004, "result": 0.136378908381, "document": 0.932547122383, "subset": 3.3078130570499997, "use": 0.0292080197316, "set": 0.171496011289, "info": 3.6299309802199997, "from": 0.000567054168866, "done": 0.845975983129, "num": 0.00031499039539700004, "instanc": 1.18089357972, "exclud": 1.67120878808, "for": 0.00031499039539700004, "depend": 0.806969815, "predict": 1.6457402376899999, "finish": 1.17210994649, "inclus": 2.16982560315, "warrant": 2.7538685948799997, "sequentialfeatureselector": 7.369978720910001, "with": 0.00119749171339, "verbos": 5.934894195619999, "determin": 0.772833019022, "split": 1.24442043932, "better": 0.6964279406, "meaning": 3.08226276571, "look": 0.6463866936, "numpi": 7.369978720910001, "veri": 0.230159793238, "intern": 0.265095377816, "accuracyscor": 7.369978720910001, "work": 0.109034567273, "combin": 0.529218310751, "forest": 1.5883097076, "promis": 1.25364519176, "column": 1.95699427938, "again": 0.411340231612, "compar": 0.6239191809269999, "remov": 0.6960488415880001, "littl": 0.438213989466, "tandem": 3.8232390339599998, "valid": 1.8889232176800002, "xtest": 7.369978720910001, "defin": 1.00368010925, "train": 0.660918312839, "great": 0.235805258079, "step": 1.03954505698, "scratch": 3.2509415461, "sklearnensembl": 7.369978720910001, "stick": 2.4456277954099996, "type": 0.707101485387, "randomforestclassifi": 7.369978720910001, "note": 0.353817568083, "fals": 1.8271477773099998, "oppos": 0.921405832541, "machin": 1.39235958062, "into": 0.0149128632287, "prior": 0.778442172521, "realiti": 1.51806363875, "abov": 0.643865229816, "requir": 0.424253510675, "traintestsplit": 7.369978720910001, "close": 0.250666759864, "what": 0.225887296827, "rememb": 1.5867691126199999, "choos": 1.43007066072, "guess": 3.22051485947, "help": 0.336207721344, "larg": 0.17037506060600002, "find": 0.547781330288, "line": 0.349430614452, "librari": 0.986809980943, "relat": 0.21310030165399999, "deal": 0.780914701253, "decid": 0.655322871893, "sebastian": 3.2890571790200003, "here": 0.8850381883700001, "lower": 0.742201929994, "report": 0.31001750903700004, "were": 0.024291143681099997, "wrapper": 5.52942908751, "long": 0.235645793878, "much": 0.17749572930100002, "symbiot": 4.53676537685, "evalu": 1.9388802431299998, "about": 0.0628434774746, "problem": 0.569140724273, "selector": 4.73808988077, "mind": 1.2786688388299998, "just": 0.289531434109, "entir": 0.46603068026999994, "sklearnmodelselect": 7.369978720910001, "dure": 0.0491209066894, "classifi": 1.6665296351499999, "check": 1.87281049562, "expens": 1.26563201674, "multidimension": 5.0774439637699995, "marri": 1.06206244535, "live": 0.266903399347, "start": 0.236443369291, "run": 0.442714975539, "criteria": 2.4632235573, "them": 0.0941833269093, "load": 1.91765354188, "keep": 0.7141523446729999, "execut": 0.804854605864, "order": 0.22014038079300002, "raschka": 7.369978720910001, "abl": 0.599303982475, "mani": 0.0433157581221, "over": 0.0249367214957, "date": 0.489080896097, "build": 0.491137452091, "algorithm": 3.33044239518, "total": 0.43567888670500005, "select": 0.704804687133, "acc": 4.74531012875, "same": 0.112059649604, "rnns": 7.369978720910001, "there": 0.0400978929255, "exist": 0.38165779408699996, "highlydimension": 7.369978720910001, "move": 0.255615859253, "sfs": 6.964513612799999, "possibl": 0.348805474891, "say": 0.562154280552, "best": 0.459227932947, "exampl": 0.40868267499899996, "knowledg": 1.2232212893899999, "take": 0.130691962197, "follow": 0.045356911094199995, "now": 0.149092945021, "tri": 0.61759152916, "code": 1.35601909597, "strict": 1.55256998618, "desir": 1.0991793428399999, "than": 0.0322608622182, "regress": 3.9359915164199997, "goe": 1.4473284897999998, "accommod": 1.7416384414200001, "also": 0.0146571578, "around": 0.19387710578200001, "bestperform": 7.369978720910001, "method": 0.944461608841, "artform": 6.58152136054, "the": 0.0, "may": 0.050709995284400004, "mayo": 3.90737271112, "bet": 3.3736145670499997, "promin": 0.8744110957960001, "want": 0.6916366062549999, "ytest": 7.369978720910001, "individu": 0.588013447985, "actual": 0.628514181648, "ytrainpr": 7.369978720910001, "backward": 2.68138692678, "which": 0.00517841384543, "like": 0.139053576545, "infeas": 5.45305610873, "upon": 0.47207177798199995, "other": 0.00987474791976, "appli": 0.8316941898119999, "benefici": 2.90522068864, "one": 0.0062553516455, "show": 0.236682766013, "begin": 0.285584668268, "see": 0.240921585492, "imag": 0.99376210729, "get": 0.579769005782, "random": 1.9727214065099998, "implement": 1.27437940907, "wonder": 1.98319270637, "arbitrarili": 3.9295606260900002, "numf": 3.8348333667400003, "file": 1.32734588723, "detect": 1.68878274493, "process": 0.527829199025, "knearest": 7.369978720910001, "independ": 0.463424162503, "includ": 0.0188846813905, "this": 0.0037864490525, "sampl": 1.9786264883900002, "wine": 2.40086010702, "part": 0.04239531098280001, "worri": 2.3323769785799997, "smaller": 0.9530830530519999, "project": 0.561601885907, "differ": 0.212321121312, "sfsnum": 7.369978720910001, "been": 0.023645982368400004, "attempt": 0.38674498075099994, "proceed": 1.23354840355, "predefin": 5.34183047362, "multiobject": 7.369978720910001, "between": 0.033953681165299995, "low": 0.7564602833490001, "kfeaturesnum": 7.369978720910001, "domain": 2.24008000599, "alway": 0.726319204572, "along": 0.260344385917, "all": 0.011402632097799998, "itch": 5.26584456664, "input": 2.50167533539, "ytrain": 7.369978720910001, "case": 0.395406268889, "that": 0.00397614837964, "built": 0.690379535065, "kfeatureidx": 7.369978720910001, "more": 0.017024931599999998, "and": 6.29901420636e-05, "list": 0.309845761506, "discuss": 0.78698452262, "these": 0.0715336194008, "float": 2.38911558515, "befor": 0.0956377718795, "lead": 0.23620402986699998, "untouch": 3.7726664603199995, "mlxtend": 7.369978720910001, "can": 0.162341096394, "ytestpr": 7.369978720910001, "comput": 1.36806891594, "make": 0.07349765782289999, "way": 0.19809150993500002, "onli": 0.025324268329099998, "each": 0.173741689304, "size": 0.9138372060609999, "read": 0.83939268088, "neighbor": 1.7546632275799998, "remain": 0.15356296309, "while": 0.04324998379380001, "data": 1.2168205848, "qualiti": 1.07600506711, "shape": 1.16420957115, "sinc": 0.0803681994577, "come": 0.28390990653000003, "must": 0.653383947388, "cross": 0.846416414759, "worth": 1.65065103492, "text": 1.14048200999, "dimension": 3.99239120489, "potenti": 0.9245764122419999, "import": 0.292818277066, "first": 0.0075872898121599995, "engin": 0.904767558276, "could": 0.18595627229000003, "when": 0.0205549888584, "have": 0.0147850023412, "score": 1.4559353207700003, "exclus": 1.22643707092, "learn": 0.842752064745, "pipelin": 3.47002829672, "winequalitywhitecsv": 7.369978720910001}, "freq": {"matthew": 1, "onc": 1, "too": 2, "troubl": 1, "comparison": 2, "form": 1, "immedi": 1, "fastai": 1, "addit": 1, "featcol": 2, "number": 6, "etc": 1, "dataset": 15, "intim": 1, "iter": 1, "suboptim": 1, "python": 1, "well": 5, "whether": 1, "approach": 5, "know": 1, "concern": 3, "chose": 1, "equal": 2, "particular": 1, "instead": 1, "traintest": 1, "how": 6, "test": 9, "place": 1, "elaps": 10, "had": 2, "especi": 2, "would": 2, "end": 2, "given": 3, "word": 1, "interest": 1, "guarante": 1, "will": 9, "full": 8, "sklearnmetr": 1, "second": 1, "repo": 1, "next": 3, "below": 4, "perform": 14, "but": 4, "demonstr": 4, "need": 1, "our": 19, "classif": 1, "xtrain": 2, "point": 1, "mlxtendfeatureselect": 1, "level": 1, "has": 2, "generat": 6, "paramet": 1, "pass": 1, "panda": 1, "subsequ": 2, "sequenti": 1, "out": 12, "model": 18, "treat": 1, "clf": 9, "disciplin": 4, "comment": 1, "cours": 1, "optim": 5, "anyhow": 1, "forward": 11, "print": 7, "not": 5, "are": 16, "altern": 2, "logist": 1, "specif": 3, "quick": 1, "poor": 1, "metric": 2, "said": 1, "though": 1, "then": 5, "index": 1, "fold": 1, "they": 3, "anoth": 2, "instal": 1, "effect": 1, "let": 2, "scienc": 1, "kdnugget": 1, "larger": 3, "github": 1, "hilitem": 5, "featur": 83, "until": 1, "provid": 1, "either": 1, "such": 3, "accuraci": 13, "those": 3, "without": 1, "some": 5, "sourc": 1, "should": 2, "both": 2, "result": 7, "document": 1, "subset": 12, "use": 26, "set": 12, "info": 1, "from": 6, "done": 11, "num": 80, "instanc": 2, "exclud": 1, "for": 15, "depend": 1, "predict": 1, "finish": 5, "inclus": 1, "warrant": 1, "sequentialfeatureselector": 1, "with": 23, "verbos": 1, "determin": 1, "split": 2, "better": 2, "meaning": 1, "look": 2, "numpi": 1, "veri": 3, "intern": 1, "accuracyscor": 1, "work": 4, "combin": 5, "forest": 2, "promis": 1, "column": 1, "again": 1, "compar": 4, "remov": 2, "littl": 1, "tandem": 1, "valid": 2, "xtest": 2, "defin": 6, "train": 9, "great": 1, "step": 13, "scratch": 1, "sklearnensembl": 1, "stick": 1, "type": 1, "randomforestclassifi": 1, "note": 1, "fals": 1, "oppos": 1, "machin": 3, "into": 1, "prior": 2, "realiti": 1, "abov": 3, "requir": 1, "traintestsplit": 1, "close": 1, "what": 4, "rememb": 1, "choos": 1, "guess": 1, "help": 1, "larg": 3, "find": 4, "line": 1, "librari": 1, "relat": 3, "deal": 2, "decid": 1, "sebastian": 1, "here": 2, "lower": 1, "report": 2, "were": 4, "wrapper": 3, "long": 2, "much": 1, "symbiot": 1, "evalu": 5, "about": 1, "problem": 1, "selector": 6, "mind": 4, "just": 1, "entir": 2, "sklearnmodelselect": 1, "dure": 1, "classifi": 6, "check": 2, "expens": 1, "multidimension": 1, "marri": 1, "live": 1, "start": 2, "run": 1, "criteria": 1, "them": 3, "load": 1, "keep": 4, "execut": 1, "order": 1, "raschka": 1, "abl": 1, "mani": 3, "over": 1, "date": 1, "build": 9, "algorithm": 9, "total": 1, "select": 47, "acc": 1, "same": 1, "rnns": 1, "there": 5, "exist": 2, "highlydimension": 1, "move": 2, "sfs": 1, "possibl": 2, "say": 2, "best": 9, "exampl": 1, "knowledg": 1, "take": 4, "follow": 1, "now": 2, "tri": 2, "code": 4, "strict": 1, "desir": 4, "than": 3, "regress": 1, "goe": 1, "accommod": 1, "also": 1, "around": 2, "bestperform": 1, "method": 7, "artform": 1, "the": 58, "may": 7, "mayo": 1, "bet": 1, "promin": 1, "want": 1, "ytest": 4, "individu": 1, "actual": 3, "ytrainpr": 2, "backward": 4, "which": 10, "like": 1, "infeas": 1, "upon": 1, "other": 2, "appli": 1, "benefici": 1, "one": 2, "show": 1, "begin": 1, "see": 3, "imag": 1, "get": 2, "random": 2, "implement": 3, "wonder": 1, "arbitrarili": 1, "numf": 4, "file": 1, "detect": 1, "process": 9, "knearest": 1, "independ": 1, "includ": 2, "this": 6, "sampl": 2, "wine": 1, "part": 1, "worri": 1, "smaller": 1, "project": 1, "differ": 3, "sfsnum": 4, "been": 1, "attempt": 1, "proceed": 1, "predefin": 2, "multiobject": 1, "between": 1, "low": 1, "kfeaturesnum": 1, "domain": 1, "alway": 1, "along": 1, "all": 7, "itch": 1, "input": 1, "ytrain": 4, "case": 3, "that": 17, "built": 4, "kfeatureidx": 1, "more": 4, "and": 28, "list": 1, "discuss": 1, "these": 10, "float": 3, "befor": 1, "lead": 1, "untouch": 1, "mlxtend": 5, "can": 6, "ytestpr": 2, "comput": 1, "make": 1, "way": 1, "onli": 2, "each": 2, "size": 2, "read": 1, "neighbor": 1, "remain": 5, "while": 1, "data": 3, "qualiti": 1, "shape": 8, "sinc": 1, "come": 1, "must": 1, "cross": 2, "worth": 1, "text": 1, "dimension": 1, "potenti": 1, "import": 8, "first": 2, "engin": 1, "could": 5, "when": 3, "have": 7, "score": 11, "exclus": 1, "learn": 3, "pipelin": 1, "winequalitywhitecsv": 1}, "idf": {"matthew": 6.908616187989999, "onc": 1.4974533106999999, "too": 1.81585268215, "troubl": 4.99088337001, "comparison": 4.9597000937199995, "form": 1.12755681818, "immedi": 2.02862254025, "fastai": 1587.6, "addit": 1.24634950542, "featcol": 1587.6, "number": 1.10142916609, "etc": 4.2066772655, "dataset": 193.609756098, "intim": 14.459016393399999, "iter": 37.4433962264, "suboptim": 588.0, "python": 56.2978723404, "well": 1.0655748708, "whether": 2.20683903253, "approach": 2.07556543339, "know": 2.59327017315, "concern": 1.8852867830400002, "chose": 4.42105263158, "equal": 2.542193755, "particular": 1.3814827706200001, "instead": 1.59461631177, "traintest": 1587.6, "how": 1.60250328051, "test": 2.65707112971, "place": 1.1004366812200002, "elaps": 87.2307692308, "had": 1.0475750577399998, "especi": 1.66712170534, "would": 1.0828729281799998, "end": 1.10680423871, "given": 1.35426085473, "word": 1.7965372864099998, "interest": 1.60331246213, "guarante": 6.57119205298, "will": 1.22481098596, "full": 1.66729678639, "sklearnmetr": 1587.6, "second": 1.1130898128, "repo": 369.209302326, "next": 1.4950560316400001, "below": 2.25607503197, "perform": 1.5313977042500002, "but": 1.01632417899, "demonstr": 2.64997496244, "need": 1.4372623574099999, "our": 2.35758835759, "classif": 8.067073170730001, "xtrain": 1587.6, "point": 1.25990000794, "mlxtendfeatureselect": 1587.6, "level": 1.6544393497299998, "has": 1.0436497502, "generat": 2.05275407292, "paramet": 17.256521739100002, "pass": 1.61818367139, "panda": 111.802816901, "subsequ": 1.7534791252500002, "sequenti": 39.5910224439, "out": 1.06016694491, "model": 2.0905978404, "treat": 3.59023066486, "clf": 1587.6, "disciplin": 6.92972501091, "comment": 3.05954904606, "cours": 2.15092805853, "optim": 11.5377906977, "anyhow": 360.818181818, "forward": 3.66566612792, "print": 3.29719626168, "not": 1.01567398119, "are": 1.02990593578, "altern": 2.1390460792200003, "logist": 14.0994671403, "specif": 1.8719490626099997, "quick": 2.205, "poor": 2.42196796339, "metric": 22.235294117600002, "said": 1.54751925139, "though": 1.36076112111, "then": 1.08657860516, "index": 6.9969149405, "fold": 12.3452566096, "they": 1.03017325287, "anoth": 1.13643521832, "instal": 3.78721374046, "effect": 1.3963060686000002, "let": 3.48616600791, "scienc": 2.31969608416, "kdnugget": 1587.6, "larger": 2.2407904022599996, "github": 1587.6, "hilitem": 1587.6, "featur": 1.52712581762, "until": 1.14852058164, "provid": 1.21552714187, "either": 1.5830092731099998, "such": 1.06151377374, "accuraci": 12.7620578778, "those": 1.19548192771, "without": 1.29547123623, "some": 1.04036697248, "sourc": 1.69760479042, "should": 1.6643254009900001, "both": 1.05215720061, "result": 1.14611608432, "document": 2.5409731114, "subset": 27.3253012048, "use": 1.0296387573799999, "set": 1.18707940781, "info": 37.7102137767, "from": 1.00056721497, "done": 2.3302509907499998, "num": 1.00031504001, "instanc": 3.2572835453400004, "exclud": 5.31859296482, "for": 1.00031504001, "depend": 2.2411067193700003, "predict": 5.18484650555, "finish": 3.22879804759, "inclus": 8.756756756760002, "warrant": 15.703264095, "sequentialfeatureselector": 1587.6, "with": 1.0011982089899998, "verbos": 378.0, "determin": 2.1658935879900003, "split": 3.4709226060300002, "better": 2.0065722952500002, "meaning": 21.8076923077, "look": 1.9086318826599997, "numpi": 1587.6, "veri": 1.25880114177, "intern": 1.30355530011, "accuracyscor": 1587.6, "work": 1.11520089913, "combin": 1.69760479042, "forest": 4.89546716004, "promis": 3.5030891438699996, "column": 7.078020508250001, "again": 1.50883862384, "compar": 1.8662278123900002, "remov": 2.0058117498400003, "littl": 1.5499365420299998, "tandem": 45.7521613833, "valid": 6.61224489796, "xtest": 1587.6, "defin": 2.72830383227, "train": 1.9365698950999999, "great": 1.26592775696, "step": 2.8279301745599996, "scratch": 25.8146341463, "sklearnensembl": 1587.6, "stick": 11.5377906977, "type": 2.0281042411900003, "randomforestclassifi": 1587.6, "note": 1.42449528937, "fals": 6.21613155834, "oppos": 2.51282051282, "machin": 4.02433460076, "into": 1.01502461479, "prior": 2.17807655371, "realiti": 4.563380281690001, "abov": 1.90382539873, "requir": 1.52844902282, "traintestsplit": 1587.6, "close": 1.2848818387799998, "what": 1.25343439128, "rememb": 4.88793103448, "choos": 4.17899447223, "guess": 25.0410094637, "help": 1.39962972759, "larg": 1.18574949585, "find": 1.7294117647099998, "line": 1.4182597820299998, "librari": 2.68266306185, "relat": 1.23750876919, "deal": 2.18346857379, "decid": 1.9257641921400002, "sebastian": 26.8175675676, "here": 2.42307692308, "lower": 2.10055570257, "report": 1.3634489866, "were": 1.02458857696, "wrapper": 252.0, "long": 1.2657259028899999, "much": 1.1942229577299999, "symbiot": 93.3882352941, "evalu": 6.9509632224199995, "about": 1.06486015159, "problem": 1.76674827509, "selector": 114.215827338, "mind": 3.5918552036199998, "just": 1.33580143037, "entir": 1.59365589239, "sklearnmodelselect": 1587.6, "dure": 1.0503473370799998, "classifi": 5.2937645882, "check": 6.50655737705, "expens": 3.5453327378300004, "multidimension": 160.363636364, "marri": 2.89233011478, "live": 1.30591428806, "start": 1.26673581744, "run": 1.55692850838, "criteria": 11.7426035503, "them": 1.09876115994, "load": 6.80497213888, "keep": 2.04245465071, "execut": 2.2363713199, "order": 1.24625166811, "raschka": 1587.6, "abl": 1.8208510150200001, "mani": 1.04426757877, "over": 1.02525024217, "date": 1.63081664099, "build": 1.6341739578, "algorithm": 27.9507042254, "total": 1.5460122699399999, "select": 2.02345144022, "acc": 115.043478261, "same": 1.11857958148, "rnns": 1587.6, "there": 1.04091266719, "exist": 1.4647107666799999, "highlydimension": 1587.6, "move": 1.29125660838, "sfs": 1058.4, "possibl": 1.4173734488, "say": 1.7544480053, "best": 1.5828514456600002, "exampl": 1.50483412322, "knowledg": 3.3981164383599998, "take": 1.13961668222, "follow": 1.04640126549, "now": 1.160780873, "tri": 1.8544562551099997, "code": 3.8807137619199996, "strict": 4.7235941684, "desir": 3.00170164492, "than": 1.03278688525, "regress": 51.2129032258, "goe": 4.251740760580001, "accommod": 5.70668583753, "also": 1.01476510067, "around": 1.21394708671, "bestperform": 1587.6, "method": 2.5714285714300003, "artform": 721.636363636, "the": 1.0, "may": 1.05201775893, "mayo": 49.7680250784, "bet": 29.1838235294, "promin": 2.39746300211, "want": 1.99698113208, "ytest": 1587.6, "individu": 1.8004082558400003, "actual": 1.87482286254, "ytrainpr": 1587.6, "backward": 14.605335786600001, "which": 1.005191845, "like": 1.14918566775, "infeas": 233.470588235, "upon": 1.60331246213, "other": 1.00992366412, "appli": 2.2972073506, "benefici": 18.269275028800003, "one": 1.00627495722, "show": 1.26703910615, "begin": 1.3305397251100002, "see": 1.27242125511, "imag": 2.70137825421, "get": 1.78562591385, "random": 7.1902173913, "implement": 3.57648118946, "wonder": 7.265903890160001, "arbitrarili": 50.8846153846, "numf": 46.285714285699996, "file": 3.7710213776699995, "detect": 5.41288782816, "process": 1.69524826482, "knearest": 1587.6, "independ": 1.58950740889, "includ": 1.0190641247799999, "this": 1.00379362671, "sampl": 7.23280182232, "wine": 11.0326615705, "part": 1.04330682789, "worri": 10.302401038300001, "smaller": 2.59369384088, "project": 1.7534791252500002, "differ": 1.23654490225, "sfsnum": 1587.6, "been": 1.0239277652399998, "attempt": 1.4721810088999998, "proceed": 3.4333910034599997, "predefin": 208.89473684200001, "multiobject": 1587.6, "between": 1.03453668708, "low": 2.13072070863, "kfeaturesnum": 1587.6, "domain": 9.39408284024, "alway": 2.06745670009, "along": 1.2973768080399999, "all": 1.01146788991, "itch": 193.609756098, "input": 12.2029208301, "ytrain": 1587.6, "case": 1.48498737256, "that": 1.00398406375, "built": 1.99447236181, "kfeatureidx": 1587.6, "more": 1.0171706817, "and": 1.00006299213, "list": 1.36321483771, "discuss": 2.19676214197, "these": 1.07415426252, "float": 10.903846153800002, "befor": 1.10036041031, "lead": 1.2664326739, "untouch": 43.495890411000005, "mlxtend": 1587.6, "can": 1.17626139142, "ytestpr": 1587.6, "comput": 3.9277585353800006, "make": 1.0762660158600001, "way": 1.2190739461, "onli": 1.0256476516600002, "each": 1.18974820144, "size": 2.49387370405, "read": 2.3149606299200003, "neighbor": 5.781500364169999, "remain": 1.16598119859, "while": 1.0441988950299999, "data": 3.37643555934, "qualiti": 2.9329392204, "shape": 3.20338983051, "sinc": 1.08368600683, "come": 1.32831325301, "must": 1.9220338983099996, "cross": 2.33127753304, "worth": 5.210370856580001, "text": 3.12827586207, "dimension": 54.1843003413, "potenti": 2.52080025405, "import": 1.3401992233700002, "first": 1.00761614623, "engin": 2.47135740971, "could": 1.2043695949, "when": 1.02076769755, "have": 1.0148948411399998, "score": 4.2884927066500005, "exclus": 3.40906162766, "learn": 2.32275054865, "pipelin": 32.1376518219, "winequalitywhitecsv": 1587.6}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Step Forward Feature Selection: A Practical Example in Python</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Step Forward Feature Selection: A Practical Example in Python Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/jobs/18/06-18-cvs-application-developer.html\" rel=\"prev\" title=\"CVS: Application Developer, Rebates Forecasting\"/>\n<link href=\"https://www.kdnuggets.com/2018/06/top-news-week-0611-0617.html\" rel=\"next\" title=\"Top Stories, Jun 11-17: Data Lake \u2013 the evolution of data processing; Generating Text with RNNs in 4 Lines of Code\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=82052\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-82052 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 18-Jun, 2018  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/06/index.html\">Jun</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/06/tutorials.html\">Tutorials, Overviews</a> \u00bb Step Forward Feature Selection: A Practical Example in Python (\u00a0<a href=\"/2018/n24.html\">18:n24</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Step Forward Feature Selection: A Practical Example in Python</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/jobs/18/06-18-cvs-application-developer.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/06/top-news-week-0611-0617.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/feature-selection\" rel=\"tag\">Feature Selection</a>, <a href=\"https://www.kdnuggets.com/tag/machine-learning\" rel=\"tag\">Machine Learning</a>, <a href=\"https://www.kdnuggets.com/tag/python\" rel=\"tag\">Python</a></div>\n<br/>\n<p class=\"excerpt\">\n     When it comes to disciplined approaches to feature selection, wrapper methods are those which marry the feature selection process to the type of model being built, evaluating feature subsets in order to detect the model performance between features, and subsequently select the best performing subset.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/matt-mayo\" rel=\"author\" title=\"Posts by Matthew Mayo\">Matthew Mayo</a>, KDnuggets.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p>Many methods for <a href=\"https://en.wikipedia.org/wiki/Feature_selection\" rel=\"noopener\" target=\"_blank\">feature selection</a> exist, some of which treat the process strictly as an artform, others as a science, while, in reality, some form of domain knowledge along with a disciplined approach are likely your best bet.</p>\n<p>When it comes to disciplined approaches to feature selection, <a href=\"https://en.wikipedia.org/wiki/Feature_selection#Wrapper_method\" rel=\"noopener\" target=\"_blank\">wrapper methods</a> are those which marry the feature selection process to the type of model being built, evaluating feature subsets in order to detect the model performance between features, and subsequently select the best performing subset. In other words, instead of existing as an independent process taking place <em>prior</em> to model building, wrapper methods attempt to optimize feature selection process for a given machine learning algorithm <em>in tandem</em> with this algorithm.</p>\n<p>2 prominent wrapper methods for feature selection are step forward feature selection and step backward features selection.</p>\n<p><center><img alt=\"Image\" src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Feature_selection_Wrapper_Method.png\" width=\"99%\"><br>\n<font size=\"-1\"><a href=\"https://en.wikipedia.org/wiki/Feature_selection\" rel=\"noopener\" target=\"_blank\">Image source</a></font></br></img></center></p>\n<p>Step forward feature selection starts with the evaluation of each individual feature, and selects that which results in the best performing selected algorithm model. What's the \"best?\" That depends entirely on the defined evaluation criteria (AUC, prediction accuracy, RMSE, etc.). Next, all possible combinations of the that selected feature and a subsequent feature are evaluated, and a second feature is selected, and so on, until the required predefined number of features is selected.</p>\n<p>Step backward feature selection is closely related, and as you may have guessed starts with the entire set of features and works backward from there, removing features to find the optimal subset of a predefined size.</p>\n<p>These are both potentially very computationally expensive. Do you have a large, multidimensional dataset? These methods may take too long to be at all useful, or may be totally infeasible. That said, with a dataset of accommodating size and dimensionality, such an approach may well be your best possible approach.</p>\n<p>To see how they work, let's take a look at step forward feature selection, specifically. Note that, as discussed, a machine learning algorithm must be defined prior to beginning our symbiotic feature selection process.</p>\n<p>Keep in mind that an optimized set of selected features using a given algorithm may or may not perform equally well with a different algorithm. If we select features using logistic regression, for example, there is no guarantee that these same features will perform optimally if we then tried them out using K-nearest neighbors, or an SVM.</p>\n<p>\u00a0</p>\n<h3>Implementing Feature Selection and Building a Model</h3>\n<p>\u00a0<br>\nSo, how do we perform step forward feature selection in Python? <a href=\"https://github.com/rasbt/mlxtend\" rel=\"noopener\" target=\"_blank\">Sebastian Raschka's mlxtend library</a> includes an implementation (<a href=\"https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/\" rel=\"noopener\" target=\"_blank\">Sequential Feature Selector</a>), and so we will use it to demonstrate. It goes without saying that you should have mlxtend installed before moving forward (check the Github repo).</br></p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://raw.githubusercontent.com/rasbt/mlxtend/master/docs/sources/img/logo.png\" width=\"50%\"/></p>\n<p>We will use a Random Forest classifier for feature selection and model building (which, again, are intimately related in the case of step forward feature selection).</p>\n<p>We need data to use for demonstration, so let's use the <a href=\"https://archive.ics.uci.edu/ml/datasets/wine+quality\" rel=\"noopener\" target=\"_blank\">wine quality dataset</a>. Specifically, I have used the untouched <code>winequality-white.csv</code> file as input in the code below.</p>\n<p>Arbitrarily, we will set the desired number of features to 5 (there are 12 in the dataset). What we are able to do is compare the evaluation scores for each iteration of the feature selection process, and so keep in mind that if we find that a lower number of features has a better score we can alternatively choose that best-performing subset to run with in our \"live\" model moving forward. Also keep in mind that setting our desired number of features too low could lead to a sub-optimal number and combination of features being decided upon (say, if some combination of 11 features in our case is better than the best combination of &lt;= 10 features we find during the selection process).</p>\n<p>Since we are more interested in demonstrating how to implement step forward feature selection than we are with the actual results on this particular dataset, we won't be overly concerned with the actual performance of our models, but we will compare the performances anyhow, as to show how it would be done in a meaningful project.</p>\n<p>First, we will make our imports, load the dataset, and split it into training and testing sets.</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #006699; font-weight: bold\">import</span> <span style=\"color: #00CCFF; font-weight: bold\">numpy</span> <span style=\"color: #006699; font-weight: bold\">as</span> <span style=\"color: #00CCFF; font-weight: bold\">np</span>\r\n<span style=\"color: #006699; font-weight: bold\">import</span> <span style=\"color: #00CCFF; font-weight: bold\">pandas</span> <span style=\"color: #006699; font-weight: bold\">as</span> <span style=\"color: #00CCFF; font-weight: bold\">pd</span>\r\n<span style=\"color: #006699; font-weight: bold\">from</span> <span style=\"color: #00CCFF; font-weight: bold\">sklearn.ensemble</span> <span style=\"color: #006699; font-weight: bold\">import</span> RandomForestClassifier\r\n<span style=\"color: #006699; font-weight: bold\">from</span> <span style=\"color: #00CCFF; font-weight: bold\">sklearn.model_selection</span> <span style=\"color: #006699; font-weight: bold\">import</span> train_test_split\r\n<span style=\"color: #006699; font-weight: bold\">from</span> <span style=\"color: #00CCFF; font-weight: bold\">sklearn.metrics</span> <span style=\"color: #006699; font-weight: bold\">import</span> accuracy_score <span style=\"color: #006699; font-weight: bold\">as</span> acc\r\n<span style=\"color: #006699; font-weight: bold\">from</span> <span style=\"color: #00CCFF; font-weight: bold\">mlxtend.feature_selection</span> <span style=\"color: #006699; font-weight: bold\">import</span> SequentialFeatureSelector <span style=\"color: #006699; font-weight: bold\">as</span> sfs\r\n\r\n<span style=\"color: #0099FF; font-style: italic\"># Read data</span>\r\ndf <span style=\"color: #555555\">=</span> pd<span style=\"color: #555555\">.</span>read_csv(<span style=\"color: #CC3300\">'winequality-white.csv'</span>, sep<span style=\"color: #555555\">=</span><span style=\"color: #CC3300\">';'</span>)\r\n\r\n<span style=\"color: #0099FF; font-style: italic\"># Train/test split</span>\r\nX_train, X_test, y_train, y_test <span style=\"color: #555555\">=</span> train_test_split(\r\n    df<span style=\"color: #555555\">.</span>values[:,:<span style=\"color: #555555\">-</span><span style=\"color: #FF6600\">1</span>],\r\n    df<span style=\"color: #555555\">.</span>values[:,<span style=\"color: #555555\">-</span><span style=\"color: #FF6600\">1</span>:],\r\n    test_size<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">0.25</span>,\r\n    random_state<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">42</span>)\r\n\r\ny_train <span style=\"color: #555555\">=</span> y_train<span style=\"color: #555555\">.</span>ravel()\r\ny_test <span style=\"color: #555555\">=</span> y_test<span style=\"color: #555555\">.</span>ravel()\r\n\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Training dataset shape:'</span>, X_train<span style=\"color: #555555\">.</span>shape, y_train<span style=\"color: #555555\">.</span>shape)\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Testing dataset shape:'</span>, X_test<span style=\"color: #555555\">.</span>shape, y_test<span style=\"color: #555555\">.</span>shape)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\nTraining dataset shape: (3673, 11) (3673,)\r\nTesting dataset shape: (1225, 11) (1225,)</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Next, we will define a classifier, as well as a step forward feature selector, and then perform our feature selection. The feature feature selector in mlxtend has some parameters we can define, so here's how we will proceed:</p>\n<ul>\n<li>First, we pass our classifier, the Random Forest classifier defined above the feature selector\n<li>Next, we define the subset of features we are looking to select (k_features=5)\n<li>We then set floating to False; see the documentation for more info on floating:\n</li></li></li></ul>\n<blockquote><p>\nThe floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded), so that a larger number of feature subset combinations can be sampled.\n</p></blockquote>\n<ul>\n<li>We set he desired level of verbosity for mlxtend to report\n<li>Importantly, we set our scoring to accuracy; this is but one metric which could be used to score our resulting models built on the selected features\n<li>mlxtend feature selector uses cross validation internally, and we set our desired folds to 5 for our demonstration\n</li></li></li></ul>\n<p>The dataset we chose isn't very large, and so the following code should not take long to execute.</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #0099FF; font-style: italic\"># Build RF classifier to use in feature selection</span>\r\nclf <span style=\"color: #555555\">=</span> RandomForestClassifier(n_estimators<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">100</span>, n_jobs<span style=\"color: #555555\">=-</span><span style=\"color: #FF6600\">1</span>)\r\n\r\n<span style=\"color: #0099FF; font-style: italic\"># Build step forward feature selection</span>\r\nsfs1 <span style=\"color: #555555\">=</span> sfs(clf,\r\n           k_features<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">5</span>,\r\n           forward<span style=\"color: #555555\">=</span><span style=\"color: #336666\">True</span>,\r\n           floating<span style=\"color: #555555\">=</span><span style=\"color: #336666\">False</span>,\r\n           verbose<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">2</span>,\r\n           scoring<span style=\"color: #555555\">=</span><span style=\"color: #CC3300\">'accuracy'</span>,\r\n           cv<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">5</span>)\r\n\r\n<span style=\"color: #0099FF; font-style: italic\"># Perform SFFS</span>\r\nsfs1 <span style=\"color: #555555\">=</span> sfs1<span style=\"color: #555555\">.</span>fit(X_train, y_train)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   24.3s finished\r\n\r\n[2018-06-12 14:47:47] Features: 1/5 -- score: 0.49768148939247264[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   22.7s finished\r\n\r\n[2018-06-12 14:48:09] Features: 2/5 -- score: 0.5442629071398873[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   21.2s finished\r\n\r\n[2018-06-12 14:48:31] Features: 3/5 -- score: 0.6052194438136681[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   20.3s finished\r\n\r\n[2018-06-12 14:48:51] Features: 4/5 -- score: 0.6261526236769334[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   17.3s finished\r\n\r\n[2018-06-12 14:49:08] Features: 5/5 -- score: 0.6444222989869156</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Our best performing model, given our scoring metric, is some subset of 5 features, with a score of 0.644 (remember that this is using cross validation, and so will be different than that which is reported on our full models below, using train and test sets). But which subset of 5 features were selected?</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #0099FF; font-style: italic\"># Which features?</span>\r\nfeat_cols <span style=\"color: #555555\">=</span> <span style=\"color: #336666\">list</span>(sfs1<span style=\"color: #555555\">.</span>k_feature_idx_)\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(feat_cols)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\n[1, 2, 3, 7, 10]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The columns at these indexes are those which were selected. Great! So what now...?</p>\n<p>We can now use those features to build a full model using our training and test sets. If we had a much larger set (i.e. many more instances as opposed to many more features), this would be especially beneficial as we could have used the feature selector above on a smaller subset of instances, determined our best performing subset of features, and then applied them to the full dataset for classification.</p>\n<p>The code below builds a classifier on <b>only</b> the subset of selected features.</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #0099FF; font-style: italic\"># Build full model with selected features</span>\r\nclf <span style=\"color: #555555\">=</span> RandomForestClassifier(n_estimators<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">1000</span>, random_state<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">42</span>, max_depth<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">4</span>)\r\nclf<span style=\"color: #555555\">.</span>fit(X_train[:, feat_cols], y_train)\r\n\r\ny_train_pred <span style=\"color: #555555\">=</span> clf<span style=\"color: #555555\">.</span>predict(X_train[:, feat_cols])\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Training accuracy on selected features: </span><span style=\"color: #AA0000\">%.3f</span><span style=\"color: #CC3300\">'</span> <span style=\"color: #555555\">%</span> acc(y_train, y_train_pred))\r\n\r\ny_test_pred <span style=\"color: #555555\">=</span> clf<span style=\"color: #555555\">.</span>predict(X_test[:, feat_cols])\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Testing accuracy on selected features: </span><span style=\"color: #AA0000\">%.3f</span><span style=\"color: #CC3300\">'</span> <span style=\"color: #555555\">%</span> acc(y_test, y_test_pred))\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\nTraining accuracy on selected features: 0.558\r\nTesting accuracy on selected features: 0.512\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Don't worry about the actual accuracies; we're concerned here with the process, not the end result.</p>\n<p>But what if we <em>were</em> concerned with the end result, and wanted to know if our feature selection troubles had been worth it? Well, we could compare the resultant accuracies of the full model built using the selected features (immediately above) with the resultant accuracies of another full model using <b>all</b> of the features, just as we do below:</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #0099FF; font-style: italic\"># Build full model on ALL features, for comparison</span>\r\nclf <span style=\"color: #555555\">=</span> RandomForestClassifier(n_estimators<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">1000</span>, random_state<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">42</span>, max_depth<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">4</span>)\r\nclf<span style=\"color: #555555\">.</span>fit(X_train, y_train)\r\n\r\ny_train_pred <span style=\"color: #555555\">=</span> clf<span style=\"color: #555555\">.</span>predict(X_train)\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Training accuracy on all features: </span><span style=\"color: #AA0000\">%.3f</span><span style=\"color: #CC3300\">'</span> <span style=\"color: #555555\">%</span> acc(y_train, y_train_pred))\r\n\r\ny_test_pred <span style=\"color: #555555\">=</span> clf<span style=\"color: #555555\">.</span>predict(X_test)\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Testing accuracy on all features: </span><span style=\"color: #AA0000\">%.3f</span><span style=\"color: #CC3300\">'</span> <span style=\"color: #555555\">%</span> acc(y_test, y_test_pred))\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\nTraining accuracy on all features: 0.566\r\nTesting accuracy on all features: 0.509\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>And there you have them for comparison. They are both poor and comparable to our model built with the selected features (though I promise this is not always the case!). With very little work, you could see how these selected features perform with a different algorithm, to help scratch that itch as to wondering whether these features selected with one algorithm are equally well performing with another.</p>\n<p>Such a feature selection method can be an effective part of a disciplined machine learning pipeline. Keep in mind that step forward (or step backward) methods, specifically, can provide problems when dealing with especially large or highly-dimensional datasets. There are ways of getting around (or <b>trying</b> to get around) these sticking points, such as sampling from the data to find the feature subset which works best, and then using these features for the modeling process on the full dataset. Of course, these are not the only disciplined approaches to feature selection either, and so checking out alternatives may be warranted when dealing with these larger datasets.</p>\n<p>\u00a0<br/>\n<b>Related</b>:</p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/03/feature-engineering-dates-fastai.html\">Quick Feature Engineering with Dates Using fast.ai</a>\n<li><a href=\"/2018/06/generating-text-rnn-4-lines-code.html\">Generating Text with RNNs in 4 Lines of Code</a>\n<li><a href=\"/2017/12/rapidminer-multi-objective-optimization-feature-selection.html\">Multi-objective Optimization for Feature Selection</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/jobs/18/06-18-cvs-application-developer.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/06/top-news-week-0611-0617.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/kdnuggets-editor.html\">Looking for a KDnuggets Editor</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning Experts</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/06/index.html\">Jun</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/06/tutorials.html\">Tutorials, Overviews</a> \u00bb Step Forward Feature Selection: A Practical Example in Python (\u00a0<a href=\"/2018/n24.html\">18:n24</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<div>\n<br/><span style=\"font-size:9px\">By subscribing, you agree to KDnuggets <a href=\"https://www.kdnuggets.com/news/privacy-policy.html\">privacy policy</a></span>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556410493\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.709 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 20:14:53 -->\n<!-- Compression = gzip -->", "content_tokenized": ["matthew", "mayo", "kdnugget", "comment", "mani", "method", "for", "featur", "select", "exist", "some", "which", "treat", "the", "process", "strict", "artform", "other", "scienc", "while", "realiti", "some", "form", "domain", "knowledg", "along", "with", "disciplin", "approach", "are", "like", "best", "bet", "when", "come", "disciplin", "approach", "featur", "select", "wrapper", "method", "are", "those", "which", "marri", "the", "featur", "select", "process", "the", "type", "model", "built", "evalu", "featur", "subset", "order", "detect", "the", "model", "perform", "between", "featur", "and", "subsequ", "select", "the", "best", "perform", "subset", "other", "word", "instead", "exist", "independ", "process", "take", "place", "prior", "model", "build", "wrapper", "method", "attempt", "optim", "featur", "select", "process", "for", "given", "machin", "learn", "algorithm", "tandem", "with", "this", "algorithm", "num", "promin", "wrapper", "method", "for", "featur", "select", "are", "step", "forward", "featur", "select", "and", "step", "backward", "featur", "select", "imag", "sourc", "step", "forward", "featur", "select", "start", "with", "the", "evalu", "each", "individu", "featur", "and", "select", "that", "which", "result", "the", "best", "perform", "select", "algorithm", "model", "what", "the", "best", "that", "depend", "entir", "the", "defin", "evalu", "criteria", "predict", "accuraci", "etc", "next", "all", "possibl", "combin", "the", "that", "select", "featur", "and", "subsequ", "featur", "are", "evalu", "and", "second", "featur", "select", "and", "until", "the", "requir", "predefin", "number", "featur", "select", "step", "backward", "featur", "select", "close", "relat", "and", "may", "have", "guess", "start", "with", "the", "entir", "set", "featur", "and", "work", "backward", "from", "there", "remov", "featur", "find", "the", "optim", "subset", "predefin", "size", "these", "are", "both", "potenti", "veri", "comput", "expens", "have", "larg", "multidimension", "dataset", "these", "method", "may", "take", "too", "long", "all", "use", "may", "total", "infeas", "that", "said", "with", "dataset", "accommod", "size", "and", "dimension", "such", "approach", "may", "well", "best", "possibl", "approach", "see", "how", "they", "work", "let", "take", "look", "step", "forward", "featur", "select", "specif", "note", "that", "discuss", "machin", "learn", "algorithm", "must", "defin", "prior", "begin", "our", "symbiot", "featur", "select", "process", "keep", "mind", "that", "optim", "set", "select", "featur", "use", "given", "algorithm", "may", "may", "not", "perform", "equal", "well", "with", "differ", "algorithm", "select", "featur", "use", "logist", "regress", "for", "exampl", "there", "guarante", "that", "these", "same", "featur", "will", "perform", "optim", "then", "tri", "them", "out", "use", "knearest", "neighbor", "implement", "featur", "select", "and", "build", "model", "how", "perform", "step", "forward", "featur", "select", "python", "sebastian", "raschka", "mlxtend", "librari", "includ", "implement", "sequenti", "featur", "selector", "and", "will", "use", "demonstr", "goe", "without", "say", "that", "should", "have", "mlxtend", "instal", "befor", "move", "forward", "check", "the", "github", "repo", "will", "use", "random", "forest", "classifi", "for", "featur", "select", "and", "model", "build", "which", "again", "are", "intim", "relat", "the", "case", "step", "forward", "featur", "select", "need", "data", "use", "for", "demonstr", "let", "use", "the", "wine", "qualiti", "dataset", "specif", "have", "use", "the", "untouch", "winequalitywhitecsv", "file", "input", "the", "code", "below", "arbitrarili", "will", "set", "the", "desir", "number", "featur", "num", "there", "are", "num", "the", "dataset", "what", "are", "abl", "compar", "the", "evalu", "score", "for", "each", "iter", "the", "featur", "select", "process", "and", "keep", "mind", "that", "find", "that", "lower", "number", "featur", "has", "better", "score", "can", "altern", "choos", "that", "bestperform", "subset", "run", "with", "our", "live", "model", "move", "forward", "also", "keep", "mind", "that", "set", "our", "desir", "number", "featur", "too", "low", "could", "lead", "suboptim", "number", "and", "combin", "featur", "decid", "upon", "say", "some", "combin", "num", "featur", "our", "case", "better", "than", "the", "best", "combin", "num", "featur", "find", "dure", "the", "select", "process", "sinc", "are", "more", "interest", "demonstr", "how", "implement", "step", "forward", "featur", "select", "than", "are", "with", "the", "actual", "result", "this", "particular", "dataset", "over", "concern", "with", "the", "actual", "perform", "our", "model", "but", "will", "compar", "the", "perform", "anyhow", "show", "how", "would", "done", "meaning", "project", "first", "will", "make", "our", "import", "load", "the", "dataset", "and", "split", "into", "train", "and", "test", "set", "generat", "use", "hilitem", "import", "numpi", "import", "panda", "from", "sklearnensembl", "import", "randomforestclassifi", "from", "sklearnmodelselect", "import", "traintestsplit", "from", "sklearnmetr", "import", "accuracyscor", "acc", "from", "mlxtendfeatureselect", "import", "sequentialfeatureselector", "sfs", "read", "data", "traintest", "split", "xtrain", "xtest", "ytrain", "ytest", "ytrain", "ytrain", "ytest", "ytest", "print", "train", "dataset", "shape", "xtrain", "shape", "ytrain", "shape", "print", "test", "dataset", "shape", "xtest", "shape", "ytest", "shape", "train", "dataset", "shape", "num", "num", "num", "test", "dataset", "shape", "num", "num", "num", "next", "will", "defin", "classifi", "well", "step", "forward", "featur", "selector", "and", "then", "perform", "our", "featur", "select", "the", "featur", "featur", "selector", "mlxtend", "has", "some", "paramet", "can", "defin", "here", "how", "will", "proceed", "first", "pass", "our", "classifi", "the", "random", "forest", "classifi", "defin", "abov", "the", "featur", "selector", "next", "defin", "the", "subset", "featur", "are", "look", "select", "kfeaturesnum", "then", "set", "float", "fals", "see", "the", "document", "for", "more", "info", "float", "the", "float", "algorithm", "have", "addit", "exclus", "inclus", "step", "remov", "featur", "onc", "they", "were", "includ", "exclud", "that", "larger", "number", "featur", "subset", "combin", "can", "sampl", "set", "desir", "level", "verbos", "for", "mlxtend", "report", "import", "set", "our", "score", "accuraci", "this", "but", "one", "metric", "which", "could", "use", "score", "our", "result", "model", "built", "the", "select", "featur", "mlxtend", "featur", "selector", "use", "cross", "valid", "intern", "and", "set", "our", "desir", "fold", "num", "for", "our", "demonstr", "the", "dataset", "chose", "veri", "larg", "and", "the", "follow", "code", "should", "not", "take", "long", "execut", "generat", "use", "hilitem", "build", "classifi", "use", "featur", "select", "clf", "build", "step", "forward", "featur", "select", "sfsnum", "perform", "sfsnum", "sfsnum", "done", "num", "out", "num", "elaps", "num", "remain", "num", "done", "num", "out", "num", "elaps", "num", "finish", "num", "num", "featur", "num", "score", "num", "done", "num", "out", "num", "elaps", "num", "remain", "num", "done", "num", "out", "num", "elaps", "num", "finish", "num", "num", "featur", "num", "score", "num", "done", "num", "out", "num", "elaps", "num", "remain", "num", "done", "num", "out", "num", "elaps", "num", "finish", "num", "num", "featur", "num", "score", "num", "done", "num", "out", "num", "elaps", "num", "remain", "num", "done", "num", "out", "num", "elaps", "num", "finish", "num", "num", "featur", "num", "score", "num", "done", "num", "out", "num", "elaps", "num", "remain", "num", "done", "num", "out", "num", "elaps", "num", "finish", "num", "num", "featur", "num", "score", "num", "our", "best", "perform", "model", "given", "our", "score", "metric", "some", "subset", "num", "featur", "with", "score", "num", "rememb", "that", "this", "use", "cross", "valid", "and", "will", "differ", "than", "that", "which", "report", "our", "full", "model", "below", "use", "train", "and", "test", "set", "but", "which", "subset", "num", "featur", "were", "select", "generat", "use", "hilitem", "which", "featur", "featcol", "list", "sfsnum", "kfeatureidx", "print", "featcol", "num", "num", "num", "num", "num", "the", "column", "these", "index", "are", "those", "which", "were", "select", "great", "what", "now", "can", "now", "use", "those", "featur", "build", "full", "model", "use", "our", "train", "and", "test", "set", "had", "much", "larger", "set", "mani", "more", "instanc", "oppos", "mani", "more", "featur", "this", "would", "especi", "benefici", "could", "have", "use", "the", "featur", "selector", "abov", "smaller", "subset", "instanc", "determin", "our", "best", "perform", "subset", "featur", "and", "then", "appli", "them", "the", "full", "dataset", "for", "classif", "the", "code", "below", "build", "classifi", "onli", "the", "subset", "select", "featur", "generat", "use", "hilitem", "build", "full", "model", "with", "select", "featur", "clf", "clf", "ytrainpr", "clf", "print", "train", "accuraci", "select", "featur", "numf", "ytestpr", "clf", "print", "test", "accuraci", "select", "featur", "numf", "train", "accuraci", "select", "featur", "num", "test", "accuraci", "select", "featur", "num", "worri", "about", "the", "actual", "accuraci", "concern", "here", "with", "the", "process", "not", "the", "end", "result", "but", "what", "were", "concern", "with", "the", "end", "result", "and", "want", "know", "our", "featur", "select", "troubl", "had", "been", "worth", "well", "could", "compar", "the", "result", "accuraci", "the", "full", "model", "built", "use", "the", "select", "featur", "immedi", "abov", "with", "the", "result", "accuraci", "anoth", "full", "model", "use", "all", "the", "featur", "just", "below", "generat", "use", "hilitem", "build", "full", "model", "featur", "for", "comparison", "clf", "clf", "ytrainpr", "clf", "print", "train", "accuraci", "all", "featur", "numf", "ytestpr", "clf", "print", "test", "accuraci", "all", "featur", "numf", "train", "accuraci", "all", "featur", "num", "test", "accuraci", "all", "featur", "num", "and", "there", "have", "them", "for", "comparison", "they", "are", "both", "poor", "and", "compar", "our", "model", "built", "with", "the", "select", "featur", "though", "promis", "this", "not", "alway", "the", "case", "with", "veri", "littl", "work", "could", "see", "how", "these", "select", "featur", "perform", "with", "differ", "algorithm", "help", "scratch", "that", "itch", "wonder", "whether", "these", "featur", "select", "with", "one", "algorithm", "are", "equal", "well", "perform", "with", "anoth", "such", "featur", "select", "method", "can", "effect", "part", "disciplin", "machin", "learn", "pipelin", "keep", "mind", "that", "step", "forward", "step", "backward", "method", "specif", "can", "provid", "problem", "when", "deal", "with", "especi", "larg", "highlydimension", "dataset", "there", "are", "way", "get", "around", "tri", "get", "around", "these", "stick", "point", "such", "sampl", "from", "the", "data", "find", "the", "featur", "subset", "which", "work", "best", "and", "then", "use", "these", "featur", "for", "the", "model", "process", "the", "full", "dataset", "cours", "these", "are", "not", "the", "onli", "disciplin", "approach", "featur", "select", "either", "and", "check", "out", "altern", "may", "warrant", "when", "deal", "with", "these", "larger", "dataset", "relat", "quick", "featur", "engin", "with", "date", "use", "fastai", "generat", "text", "with", "rnns", "num", "line", "code", "multiobject", "optim", "for", "featur", "select"], "timestamp_scraper": 1556479566.055955, "title": "Step Forward Feature Selection: A Practical Example in Python", "read_time": 525.3, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/matt-mayo\" rel=\"author\" title=\"Posts by Matthew Mayo\">Matthew Mayo</a>, KDnuggets.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p>Many methods for <a href=\"https://en.wikipedia.org/wiki/Feature_selection\" rel=\"noopener\" target=\"_blank\">feature selection</a> exist, some of which treat the process strictly as an artform, others as a science, while, in reality, some form of domain knowledge along with a disciplined approach are likely your best bet.</p>\n<p>When it comes to disciplined approaches to feature selection, <a href=\"https://en.wikipedia.org/wiki/Feature_selection#Wrapper_method\" rel=\"noopener\" target=\"_blank\">wrapper methods</a> are those which marry the feature selection process to the type of model being built, evaluating feature subsets in order to detect the model performance between features, and subsequently select the best performing subset. In other words, instead of existing as an independent process taking place <em>prior</em> to model building, wrapper methods attempt to optimize feature selection process for a given machine learning algorithm <em>in tandem</em> with this algorithm.</p>\n<p>2 prominent wrapper methods for feature selection are step forward feature selection and step backward features selection.</p>\n<p><center><img alt=\"Image\" src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Feature_selection_Wrapper_Method.png\" width=\"99%\"><br>\n<font size=\"-1\"><a href=\"https://en.wikipedia.org/wiki/Feature_selection\" rel=\"noopener\" target=\"_blank\">Image source</a></font></br></img></center></p>\n<p>Step forward feature selection starts with the evaluation of each individual feature, and selects that which results in the best performing selected algorithm model. What's the \"best?\" That depends entirely on the defined evaluation criteria (AUC, prediction accuracy, RMSE, etc.). Next, all possible combinations of the that selected feature and a subsequent feature are evaluated, and a second feature is selected, and so on, until the required predefined number of features is selected.</p>\n<p>Step backward feature selection is closely related, and as you may have guessed starts with the entire set of features and works backward from there, removing features to find the optimal subset of a predefined size.</p>\n<p>These are both potentially very computationally expensive. Do you have a large, multidimensional dataset? These methods may take too long to be at all useful, or may be totally infeasible. That said, with a dataset of accommodating size and dimensionality, such an approach may well be your best possible approach.</p>\n<p>To see how they work, let's take a look at step forward feature selection, specifically. Note that, as discussed, a machine learning algorithm must be defined prior to beginning our symbiotic feature selection process.</p>\n<p>Keep in mind that an optimized set of selected features using a given algorithm may or may not perform equally well with a different algorithm. If we select features using logistic regression, for example, there is no guarantee that these same features will perform optimally if we then tried them out using K-nearest neighbors, or an SVM.</p>\n<p>\u00a0</p>\n<h3>Implementing Feature Selection and Building a Model</h3>\n<p>\u00a0<br>\nSo, how do we perform step forward feature selection in Python? <a href=\"https://github.com/rasbt/mlxtend\" rel=\"noopener\" target=\"_blank\">Sebastian Raschka's mlxtend library</a> includes an implementation (<a href=\"https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/\" rel=\"noopener\" target=\"_blank\">Sequential Feature Selector</a>), and so we will use it to demonstrate. It goes without saying that you should have mlxtend installed before moving forward (check the Github repo).</br></p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://raw.githubusercontent.com/rasbt/mlxtend/master/docs/sources/img/logo.png\" width=\"50%\"/></p>\n<p>We will use a Random Forest classifier for feature selection and model building (which, again, are intimately related in the case of step forward feature selection).</p>\n<p>We need data to use for demonstration, so let's use the <a href=\"https://archive.ics.uci.edu/ml/datasets/wine+quality\" rel=\"noopener\" target=\"_blank\">wine quality dataset</a>. Specifically, I have used the untouched <code>winequality-white.csv</code> file as input in the code below.</p>\n<p>Arbitrarily, we will set the desired number of features to 5 (there are 12 in the dataset). What we are able to do is compare the evaluation scores for each iteration of the feature selection process, and so keep in mind that if we find that a lower number of features has a better score we can alternatively choose that best-performing subset to run with in our \"live\" model moving forward. Also keep in mind that setting our desired number of features too low could lead to a sub-optimal number and combination of features being decided upon (say, if some combination of 11 features in our case is better than the best combination of &lt;= 10 features we find during the selection process).</p>\n<p>Since we are more interested in demonstrating how to implement step forward feature selection than we are with the actual results on this particular dataset, we won't be overly concerned with the actual performance of our models, but we will compare the performances anyhow, as to show how it would be done in a meaningful project.</p>\n<p>First, we will make our imports, load the dataset, and split it into training and testing sets.</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #006699; font-weight: bold\">import</span> <span style=\"color: #00CCFF; font-weight: bold\">numpy</span> <span style=\"color: #006699; font-weight: bold\">as</span> <span style=\"color: #00CCFF; font-weight: bold\">np</span>\r\n<span style=\"color: #006699; font-weight: bold\">import</span> <span style=\"color: #00CCFF; font-weight: bold\">pandas</span> <span style=\"color: #006699; font-weight: bold\">as</span> <span style=\"color: #00CCFF; font-weight: bold\">pd</span>\r\n<span style=\"color: #006699; font-weight: bold\">from</span> <span style=\"color: #00CCFF; font-weight: bold\">sklearn.ensemble</span> <span style=\"color: #006699; font-weight: bold\">import</span> RandomForestClassifier\r\n<span style=\"color: #006699; font-weight: bold\">from</span> <span style=\"color: #00CCFF; font-weight: bold\">sklearn.model_selection</span> <span style=\"color: #006699; font-weight: bold\">import</span> train_test_split\r\n<span style=\"color: #006699; font-weight: bold\">from</span> <span style=\"color: #00CCFF; font-weight: bold\">sklearn.metrics</span> <span style=\"color: #006699; font-weight: bold\">import</span> accuracy_score <span style=\"color: #006699; font-weight: bold\">as</span> acc\r\n<span style=\"color: #006699; font-weight: bold\">from</span> <span style=\"color: #00CCFF; font-weight: bold\">mlxtend.feature_selection</span> <span style=\"color: #006699; font-weight: bold\">import</span> SequentialFeatureSelector <span style=\"color: #006699; font-weight: bold\">as</span> sfs\r\n\r\n<span style=\"color: #0099FF; font-style: italic\"># Read data</span>\r\ndf <span style=\"color: #555555\">=</span> pd<span style=\"color: #555555\">.</span>read_csv(<span style=\"color: #CC3300\">'winequality-white.csv'</span>, sep<span style=\"color: #555555\">=</span><span style=\"color: #CC3300\">';'</span>)\r\n\r\n<span style=\"color: #0099FF; font-style: italic\"># Train/test split</span>\r\nX_train, X_test, y_train, y_test <span style=\"color: #555555\">=</span> train_test_split(\r\n    df<span style=\"color: #555555\">.</span>values[:,:<span style=\"color: #555555\">-</span><span style=\"color: #FF6600\">1</span>],\r\n    df<span style=\"color: #555555\">.</span>values[:,<span style=\"color: #555555\">-</span><span style=\"color: #FF6600\">1</span>:],\r\n    test_size<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">0.25</span>,\r\n    random_state<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">42</span>)\r\n\r\ny_train <span style=\"color: #555555\">=</span> y_train<span style=\"color: #555555\">.</span>ravel()\r\ny_test <span style=\"color: #555555\">=</span> y_test<span style=\"color: #555555\">.</span>ravel()\r\n\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Training dataset shape:'</span>, X_train<span style=\"color: #555555\">.</span>shape, y_train<span style=\"color: #555555\">.</span>shape)\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Testing dataset shape:'</span>, X_test<span style=\"color: #555555\">.</span>shape, y_test<span style=\"color: #555555\">.</span>shape)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\nTraining dataset shape: (3673, 11) (3673,)\r\nTesting dataset shape: (1225, 11) (1225,)</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Next, we will define a classifier, as well as a step forward feature selector, and then perform our feature selection. The feature feature selector in mlxtend has some parameters we can define, so here's how we will proceed:</p>\n<ul>\n<li>First, we pass our classifier, the Random Forest classifier defined above the feature selector\n<li>Next, we define the subset of features we are looking to select (k_features=5)\n<li>We then set floating to False; see the documentation for more info on floating:\n</li></li></li></ul>\n<blockquote><p>\nThe floating algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded), so that a larger number of feature subset combinations can be sampled.\n</p></blockquote>\n<ul>\n<li>We set he desired level of verbosity for mlxtend to report\n<li>Importantly, we set our scoring to accuracy; this is but one metric which could be used to score our resulting models built on the selected features\n<li>mlxtend feature selector uses cross validation internally, and we set our desired folds to 5 for our demonstration\n</li></li></li></ul>\n<p>The dataset we chose isn't very large, and so the following code should not take long to execute.</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #0099FF; font-style: italic\"># Build RF classifier to use in feature selection</span>\r\nclf <span style=\"color: #555555\">=</span> RandomForestClassifier(n_estimators<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">100</span>, n_jobs<span style=\"color: #555555\">=-</span><span style=\"color: #FF6600\">1</span>)\r\n\r\n<span style=\"color: #0099FF; font-style: italic\"># Build step forward feature selection</span>\r\nsfs1 <span style=\"color: #555555\">=</span> sfs(clf,\r\n           k_features<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">5</span>,\r\n           forward<span style=\"color: #555555\">=</span><span style=\"color: #336666\">True</span>,\r\n           floating<span style=\"color: #555555\">=</span><span style=\"color: #336666\">False</span>,\r\n           verbose<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">2</span>,\r\n           scoring<span style=\"color: #555555\">=</span><span style=\"color: #CC3300\">'accuracy'</span>,\r\n           cv<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">5</span>)\r\n\r\n<span style=\"color: #0099FF; font-style: italic\"># Perform SFFS</span>\r\nsfs1 <span style=\"color: #555555\">=</span> sfs1<span style=\"color: #555555\">.</span>fit(X_train, y_train)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   24.3s finished\r\n\r\n[2018-06-12 14:47:47] Features: 1/5 -- score: 0.49768148939247264[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   22.7s finished\r\n\r\n[2018-06-12 14:48:09] Features: 2/5 -- score: 0.5442629071398873[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   21.2s finished\r\n\r\n[2018-06-12 14:48:31] Features: 3/5 -- score: 0.6052194438136681[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   20.3s finished\r\n\r\n[2018-06-12 14:48:51] Features: 4/5 -- score: 0.6261526236769334[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s remaining:    0.0s\r\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   17.3s finished\r\n\r\n[2018-06-12 14:49:08] Features: 5/5 -- score: 0.6444222989869156</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Our best performing model, given our scoring metric, is some subset of 5 features, with a score of 0.644 (remember that this is using cross validation, and so will be different than that which is reported on our full models below, using train and test sets). But which subset of 5 features were selected?</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #0099FF; font-style: italic\"># Which features?</span>\r\nfeat_cols <span style=\"color: #555555\">=</span> <span style=\"color: #336666\">list</span>(sfs1<span style=\"color: #555555\">.</span>k_feature_idx_)\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(feat_cols)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\n[1, 2, 3, 7, 10]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The columns at these indexes are those which were selected. Great! So what now...?</p>\n<p>We can now use those features to build a full model using our training and test sets. If we had a much larger set (i.e. many more instances as opposed to many more features), this would be especially beneficial as we could have used the feature selector above on a smaller subset of instances, determined our best performing subset of features, and then applied them to the full dataset for classification.</p>\n<p>The code below builds a classifier on <b>only</b> the subset of selected features.</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #0099FF; font-style: italic\"># Build full model with selected features</span>\r\nclf <span style=\"color: #555555\">=</span> RandomForestClassifier(n_estimators<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">1000</span>, random_state<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">42</span>, max_depth<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">4</span>)\r\nclf<span style=\"color: #555555\">.</span>fit(X_train[:, feat_cols], y_train)\r\n\r\ny_train_pred <span style=\"color: #555555\">=</span> clf<span style=\"color: #555555\">.</span>predict(X_train[:, feat_cols])\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Training accuracy on selected features: </span><span style=\"color: #AA0000\">%.3f</span><span style=\"color: #CC3300\">'</span> <span style=\"color: #555555\">%</span> acc(y_train, y_train_pred))\r\n\r\ny_test_pred <span style=\"color: #555555\">=</span> clf<span style=\"color: #555555\">.</span>predict(X_test[:, feat_cols])\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Testing accuracy on selected features: </span><span style=\"color: #AA0000\">%.3f</span><span style=\"color: #CC3300\">'</span> <span style=\"color: #555555\">%</span> acc(y_test, y_test_pred))\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\nTraining accuracy on selected features: 0.558\r\nTesting accuracy on selected features: 0.512\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Don't worry about the actual accuracies; we're concerned here with the process, not the end result.</p>\n<p>But what if we <em>were</em> concerned with the end result, and wanted to know if our feature selection troubles had been worth it? Well, we could compare the resultant accuracies of the full model built using the selected features (immediately above) with the resultant accuracies of another full model using <b>all</b> of the features, just as we do below:</p>\n<p><!-- HTML generated using hilite.me --></p>\n<div style=\"background: #f0f3f3; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .2em;padding:.8em .6em;\">\n<pre style=\"margin: 0; line-height: 125%\"><span style=\"color: #0099FF; font-style: italic\"># Build full model on ALL features, for comparison</span>\r\nclf <span style=\"color: #555555\">=</span> RandomForestClassifier(n_estimators<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">1000</span>, random_state<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">42</span>, max_depth<span style=\"color: #555555\">=</span><span style=\"color: #FF6600\">4</span>)\r\nclf<span style=\"color: #555555\">.</span>fit(X_train, y_train)\r\n\r\ny_train_pred <span style=\"color: #555555\">=</span> clf<span style=\"color: #555555\">.</span>predict(X_train)\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Training accuracy on all features: </span><span style=\"color: #AA0000\">%.3f</span><span style=\"color: #CC3300\">'</span> <span style=\"color: #555555\">%</span> acc(y_train, y_train_pred))\r\n\r\ny_test_pred <span style=\"color: #555555\">=</span> clf<span style=\"color: #555555\">.</span>predict(X_test)\r\n<span style=\"color: #006699; font-weight: bold\">print</span>(<span style=\"color: #CC3300\">'Testing accuracy on all features: </span><span style=\"color: #AA0000\">%.3f</span><span style=\"color: #CC3300\">'</span> <span style=\"color: #555555\">%</span> acc(y_test, y_test_pred))\r\n</pre>\n</div>\n<p>\u00a0</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>\r\nTraining accuracy on all features: 0.566\r\nTesting accuracy on all features: 0.509\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>And there you have them for comparison. They are both poor and comparable to our model built with the selected features (though I promise this is not always the case!). With very little work, you could see how these selected features perform with a different algorithm, to help scratch that itch as to wondering whether these features selected with one algorithm are equally well performing with another.</p>\n<p>Such a feature selection method can be an effective part of a disciplined machine learning pipeline. Keep in mind that step forward (or step backward) methods, specifically, can provide problems when dealing with especially large or highly-dimensional datasets. There are ways of getting around (or <b>trying</b> to get around) these sticking points, such as sampling from the data to find the feature subset which works best, and then using these features for the modeling process on the full dataset. Of course, these are not the only disciplined approaches to feature selection either, and so checking out alternatives may be warranted when dealing with these larger datasets.</p>\n<p>\u00a0<br/>\n<b>Related</b>:</p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/03/feature-engineering-dates-fastai.html\">Quick Feature Engineering with Dates Using fast.ai</a>\n<li><a href=\"/2018/06/generating-text-rnn-4-lines-code.html\">Generating Text with RNNs in 4 Lines of Code</a>\n<li><a href=\"/2017/12/rapidminer-multi-objective-optimization-feature-selection.html\">Multi-objective Optimization for Feature Selection</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}