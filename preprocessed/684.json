{"content": "By James Le , Machine Learning Engineer. comments Interest in\u00a0 machine learning \u00a0has exploded over the past decade. You see machine learning in computer science programs, industry conferences, and the Wall Street Journal almost daily. For all the talk about machine learning, many conflate what it can do with what they wish it could do. Fundamentally, machine learning is using algorithms to extract information from raw data and represent it in some type of model. We use this model to infer things about other data we have not yet modeled. Neural networks \u00a0are one type of model for machine learning; they have been around for at least 50 years. The fundamental unit of a neural network is a node, which is loosely based on the biological neuron in the mammalian brain. The connections between neurons are also modeled on biological brains, as is the way these connections develop over time (with \u201ctraining\u201d). In the mid-1980s and early 1990s, many important architectural advancements were made in neural networks. However, the amount of time and data needed to get good results slowed adoption, and thus interest cooled. In the early 2000s, computational power expanded exponentially and the industry saw a \u201cCambrian explosion\u201d of computational techniques that were not possible prior to this.\u00a0 Deep learning \u00a0emerged from that decade\u2019s explosive computational growth as a serious contender in the field, winning many important machine learning competitions. The interest has not cooled as of 2017; today, we see deep learning mentioned in every corner of machine learning. To get myself into the craze, I took\u00a0 Udacity\u2019s \u201cDeep Learning\u201d course , which is a great introduction to the motivation of deep learning and the design of intelligent systems that learn from complex and/or large-scale datasets in\u00a0 TensorFlow . For the class projects, I used and developed neural networks for image recognition with convolutions, natural language processing with embeddings and character based text generation with Recurrent Neural Network / Long Short-Term Memory. All the code in Jupiter Notebook can be found on\u00a0 this GitHub repository . Here is an outcome of one of the assignments, a t-SNE projection of word vectors, clustered by similarity. Most recently, I have started reading academic papers on the subject. From my research, here are several publications that have been hugely influential to the development of the field: NYU\u2019s\u00a0 Gradient-Based Learning Applied to Document Recognition (1998), which introduces Convolutional Neural Network to the Machine Learning world. Toronto\u2019s\u00a0 Deep Boltzmann Machines \u00a0(2009), which presents a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Stanford & Google\u2019s\u00a0 Building High-Level Features Using Large-Scale Unsupervised Learning \u00a0(2012), which addresses the problem of building high-level, class-specific feature detectors from only unlabeled data. Berkeley\u2019s\u00a0 DeCAF\u200a\u2014\u200aA Deep Convolutional Activation Feature for Generic Visual Recognition \u00a0(2013), which releases DeCAF, an open-source implementation of the deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms. DeepMind\u2019s\u00a0 Playing Atari with Deep Reinforcement Learning \u00a0(2016), which presents the 1st deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. There is an abundant amount of great knowledge about deep learning I have learnt via research and learning. Here I want to share the\u00a0 10 powerful deep learning methods \u00a0AI engineers can apply to their machine learning problems. But first of all, let\u2019s define what deep learning is. Deep learning has been a challenge to define for many because it has changed forms slowly over the past decade. To set deep learning in context visually, the figure below illustrates the conception of the relationship between AI, machine learning, and deep learning. The field of AI is broad and has been around for a long time. Deep learning is a subset of the field of machine learning, which is a subfield of AI. The facets that differentiate deep learning networks in general from \u201ccanonical\u201d feed-forward multilayer networks are as follows: More neurons than previous networks More complex ways of connecting layers \u201cCambrian explosion\u201d of computing power to train Automatic feature extraction When I say \u201cmore neurons\u201d, I mean that the neuron count has risen over the years to express more complex models. Layers also have evolved from each layer being fully connected in multilayer networks to locally connected patches of neurons between layers in Convolutional Neural Networks and recurrent connections to the same neuron in Recurrent Neural Networks (in addition to the connections from the previous layer). Deep learning then can be defined as neural networks with a large number of parameters and layers in one of four fundamental network architectures: Unsupervised Pre-trained Networks Convolutional Neural Networks Recurrent Neural Networks Recursive Neural Networks In this post, I am mainly interested in the latter 3 architectures. A\u00a0 Convolutional Neural Network \u00a0is basically a standard neural network that has been extended across space using shared weights. CNN is designed to recognize images by having convolutions inside, which see the edges of an object recognized on the image. A\u00a0 Recurrent Neural Network \u00a0is basically a standard neural network that has been extended across time by having edges which feed into the next time step instead of into the next layer in the same time step. RNN is designed to recognize sequences, for example, a speech signal or a text. It has cycles inside that implies the presence of short memory in the net. A\u00a0 Recursive Neural Network \u00a0is more like a hierarchical network where there is really no time aspect to the input sequence but the input has to be processed hierarchically in a tree fashion. The 10 methods below can be applied to all of these architectures. 1\u200a\u2014\u200aBack-Propagation Back-prop is simply a method to compute the partial derivatives (or gradient) of a function, which has the form as a function composition (as in Neural Nets). When you solve an optimization problem using a gradient-based method (gradient descent is just one of them), you want to compute the function gradient at each iteration. For a Neural Nets, the objective function has the form of a composition. How do you compute the gradient? There are 2 common ways to do it: (i)\u00a0 Analytic differentiation . You know the form of the function. You just compute the derivatives using the chain rule (basic calculus). (ii)\u00a0 Approximate differentiation using finite difference . This method is computationally expensive because the number of function evaluation is\u00a0  , where\u00a0 N \u00a0is the number of parameters. This is expensive, compared to analytic differentiation. Finite difference, however, is commonly used to validate a back-prop implementation when debugging. 2\u200a\u2014\u200aStochastic Gradient\u00a0Descent An intuitive way to think of Gradient Descent is to imagine the path of a river originating from top of a mountain. The goal of gradient descent is exactly what the river strives to achieve\u200a\u2014\u200anamely, reach the bottom most point (at the foothill) climbing down from the mountain. Now, if the terrain of the mountain is shaped in such a way that the river doesn\u2019t have to stop anywhere completely before arriving at its final destination (which is the lowest point at the foothill, then this is the ideal case we desire. In Machine Learning, this amounts to saying, we have found the global mimimum (or optimum) of the solution starting from the initial point (top of the hill). However, it could be that the nature of terrain forces several pits in the path of the river, which could force the river to get trapped and stagnate. In Machine Learning terms, such pits are termed as local minima solutions, which is not desirable. There are a bunch of ways to get out of this (which I am not discussing). Gradient Descent therefore is prone to be stuck in local minimum, depending on the nature of the terrain (or function in ML terms). But, when you have a special kind of mountain terrain (which is shaped like a bowl, in ML terms this is called a Convex Function), the algorithm is always guaranteed to find the optimum. You can visualize this picturing a river again. These kind of special terrains (a.k.a convex functions) are always a blessing for optimization in ML. Also, depending on where at the top of the mountain you initial start from (ie. initial values of the function), you might end up following a different path. Similarly, depending on the speed at the river climbs down (ie. the learning rate or step size for the gradient descent algorithm), you might arrive at the final destination in a different manner. Both of these criteria can affect whether you fall into a pit (local minima) or are able to avoid it. 3\u200a\u2014\u200aLearning Rate\u00a0Decay Adapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time. Sometimes this is called\u00a0 learning rate annealing \u00a0or\u00a0 adaptive learning rates . The simplest and perhaps most used adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure. This has the effect of quickly learning good weights early and fine tuning them later. Two popular and easy to use learning rate decay are as follows: Decrease the learning rate gradually based on the epoch. Decrease the learning rate using punctuated large drops at specific epochs.", "title_html": "<h1 id=\"title\">The 10 Deep Learning Methods AI Practitioners Need to Apply</h1> ", "url": "https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html", "tfidf": {"tfidf": {"mimimum": 1587.6, "googl": 11.388809182200001, "anneal": 260.262295082, "vector": 25.898858075, "corner": 5.873473917869999, "form": 4.51022727272, "signal": 5.12459651388, "kind": 5.1612483745199995, "assign": 3.83663605607, "bowl": 9.06164383562, "addit": 1.24634950542, "berkeley": 11.6735294118, "number": 3.30428749827, "follow": 3.1392037964699995, "recogn": 7.64862694716, "risen": 17.6792873051, "decreas": 13.56923076924, "dataset": 193.609756098, "function": 24.954416849999998, "class": 2.11651779763, "decay": 32.170212766, "descent": 59.46067415728999, "manner": 3.93164933135, "common": 2.8051948051999998, "valu": 4.555523672880001, "fall": 1.6945244956799999, "whether": 2.20683903253, "tree": 4.127925117, "complet": 1.24021560816, "path": 13.926315789479998, "slowli": 6.080428954419999, "know": 2.59327017315, "their": 1.01547908405, "paradigm": 24.2752293578, "feedforward": 1443.27272727, "automat": 6.787516032490001, "calculus": 62.2588235294, "instead": 1.59461631177, "explod": 12.432263116700002, "express": 1.9120799710900003, "extend": 3.9209681402800007, "how": 1.60250328051, "paramet": 51.769565217300006, "present": 2.51103202846, "subfield": 226.8, "bunch": 40.5, "jame": 1.9313868613099998, "increas": 1.32024948025, "special": 2.9763779527599996, "initi": 4.050000000000001, "unlabel": 992.25, "end": 1.10680423871, "found": 2.2277415281, "releas": 1.8377126982299998, "word": 1.7965372864099998, "interest": 6.41324984852, "guarante": 6.57119205298, "deep": 68.93144424126, "depend": 6.723320158110001, "associ": 1.3263157894700002, "recurr": 177.9820627805, "unit": 1.15394679459, "aka": 20.834645669300002, "updat": 5.56466876972, "toronto": 8.849498327760001, "neuron": 449.9271255059, "below": 4.51215006394, "perform": 1.5313977042500002, "but": 3.04897253697, "success": 1.32002993265, "were": 2.04917715392, "need": 1.4372623574099999, "all": 5.05733944955, "subject": 1.8715077213299998, "final": 2.6801721955, "tune": 10.4173228346, "point": 3.7797000238200003, "hidden": 7.81299212598, "decad": 6.4171382376600015, "has": 13.567446752599999, "generat": 2.05275407292, "arriv": 4.06347581264, "stop": 2.1783754116400003, "specif": 1.8719490626099997, "tensorflow": 1587.6, "use": 14.414942603319998, "optim": 34.6133720931, "out": 1.06016694491, "model": 14.6341848828, "optimum": 116.3076923076, "good": 3.03963239518, "alway": 4.13491340018, "slow": 4.04793472718, "concept": 5.31414225942, "cours": 2.15092805853, "competit": 3.06960556845, "huge": 4.38927287808, "count": 3.48157894737, "learnt": 56.9032258065, "field": 7.1160914388, "global": 3.30612244898, "short": 1.41295834817, "climb": 18.568421052639998, "not": 5.07836990595, "wish": 3.67755385685, "growth": 3.15062512403, "mammalian": 76.6956521739, "techniqu": 7.458773784360001, "four": 1.20950784702, "deepmind": 1587.6, "quick": 2.205, "easi": 5.2937645882, "adopt": 2.0442956477000003, "jupit": 39.007371007399996, "solv": 7.26923076923, "than": 1.03278688525, "scienc": 2.31969608416, "they": 2.06034650574, "design": 4.37475888675, "reach": 1.49801849406, "effect": 1.3963060686000002, "let": 3.48616600791, "world": 1.11340206186, "minimum": 6.02962400304, "outcom": 7.48867924528, "larger": 2.2407904022599996, "github": 1587.6, "node": 44.3463687151, "featur": 7.6356290881, "feed": 7.77853993141, "stuck": 18.945107398599998, "algorithm": 111.8028169016, "daili": 2.76971388695, "develop": 3.5867158671600006, "such": 3.18454132122, "terrain": 66.4824120605, "sever": 2.14482572278, "via": 2.2978723404299997, "talk": 3.0303493033, "network": 64.842346022, "confer": 2.8324710080299997, "deriv": 5.5675960021, "andor": 690.260869565, "control": 1.46959178006, "both": 1.05215720061, "result": 1.14611608432, "document": 2.5409731114, "subset": 27.3253012048, "cluster": 12.5007874016, "repositori": 44.974504249300004, "rate": 27.826344883349996, "biolog": 13.21348314606, "bless": 11.141052631600001, "languag": 2.29488291414, "set": 1.18707940781, "two": 1.01379310345, "from": 13.00737379461, "anywher": 10.1638924456, "num": 16.00504064016, "impli": 5.5125, "partial": 3.6131087847099996, "shortterm": 1587.6, "for": 14.00441056014, "mountain": 17.5891867937, "bottom": 6.27261951798, "fulli": 2.79015817223, "with": 9.010783880909997, "forc": 2.6479859895, "new": 1.0178880554, "are": 13.38877716514, "solut": 9.4556283502, "minima": 690.2608695660001, "debug": 180.409090909, "popular": 1.50769230769, "highdimension": 1587.6, "tsne": 1587.6, "extract": 15.406113537120001, "think": 2.90715986083, "public": 1.22424429365, "later": 2.17300848618, "net": 20.88947368422, "udac": 1587.6, "iter": 37.4433962264, "compar": 1.8662278123900002, "then": 2.17315721032, "infer": 21.1398135819, "valid": 6.61224489796, "train": 13.5559892657, "great": 2.53185551392, "motiv": 5.01611374408, "goal": 3.28152128979, "convex": 207.529411764, "type": 4.056208482380001, "backprop": 3175.2, "conflat": 70.2477876106, "reinforc": 12.907317073180002, "notebook": 40.1924050633, "again": 1.50883862384, "space": 2.39818731118, "paper": 2.6628648104700003, "prior": 2.17807655371, "where": 3.20145190563, "figur": 2.0343413634, "canon": 10.8220858896, "what": 5.01373756512, "local": 6.06880733944, "presenc": 2.7476635514, "prone": 18.144000000000002, "exponenti": 39.2, "relationship": 2.39132399458, "influenti": 4.520501138949999, "main": 1.25303867403, "strive": 21.9585062241, "larg": 3.55724848755, "find": 1.7294117647099998, "largescal": 3175.2, "brain": 17.858267716540002, "least": 1.6165359943000002, "approxim": 2.2132998745299997, "base": 3.43884476535, "natur": 4.61780104713, "can": 9.41009113136, "pictur": 3.4953764861300005, "atari": 92.3023255814, "post": 2.23826307627, "here": 7.26923076924, "ideal": 4.65571847507, "punctuat": 42.1114058355, "sensori": 40.2944162437, "speed": 3.8703071672400005, "stagnat": 34.8157894737, "long": 2.5314518057799997, "layer": 65.13230769232, "decaf": 3175.2, "evalu": 6.9509632224199995, "contain": 1.59814777532, "about": 3.19458045477, "problem": 5.30024482527, "epoch": 76.695652174, "boltzmann": 466.94117647, "neural": 1129.7528089887, "thing": 2.4065484311099996, "context": 4.25972632144, "just": 2.67160286074, "adapt": 9.96818752617, "saw": 1.94845360825, "simpli": 2.5192002538900002, "dure": 1.0503473370799998, "defin": 8.184911496809999, "expens": 7.090665475660001, "serious": 2.583984375, "recognit": 13.200665188470001, "term": 5.58080674928, "across": 5.19559288752, "complex": 7.0206367924499995, "destin": 12.100609756099999, "start": 3.8002074523200005, "latter": 2.34159292035, "took": 1.4009883515700001, "intuit": 27.7068062827, "enabl": 3.5421686747, "criteria": 11.7426035503, "them": 2.19752231988, "standard": 3.7831526271800007, "avoid": 2.45986984816, "expand": 2.2260235558000003, "other": 1.00992366412, "abl": 3.6417020300400003, "mani": 5.22133789385, "appli": 6.8916220518, "over": 5.1262512108500005, "step": 8.48379052368, "perhap": 3.14812611541, "build": 3.2683479156, "wall": 3.07853403141, "pretrain": 1587.6, "journal": 2.36884512086, "broad": 4.27693965517, "earli": 3.3740436384300003, "facet": 49.7680250784, "same": 2.23715916296, "exact": 3.46864758575, "there": 4.16365066876, "explos": 20.16426756987, "connect": 13.190741839730002, "inform": 1.5753125620200001, "trap": 7.4570220760899995, "possibl": 1.4173734488, "conduct": 2.2637958077900002, "say": 3.5088960106, "contend": 9.08757870635, "top": 5.516330785260001, "origin": 1.13724928367, "illustr": 3.6614391143900002, "loos": 7.065420560750001, "cambrian": 373.552941176, "knowledg": 3.3981164383599998, "visual": 20.91010865988, "previous": 2.85693719632, "now": 1.160780873, "might": 4.312372674180001, "name": 1.10211732037, "code": 3.8807137619199996, "foothil": 85.5849056604, "desir": 6.00340328984, "represent": 5.928304705, "basic": 8.190541702500001, "play": 1.46390041494, "realli": 4.7476076555, "power": 4.018901358539999, "edg": 8.91409320606, "also": 3.04429530201, "simplest": 28.0494699647, "around": 2.42789417342, "mean": 1.44906900329, "into": 4.06009845916, "fashion": 4.85207823961, "composit": 9.259842519680001, "stochast": 256.06451613, "research": 5.8260550458600004, "therefor": 4.66803881212, "the": 107.0, "polici": 2.52963671128, "lowest": 6.549504950499999, "past": 4.03404904078, "evolv": 4.60173913043, "challeng": 2.55816951337, "machin": 64.38935361216, "want": 3.99396226416, "street": 2.36707917102, "generic": 17.9592760181, "system": 1.38739840951, "sometim": 1.7126213592200001, "howev": 3.2835573939899994, "share": 3.7132499123000002, "variabl": 8.747107438019999, "win": 2.75290445639, "that": 14.055776892499999, "numst": 2.6297830047999997, "which": 16.08306952, "intellig": 4.19334389857, "activ": 2.92807082258, "thus": 1.6463756092500001, "raw": 10.6478873239, "introduct": 2.7808723068799996, "implement": 7.15296237892, "procedur": 17.60739371535, "one": 4.02509982888, "abund": 9.563855421689999, "begin": 1.3305397251100002, "see": 3.81726376533, "some": 1.04036697248, "becaus": 2.2990369994999997, "imag": 8.10413476263, "river": 17.6428004445, "highlevel": 3175.2, "craze": 61.7743190661, "classspecif": 1587.6, "get": 7.1425036554, "fundament": 15.98791540785, "like": 2.2983713355, "myself": 14.5517873511, "imagin": 6.598503740650001, "opensourc": 1587.6, "process": 3.39049652964, "backpropag": 1587.6, "memori": 5.14785992218, "yet": 2.1258703802900003, "recent": 1.54405757635, "cool": 13.715766738660001, "next": 2.9901120632800002, "drop": 2.4594887684, "rang": 1.7848229342299997, "time": 9.10147143132, "finit": 56.3978685612, "engin": 4.94271481942, "chang": 2.3617970842, "similar": 2.75028150714, "smaller": 5.18738768176, "project": 3.5069582505000003, "differ": 4.946179609, "embed": 16.835630965, "been": 6.143566591439999, "most": 3.06289389069, "between": 3.1036100612399995, "analyt": 34.513043478200004, "stanford": 12.6, "recurs": 182.4827586206, "shape": 6.40677966102, "along": 1.2973768080399999, "midnum": 1587.6, "charact": 2.51720310766, "today": 1.74961428257, "program": 2.02139037433, "gradient": 418.89182058000006, "almost": 1.53584212054, "case": 1.48498737256, "fine": 4.02229541424, "weight": 14.636754763379997, "architectur": 20.51162790696, "rule": 1.7415533128599998, "experiment": 6.07112810707, "call": 2.1353059852, "academ": 3.8921304241199994, "object": 4.697736351540001, "more": 5.085853408499999, "and": 21.001322834729997, "discuss": 2.19676214197, "achiev": 1.87216981132, "these": 5.3707713126000005, "benefit": 3.06841901817, "unsupervis": 690.2608695660001, "amount": 6.8108108108100005, "general": 1.1218202374200001, "made": 2.14077669902, "could": 3.6131087846999996, "emerg": 2.1131372288, "befor": 1.10036041031, "comment": 3.05954904606, "affect": 2.4794627518400003, "detector": 45.6206896552, "patch": 16.764519535399998, "introduc": 1.7258397651900002, "comput": 39.277585353800006, "make": 1.0762660158600001, "way": 7.3144436766, "pit": 34.5631349781, "onli": 1.0256476516600002, "each": 2.37949640288, "hill": 2.89233011478, "size": 2.49387370405, "gradientbas": 3175.2, "industri": 4.04638715432, "this": 13.049317147230001, "speech": 3.8227787141800005, "read": 2.3149606299200003, "advanc": 1.9997480791, "cycl": 5.40919931857, "chain": 5.17639387023, "data": 13.50574223736, "reduc": 3.97396745932, "address": 2.86157173756, "method": 12.857142857150002, "sequenc": 12.14225621414, "repres": 1.46972782818, "convolut": 808.968152864, "exampl": 1.50483412322, "aspect": 3.0893169877399997, "text": 6.25655172414, "insid": 5.479206212259999, "differenti": 31.038123167159995, "direct": 1.22226499346, "import": 2.6803984467400004, "first": 1.00761614623, "everi": 1.47917637194, "vision": 4.88041807562, "input": 36.6087624903, "have": 12.178738093679998, "mention": 2.53894130817, "year": 2.0970873786400004, "gradual": 3.7890214797099997, "down": 2.71779508688, "learn": 116.13752743250001, "when": 5.10383848775, "multilay": 648.0, "hierarch": 60.48}, "logtfidf": {"mimimum": 7.369978720910001, "googl": 2.43263122258, "anneal": 5.561689949730001, "vector": 3.25419887797, "corner": 1.77044626763, "form": 0.480212736764, "signal": 1.6340517929299998, "kind": 1.896062605434, "assign": 1.3445959556, "bowl": 2.20405054241, "addit": 0.220218882972, "berkeley": 2.4573238351700004, "number": 0.2898257352558, "follow": 0.1360707332826, "recogn": 2.807741577093, "risen": 2.8723937456, "decreas": 4.52757749232, "dataset": 5.26584456664, "function": 9.14465741594, "class": 0.7497721899330001, "decay": 5.555787548940001, "descent": 14.975835045150001, "manner": 1.36905901503, "common": 0.676651610542, "valu": 1.646386620296, "fall": 0.527402167952, "whether": 0.791561189647, "tree": 1.41777488775, "complet": 0.215285242047, "path": 4.605503951549999, "slowli": 1.8050752452, "know": 0.952919694398, "their": 0.015360505122700001, "paradigm": 3.18945646245, "feedforward": 7.2746685411000005, "automat": 1.9150850473199998, "calculus": 4.1313002687400004, "instead": 0.46663315041500003, "explod": 2.52029495787, "express": 0.648191639641, "extend": 1.346382834622, "how": 0.47156695693000006, "paramet": 8.544570431579999, "present": 0.455093309598, "subfield": 5.4240685718499995, "bunch": 3.70130197411, "jame": 0.658238325853, "increas": 0.277820718929, "special": 0.7951198572020001, "initi": 0.90031377735, "unlabel": 6.89997509166, "end": 0.101476798618, "found": 0.215682248096, "releas": 0.608521699544, "word": 0.585861082385, "interest": 1.8882871119279998, "guarante": 1.8826952548500002, "deep": 24.4847959262, "depend": 2.420909445, "associ": 0.28240501535100004, "recurr": 17.8612243094, "unit": 0.143188061817, "aka": 3.03661725822, "updat": 1.7164374626899999, "toronto": 2.1803607712799997, "neuron": 29.14222834089, "below": 1.627253183872, "perform": 0.42618085058, "but": 0.0485771162157, "success": 0.27765441259199997, "were": 0.048582287362199994, "need": 0.362740163442, "all": 0.057013160488999994, "subject": 0.6267443740950001, "final": 0.585467727896, "tune": 2.3434700776599997, "point": 0.6930970770989999, "hidden": 2.0557880052, "decad": 2.281079916846, "has": 0.5554112831124001, "generat": 0.719182341736, "arriv": 1.4177830765759998, "stop": 0.778579374963, "specif": 0.626980167541, "tensorflow": 7.369978720910001, "use": 0.4089122762424, "optim": 7.336883386229999, "out": 0.0584263909193, "model": 5.162150511777001, "optimum": 8.12618403744, "good": 0.837178809814, "alway": 1.452638409144, "slow": 1.39820680715, "concept": 1.954448874206, "cours": 0.765899404133, "competit": 1.12154907401, "huge": 1.47916358195, "count": 1.24748591139, "learnt": 4.04135203208, "field": 2.3042570334040002, "global": 1.1957760371200001, "short": 0.345685625679, "climb": 4.456630328819999, "not": 0.0777620650375, "wish": 1.30224781835, "growth": 1.1476008852200001, "mammalian": 4.33984502064, "techniqu": 2.63248769614, "four": 0.190213538869, "deepmind": 7.369978720910001, "quick": 0.790727508899, "easi": 1.6665296351499999, "adopt": 0.7150533036110001, "jupit": 3.6637506284599994, "solv": 1.9836504770400003, "than": 0.0322608622182, "scienc": 0.841436178891, "they": 0.0594539895352, "design": 1.131717354066, "reach": 0.40414323085000003, "effect": 0.333830227158, "let": 1.2488025672799998, "world": 0.107420248621, "minimum": 1.79668465441, "outcom": 2.01339244624, "larger": 0.806828661778, "github": 7.369978720910001, "node": 3.7920308275, "featur": 2.11693709071, "feed": 2.05136865109, "stuck": 2.94154571342, "algorithm": 13.32176958072, "daili": 1.01874402495, "develop": 0.535874084739, "such": 0.179087933418, "terrain": 12.937497599750001, "sever": 0.13982224079379998, "via": 0.831983625414, "talk": 1.10867789449, "network": 23.8270763263, "confer": 1.0411494784, "deriv": 2.0476323655, "andor": 6.5370695979699995, "control": 0.38498466158600003, "both": 0.050842533389300004, "result": 0.136378908381, "document": 0.932547122383, "subset": 3.3078130570499997, "cluster": 2.52579163445, "repositori": 3.8060957569699996, "rate": 9.893440338158001, "biolog": 3.77618115634, "bless": 2.4106367212, "languag": 0.8306818244059999, "set": 0.171496011289, "two": 0.0136988443582, "from": 0.007371704195258, "anywher": 2.3188414835, "num": 0.005039846326352001, "impli": 1.7070182407700003, "partial": 1.28456856096, "shortterm": 7.369978720910001, "for": 0.0044098655355580005, "mountain": 6.289232070399999, "bottom": 1.8361940533599999, "fulli": 1.02609828678, "with": 0.01077742542051, "forc": 0.561304333048, "new": 0.0177299468511, "are": 0.3830771565751, "solut": 3.10692595254, "minima": 11.687844834819998, "debug": 5.19522699942, "popular": 0.41058020877499996, "highdimension": 7.369978720910001, "tsne": 7.369978720910001, "extract": 4.08323446602, "think": 1.06717661175, "public": 0.20232375048700002, "later": 0.1659308519756, "net": 5.82189927585, "udac": 7.369978720910001, "iter": 3.62283035867, "compar": 0.6239191809269999, "then": 0.16606773046179998, "infer": 3.0511581621399997, "valid": 1.8889232176800002, "train": 4.626428189873001, "great": 0.471610516158, "motiv": 1.61265547932, "goal": 1.18830712273, "convex": 9.28425178502, "type": 1.414202970774, "backprop": 14.739957441820001, "conflat": 4.252028814630001, "reinforc": 3.72929436996, "notebook": 3.693678049, "again": 0.411340231612, "space": 0.874713164972, "paper": 0.979402539665, "prior": 0.778442172521, "where": 0.19497641623710002, "figur": 0.7101721121600001, "canon": 2.38158903576, "what": 0.903549187308, "local": 1.667470960824, "presenc": 1.01075093288, "prone": 2.89833992755, "exponenti": 3.6686767468, "relationship": 0.871847185184, "influenti": 1.50862285915, "main": 0.225571540588, "strive": 3.0891545917400003, "larg": 0.511125181818, "find": 0.547781330288, "largescal": 14.739957441820001, "brain": 4.37863879566, "least": 0.480285584745, "approxim": 0.7944845577770001, "base": 0.40956990686100003, "natur": 1.293919017876, "can": 1.298728771152, "pictur": 1.25144109124, "atari": 4.52506933709, "post": 0.8057001527009999, "here": 2.6551145651100003, "ideal": 1.53809624363, "punctuat": 3.7403186264499997, "sensori": 3.69621290461, "speed": 1.3533338752700002, "stagnat": 3.55007100439, "long": 0.471291587756, "layer": 16.775833298800002, "decaf": 14.739957441820001, "evalu": 1.9388802431299998, "contain": 0.468845318236, "about": 0.18853043242380002, "problem": 1.707422172819, "epoch": 7.293395680160001, "boltzmann": 10.90611221746, "neural": 77.6209879545, "thing": 0.8781935346799999, "context": 1.44920491442, "just": 0.579062868218, "adapt": 3.6023594580600005, "saw": 0.667036036556, "simpli": 0.923941491586, "dure": 0.0491209066894, "defin": 3.01104032775, "expens": 2.53126403348, "serious": 0.949332539075, "recognit": 4.444964798160001, "term": 1.3321559341840001, "across": 1.647595367823, "complex": 2.5507249092929998, "destin": 3.6002173276800002, "start": 0.709330107873, "latter": 0.850831432969, "took": 0.337177952953, "intuit": 3.3216780971900004, "enabl": 1.26473915954, "criteria": 2.4632235573, "them": 0.1883666538186, "standard": 1.27482101964, "avoid": 0.900108441291, "expand": 0.80021683492, "other": 0.00987474791976, "abl": 1.19860796495, "mani": 0.2165787906105, "appli": 2.4950825694359997, "over": 0.1246836074785, "step": 3.11863517094, "perhap": 1.14680739183, "build": 0.982274904182, "wall": 1.12445351985, "pretrain": 7.369978720910001, "journal": 0.8624025456059999, "broad": 1.45323772, "earli": 0.35249888732400003, "facet": 3.90737271112, "same": 0.224119299208, "exact": 1.2437647732500001, "there": 0.160391571702, "explos": 5.7158994514499994, "connect": 4.435235410774, "inform": 0.454453704662, "trap": 2.00915614901, "possibl": 0.348805474891, "conduct": 0.817042965366, "say": 1.124308561104, "contend": 2.2069085037700003, "top": 1.8273019133640003, "origin": 0.128612437587, "illustr": 1.2978562707799999, "loos": 1.9552125417200001, "cambrian": 10.45982511482, "knowledg": 1.2232212893899999, "visual": 6.6157533954400005, "previous": 0.713205920126, "now": 0.149092945021, "might": 1.5366821530680002, "name": 0.09723316638430002, "code": 1.35601909597, "foothil": 7.5127235026, "desir": 2.1983586856799997, "represent": 1.7797382876499999, "basic": 3.01310324685, "play": 0.38110439064199997, "realli": 1.5576408397, "power": 0.8771888481450001, "edg": 2.9889727000999997, "also": 0.0439714734, "simplest": 3.3339697356999998, "around": 0.38775421156400003, "mean": 0.37092128352, "into": 0.0596514529148, "fashion": 1.57940711618, "composit": 3.0650797228799997, "stochast": 9.7045644966, "research": 1.991183454414, "therefor": 1.695183696672, "the": 0.0, "polici": 0.92807570005, "lowest": 1.8793894667099997, "past": 1.4032468315220001, "evolv": 1.52643430388, "challeng": 0.9392919688950001, "machin": 22.27775328992, "want": 1.3832732125099998, "street": 0.8616567778030001, "generic": 2.8881067512700005, "system": 0.327430345585, "sometim": 0.538025155343, "howev": 0.27094535204250003, "share": 1.237520599494, "variabl": 2.1687230672, "win": 1.01265652029, "that": 0.05566607731496, "numst": 0.966901335107, "which": 0.08285462152688, "intellig": 1.43349848213, "activ": 0.762393206568, "thus": 0.49857627139300004, "raw": 2.36536149914, "introduct": 1.02276465794, "implement": 2.54875881814, "procedur": 5.30911986786, "one": 0.025021406582, "abund": 2.25799093255, "begin": 0.285584668268, "see": 0.722764756476, "some": 0.0395735090645, "becaus": 0.27868631765, "imag": 2.98128632187, "river": 6.470923509967, "highlevel": 14.739957441820001, "craze": 4.12348772901, "classspecif": 7.369978720910001, "get": 2.319076023128, "fundament": 5.01966258357, "like": 0.27810715309, "myself": 2.67771382807, "imagin": 1.88684291737, "opensourc": 7.369978720910001, "process": 1.05565839805, "backpropag": 7.369978720910001, "memori": 1.8908677973199999, "yet": 0.754181309241, "recent": 0.434413741288, "cool": 3.8507976947600002, "next": 0.804327370998, "drop": 0.8999535106219999, "rang": 0.579319213803, "time": 0.1009036697634, "finit": 6.67856837152, "engin": 1.809535116552, "chang": 0.332551250116, "similar": 0.637112184228, "smaller": 1.9061661061039998, "project": 1.123203771814, "differ": 0.849284485248, "embed": 2.82349753127, "been": 0.14187589421040003, "most": 0.06224368888679999, "between": 0.10186104349589999, "analyt": 5.696380287719999, "stanford": 2.53369681396, "recurs": 9.027017029380001, "shape": 2.3284191423, "along": 0.260344385917, "midnum": 7.369978720910001, "charact": 0.923148407239, "today": 0.559395353679, "program": 0.7037855787649999, "gradient": 37.3502760882, "almost": 0.42907884333400004, "case": 0.395406268889, "fine": 1.39185273824, "weight": 4.7547705783600005, "architectur": 6.53879031676, "rule": 0.554777423537, "experiment": 1.8035444374, "call": 0.1309255488976, "academ": 1.35895667459, "object": 1.707867169606, "more": 0.08512465799999999, "and": 0.0013227929833356, "discuss": 0.78698452262, "achiev": 0.6270980851169999, "these": 0.357668097004, "benefit": 1.12116245116, "unsupervis": 11.687844834819998, "amount": 2.459696658597, "general": 0.114952578063, "made": 0.1360430521946, "could": 0.5578688168700001, "emerg": 0.748173681534, "befor": 0.0956377718795, "comment": 1.11826753454, "affect": 0.908041904384, "detector": 3.8203613341300007, "patch": 2.81926472072, "introduc": 0.5457137524260001, "comput": 13.6806891594, "make": 0.07349765782289999, "way": 1.1885490596100001, "pit": 7.332526086989999, "onli": 0.025324268329099998, "each": 0.347483378608, "hill": 1.06206244535, "size": 0.9138372060609999, "gradientbas": 14.739957441820001, "industri": 1.4093544835499998, "this": 0.0492238376825, "speech": 1.3409775702700002, "read": 0.83939268088, "advanc": 0.6930212121780001, "cycl": 1.68810108164, "chain": 1.64410864979, "data": 4.8672823392, "reduc": 1.373235550286, "address": 1.05137103247, "method": 4.7223080442050005, "sequenc": 3.6070888748, "repres": 0.38507723275, "convolut": 36.9305440684, "exampl": 0.40868267499899996, "aspect": 1.12795002691, "text": 2.28096401998, "insid": 2.01562611626, "differenti": 8.195687469560001, "direct": 0.200705689496, "import": 0.585636554132, "first": 0.0075872898121599995, "everi": 0.391485427421, "vision": 1.58523088743, "input": 7.50502600617, "have": 0.1774200280944, "mention": 0.931747186336, "year": 0.09480447778920001, "gradual": 1.3321078009899998, "down": 0.613347482372, "learn": 42.13760323725, "when": 0.102774944292, "multilay": 11.56148703158, "hierarch": 6.818331102619999}, "logidf": {"mimimum": 7.369978720910001, "googl": 2.43263122258, "anneal": 5.561689949730001, "vector": 3.25419887797, "corner": 1.77044626763, "form": 0.120053184191, "signal": 1.6340517929299998, "kind": 0.948031302717, "assign": 1.3445959556, "bowl": 2.20405054241, "addit": 0.220218882972, "berkeley": 2.4573238351700004, "number": 0.0966085784186, "follow": 0.045356911094199995, "recogn": 0.935913859031, "risen": 2.8723937456, "decreas": 1.50919249744, "dataset": 5.26584456664, "function": 0.914465741594, "class": 0.7497721899330001, "decay": 2.7778937744700003, "descent": 2.13940500645, "manner": 1.36905901503, "common": 0.338325805271, "valu": 0.823193310148, "fall": 0.527402167952, "whether": 0.791561189647, "tree": 1.41777488775, "complet": 0.215285242047, "path": 1.5351679838499999, "slowli": 1.8050752452, "know": 0.952919694398, "their": 0.015360505122700001, "paradigm": 3.18945646245, "feedforward": 7.2746685411000005, "automat": 1.9150850473199998, "calculus": 4.1313002687400004, "instead": 0.46663315041500003, "explod": 2.52029495787, "express": 0.648191639641, "extend": 0.673191417311, "how": 0.47156695693000006, "paramet": 2.8481901438599997, "present": 0.227546654799, "subfield": 5.4240685718499995, "bunch": 3.70130197411, "jame": 0.658238325853, "increas": 0.277820718929, "special": 0.39755992860100003, "initi": 0.30010459245, "unlabel": 6.89997509166, "end": 0.101476798618, "found": 0.107841124048, "releas": 0.608521699544, "word": 0.585861082385, "interest": 0.47207177798199995, "guarante": 1.8826952548500002, "deep": 1.2886734698, "depend": 0.806969815, "associ": 0.28240501535100004, "recurr": 3.5722448618800002, "unit": 0.143188061817, "aka": 3.03661725822, "updat": 1.7164374626899999, "toronto": 2.1803607712799997, "neuron": 4.16317547727, "below": 0.813626591936, "perform": 0.42618085058, "but": 0.0161923720719, "success": 0.27765441259199997, "were": 0.024291143681099997, "need": 0.362740163442, "all": 0.011402632097799998, "subject": 0.6267443740950001, "final": 0.292733863948, "tune": 2.3434700776599997, "point": 0.23103235903299998, "hidden": 2.0557880052, "decad": 0.760359972282, "has": 0.0427239448548, "generat": 0.719182341736, "arriv": 0.7088915382879999, "stop": 0.778579374963, "specif": 0.626980167541, "tensorflow": 7.369978720910001, "use": 0.0292080197316, "optim": 2.4456277954099996, "out": 0.0584263909193, "model": 0.7374500731110001, "optimum": 4.06309201872, "good": 0.418589404907, "alway": 0.726319204572, "slow": 1.39820680715, "concept": 0.977224437103, "cours": 0.765899404133, "competit": 1.12154907401, "huge": 1.47916358195, "count": 1.24748591139, "learnt": 4.04135203208, "field": 0.5760642583510001, "global": 1.1957760371200001, "short": 0.345685625679, "climb": 2.2283151644099997, "not": 0.0155524130075, "wish": 1.30224781835, "growth": 1.1476008852200001, "mammalian": 4.33984502064, "techniqu": 1.31624384807, "four": 0.190213538869, "deepmind": 7.369978720910001, "quick": 0.790727508899, "easi": 1.6665296351499999, "adopt": 0.7150533036110001, "jupit": 3.6637506284599994, "solv": 1.9836504770400003, "than": 0.0322608622182, "scienc": 0.841436178891, "they": 0.0297269947676, "design": 0.377239118022, "reach": 0.40414323085000003, "effect": 0.333830227158, "let": 1.2488025672799998, "world": 0.107420248621, "minimum": 1.79668465441, "outcom": 2.01339244624, "larger": 0.806828661778, "github": 7.369978720910001, "node": 3.7920308275, "featur": 0.423387418142, "feed": 2.05136865109, "stuck": 2.94154571342, "algorithm": 3.33044239518, "daili": 1.01874402495, "develop": 0.178624694913, "such": 0.059695977806, "terrain": 2.58749951995, "sever": 0.06991112039689999, "via": 0.831983625414, "talk": 1.10867789449, "network": 0.9530830530519999, "confer": 1.0411494784, "deriv": 1.02381618275, "andor": 6.5370695979699995, "control": 0.38498466158600003, "both": 0.050842533389300004, "result": 0.136378908381, "document": 0.932547122383, "subset": 3.3078130570499997, "cluster": 2.52579163445, "repositori": 3.8060957569699996, "rate": 0.761033872166, "biolog": 1.88809057817, "bless": 2.4106367212, "languag": 0.8306818244059999, "set": 0.171496011289, "two": 0.0136988443582, "from": 0.000567054168866, "anywher": 2.3188414835, "num": 0.00031499039539700004, "impli": 1.7070182407700003, "partial": 1.28456856096, "shortterm": 7.369978720910001, "for": 0.00031499039539700004, "mountain": 1.2578464140799999, "bottom": 1.8361940533599999, "fulli": 1.02609828678, "with": 0.00119749171339, "forc": 0.280652166524, "new": 0.0177299468511, "are": 0.0294674735827, "solut": 1.55346297627, "minima": 5.843922417409999, "debug": 5.19522699942, "popular": 0.41058020877499996, "highdimension": 7.369978720910001, "tsne": 7.369978720910001, "extract": 2.04161723301, "think": 1.06717661175, "public": 0.20232375048700002, "later": 0.0829654259878, "net": 1.9406330919499999, "udac": 7.369978720910001, "iter": 3.62283035867, "compar": 0.6239191809269999, "then": 0.08303386523089999, "infer": 3.0511581621399997, "valid": 1.8889232176800002, "train": 0.660918312839, "great": 0.235805258079, "motiv": 1.61265547932, "goal": 1.18830712273, "convex": 4.64212589251, "type": 0.707101485387, "backprop": 7.369978720910001, "conflat": 4.252028814630001, "reinforc": 1.86464718498, "notebook": 3.693678049, "again": 0.411340231612, "space": 0.874713164972, "paper": 0.979402539665, "prior": 0.778442172521, "where": 0.0649921387457, "figur": 0.7101721121600001, "canon": 2.38158903576, "what": 0.225887296827, "local": 0.416867740206, "presenc": 1.01075093288, "prone": 2.89833992755, "exponenti": 3.6686767468, "relationship": 0.871847185184, "influenti": 1.50862285915, "main": 0.225571540588, "strive": 3.0891545917400003, "larg": 0.17037506060600002, "find": 0.547781330288, "largescal": 7.369978720910001, "brain": 2.18931939783, "least": 0.480285584745, "approxim": 0.7944845577770001, "base": 0.13652330228700002, "natur": 0.431306339292, "can": 0.162341096394, "pictur": 1.25144109124, "atari": 4.52506933709, "post": 0.8057001527009999, "here": 0.8850381883700001, "ideal": 1.53809624363, "punctuat": 3.7403186264499997, "sensori": 3.69621290461, "speed": 1.3533338752700002, "stagnat": 3.55007100439, "long": 0.235645793878, "layer": 2.0969791623500003, "decaf": 7.369978720910001, "evalu": 1.9388802431299998, "contain": 0.468845318236, "about": 0.0628434774746, "problem": 0.569140724273, "epoch": 3.6466978400800003, "boltzmann": 5.45305610873, "neural": 4.0853151555, "thing": 0.8781935346799999, "context": 1.44920491442, "just": 0.289531434109, "adapt": 1.2007864860200002, "saw": 0.667036036556, "simpli": 0.923941491586, "dure": 0.0491209066894, "defin": 1.00368010925, "expens": 1.26563201674, "serious": 0.949332539075, "recognit": 1.4816549327200002, "term": 0.33303898354600003, "across": 0.549198455941, "complex": 0.8502416364309999, "destin": 1.8001086638400001, "start": 0.236443369291, "latter": 0.850831432969, "took": 0.337177952953, "intuit": 3.3216780971900004, "enabl": 1.26473915954, "criteria": 2.4632235573, "them": 0.0941833269093, "standard": 0.63741050982, "avoid": 0.900108441291, "expand": 0.80021683492, "other": 0.00987474791976, "abl": 0.599303982475, "mani": 0.0433157581221, "appli": 0.8316941898119999, "over": 0.0249367214957, "step": 1.03954505698, "perhap": 1.14680739183, "build": 0.491137452091, "wall": 1.12445351985, "pretrain": 7.369978720910001, "journal": 0.8624025456059999, "broad": 1.45323772, "earli": 0.117499629108, "facet": 3.90737271112, "same": 0.112059649604, "exact": 1.2437647732500001, "there": 0.0400978929255, "explos": 1.90529981715, "connect": 0.633605058682, "inform": 0.454453704662, "trap": 2.00915614901, "possibl": 0.348805474891, "conduct": 0.817042965366, "say": 0.562154280552, "contend": 2.2069085037700003, "top": 0.609100637788, "origin": 0.128612437587, "illustr": 1.2978562707799999, "loos": 1.9552125417200001, "cambrian": 5.22991255741, "knowledg": 1.2232212893899999, "visual": 1.6539383488600001, "previous": 0.356602960063, "now": 0.149092945021, "might": 0.7683410765340001, "name": 0.09723316638430002, "code": 1.35601909597, "foothil": 3.7563617513, "desir": 1.0991793428399999, "represent": 1.7797382876499999, "basic": 1.00436774895, "play": 0.38110439064199997, "realli": 1.5576408397, "power": 0.292396282715, "edg": 1.4944863500499999, "also": 0.0146571578, "simplest": 3.3339697356999998, "around": 0.19387710578200001, "mean": 0.37092128352, "into": 0.0149128632287, "fashion": 1.57940711618, "composit": 1.5325398614399999, "stochast": 4.8522822483, "research": 0.663727818138, "therefor": 0.847591848336, "the": 0.0, "polici": 0.92807570005, "lowest": 1.8793894667099997, "past": 0.7016234157610001, "evolv": 1.52643430388, "challeng": 0.9392919688950001, "machin": 1.39235958062, "want": 0.6916366062549999, "street": 0.8616567778030001, "generic": 2.8881067512700005, "system": 0.327430345585, "sometim": 0.538025155343, "howev": 0.0903151173475, "share": 0.618760299747, "variabl": 2.1687230672, "win": 1.01265652029, "that": 0.00397614837964, "numst": 0.966901335107, "which": 0.00517841384543, "intellig": 1.43349848213, "activ": 0.381196603284, "thus": 0.49857627139300004, "raw": 2.36536149914, "introduct": 1.02276465794, "implement": 1.27437940907, "procedur": 1.76970662262, "one": 0.0062553516455, "abund": 2.25799093255, "begin": 0.285584668268, "see": 0.240921585492, "some": 0.0395735090645, "becaus": 0.139343158825, "imag": 0.99376210729, "river": 0.924417644281, "highlevel": 7.369978720910001, "craze": 4.12348772901, "classspecif": 7.369978720910001, "get": 0.579769005782, "fundament": 1.67322086119, "like": 0.139053576545, "myself": 2.67771382807, "imagin": 1.88684291737, "opensourc": 7.369978720910001, "process": 0.527829199025, "backpropag": 7.369978720910001, "memori": 0.9454338986599999, "yet": 0.754181309241, "recent": 0.434413741288, "cool": 1.9253988473800001, "next": 0.402163685499, "drop": 0.8999535106219999, "rang": 0.579319213803, "time": 0.0112115188626, "finit": 3.33928418576, "engin": 0.904767558276, "chang": 0.166275625058, "similar": 0.318556092114, "smaller": 0.9530830530519999, "project": 0.561601885907, "differ": 0.212321121312, "embed": 2.82349753127, "been": 0.023645982368400004, "most": 0.020747896295599998, "between": 0.033953681165299995, "analyt": 2.8481901438599997, "stanford": 2.53369681396, "recurs": 4.513508514690001, "shape": 1.16420957115, "along": 0.260344385917, "midnum": 7.369978720910001, "charact": 0.923148407239, "today": 0.559395353679, "program": 0.7037855787649999, "gradient": 3.73502760882, "almost": 0.42907884333400004, "case": 0.395406268889, "fine": 1.39185273824, "weight": 1.58492352612, "architectur": 1.63469757919, "rule": 0.554777423537, "experiment": 1.8035444374, "call": 0.0654627744488, "academ": 1.35895667459, "object": 0.853933584803, "more": 0.017024931599999998, "and": 6.29901420636e-05, "discuss": 0.78698452262, "achiev": 0.6270980851169999, "these": 0.0715336194008, "benefit": 1.12116245116, "unsupervis": 5.843922417409999, "amount": 0.819898886199, "general": 0.114952578063, "made": 0.0680215260973, "could": 0.18595627229000003, "emerg": 0.748173681534, "befor": 0.0956377718795, "comment": 1.11826753454, "affect": 0.908041904384, "detector": 3.8203613341300007, "patch": 2.81926472072, "introduc": 0.5457137524260001, "comput": 1.36806891594, "make": 0.07349765782289999, "way": 0.19809150993500002, "pit": 2.4441753623299998, "onli": 0.025324268329099998, "each": 0.173741689304, "hill": 1.06206244535, "size": 0.9138372060609999, "gradientbas": 7.369978720910001, "industri": 0.7046772417749999, "this": 0.0037864490525, "speech": 1.3409775702700002, "read": 0.83939268088, "advanc": 0.6930212121780001, "cycl": 1.68810108164, "chain": 1.64410864979, "data": 1.2168205848, "reduc": 0.686617775143, "address": 1.05137103247, "method": 0.944461608841, "sequenc": 1.8035444374, "repres": 0.38507723275, "convolut": 4.61631800855, "exampl": 0.40868267499899996, "aspect": 1.12795002691, "text": 1.14048200999, "insid": 1.00781305813, "differenti": 2.0489218673900003, "direct": 0.200705689496, "import": 0.292818277066, "first": 0.0075872898121599995, "everi": 0.391485427421, "vision": 1.58523088743, "input": 2.50167533539, "have": 0.0147850023412, "mention": 0.931747186336, "year": 0.047402238894600005, "gradual": 1.3321078009899998, "down": 0.306673741186, "learn": 0.842752064745, "when": 0.0205549888584, "multilay": 5.78074351579, "hierarch": 3.4091655513099997}, "freq": {"mimimum": 1, "googl": 1, "anneal": 1, "vector": 1, "corner": 1, "form": 4, "signal": 1, "kind": 2, "assign": 1, "bowl": 1, "addit": 1, "berkeley": 1, "number": 3, "follow": 3, "recogn": 3, "risen": 1, "decreas": 3, "dataset": 1, "function": 10, "class": 1, "decay": 2, "descent": 7, "manner": 1, "common": 2, "valu": 2, "fall": 1, "whether": 1, "tree": 1, "complet": 1, "path": 3, "slowli": 1, "know": 1, "their": 1, "paradigm": 1, "feedforward": 1, "automat": 1, "calculus": 1, "instead": 1, "explod": 1, "express": 1, "extend": 2, "how": 1, "paramet": 3, "present": 2, "subfield": 1, "bunch": 1, "jame": 1, "increas": 1, "special": 2, "initi": 3, "unlabel": 1, "end": 1, "found": 2, "releas": 1, "word": 1, "interest": 4, "guarante": 1, "deep": 19, "depend": 3, "associ": 1, "recurr": 5, "unit": 1, "aka": 1, "updat": 1, "toronto": 1, "neuron": 7, "below": 2, "perform": 1, "but": 3, "success": 1, "were": 2, "need": 1, "all": 5, "subject": 1, "final": 2, "tune": 1, "point": 3, "hidden": 1, "decad": 3, "has": 13, "generat": 1, "arriv": 2, "stop": 1, "specif": 1, "tensorflow": 1, "use": 14, "optim": 3, "out": 1, "model": 7, "optimum": 2, "good": 2, "alway": 2, "slow": 1, "concept": 2, "cours": 1, "competit": 1, "huge": 1, "count": 1, "learnt": 1, "field": 4, "global": 1, "short": 1, "climb": 2, "not": 5, "wish": 1, "growth": 1, "mammalian": 1, "techniqu": 2, "four": 1, "deepmind": 1, "quick": 1, "easi": 1, "adopt": 1, "jupit": 1, "solv": 1, "than": 1, "scienc": 1, "they": 2, "design": 3, "reach": 1, "effect": 1, "let": 1, "world": 1, "minimum": 1, "outcom": 1, "larger": 1, "github": 1, "node": 1, "featur": 5, "feed": 1, "stuck": 1, "algorithm": 4, "daili": 1, "develop": 3, "such": 3, "terrain": 5, "sever": 2, "via": 1, "talk": 1, "network": 25, "confer": 1, "deriv": 2, "andor": 1, "control": 1, "both": 1, "result": 1, "document": 1, "subset": 1, "cluster": 1, "repositori": 1, "rate": 13, "biolog": 2, "bless": 1, "languag": 1, "set": 1, "two": 1, "from": 13, "anywher": 1, "num": 16, "impli": 1, "partial": 1, "shortterm": 1, "for": 14, "mountain": 5, "bottom": 1, "fulli": 1, "with": 9, "forc": 2, "new": 1, "are": 13, "solut": 2, "minima": 2, "debug": 1, "popular": 1, "highdimension": 1, "tsne": 1, "extract": 2, "think": 1, "public": 1, "later": 2, "net": 3, "udac": 1, "iter": 1, "compar": 1, "then": 2, "infer": 1, "valid": 1, "train": 7, "great": 2, "motiv": 1, "goal": 1, "convex": 2, "type": 2, "backprop": 2, "conflat": 1, "reinforc": 2, "notebook": 1, "again": 1, "space": 1, "paper": 1, "prior": 1, "where": 3, "figur": 1, "canon": 1, "what": 4, "local": 4, "presenc": 1, "prone": 1, "exponenti": 1, "relationship": 1, "influenti": 1, "main": 1, "strive": 1, "larg": 3, "find": 1, "largescal": 2, "brain": 2, "least": 1, "approxim": 1, "base": 3, "natur": 3, "can": 8, "pictur": 1, "atari": 1, "post": 1, "here": 3, "ideal": 1, "punctuat": 1, "sensori": 1, "speed": 1, "stagnat": 1, "long": 2, "layer": 8, "decaf": 2, "evalu": 1, "contain": 1, "about": 3, "problem": 3, "epoch": 2, "boltzmann": 2, "neural": 19, "thing": 1, "context": 1, "just": 2, "adapt": 3, "saw": 1, "simpli": 1, "dure": 1, "defin": 3, "expens": 2, "serious": 1, "recognit": 3, "term": 4, "across": 3, "complex": 3, "destin": 2, "start": 3, "latter": 1, "took": 1, "intuit": 1, "enabl": 1, "criteria": 1, "them": 2, "standard": 2, "avoid": 1, "expand": 1, "other": 1, "abl": 2, "mani": 5, "appli": 3, "over": 5, "step": 3, "perhap": 1, "build": 2, "wall": 1, "pretrain": 1, "journal": 1, "broad": 1, "earli": 3, "facet": 1, "same": 2, "exact": 1, "there": 4, "explos": 3, "connect": 7, "inform": 1, "trap": 1, "possibl": 1, "conduct": 1, "say": 2, "contend": 1, "top": 3, "origin": 1, "illustr": 1, "loos": 1, "cambrian": 2, "knowledg": 1, "visual": 4, "previous": 2, "now": 1, "might": 2, "name": 1, "code": 1, "foothil": 2, "desir": 2, "represent": 1, "basic": 3, "play": 1, "realli": 1, "power": 3, "edg": 2, "also": 3, "simplest": 1, "around": 2, "mean": 1, "into": 4, "fashion": 1, "composit": 2, "stochast": 2, "research": 3, "therefor": 2, "the": 107, "polici": 1, "lowest": 1, "past": 2, "evolv": 1, "challeng": 1, "machin": 16, "want": 2, "street": 1, "generic": 1, "system": 1, "sometim": 1, "howev": 3, "share": 2, "variabl": 1, "win": 1, "that": 14, "numst": 1, "which": 16, "intellig": 1, "activ": 2, "thus": 1, "raw": 1, "introduct": 1, "implement": 2, "procedur": 3, "one": 4, "abund": 1, "begin": 1, "see": 3, "some": 1, "becaus": 2, "imag": 3, "river": 7, "highlevel": 2, "craze": 1, "classspecif": 1, "get": 4, "fundament": 3, "like": 2, "myself": 1, "imagin": 1, "opensourc": 1, "process": 2, "backpropag": 1, "memori": 2, "yet": 1, "recent": 1, "cool": 2, "next": 2, "drop": 1, "rang": 1, "time": 9, "finit": 2, "engin": 2, "chang": 2, "similar": 2, "smaller": 2, "project": 2, "differ": 4, "embed": 1, "been": 6, "most": 3, "between": 3, "analyt": 2, "stanford": 1, "recurs": 2, "shape": 2, "along": 1, "midnum": 1, "charact": 1, "today": 1, "program": 1, "gradient": 10, "almost": 1, "case": 1, "fine": 1, "weight": 3, "architectur": 4, "rule": 1, "experiment": 1, "call": 2, "academ": 1, "object": 2, "more": 5, "and": 21, "discuss": 1, "achiev": 1, "these": 5, "benefit": 1, "unsupervis": 2, "amount": 3, "general": 1, "made": 2, "could": 3, "emerg": 1, "befor": 1, "comment": 1, "affect": 1, "detector": 1, "patch": 1, "introduc": 1, "comput": 10, "make": 1, "way": 6, "pit": 3, "onli": 1, "each": 2, "hill": 1, "size": 1, "gradientbas": 2, "industri": 2, "this": 13, "speech": 1, "read": 1, "advanc": 1, "cycl": 1, "chain": 1, "data": 4, "reduc": 2, "address": 1, "method": 5, "sequenc": 2, "repres": 1, "convolut": 8, "exampl": 1, "aspect": 1, "text": 2, "insid": 2, "differenti": 4, "direct": 1, "import": 2, "first": 1, "everi": 1, "vision": 1, "input": 3, "have": 12, "mention": 1, "year": 2, "gradual": 1, "down": 2, "learn": 50, "when": 5, "multilay": 2, "hierarch": 2}, "idf": {"mimimum": 1587.6, "googl": 11.388809182200001, "anneal": 260.262295082, "vector": 25.898858075, "corner": 5.873473917869999, "form": 1.12755681818, "signal": 5.12459651388, "kind": 2.5806241872599998, "assign": 3.83663605607, "bowl": 9.06164383562, "addit": 1.24634950542, "berkeley": 11.6735294118, "number": 1.10142916609, "follow": 1.04640126549, "recogn": 2.54954231572, "risen": 17.6792873051, "decreas": 4.5230769230800005, "dataset": 193.609756098, "function": 2.495441685, "class": 2.11651779763, "decay": 16.085106383, "descent": 8.494382022469999, "manner": 3.93164933135, "common": 1.4025974025999999, "valu": 2.2777618364400003, "fall": 1.6945244956799999, "whether": 2.20683903253, "tree": 4.127925117, "complet": 1.24021560816, "path": 4.6421052631599995, "slowli": 6.080428954419999, "know": 2.59327017315, "their": 1.01547908405, "paradigm": 24.2752293578, "feedforward": 1443.27272727, "automat": 6.787516032490001, "calculus": 62.2588235294, "instead": 1.59461631177, "explod": 12.432263116700002, "express": 1.9120799710900003, "extend": 1.9604840701400004, "how": 1.60250328051, "paramet": 17.256521739100002, "present": 1.25551601423, "subfield": 226.8, "bunch": 40.5, "jame": 1.9313868613099998, "increas": 1.32024948025, "special": 1.4881889763799998, "initi": 1.35, "unlabel": 992.25, "end": 1.10680423871, "found": 1.11387076405, "releas": 1.8377126982299998, "word": 1.7965372864099998, "interest": 1.60331246213, "guarante": 6.57119205298, "deep": 3.6279707495399998, "depend": 2.2411067193700003, "associ": 1.3263157894700002, "recurr": 35.5964125561, "unit": 1.15394679459, "aka": 20.834645669300002, "updat": 5.56466876972, "toronto": 8.849498327760001, "neuron": 64.2753036437, "below": 2.25607503197, "perform": 1.5313977042500002, "but": 1.01632417899, "success": 1.32002993265, "were": 1.02458857696, "need": 1.4372623574099999, "all": 1.01146788991, "subject": 1.8715077213299998, "final": 1.34008609775, "tune": 10.4173228346, "point": 1.25990000794, "hidden": 7.81299212598, "decad": 2.1390460792200003, "has": 1.0436497502, "generat": 2.05275407292, "arriv": 2.03173790632, "stop": 2.1783754116400003, "specif": 1.8719490626099997, "tensorflow": 1587.6, "use": 1.0296387573799999, "optim": 11.5377906977, "out": 1.06016694491, "model": 2.0905978404, "optimum": 58.1538461538, "good": 1.51981619759, "alway": 2.06745670009, "slow": 4.04793472718, "concept": 2.65707112971, "cours": 2.15092805853, "competit": 3.06960556845, "huge": 4.38927287808, "count": 3.48157894737, "learnt": 56.9032258065, "field": 1.7790228597, "global": 3.30612244898, "short": 1.41295834817, "climb": 9.284210526319999, "not": 1.01567398119, "wish": 3.67755385685, "growth": 3.15062512403, "mammalian": 76.6956521739, "techniqu": 3.7293868921800004, "four": 1.20950784702, "deepmind": 1587.6, "quick": 2.205, "easi": 5.2937645882, "adopt": 2.0442956477000003, "jupit": 39.007371007399996, "solv": 7.26923076923, "than": 1.03278688525, "scienc": 2.31969608416, "they": 1.03017325287, "design": 1.45825296225, "reach": 1.49801849406, "effect": 1.3963060686000002, "let": 3.48616600791, "world": 1.11340206186, "minimum": 6.02962400304, "outcom": 7.48867924528, "larger": 2.2407904022599996, "github": 1587.6, "node": 44.3463687151, "featur": 1.52712581762, "feed": 7.77853993141, "stuck": 18.945107398599998, "algorithm": 27.9507042254, "daili": 2.76971388695, "develop": 1.1955719557200002, "such": 1.06151377374, "terrain": 13.2964824121, "sever": 1.07241286139, "via": 2.2978723404299997, "talk": 3.0303493033, "network": 2.59369384088, "confer": 2.8324710080299997, "deriv": 2.78379800105, "andor": 690.260869565, "control": 1.46959178006, "both": 1.05215720061, "result": 1.14611608432, "document": 2.5409731114, "subset": 27.3253012048, "cluster": 12.5007874016, "repositori": 44.974504249300004, "rate": 2.14048806795, "biolog": 6.60674157303, "bless": 11.141052631600001, "languag": 2.29488291414, "set": 1.18707940781, "two": 1.01379310345, "from": 1.00056721497, "anywher": 10.1638924456, "num": 1.00031504001, "impli": 5.5125, "partial": 3.6131087847099996, "shortterm": 1587.6, "for": 1.00031504001, "mountain": 3.51783735874, "bottom": 6.27261951798, "fulli": 2.79015817223, "with": 1.0011982089899998, "forc": 1.32399299475, "new": 1.0178880554, "are": 1.02990593578, "solut": 4.7278141751, "minima": 345.13043478300006, "debug": 180.409090909, "popular": 1.50769230769, "highdimension": 1587.6, "tsne": 1587.6, "extract": 7.703056768560001, "think": 2.90715986083, "public": 1.22424429365, "later": 1.08650424309, "net": 6.96315789474, "udac": 1587.6, "iter": 37.4433962264, "compar": 1.8662278123900002, "then": 1.08657860516, "infer": 21.1398135819, "valid": 6.61224489796, "train": 1.9365698950999999, "great": 1.26592775696, "motiv": 5.01611374408, "goal": 3.28152128979, "convex": 103.764705882, "type": 2.0281042411900003, "backprop": 1587.6, "conflat": 70.2477876106, "reinforc": 6.453658536590001, "notebook": 40.1924050633, "again": 1.50883862384, "space": 2.39818731118, "paper": 2.6628648104700003, "prior": 2.17807655371, "where": 1.06715063521, "figur": 2.0343413634, "canon": 10.8220858896, "what": 1.25343439128, "local": 1.51720183486, "presenc": 2.7476635514, "prone": 18.144000000000002, "exponenti": 39.2, "relationship": 2.39132399458, "influenti": 4.520501138949999, "main": 1.25303867403, "strive": 21.9585062241, "larg": 1.18574949585, "find": 1.7294117647099998, "largescal": 1587.6, "brain": 8.929133858270001, "least": 1.6165359943000002, "approxim": 2.2132998745299997, "base": 1.14628158845, "natur": 1.5392670157100001, "can": 1.17626139142, "pictur": 3.4953764861300005, "atari": 92.3023255814, "post": 2.23826307627, "here": 2.42307692308, "ideal": 4.65571847507, "punctuat": 42.1114058355, "sensori": 40.2944162437, "speed": 3.8703071672400005, "stagnat": 34.8157894737, "long": 1.2657259028899999, "layer": 8.14153846154, "decaf": 1587.6, "evalu": 6.9509632224199995, "contain": 1.59814777532, "about": 1.06486015159, "problem": 1.76674827509, "epoch": 38.347826087, "boltzmann": 233.470588235, "neural": 59.4606741573, "thing": 2.4065484311099996, "context": 4.25972632144, "just": 1.33580143037, "adapt": 3.32272917539, "saw": 1.94845360825, "simpli": 2.5192002538900002, "dure": 1.0503473370799998, "defin": 2.72830383227, "expens": 3.5453327378300004, "serious": 2.583984375, "recognit": 4.40022172949, "term": 1.39520168732, "across": 1.7318642958400001, "complex": 2.34021226415, "destin": 6.0503048780499995, "start": 1.26673581744, "latter": 2.34159292035, "took": 1.4009883515700001, "intuit": 27.7068062827, "enabl": 3.5421686747, "criteria": 11.7426035503, "them": 1.09876115994, "standard": 1.8915763135900003, "avoid": 2.45986984816, "expand": 2.2260235558000003, "other": 1.00992366412, "abl": 1.8208510150200001, "mani": 1.04426757877, "appli": 2.2972073506, "over": 1.02525024217, "step": 2.8279301745599996, "perhap": 3.14812611541, "build": 1.6341739578, "wall": 3.07853403141, "pretrain": 1587.6, "journal": 2.36884512086, "broad": 4.27693965517, "earli": 1.12468121281, "facet": 49.7680250784, "same": 1.11857958148, "exact": 3.46864758575, "there": 1.04091266719, "explos": 6.72142252329, "connect": 1.8843916913900003, "inform": 1.5753125620200001, "trap": 7.4570220760899995, "possibl": 1.4173734488, "conduct": 2.2637958077900002, "say": 1.7544480053, "contend": 9.08757870635, "top": 1.8387769284200002, "origin": 1.13724928367, "illustr": 3.6614391143900002, "loos": 7.065420560750001, "cambrian": 186.776470588, "knowledg": 3.3981164383599998, "visual": 5.22752716497, "previous": 1.42846859816, "now": 1.160780873, "might": 2.1561863370900003, "name": 1.10211732037, "code": 3.8807137619199996, "foothil": 42.7924528302, "desir": 3.00170164492, "represent": 5.928304705, "basic": 2.7301805675, "play": 1.46390041494, "realli": 4.7476076555, "power": 1.3396337861799998, "edg": 4.45704660303, "also": 1.01476510067, "simplest": 28.0494699647, "around": 1.21394708671, "mean": 1.44906900329, "into": 1.01502461479, "fashion": 4.85207823961, "composit": 4.629921259840001, "stochast": 128.032258065, "research": 1.9420183486200002, "therefor": 2.33401940606, "the": 1.0, "polici": 2.52963671128, "lowest": 6.549504950499999, "past": 2.01702452039, "evolv": 4.60173913043, "challeng": 2.55816951337, "machin": 4.02433460076, "want": 1.99698113208, "street": 2.36707917102, "generic": 17.9592760181, "system": 1.38739840951, "sometim": 1.7126213592200001, "howev": 1.0945191313299998, "share": 1.8566249561500001, "variabl": 8.747107438019999, "win": 2.75290445639, "that": 1.00398406375, "numst": 2.6297830047999997, "which": 1.005191845, "intellig": 4.19334389857, "activ": 1.46403541129, "thus": 1.6463756092500001, "raw": 10.6478873239, "introduct": 2.7808723068799996, "implement": 3.57648118946, "procedur": 5.8691312384500005, "one": 1.00627495722, "abund": 9.563855421689999, "begin": 1.3305397251100002, "see": 1.27242125511, "some": 1.04036697248, "becaus": 1.1495184997499999, "imag": 2.70137825421, "river": 2.5204000635, "highlevel": 1587.6, "craze": 61.7743190661, "classspecif": 1587.6, "get": 1.78562591385, "fundament": 5.32930513595, "like": 1.14918566775, "myself": 14.5517873511, "imagin": 6.598503740650001, "opensourc": 1587.6, "process": 1.69524826482, "backpropag": 1587.6, "memori": 2.57392996109, "yet": 2.1258703802900003, "recent": 1.54405757635, "cool": 6.8578833693300005, "next": 1.4950560316400001, "drop": 2.4594887684, "rang": 1.7848229342299997, "time": 1.01127460348, "finit": 28.1989342806, "engin": 2.47135740971, "chang": 1.1808985421, "similar": 1.37514075357, "smaller": 2.59369384088, "project": 1.7534791252500002, "differ": 1.23654490225, "embed": 16.835630965, "been": 1.0239277652399998, "most": 1.02096463023, "between": 1.03453668708, "analyt": 17.256521739100002, "stanford": 12.6, "recurs": 91.2413793103, "shape": 3.20338983051, "along": 1.2973768080399999, "midnum": 1587.6, "charact": 2.51720310766, "today": 1.74961428257, "program": 2.02139037433, "gradient": 41.889182058, "almost": 1.53584212054, "case": 1.48498737256, "fine": 4.02229541424, "weight": 4.878918254459999, "architectur": 5.12790697674, "rule": 1.7415533128599998, "experiment": 6.07112810707, "call": 1.0676529926, "academ": 3.8921304241199994, "object": 2.3488681757700003, "more": 1.0171706817, "and": 1.00006299213, "discuss": 2.19676214197, "achiev": 1.87216981132, "these": 1.07415426252, "benefit": 3.06841901817, "unsupervis": 345.13043478300006, "amount": 2.27027027027, "general": 1.1218202374200001, "made": 1.07038834951, "could": 1.2043695949, "emerg": 2.1131372288, "befor": 1.10036041031, "comment": 3.05954904606, "affect": 2.4794627518400003, "detector": 45.6206896552, "patch": 16.764519535399998, "introduc": 1.7258397651900002, "comput": 3.9277585353800006, "make": 1.0762660158600001, "way": 1.2190739461, "pit": 11.5210449927, "onli": 1.0256476516600002, "each": 1.18974820144, "hill": 2.89233011478, "size": 2.49387370405, "gradientbas": 1587.6, "industri": 2.02319357716, "this": 1.00379362671, "speech": 3.8227787141800005, "read": 2.3149606299200003, "advanc": 1.9997480791, "cycl": 5.40919931857, "chain": 5.17639387023, "data": 3.37643555934, "reduc": 1.98698372966, "address": 2.86157173756, "method": 2.5714285714300003, "sequenc": 6.07112810707, "repres": 1.46972782818, "convolut": 101.121019108, "exampl": 1.50483412322, "aspect": 3.0893169877399997, "text": 3.12827586207, "insid": 2.7396031061299997, "differenti": 7.759530791789999, "direct": 1.22226499346, "import": 1.3401992233700002, "first": 1.00761614623, "everi": 1.47917637194, "vision": 4.88041807562, "input": 12.2029208301, "have": 1.0148948411399998, "mention": 2.53894130817, "year": 1.0485436893200002, "gradual": 3.7890214797099997, "down": 1.35889754344, "learn": 2.32275054865, "when": 1.02076769755, "multilay": 324.0, "hierarch": 30.24}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  The 10 Deep Learning Methods AI Practitioners Need to Apply</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb The 10 Deep Learning Methods AI Practitioners Need to Apply Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/12/meaningcloud-voice-customer-simplified-webinar.html\" rel=\"prev\" title=\"The Fastest Way to Benefit from Text Analytics, Dec 20 Webinar\"/>\n<link href=\"https://www.kdnuggets.com/2017/12/improve-machine-learning-performance-lessons-andrew-ng.html\" rel=\"next\" title=\"How to Improve Machine Learning Performance? Lessons from Andrew Ng\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=75579\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-75579 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 13-Dec, 2017  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/12/index.html\">Dec</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/12/tutorials.html\">Tutorials, Overviews</a> \u00bb The 10 Deep Learning Methods AI Practitioners Need to Apply (\u00a0<a href=\"/2017/n48.html\">17:n48</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">The 10 Deep Learning Methods AI Practitioners Need to Apply</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/12/meaningcloud-voice-customer-simplified-webinar.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/12/improve-machine-learning-performance-lessons-andrew-ng.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/backpropagation\" rel=\"tag\">Backpropagation</a>, <a href=\"https://www.kdnuggets.com/tag/convolutional-neural-networks\" rel=\"tag\">Convolutional Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/dropout\" rel=\"tag\">Dropout</a>, <a href=\"https://www.kdnuggets.com/tag/gradient-descent\" rel=\"tag\">Gradient Descent</a>, <a href=\"https://www.kdnuggets.com/tag/lstm\" rel=\"tag\">LSTM</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/transfer-learning\" rel=\"tag\">Transfer Learning</a></div>\n<br/>\n<p class=\"excerpt\">\n     Deep learning emerged from that decade\u2019s explosive computational growth as a serious contender in the field, winning many important machine learning competitions. The interest has not cooled as of 2017; today, we see deep learning mentioned in every corner of machine learning.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/james-le\" rel=\"author\" title=\"Posts by James Le\">James Le</a>, Machine Learning Engineer.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html?page=2#comments\">comments</a></div>\n<p><img alt=\"Header image\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/2000/1*CPI-6ZtpYfMyV3bTt8EumQ.jpeg\" width=\"99%\"/></p>\n<p>Interest in\u00a0<strong>machine learning</strong>\u00a0has exploded over the past decade. You see machine learning in computer science programs, industry conferences, and the Wall Street Journal almost daily. For all the talk about machine learning, many conflate what it can do with what they wish it could do. Fundamentally, machine learning is using algorithms to extract information from raw data and represent it in some type of model. We use this model to infer things about other data we have not yet modeled.</p>\n<p><strong>Neural networks</strong>\u00a0are one type of model for machine learning; they have been around for at least 50 years. The fundamental unit of a neural network is a node, which is loosely based on the biological neuron in the mammalian brain. The connections between neurons are also modeled on biological brains, as is the way these connections develop over time (with \u201ctraining\u201d).</p>\n<p>In the mid-1980s and early 1990s, many important architectural advancements were made in neural networks. However, the amount of time and data needed to get good results slowed adoption, and thus interest cooled. In the early 2000s, computational power expanded exponentially and the industry saw a \u201cCambrian explosion\u201d of computational techniques that were not possible prior to this.\u00a0<strong>Deep learning</strong>\u00a0emerged from that decade\u2019s explosive computational growth as a serious contender in the field, winning many important machine learning competitions. The interest has not cooled as of 2017; today, we see deep learning mentioned in every corner of machine learning.</p>\n<p>To get myself into the craze, I took\u00a0<a href=\"https://www.udacity.com/course/deep-learning--ud730\" target='_blank rel=\"noopener noreferrer\"'><strong>Udacity\u2019s \u201cDeep Learning\u201d course</strong></a>, which is a great introduction to the motivation of deep learning and the design of intelligent systems that learn from complex and/or large-scale datasets in\u00a0<strong>TensorFlow</strong>. For the class projects, I used and developed neural networks for image recognition with convolutions, natural language processing with embeddings and character based text generation with Recurrent Neural Network / Long Short-Term Memory. All the code in Jupiter Notebook can be found on\u00a0<a href=\"https://github.com/khanhnamle1994/deep-learning\" target='_blank rel=\"noopener noreferrer\"'>this GitHub repository</a>.</p>\n<p>Here is an outcome of one of the assignments, a t-SNE projection of word vectors, clustered by similarity.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*5gUqFLOyQrB4kgjAicn0og.png\" width=\"99%\"/></p>\n<p>Most recently, I have started reading academic papers on the subject. From my research, here are several publications that have been hugely influential to the development of the field:</p>\n<ul>\n<li>NYU\u2019s\u00a0<a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>Gradient-Based Learning Applied to Document Recognition</strong></a> (1998), which introduces Convolutional Neural Network to the Machine Learning world.\n<li>Toronto\u2019s\u00a0<a href=\"http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>Deep Boltzmann Machines</strong></a>\u00a0(2009), which presents a new learning algorithm for Boltzmann machines that contain many layers of hidden variables.\n<li>Stanford &amp; Google\u2019s\u00a0<a href=\"http://icml.cc/2012/papers/73.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>Building High-Level Features Using Large-Scale Unsupervised Learning</strong></a>\u00a0(2012), which addresses the problem of building high-level, class-specific feature detectors from only unlabeled data.\n<li>Berkeley\u2019s\u00a0<a href=\"http://proceedings.mlr.press/v32/donahue14.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>DeCAF\u200a\u2014\u200aA Deep Convolutional Activation Feature for Generic Visual Recognition</strong></a>\u00a0(2013), which releases DeCAF, an open-source implementation of the deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.\n<li>DeepMind\u2019s\u00a0<a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>Playing Atari with Deep Reinforcement Learning</strong></a>\u00a0(2016), which presents the 1st deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.\n</li></li></li></li></li></ul>\n<p>There is an abundant amount of great knowledge about deep learning I have learnt via research and learning. Here I want to share the\u00a0<strong>10 powerful deep learning methods</strong>\u00a0AI engineers can apply to their machine learning problems. But first of all, let\u2019s define what deep learning is. Deep learning has been a challenge to define for many because it has changed forms slowly over the past decade. To set deep learning in context visually, the figure below illustrates the conception of the relationship between AI, machine learning, and deep learning.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*k5P2e-b_rhH2X4u9qMsrWg.jpeg\" width=\"80%\"/></p>\n<p>The field of AI is broad and has been around for a long time. Deep learning is a subset of the field of machine learning, which is a subfield of AI. The facets that differentiate deep learning networks in general from \u201ccanonical\u201d feed-forward multilayer networks are as follows:</p>\n<ul>\n<li>More neurons than previous networks\n<li>More complex ways of connecting layers\n<li>\u201cCambrian explosion\u201d of computing power to train\n<li>Automatic feature extraction\n</li></li></li></li></ul>\n<p>When I say \u201cmore neurons\u201d, I mean that the neuron count has risen over the years to express more complex models. Layers also have evolved from each layer being fully connected in multilayer networks to locally connected patches of neurons between layers in Convolutional Neural Networks and recurrent connections to the same neuron in Recurrent Neural Networks (in addition to the connections from the previous layer).<br>\nDeep learning then can be defined as neural networks with a large number of parameters and layers in one of four fundamental network architectures:</br></p>\n<ul>\n<li>Unsupervised Pre-trained Networks\n<li>Convolutional Neural Networks\n<li>Recurrent Neural Networks\n<li>Recursive Neural Networks\n</li></li></li></li></ul>\n<p>In this post, I am mainly interested in the latter 3 architectures. A\u00a0<strong>Convolutional Neural Network</strong>\u00a0is basically a standard neural network that has been extended across space using shared weights. CNN is designed to recognize images by having convolutions inside, which see the edges of an object recognized on the image. A\u00a0<strong>Recurrent Neural Network</strong>\u00a0is basically a standard neural network that has been extended across time by having edges which feed into the next time step instead of into the next layer in the same time step. RNN is designed to recognize sequences, for example, a speech signal or a text. It has cycles inside that implies the presence of short memory in the net. A\u00a0<strong>Recursive Neural Network</strong>\u00a0is more like a hierarchical network where there is really no time aspect to the input sequence but the input has to be processed hierarchically in a tree fashion. The 10 methods below can be applied to all of these architectures.</p>\n<p><b><strong>1\u200a\u2014\u200aBack-Propagation</strong></b></p>\n<p>Back-prop is simply a method to compute the partial derivatives (or gradient) of a function, which has the form as a function composition (as in Neural Nets). When you solve an optimization problem using a gradient-based method (gradient descent is just one of them), you want to compute the function gradient at each iteration.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/600/1*gRAmdOHLaf-AfgyBfsa0JQ.png\" width=\"70%\"/></p>\n<p>For a Neural Nets, the objective function has the form of a composition. How do you compute the gradient? There are 2 common ways to do it: (i)\u00a0<strong>Analytic differentiation</strong>. You know the form of the function. You just compute the derivatives using the chain rule (basic calculus). (ii)\u00a0<strong>Approximate differentiation using finite difference</strong>. This method is computationally expensive because the number of function evaluation is\u00a0<em>O(N)</em>, where\u00a0<em>N</em>\u00a0is the number of parameters. This is expensive, compared to analytic differentiation. Finite difference, however, is commonly used to validate a back-prop implementation when debugging.</p>\n<p><b><strong>2\u200a\u2014\u200aStochastic Gradient\u00a0Descent</strong></b></p>\n<p>An intuitive way to think of Gradient Descent is to imagine the path of a river originating from top of a mountain. The goal of gradient descent is exactly what the river strives to achieve\u200a\u2014\u200anamely, reach the bottom most point (at the foothill) climbing down from the mountain.</p>\n<p>Now, if the terrain of the mountain is shaped in such a way that the river doesn\u2019t have to stop anywhere completely before arriving at its final destination (which is the lowest point at the foothill, then this is the ideal case we desire. In Machine Learning, this amounts to saying, we have found the global mimimum (or optimum) of the solution starting from the initial point (top of the hill). However, it could be that the nature of terrain forces several pits in the path of the river, which could force the river to get trapped and stagnate. In Machine Learning terms, such pits are termed as local minima solutions, which is not desirable. There are a bunch of ways to get out of this (which I am not discussing).</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*fByskqwqf8UQaU-1sHE4jg.png\" width=\"70%\"/></p>\n<p>Gradient Descent therefore is prone to be stuck in local minimum, depending on the nature of the terrain (or function in ML terms). But, when you have a special kind of mountain terrain (which is shaped like a bowl, in ML terms this is called a Convex Function), the algorithm is always guaranteed to find the optimum. You can visualize this picturing a river again. These kind of special terrains (a.k.a convex functions) are always a blessing for optimization in ML. Also, depending on where at the top of the mountain you initial start from (ie. initial values of the function), you might end up following a different path. Similarly, depending on the speed at the river climbs down (ie. the learning rate or step size for the gradient descent algorithm), you might arrive at the final destination in a different manner. Both of these criteria can affect whether you fall into a pit (local minima) or are able to avoid it.</p>\n<p><b><strong>3\u200a\u2014\u200aLearning Rate\u00a0Decay</strong></b></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/600/1*jzIVDhOzzrHV9m8cEsTiSA.jpeg\" width=\"70%\"/></p>\n<p>Adapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time. Sometimes this is called\u00a0<em>learning rate annealing</em>\u00a0or\u00a0<em>adaptive learning rates</em>. The simplest and perhaps most used adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure. This has the effect of quickly learning good weights early and fine tuning them later.</p>\n<p>Two popular and easy to use learning rate decay are as follows:</p>\n<ul>\n<li>Decrease the learning rate gradually based on the epoch.\n<li>Decrease the learning rate using punctuated large drops at specific epochs.\n</li></li></ul>\n</div>\n<div class=\"page-link\"><p>Pages: 1 <a href=\"https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html/2\">2</a></p></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/12/meaningcloud-voice-customer-simplified-webinar.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/12/improve-machine-learning-performance-lessons-andrew-ng.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/kdnuggets-editor.html\">Looking for a KDnuggets Editor</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning Experts</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/12/index.html\">Dec</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/12/tutorials.html\">Tutorials, Overviews</a> \u00bb The 10 Deep Learning Methods AI Practitioners Need to Apply (\u00a0<a href=\"/2017/n48.html\">17:n48</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<div>\n<br/><span style=\"font-size:9px\">By subscribing, you agree to KDnuggets <a href=\"https://www.kdnuggets.com/news/privacy-policy.html\">privacy policy</a></span>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556434267\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.705 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-28 02:51:07 -->\n<!-- Compression = gzip -->", "content_tokenized": ["jame", "machin", "learn", "engin", "comment", "interest", "machin", "learn", "has", "explod", "over", "the", "past", "decad", "see", "machin", "learn", "comput", "scienc", "program", "industri", "confer", "and", "the", "wall", "street", "journal", "almost", "daili", "for", "all", "the", "talk", "about", "machin", "learn", "mani", "conflat", "what", "can", "with", "what", "they", "wish", "could", "fundament", "machin", "learn", "use", "algorithm", "extract", "inform", "from", "raw", "data", "and", "repres", "some", "type", "model", "use", "this", "model", "infer", "thing", "about", "other", "data", "have", "not", "yet", "model", "neural", "network", "are", "one", "type", "model", "for", "machin", "learn", "they", "have", "been", "around", "for", "least", "num", "year", "the", "fundament", "unit", "neural", "network", "node", "which", "loos", "base", "the", "biolog", "neuron", "the", "mammalian", "brain", "the", "connect", "between", "neuron", "are", "also", "model", "biolog", "brain", "the", "way", "these", "connect", "develop", "over", "time", "with", "train", "the", "midnum", "and", "earli", "num", "mani", "import", "architectur", "advanc", "were", "made", "neural", "network", "howev", "the", "amount", "time", "and", "data", "need", "get", "good", "result", "slow", "adopt", "and", "thus", "interest", "cool", "the", "earli", "num", "comput", "power", "expand", "exponenti", "and", "the", "industri", "saw", "cambrian", "explos", "comput", "techniqu", "that", "were", "not", "possibl", "prior", "this", "deep", "learn", "emerg", "from", "that", "decad", "explos", "comput", "growth", "serious", "contend", "the", "field", "win", "mani", "import", "machin", "learn", "competit", "the", "interest", "has", "not", "cool", "num", "today", "see", "deep", "learn", "mention", "everi", "corner", "machin", "learn", "get", "myself", "into", "the", "craze", "took", "udac", "deep", "learn", "cours", "which", "great", "introduct", "the", "motiv", "deep", "learn", "and", "the", "design", "intellig", "system", "that", "learn", "from", "complex", "andor", "largescal", "dataset", "tensorflow", "for", "the", "class", "project", "use", "and", "develop", "neural", "network", "for", "imag", "recognit", "with", "convolut", "natur", "languag", "process", "with", "embed", "and", "charact", "base", "text", "generat", "with", "recurr", "neural", "network", "long", "shortterm", "memori", "all", "the", "code", "jupit", "notebook", "can", "found", "this", "github", "repositori", "here", "outcom", "one", "the", "assign", "tsne", "project", "word", "vector", "cluster", "similar", "most", "recent", "have", "start", "read", "academ", "paper", "the", "subject", "from", "research", "here", "are", "sever", "public", "that", "have", "been", "huge", "influenti", "the", "develop", "the", "field", "gradientbas", "learn", "appli", "document", "recognit", "num", "which", "introduc", "convolut", "neural", "network", "the", "machin", "learn", "world", "toronto", "deep", "boltzmann", "machin", "num", "which", "present", "new", "learn", "algorithm", "for", "boltzmann", "machin", "that", "contain", "mani", "layer", "hidden", "variabl", "stanford", "googl", "build", "highlevel", "featur", "use", "largescal", "unsupervis", "learn", "num", "which", "address", "the", "problem", "build", "highlevel", "classspecif", "featur", "detector", "from", "onli", "unlabel", "data", "berkeley", "decaf", "deep", "convolut", "activ", "featur", "for", "generic", "visual", "recognit", "num", "which", "releas", "decaf", "opensourc", "implement", "the", "deep", "convolut", "activ", "featur", "along", "with", "all", "associ", "network", "paramet", "enabl", "vision", "research", "abl", "conduct", "experiment", "with", "deep", "represent", "across", "rang", "visual", "concept", "learn", "paradigm", "deepmind", "play", "atari", "with", "deep", "reinforc", "learn", "num", "which", "present", "the", "numst", "deep", "learn", "model", "success", "learn", "control", "polici", "direct", "from", "highdimension", "sensori", "input", "use", "reinforc", "learn", "there", "abund", "amount", "great", "knowledg", "about", "deep", "learn", "have", "learnt", "via", "research", "and", "learn", "here", "want", "share", "the", "num", "power", "deep", "learn", "method", "engin", "can", "appli", "their", "machin", "learn", "problem", "but", "first", "all", "let", "defin", "what", "deep", "learn", "deep", "learn", "has", "been", "challeng", "defin", "for", "mani", "becaus", "has", "chang", "form", "slowli", "over", "the", "past", "decad", "set", "deep", "learn", "context", "visual", "the", "figur", "below", "illustr", "the", "concept", "the", "relationship", "between", "machin", "learn", "and", "deep", "learn", "the", "field", "broad", "and", "has", "been", "around", "for", "long", "time", "deep", "learn", "subset", "the", "field", "machin", "learn", "which", "subfield", "the", "facet", "that", "differenti", "deep", "learn", "network", "general", "from", "canon", "feedforward", "multilay", "network", "are", "follow", "more", "neuron", "than", "previous", "network", "more", "complex", "way", "connect", "layer", "cambrian", "explos", "comput", "power", "train", "automat", "featur", "extract", "when", "say", "more", "neuron", "mean", "that", "the", "neuron", "count", "has", "risen", "over", "the", "year", "express", "more", "complex", "model", "layer", "also", "have", "evolv", "from", "each", "layer", "fulli", "connect", "multilay", "network", "local", "connect", "patch", "neuron", "between", "layer", "convolut", "neural", "network", "and", "recurr", "connect", "the", "same", "neuron", "recurr", "neural", "network", "addit", "the", "connect", "from", "the", "previous", "layer", "deep", "learn", "then", "can", "defin", "neural", "network", "with", "larg", "number", "paramet", "and", "layer", "one", "four", "fundament", "network", "architectur", "unsupervis", "pretrain", "network", "convolut", "neural", "network", "recurr", "neural", "network", "recurs", "neural", "network", "this", "post", "main", "interest", "the", "latter", "num", "architectur", "convolut", "neural", "network", "basic", "standard", "neural", "network", "that", "has", "been", "extend", "across", "space", "use", "share", "weight", "design", "recogn", "imag", "have", "convolut", "insid", "which", "see", "the", "edg", "object", "recogn", "the", "imag", "recurr", "neural", "network", "basic", "standard", "neural", "network", "that", "has", "been", "extend", "across", "time", "have", "edg", "which", "feed", "into", "the", "next", "time", "step", "instead", "into", "the", "next", "layer", "the", "same", "time", "step", "design", "recogn", "sequenc", "for", "exampl", "speech", "signal", "text", "has", "cycl", "insid", "that", "impli", "the", "presenc", "short", "memori", "the", "net", "recurs", "neural", "network", "more", "like", "hierarch", "network", "where", "there", "realli", "time", "aspect", "the", "input", "sequenc", "but", "the", "input", "has", "process", "hierarch", "tree", "fashion", "the", "num", "method", "below", "can", "appli", "all", "these", "architectur", "num", "backpropag", "backprop", "simpli", "method", "comput", "the", "partial", "deriv", "gradient", "function", "which", "has", "the", "form", "function", "composit", "neural", "net", "when", "solv", "optim", "problem", "use", "gradientbas", "method", "gradient", "descent", "just", "one", "them", "want", "comput", "the", "function", "gradient", "each", "iter", "for", "neural", "net", "the", "object", "function", "has", "the", "form", "composit", "how", "comput", "the", "gradient", "there", "are", "num", "common", "way", "analyt", "differenti", "know", "the", "form", "the", "function", "just", "comput", "the", "deriv", "use", "the", "chain", "rule", "basic", "calculus", "approxim", "differenti", "use", "finit", "differ", "this", "method", "comput", "expens", "becaus", "the", "number", "function", "evalu", "where", "the", "number", "paramet", "this", "expens", "compar", "analyt", "differenti", "finit", "differ", "howev", "common", "use", "valid", "backprop", "implement", "when", "debug", "num", "stochast", "gradient", "descent", "intuit", "way", "think", "gradient", "descent", "imagin", "the", "path", "river", "origin", "from", "top", "mountain", "the", "goal", "gradient", "descent", "exact", "what", "the", "river", "strive", "achiev", "name", "reach", "the", "bottom", "most", "point", "the", "foothil", "climb", "down", "from", "the", "mountain", "now", "the", "terrain", "the", "mountain", "shape", "such", "way", "that", "the", "river", "have", "stop", "anywher", "complet", "befor", "arriv", "final", "destin", "which", "the", "lowest", "point", "the", "foothil", "then", "this", "the", "ideal", "case", "desir", "machin", "learn", "this", "amount", "say", "have", "found", "the", "global", "mimimum", "optimum", "the", "solut", "start", "from", "the", "initi", "point", "top", "the", "hill", "howev", "could", "that", "the", "natur", "terrain", "forc", "sever", "pit", "the", "path", "the", "river", "which", "could", "forc", "the", "river", "get", "trap", "and", "stagnat", "machin", "learn", "term", "such", "pit", "are", "term", "local", "minima", "solut", "which", "not", "desir", "there", "are", "bunch", "way", "get", "out", "this", "which", "not", "discuss", "gradient", "descent", "therefor", "prone", "stuck", "local", "minimum", "depend", "the", "natur", "the", "terrain", "function", "term", "but", "when", "have", "special", "kind", "mountain", "terrain", "which", "shape", "like", "bowl", "term", "this", "call", "convex", "function", "the", "algorithm", "alway", "guarante", "find", "the", "optimum", "can", "visual", "this", "pictur", "river", "again", "these", "kind", "special", "terrain", "aka", "convex", "function", "are", "alway", "bless", "for", "optim", "also", "depend", "where", "the", "top", "the", "mountain", "initi", "start", "from", "initi", "valu", "the", "function", "might", "end", "follow", "differ", "path", "similar", "depend", "the", "speed", "the", "river", "climb", "down", "the", "learn", "rate", "step", "size", "for", "the", "gradient", "descent", "algorithm", "might", "arriv", "the", "final", "destin", "differ", "manner", "both", "these", "criteria", "can", "affect", "whether", "fall", "into", "pit", "local", "minima", "are", "abl", "avoid", "num", "learn", "rate", "decay", "adapt", "the", "learn", "rate", "for", "stochast", "gradient", "descent", "optim", "procedur", "can", "increas", "perform", "and", "reduc", "train", "time", "sometim", "this", "call", "learn", "rate", "anneal", "adapt", "learn", "rate", "the", "simplest", "and", "perhap", "most", "use", "adapt", "learn", "rate", "dure", "train", "are", "techniqu", "that", "reduc", "the", "learn", "rate", "over", "time", "these", "have", "the", "benefit", "make", "larg", "chang", "the", "begin", "the", "train", "procedur", "when", "larger", "learn", "rate", "valu", "are", "use", "and", "decreas", "the", "learn", "rate", "such", "that", "smaller", "rate", "and", "therefor", "smaller", "train", "updat", "are", "made", "weight", "later", "the", "train", "procedur", "this", "has", "the", "effect", "quick", "learn", "good", "weight", "earli", "and", "fine", "tune", "them", "later", "two", "popular", "and", "easi", "use", "learn", "rate", "decay", "are", "follow", "decreas", "the", "learn", "rate", "gradual", "base", "the", "epoch", "decreas", "the", "learn", "rate", "use", "punctuat", "larg", "drop", "specif", "epoch"], "timestamp_scraper": 1556482464.387009, "title": "The 10 Deep Learning Methods AI Practitioners Need to Apply", "read_time": 477.9, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/james-le\" rel=\"author\" title=\"Posts by James Le\">James Le</a>, Machine Learning Engineer.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html?page=2#comments\">comments</a></div>\n<p><img alt=\"Header image\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/2000/1*CPI-6ZtpYfMyV3bTt8EumQ.jpeg\" width=\"99%\"/></p>\n<p>Interest in\u00a0<strong>machine learning</strong>\u00a0has exploded over the past decade. You see machine learning in computer science programs, industry conferences, and the Wall Street Journal almost daily. For all the talk about machine learning, many conflate what it can do with what they wish it could do. Fundamentally, machine learning is using algorithms to extract information from raw data and represent it in some type of model. We use this model to infer things about other data we have not yet modeled.</p>\n<p><strong>Neural networks</strong>\u00a0are one type of model for machine learning; they have been around for at least 50 years. The fundamental unit of a neural network is a node, which is loosely based on the biological neuron in the mammalian brain. The connections between neurons are also modeled on biological brains, as is the way these connections develop over time (with \u201ctraining\u201d).</p>\n<p>In the mid-1980s and early 1990s, many important architectural advancements were made in neural networks. However, the amount of time and data needed to get good results slowed adoption, and thus interest cooled. In the early 2000s, computational power expanded exponentially and the industry saw a \u201cCambrian explosion\u201d of computational techniques that were not possible prior to this.\u00a0<strong>Deep learning</strong>\u00a0emerged from that decade\u2019s explosive computational growth as a serious contender in the field, winning many important machine learning competitions. The interest has not cooled as of 2017; today, we see deep learning mentioned in every corner of machine learning.</p>\n<p>To get myself into the craze, I took\u00a0<a href=\"https://www.udacity.com/course/deep-learning--ud730\" target='_blank rel=\"noopener noreferrer\"'><strong>Udacity\u2019s \u201cDeep Learning\u201d course</strong></a>, which is a great introduction to the motivation of deep learning and the design of intelligent systems that learn from complex and/or large-scale datasets in\u00a0<strong>TensorFlow</strong>. For the class projects, I used and developed neural networks for image recognition with convolutions, natural language processing with embeddings and character based text generation with Recurrent Neural Network / Long Short-Term Memory. All the code in Jupiter Notebook can be found on\u00a0<a href=\"https://github.com/khanhnamle1994/deep-learning\" target='_blank rel=\"noopener noreferrer\"'>this GitHub repository</a>.</p>\n<p>Here is an outcome of one of the assignments, a t-SNE projection of word vectors, clustered by similarity.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*5gUqFLOyQrB4kgjAicn0og.png\" width=\"99%\"/></p>\n<p>Most recently, I have started reading academic papers on the subject. From my research, here are several publications that have been hugely influential to the development of the field:</p>\n<ul>\n<li>NYU\u2019s\u00a0<a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>Gradient-Based Learning Applied to Document Recognition</strong></a> (1998), which introduces Convolutional Neural Network to the Machine Learning world.\n<li>Toronto\u2019s\u00a0<a href=\"http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>Deep Boltzmann Machines</strong></a>\u00a0(2009), which presents a new learning algorithm for Boltzmann machines that contain many layers of hidden variables.\n<li>Stanford &amp; Google\u2019s\u00a0<a href=\"http://icml.cc/2012/papers/73.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>Building High-Level Features Using Large-Scale Unsupervised Learning</strong></a>\u00a0(2012), which addresses the problem of building high-level, class-specific feature detectors from only unlabeled data.\n<li>Berkeley\u2019s\u00a0<a href=\"http://proceedings.mlr.press/v32/donahue14.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>DeCAF\u200a\u2014\u200aA Deep Convolutional Activation Feature for Generic Visual Recognition</strong></a>\u00a0(2013), which releases DeCAF, an open-source implementation of the deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.\n<li>DeepMind\u2019s\u00a0<a href=\"https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\" target='_blank rel=\"noopener noreferrer\"'><strong>Playing Atari with Deep Reinforcement Learning</strong></a>\u00a0(2016), which presents the 1st deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning.\n</li></li></li></li></li></ul>\n<p>There is an abundant amount of great knowledge about deep learning I have learnt via research and learning. Here I want to share the\u00a0<strong>10 powerful deep learning methods</strong>\u00a0AI engineers can apply to their machine learning problems. But first of all, let\u2019s define what deep learning is. Deep learning has been a challenge to define for many because it has changed forms slowly over the past decade. To set deep learning in context visually, the figure below illustrates the conception of the relationship between AI, machine learning, and deep learning.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*k5P2e-b_rhH2X4u9qMsrWg.jpeg\" width=\"80%\"/></p>\n<p>The field of AI is broad and has been around for a long time. Deep learning is a subset of the field of machine learning, which is a subfield of AI. The facets that differentiate deep learning networks in general from \u201ccanonical\u201d feed-forward multilayer networks are as follows:</p>\n<ul>\n<li>More neurons than previous networks\n<li>More complex ways of connecting layers\n<li>\u201cCambrian explosion\u201d of computing power to train\n<li>Automatic feature extraction\n</li></li></li></li></ul>\n<p>When I say \u201cmore neurons\u201d, I mean that the neuron count has risen over the years to express more complex models. Layers also have evolved from each layer being fully connected in multilayer networks to locally connected patches of neurons between layers in Convolutional Neural Networks and recurrent connections to the same neuron in Recurrent Neural Networks (in addition to the connections from the previous layer).<br>\nDeep learning then can be defined as neural networks with a large number of parameters and layers in one of four fundamental network architectures:</br></p>\n<ul>\n<li>Unsupervised Pre-trained Networks\n<li>Convolutional Neural Networks\n<li>Recurrent Neural Networks\n<li>Recursive Neural Networks\n</li></li></li></li></ul>\n<p>In this post, I am mainly interested in the latter 3 architectures. A\u00a0<strong>Convolutional Neural Network</strong>\u00a0is basically a standard neural network that has been extended across space using shared weights. CNN is designed to recognize images by having convolutions inside, which see the edges of an object recognized on the image. A\u00a0<strong>Recurrent Neural Network</strong>\u00a0is basically a standard neural network that has been extended across time by having edges which feed into the next time step instead of into the next layer in the same time step. RNN is designed to recognize sequences, for example, a speech signal or a text. It has cycles inside that implies the presence of short memory in the net. A\u00a0<strong>Recursive Neural Network</strong>\u00a0is more like a hierarchical network where there is really no time aspect to the input sequence but the input has to be processed hierarchically in a tree fashion. The 10 methods below can be applied to all of these architectures.</p>\n<p><b><strong>1\u200a\u2014\u200aBack-Propagation</strong></b></p>\n<p>Back-prop is simply a method to compute the partial derivatives (or gradient) of a function, which has the form as a function composition (as in Neural Nets). When you solve an optimization problem using a gradient-based method (gradient descent is just one of them), you want to compute the function gradient at each iteration.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/600/1*gRAmdOHLaf-AfgyBfsa0JQ.png\" width=\"70%\"/></p>\n<p>For a Neural Nets, the objective function has the form of a composition. How do you compute the gradient? There are 2 common ways to do it: (i)\u00a0<strong>Analytic differentiation</strong>. You know the form of the function. You just compute the derivatives using the chain rule (basic calculus). (ii)\u00a0<strong>Approximate differentiation using finite difference</strong>. This method is computationally expensive because the number of function evaluation is\u00a0<em>O(N)</em>, where\u00a0<em>N</em>\u00a0is the number of parameters. This is expensive, compared to analytic differentiation. Finite difference, however, is commonly used to validate a back-prop implementation when debugging.</p>\n<p><b><strong>2\u200a\u2014\u200aStochastic Gradient\u00a0Descent</strong></b></p>\n<p>An intuitive way to think of Gradient Descent is to imagine the path of a river originating from top of a mountain. The goal of gradient descent is exactly what the river strives to achieve\u200a\u2014\u200anamely, reach the bottom most point (at the foothill) climbing down from the mountain.</p>\n<p>Now, if the terrain of the mountain is shaped in such a way that the river doesn\u2019t have to stop anywhere completely before arriving at its final destination (which is the lowest point at the foothill, then this is the ideal case we desire. In Machine Learning, this amounts to saying, we have found the global mimimum (or optimum) of the solution starting from the initial point (top of the hill). However, it could be that the nature of terrain forces several pits in the path of the river, which could force the river to get trapped and stagnate. In Machine Learning terms, such pits are termed as local minima solutions, which is not desirable. There are a bunch of ways to get out of this (which I am not discussing).</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*fByskqwqf8UQaU-1sHE4jg.png\" width=\"70%\"/></p>\n<p>Gradient Descent therefore is prone to be stuck in local minimum, depending on the nature of the terrain (or function in ML terms). But, when you have a special kind of mountain terrain (which is shaped like a bowl, in ML terms this is called a Convex Function), the algorithm is always guaranteed to find the optimum. You can visualize this picturing a river again. These kind of special terrains (a.k.a convex functions) are always a blessing for optimization in ML. Also, depending on where at the top of the mountain you initial start from (ie. initial values of the function), you might end up following a different path. Similarly, depending on the speed at the river climbs down (ie. the learning rate or step size for the gradient descent algorithm), you might arrive at the final destination in a different manner. Both of these criteria can affect whether you fall into a pit (local minima) or are able to avoid it.</p>\n<p><b><strong>3\u200a\u2014\u200aLearning Rate\u00a0Decay</strong></b></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/600/1*jzIVDhOzzrHV9m8cEsTiSA.jpeg\" width=\"70%\"/></p>\n<p>Adapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time. Sometimes this is called\u00a0<em>learning rate annealing</em>\u00a0or\u00a0<em>adaptive learning rates</em>. The simplest and perhaps most used adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure. This has the effect of quickly learning good weights early and fine tuning them later.</p>\n<p>Two popular and easy to use learning rate decay are as follows:</p>\n<ul>\n<li>Decrease the learning rate gradually based on the epoch.\n<li>Decrease the learning rate using punctuated large drops at specific epochs.\n</li></li></ul>\n</div> ", "website": "kdnuggets"}