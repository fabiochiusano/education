{"content": "comments By Jekaterina Kokatjuhha . I was searching for flight tickets and noticed that ticket prices fluctuate during the day. I tried to find out when the best time to buy tickets is, but there was nothing on the Web that helped. I built a small program to automatically collect the data from the web\u200a\u2014\u200aa so-called scraper. It extracted information for my specific flight destination on predetermined dates and notified me when the price got lower. Web scraping is a technique used to extract data from websites through an automated process. I learned a lot from this experience with Web scraping, and I want to share it. This post is intended for people who are interested to know about the common design patterns, pitfalls and rules related to the web scraping. The article presents several use cases and a collection of typical problems , such as how not to be detected , dos and don\u2019ts , and how to speed up (parallelization) your scraper. Everything will be accompanied by python snippets, so that you can start straight away. This document will also go through several useful python packages. Use Cases There are many reasons and use cases why you would want to scrape data. Let me list some of them: scrape pages of a e-retailer to spot if some of the clothes you want to buy got discounted compare prices of several clothes brands by scraping their pages price of the flight tickets can vary during the day. One could crawl the travel website and get alarmed once the price was lowered analyze the action websites to answer the question if starting bid should be low or high to attract more bidders or if the longer auction correlates with a higher end bid Tutorial Structure of the tutorial: Available packages Basic code Pitfalls Dos and dont\u2019s Speed up\u200a\u2014\u200aparallelization Before we start: Be NICE to the servers; you DON\u2019T want to crash a website. 1. Available packages and tools There is no universal solution for web scraping because the way data is stored on each website is usually specific to that site. In fact, if you want to scrape the data, you need to understand the website\u2019s structure and either build your own solution or use a highly customizable one. However, you don\u2019t need to reinvent the wheel: there are many packages that do the most work for you. Depending on your programming skills and your intended use case, you might find different packages more or less useful. 1.1 Inspect option Most of the time you will finding yourself inspecting the HTML the website. You can easily do it with an \u201cinspect\u201d option of your browser. \u00a0 \u00a0 The section of the website that holds my name, my avatar and my description is called hero hero--profile u-flexTOP (how interesting that Medium calls its writers \u2018heroes\u2019 :)). The class that holds my name is called ui-h2 hero-title and the description is contained within the class ui-body hero-description . You can read more about HTML tags , and differences between classes and ids here . 1.2 Scrapy There is a stand-alone ready-to-use data extracting framework called Scrapy . Apart from extracting HTML the package offers lots of functionalities like exporting data in formats, logging etc. It is also highly customisable: run different spiders on different processes, disable cookies 1 and set download delays 2 . It can also be used to extract data using API. However, the learning curve is not smooth for the new programmers: you need to read tutorials and examples to get started. 1 Some sites use cookies to identify bots. 2 The website can get overloaded due to a huge amount of crawling requests. For my use case it was too much \u2018out of the box\u2019: I just wanted to extract the links from all pages, access each link and extract information out of it. 1.3 BeautifulSoup with Requests BeautifulSoup is a library that allows you to parse the HTML source code in a beautiful way. Along with it you need a Request library that will fetch the content of the URL. However, you should take care of everything else like error handling, how to export data, how to parallelize the web scraper, etc. I chose BeautifulSoup as it would force me to figure out a lot of stuff that Scrapy handles on its own, and hopefully help me learn faster from my mistakes. 2. Basic code It\u2019s very straightforward to start scraping a website. Most of the time you will find yourself inspecting HTML of the website to access the classes and IDs you need. Lets say we have a following html structure and we want to extract the main_price elements. Note: discounted_price element is optional. The basic code would be to import the libraries, do the request, parse the html and then to find the class main_price . It can happen that the class main_price is present in another section of the website. To avoid extracting unnecessary class main_price from any other part of the webpage we could have first addressed the id listings_prices and only then find all elements with class main_price . 3. Pitfalls 3.1 Check robots.txt The scraping rules of the websites can be found in the robots.txt file. You can find it by writing robots.txt after the main domain, e.g www.website_to_scrape.com/robots.txt . These rules identify which parts of the websites are not allowed to be automatically extracted or how frequently a bot is allowed to request a page. Most people don\u2019t care about it, but try to be respectful and at least look at the rules even if you don\u2019t plan to follow them. 3.2 HTML can be evil HTML tags can contain id, class or both. HTML id specifies a unique id and HTML class is non-unique. Changes in the class name or element could either break your code or deliver wrong results. There are two ways to avoid it or at least to be alerted about it: Use specific id rather than class since it is less likely to be changed Check if the element returns None price = \r # check if the element with such id exists or not\r if price is None:\r # NOTIFY! LOG IT, COUNT IT\r else:\r # do something\r \u00a0 However, because some fields can be optional (like discounted_price in our HTML example), corresponding elements would not appear on each listing. In this case you can count the percentage of how many times this specific element returned None to the number of listings. If it is 100%, you might want to check if the element name was changed. 3.3 User agent spoofing Every time you visit a website, it gets your browser information via user agent . Some websites won\u2019t show you any content unless you provide a user agent. Also, some sites offer different content to different browsers. Websites do not want to block genuine users but you would look suspicious if you send 200 requests/second with the same user agent. A way out might be either to generate (almost) random user agent or to set one yourself. # library to generate user agent\r from user_agent import generate_user_agent\r # generate a user agent\r headers = {'User-Agent': )}\r #headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux i686 on x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.63 Safari/537.36'}\r page_response = \r \u00a0 3.4 Timeout request By default, Request will keep waiting for a response indefinitely. Therefore, it is advised to set the timeout parameter. # timeout is set to 5 seconds\r page_response = \r \u00a0 3.5 Did I get blocked? Frequent appearance of the status codes like 404 (Not Found), 403 (Forbidden), 408 (Request Timeout) might indicate that you got blocked. You may want to check for those error codes and proceed accordingly.\u00a0Also, be ready to handle exceptions from the request. try:\r page_response = \r if page_response.status_code == 200:\r # extract\r else:\r \r # notify, try again\r except requests.Timeout as e:\r \r )\r except # other exception\r \u00a0 3.6 IP Rotation Even if you randomize your user agent, all your requests will be from the same IP address. That doesn\u2019t sound abnormal because libraries, universities, and also companies have only a few IP addresses. However, if there are uncommonly many requests coming from a single IP address, a server can detect it. Using shared proxies, VPNs or TOR can help you become a ghost ;). proxies = {'http' : ':0000', \r 'https': ':0000'}\r page_response =  \r \u00a0 By using a shared proxy, the website will see the IP address of the proxy server and not yours. A VPN connects you to another network and the IP address of the VPN provider will be sent to the website. 3.7 Honeypots Honeypots are means to detect crawlers or scrapers. These can be \u2018hidden\u2019 links that are not visible to the users but can be extracted by scrapers/spiders. Such links will have a CSS style set to display:none, they can be blended by having the color of the background, or even be moved off of the visible area of the page. Once your crawler visits such a link, your IP address can be flagged for further investigation, or even be instantly blocked. Another way to spot crawlers is to add links with infinitely deep directory trees. Then one would need to limit the number of retrieved pages or limit the traversal depth. 4. Dos and Don'ts Before scraping, check if there is a public API available. Public APIs provide easier and faster (and legal) data retrieval than web scraping. Check out Twitter API that provides APIs for different purposes. In case you scrape lots of data, you might want to consider using a database to be able to analyze or retrieve it fast. Follow this tutorial on how to create a local database with python. Be polite. As this answer suggests, it is recommended to let people know that you are scraping their website so they can better respond to the problems your bot might cause. Again, do not overload the website by sending hundreds of requests per second. 5. Speed up\u200a\u2014\u200aparallelization If you decide to parallelize your program, be careful with your implementation so you don\u2019t slam the server. And be sure you read the Dos and Don\u2019ts section. Check out the the definitions of parallelization vs concurrency, processors and threads here and here . If you extract a huge amount of information from the page and do some preprocessing of the data while scraping, the number of requests per second you send to the page can be relatively low. For my other project where I scraped apartment rental prices, I did heavy preprocessing of the data while scraping, which resulted in 1 request/second. In order to scrape 4K ads, my program would run for about one hour. In order to send requests in parallel you might want to use a multiprocessing package. Let\u2019s say we have 100 pages and we want to assign every processor equal amount of pages to work with. If n is the number of CPUs, you can evenly chunk all pages into the n bins and assign each bin to a processor. Each process will have its own name, target function and the arguments to work with. The name of the process can be used afterwards to enable writing data to a specific file. I assigned 1K pages to each of my 4 CPUs which yielded 4 requests/second and reduced the scraping time to around 17 mins. import numpy as np\r import multiprocessing as multi\r \r def :\r \"\"\"Splits the list into n chunks\"\"\"\r return \r \r cpus = \r workers = []\r page_list = ['www.website.com/page1.html', 'www.website.com/page2.html'\r 'www.website.com/page3.html', 'www.website.com/page4.html']\r \r page_bins = \r \r for cpu in :\r  + \"\\n\")\r # Process that will send corresponding list of pages \r # to the function perform_extraction\r worker = , \r target=perform_extraction, \r args=(page_bins[cpu],))\r \r \r \r for worker in workers:\r \r \r def :\r \"\"\"Extracts data, does preprocessing, writes the data\"\"\"\r # do requests and BeautifulSoup\r # preprocess the data\r file_name = .name+'.txt'\r # write into current process file\r \u00a0 Happy scraping! Bio: Jekaterina Kokatjuhha is a passionate Bioinformatician with interest in Machine Learning and Data Science. Original . Reposted with permission. Related Using AutoML to Generate Machine Learning Pipelines with TPOT A Primer on Web Scraping in R Web Scraping for Data Science with Python", "title_html": "<h1 id=\"title\"><img align=\"right\" alt=\"Gold Blog\" src=\"/images/tkb-1802-g.png\" width=\"94\"/>Web Scraping Tutorial with Python: Tips and Tricks</h1> ", "url": "https://www.kdnuggets.com/2018/02/web-scraping-tutorial-python.html", "tfidf": {"tfidf": {"after": 1.02070207021, "understand": 2.96858638743, "web": 51.7133550489, "vari": 2.4970116388799997, "onc": 2.9949066213999997, "page": 26.47697241826, "heavi": 2.88707037643, "proxi": 160.7696202532, "delay": 4.23247134098, "assign": 11.50990816821, "http": 3.17202797203, "scrapersspid": 1443.27272727, "would": 7.580110497259999, "number": 4.40571666436, "blend": 12.1935483871, "specif": 9.359745313049999, "function": 7.486325055, "class": 25.39821357156, "happi": 6.125, "log": 13.6744186047, "python": 225.1914893616, "afterward": 4.72640666865, "disabl": 8.93918918919, "vpns": 1443.27272727, "sound": 3.11294117647, "tree": 4.127925117, "requeststimeout": 1443.27272727, "permiss": 6.280063291139999, "yield": 6.46943765281, "know": 5.1865403463, "their": 2.0309581681, "applewebkitnum": 1443.27272727, "bid": 17.191120736320002, "notifi": 55.220869565200005, "legal": 2.6000655093400002, "chose": 4.42105263158, "equal": 2.542193755, "low": 4.26144141726, "creat": 1.2492917847, "kokatjuhha": 2886.54545454, "how": 12.82002624408, "programm": 5.181462140990001, "repost": 933.882352941, "present": 2.51103202846, "chunk": 162.0, "than": 2.0655737705, "hundr": 2.4698195395099996, "due": 1.23789473684, "end": 1.10680423871, "found": 2.2277415281, "correl": 13.1860465116, "jekaterina": 2886.54545454, "interest": 4.80993738639, "slam": 26.7723440135, "deep": 3.6279707495399998, "color": 3.8255421686699997, "safarinum": 1443.27272727, "rotat": 8.53548387097, "consid": 1.2397313759200002, "site": 5.91652173912, "inum": 97.3987730061, "accompani": 3.38146964856, "snippet": 135.692307692, "purpos": 2.23416830847, "new": 1.0178880554, "but": 4.06529671596, "definit": 3.24, "need": 8.62357414446, "our": 2.35758835759, "longer": 2.02319357716, "background": 4.02739726027, "off": 1.5121440137200002, "overload": 122.123076923, "mozillanum": 1443.27272727, "header": 167.11578947360002, "hidden": 7.81299212598, "straight": 6.203985932, "uflextop": 1443.27272727, "generat": 8.21101629168, "hero": 15.115201523340001, "crawler": 1161.658536585, "action": 1.81855670103, "use": 19.56313639022, "experi": 1.87062566278, "default": 21.1398135819, "nametxt": 1443.27272727, "generateuserag": 1443.27272727, "out": 7.42116861437, "mistak": 8.71350164654, "say": 3.5088960106, "format": 2.53125, "mainpric": 7216.363636349999, "specifi": 6.920662598080001, "browser": 204.4120171674, "huge": 8.77854575616, "cpus": 523.3846153860001, "around": 1.21394708671, "field": 1.7790228597, "reason": 1.72340425532, "ghost": 11.2595744681, "suggest": 1.7571665744299998, "xnum": 39.4925373134, "respect": 1.6443293630200002, "dont": 835.5789473680001, "etc": 8.413354531, "such": 4.24605509496, "not": 10.1567398119, "store": 3.44680851064, "worker": 14.737526108159999, "happen": 2.96359902931, "infinit": 16.0688259109, "handl": 11.76871756857, "stuff": 23.3127753304, "then": 3.25973581548, "customiz": 407.07692307699995, "they": 2.06034650574, "design": 1.45825296225, "travers": 22.8103448276, "anoth": 3.40930565496, "descript": 8.01009081736, "allow": 3.8148177813300004, "scienc": 4.63939216832, "attract": 2.53326950694, "section": 6.385306341329999, "away": 1.85142857143, "export": 13.454237288139998, "filenam": 496.125, "respons": 1.5066907089300001, "appear": 2.6429165972999997, "that": 18.0717131475, "provid": 4.86210856748, "twitter": 33.213389121300004, "respond": 3.17329602239, "offer": 3.07793718496, "either": 4.749027819329999, "accord": 1.27589809531, "got": 10.859097127230001, "sent": 2.32683570277, "should": 3.3286508019800003, "pattern": 3.79173632673, "spot": 9.05905848788, "sever": 3.2172385841699995, "those": 1.19548192771, "with": 17.020369552829997, "network": 2.59369384088, "lot": 17.63510136072, "box": 4.12685209254, "sourc": 1.69760479042, "small": 1.3594793629, "collect": 3.28219971056, "add": 4.61243463103, "may": 1.05201775893, "processor": 113.942583732, "call": 4.2706119704, "result": 2.29223216864, "document": 2.5409731114, "fetch": 58.583025830299995, "bioinformatician": 1443.27272727, "content": 10.6265060241, "herotitl": 1443.27272727, "html": 142.3856502242, "less": 2.93809567872, "set": 5.93539703905, "caus": 1.38521943984, "ani": 2.26767604628, "question": 2.20408163265, "multi": 91.2413793103, "write": 8.230171073080001, "from": 12.00680657964, "webpag": 51.8823529412, "abnorm": 29.8983050847, "visibl": 9.37190082644, "limit": 3.0373062942400004, "work": 3.34560269739, "pageresponsestatuscod": 1443.27272727, "num": 33.01039632033, "pitfal": 535.146067416, "els": 16.33333333332, "correspond": 6.64963350786, "curv": 11.1098670399, "for": 16.00504064016, "depend": 2.2411067193700003, "dos": 144.0, "evil": 7.714285714289999, "forc": 1.32399299475, "are": 9.26915342202, "solut": 9.4556283502, "split": 3.4709226060300002, "better": 2.0065722952500002, "compani": 1.5523613963, "cpu": 161.1776649746, "current": 1.5325803649, "frequent": 4.21002386634, "preprocess": 4884.92307692, "look": 3.8172637653199994, "option": 16.19586840092, "numpi": 1443.27272727, "veri": 1.25880114177, "extract": 107.84279475984, "higher": 2.1218925421, "brand": 5.5259310824900005, "writer": 2.75816539263, "unless": 5.44818119423, "public": 2.4484885873, "pars": 291.302752294, "flight": 13.378651685400001, "again": 3.01767724768, "compar": 1.8662278123900002, "medium": 7.00617828773, "doe": 1.70581282905, "even": 5.8230633803, "scraper": 1270.08, "could": 3.6131087846999996, "uncommon": 14.0620017715, "pagerespons": 5773.09090908, "scrapi": 4329.81818181, "avoid": 4.91973969632, "targetperformextract": 1443.27272727, "rental": 24.8840125392, "error": 12.08219178082, "investig": 3.11721971333, "move": 1.29125660838, "comment": 3.05954904606, "note": 1.42449528937, "robotstxt": 4329.81818181, "analyz": 19.37278828554, "machin": 8.04866920152, "into": 3.04507384437, "automat": 13.575032064980002, "auction": 19.1277108434, "won": 2.31732593782, "sure": 7.453521126760001, "customis": 236.955223881, "eretail": 1443.27272727, "where": 1.06715063521, "figur": 2.0343413634, "buy": 10.24919302776, "local": 1.51720183486, "let": 13.94466403164, "predetermin": 69.0260869565, "help": 4.19888918277, "main": 1.25303867403, "find": 12.10588235297, "wrong": 5.478260869570001, "least": 3.2330719886000003, "relat": 3.71252630757, "forbidden": 12.082191780799999, "fact": 1.73375559681, "hour": 2.25960717336, "who": 1.06279287723, "decid": 1.9257641921400002, "univers": 2.49779735682, "hold": 3.3102585488, "post": 2.23826307627, "here": 7.26923076924, "lower": 4.20111140514, "speed": 11.610921501720002, "paramet": 17.256521739100002, "apart": 6.2064112588, "much": 1.1942229577299999, "contain": 3.19629555064, "parallel": 32.054225555239995, "care": 7.48279654359, "about": 5.324300757950001, "problem": 3.53349655018, "run": 3.11385701676, "have": 7.104263887979998, "just": 1.33580143037, "enabl": 3.5421686747, "crawl": 93.6637168142, "requestsecond": 1443.27272727, "dure": 2.1006946741599997, "beauti": 4.79347826087, "visit": 4.41245136186, "check": 52.0524590164, "recommend": 3.9142011834300003, "flag": 5.95275590551, "suspici": 21.0557029178, "destin": 6.0503048780499995, "tool": 4.99716713881, "start": 6.3336790872, "noth": 3.46410648047, "articl": 2.01805008262, "fluctuat": 21.570652173899997, "singl": 1.60948905109, "them": 2.19752231988, "request": 46.35779637915, "keep": 2.04245465071, "uibodi": 1443.27272727, "scrape": 1552.3200000000002, "order": 2.49250333622, "wwwwebsitetoscrapecomrobotstxt": 1443.27272727, "whi": 3.2566153846200003, "autom": 19.8202247191, "usual": 1.72508964468, "abl": 1.8208510150200001, "intend": 4.697736351540001, "uniqu": 3.01595744681, "the": 106.0, "build": 1.6341739578, "indefinit": 17.0160771704, "uihnum": 1443.27272727, "display": 2.93456561922, "framework": 8.200413223139998, "too": 1.81585268215, "same": 2.23715916296, "thread": 24.2381679389, "bot": 512.1290322570001, "there": 8.32730133752, "exist": 1.4647107666799999, "connect": 1.8843916913900003, "inform": 6.301250248080001, "retriev": 6.504780114720001, "best": 1.5828514456600002, "exampl": 3.00966824644, "pagebin": 2886.54545454, "inspect": 47.0051813472, "take": 1.13961668222, "alert": 13.6041131105, "follow": 3.1392037964699995, "data": 64.15227562746, "might": 15.093304359630002, "pagelist": 1443.27272727, "further": 1.3618116315, "name": 6.612703922220001, "code": 27.164996333439998, "profil": 4.8314059647, "becaus": 3.4485554992499994, "per": 3.9195161091199995, "high": 3.44331983805, "basic": 8.190541702500001, "advis": 5.0145293746099995, "indic": 2.0826446281, "break": 2.42863698944, "honeypot": 2646.0, "tutori": 237.8426966292, "also": 6.08859060402, "typic": 2.2541530597799997, "readi": 5.15789473684, "mani": 4.17707031508, "everyth": 9.6393442623, "mean": 1.44906900329, "style": 2.37807070102, "gecko": 317.52, "will": 14.69773183152, "notic": 4.36994219653, "bin": 44.6582278482, "timeout": 1094.896551724, "ticket": 37.8, "plan": 1.5356935577500002, "primer": 61.7743190661, "therefor": 2.33401940606, "area": 1.3881262568900001, "becom": 1.12492028626, "tri": 7.417825020439999, "tag": 39.4925373134, "peopl": 3.6396148555800005, "spoof": 46.8318584071, "polit": 1.76851954996, "via": 2.2978723404299997, "want": 25.96075471704, "percentag": 6.08275862069, "argument": 5.09335899904, "genuin": 11.4793926247, "reinvent": 46.4210526316, "howev": 5.4725956566499985, "standalon": 70.875, "share": 5.56987486845, "bio": 42.336000000000006, "nonuniqu": 1443.27272727, "def": 163.67010309280002, "which": 3.015575535, "databas": 16.49454545454, "random": 14.3804347826, "skill": 3.6989748369099997, "wwwwebsitecompagenumhtml": 5773.09090908, "other": 3.02977099236, "min": 33.3529411765, "techniqu": 3.7293868921800004, "one": 5.031374786100001, "show": 1.26703910615, "second": 3.3392694383999997, "fast": 4.8729281768, "hope": 2.50884955752, "avatar": 65.0655737705, "see": 1.27242125511, "some": 7.28256880736, "https": 20.780104712, "unnecessari": 17.4845814978, "get": 8.92812956925, "easier": 7.84, "like": 6.8951140065, "implement": 3.57648118946, "through": 2.14149861738, "crash": 6.3100158982500005, "file": 11.313064133009998, "agent": 34.06866952792, "process": 10.17148958892, "both": 1.05215720061, "chromenum": 1443.27272727, "element": 21.24037460979, "except": 6.87793783168, "structur": 6.174228675149999, "rule": 6.966213251439999, "travel": 1.9655812801800001, "straightforward": 27.7552447552, "access": 3.7469907953800003, "this": 7.02655538697, "beautifulsoup": 5773.09090908, "count": 3.48157894737, "instant": 11.504347826099998, "time": 6.06764762088, "api": 168.89361702120001, "part": 2.08661365578, "userag": 4329.81818181, "chang": 3.5426956263, "requestssecond": 2886.54545454, "project": 1.7534791252500002, "differ": 8.65581431575, "herodescript": 1443.27272727, "most": 4.08385852092, "between": 1.03453668708, "multiprocess": 1512.0, "return": 4.185972930209999, "server": 152.287769784, "cooki": 132.3, "domain": 9.39408284024, "detect": 16.23866348448, "along": 1.2973768080399999, "all": 4.04587155964, "readytous": 1443.27272727, "discount": 21.570652173899997, "socal": 1134.0, "performextract": 1443.27272727, "program": 8.08556149732, "someth": 3.28152128979, "almost": 1.53584212054, "passion": 8.14571575167, "case": 10.394911607920001, "spider": 34.6637554585, "day": 2.36743215032, "discountedpric": 2886.54545454, "built": 1.99447236181, "two": 1.01379310345, "more": 3.0515120451, "and": 45.002834645849994, "list": 6.81607418855, "bidder": 92.8421052632, "automl": 1443.27272727, "smooth": 11.086592178800002, "these": 2.14830852504, "deliv": 3.4039451114899997, "yourself": 79.7788944723, "amount": 6.8108108108100005, "websit": 52.95362134692, "faster": 15.22877697842, "price": 27.870967741919998, "befor": 2.20072082062, "librari": 13.41331530925, "none": 16.26222791292, "download": 14.6457564576, "can": 27.05401200266, "avail": 5.1865403463, "linux": 65.0655737705, "way": 6.0953697305, "search": 3.2539454806299997, "onli": 2.0512953033200003, "each": 7.13848920864, "wait": 4.55421686747, "concurr": 15.413592233, "few": 1.31729173581, "cloth": 9.39686297722, "depth": 8.24299065421, "read": 6.944881889760001, "packag": 54.79881656802, "link": 12.90906626916, "common": 1.4025974025999999, "alarm": 12.96, "block": 12.81097437968, "directori": 14.768372093, "reduc": 1.98698372966, "address": 20.03100216292, "rather": 1.55692850838, "sinc": 1.08368600683, "status": 2.4636871508400002, "arg": 311.294117647, "wheel": 8.95936794582, "come": 1.32831325301, "origin": 1.13724928367, "send": 18.7526576896, "identifi": 4.60374075686, "import": 5.360796893480001, "first": 1.00761614623, "everi": 2.95835274388, "proceed": 3.4333910034599997, "when": 2.0415353951, "date": 1.63081664099, "while": 2.0883977900599997, "listingspric": 1443.27272727, "easili": 3.6938110749199997, "target": 3.2189781021900004, "within": 1.2369302688, "answer": 9.29780380674, "learn": 11.61375274325, "own": 3.5353325415600003, "pipelin": 32.1376518219, "user": 77.1053909665}, "logtfidf": {"after": 0.020490694648099998, "understand": 1.0880858756799998, "web": 16.431309733200003, "vari": 0.915094672432, "onc": 0.80753174471, "page": 9.247238421343, "heavi": 1.0602422774, "proxi": 14.774712196, "delay": 1.44278606382, "assign": 4.0337878668, "http": 1.15437112215, "scrapersspid": 7.2746685411000005, "would": 0.5573233957529, "number": 0.3864343136744, "blend": 2.50090699113, "specif": 3.1349008377050005, "function": 2.743397224782, "class": 8.997266279196001, "happi": 1.81237875643, "log": 2.61552683221, "python": 16.12262697184, "afterward": 1.5531652242899998, "disabl": 2.19044489035, "vpns": 7.2746685411000005, "sound": 1.13556799519, "tree": 1.41777488775, "requeststimeout": 7.2746685411000005, "permiss": 1.8373800586400002, "yield": 1.86708918863, "know": 1.905839388796, "their": 0.030721010245400002, "applewebkitnum": 7.2746685411000005, "bid": 4.3024916674, "notifi": 6.63638754622, "legal": 0.955536640608, "chose": 1.48637781968, "equal": 0.933027391343, "low": 1.5129205666980001, "creat": 0.222576818514, "kokatjuhha": 14.549337082200001, "how": 3.7725356554400005, "programm": 1.6450872830399998, "repost": 6.83935046985, "present": 0.455093309598, "chunk": 8.788898309339999, "than": 0.0645217244364, "hundr": 0.904145087046, "due": 0.21341214386399998, "end": 0.101476798618, "found": 0.215682248096, "correl": 2.57915918803, "jekaterina": 14.549337082200001, "interest": 1.416215333946, "slam": 3.28736941491, "deep": 1.2886734698, "color": 1.3417002006799998, "safarinum": 7.2746685411000005, "rotat": 2.1442320472, "consid": 0.214894723824, "site": 2.0374093304699996, "inum": 4.5788136131, "accompani": 1.21831042226, "snippet": 4.91038987911, "purpos": 0.803869037322, "new": 0.0177299468511, "but": 0.0647694882876, "definit": 1.1755733298, "need": 2.176440980652, "our": 0.8576392141820001, "longer": 0.7046772417749999, "background": 1.3931203261899998, "off": 0.41352852038800003, "overload": 8.223764365780001, "mozillanum": 7.2746685411000005, "header": 8.851079483480001, "hidden": 2.0557880052, "straight": 1.82519197774, "uflextop": 7.2746685411000005, "generat": 2.876729366944, "hero": 4.851266016449999, "crawler": 17.8769752416, "action": 0.598043165069, "use": 0.5549523749004001, "experi": 0.626272953933, "default": 3.0511581621399997, "nametxt": 7.2746685411000005, "generateuserag": 7.2746685411000005, "out": 0.40898473643509997, "mistak": 2.1648737360799997, "say": 1.124308561104, "format": 0.9287132518729999, "mainpric": 36.3733427055, "specifi": 1.93451151621, "browser": 12.664576081020002, "huge": 2.9583271639, "cpus": 15.48511292217, "around": 0.19387710578200001, "field": 0.5760642583510001, "reason": 0.544301552962, "ghost": 2.42121883053, "suggest": 0.563702610877, "xnum": 3.6761117252800006, "respect": 0.49733261904, "dont": 12.069955308359999, "etc": 2.8733461759400005, "such": 0.238783911224, "not": 0.155524130075, "store": 1.2374487335200002, "worker": 5.2164107051599995, "happen": 1.08640441802, "infinit": 2.7768811161599998, "handl": 4.10049800709, "stuff": 3.1490015077499995, "then": 0.24910159569269996, "customiz": 6.009002167769999, "they": 0.0594539895352, "design": 0.377239118022, "travers": 3.1272141535699998, "anoth": 0.38368908495599996, "descript": 2.7751098369, "allow": 0.720841833567, "scienc": 1.682872357782, "attract": 0.929510763678, "section": 2.266161533844, "away": 0.615957541869, "export": 3.81229383176, "filenam": 6.2068279111, "respons": 0.40991566230300003, "appear": 0.557471796986, "that": 0.07157067083351999, "provid": 0.7807113773000001, "twitter": 3.50295308141, "respond": 1.15477080241, "offer": 0.862224893804, "either": 1.377982916445, "accord": 0.243650319127, "got": 3.8591726547899996, "sent": 0.844509277088, "should": 1.018839753516, "pattern": 1.33282404788, "spot": 3.0212360288599998, "sever": 0.20973336119069996, "those": 0.17854939087299998, "with": 0.020357359127630002, "network": 0.9530830530519999, "lot": 5.934387801000001, "box": 1.41751491115, "sourc": 0.529218310751, "small": 0.307101805059, "collect": 0.99073332104, "add": 1.52875583713, "may": 0.050709995284400004, "processor": 10.911247144139999, "call": 0.2618510977952, "result": 0.272757816762, "document": 0.932547122383, "fetch": 4.07044499302, "bioinformatician": 7.2746685411000005, "content": 3.79421747862, "herotitl": 7.2746685411000005, "html": 8.53078408488, "less": 0.7692289252, "set": 0.857480056445, "caus": 0.325858567406, "ani": 0.251216716732, "question": 0.790310929014, "multi": 4.513508514690001, "write": 2.886049759508, "from": 0.0068046500263919995, "webpag": 3.9489787119499997, "abnorm": 3.3978017926599997, "visibl": 3.0891375162599997, "limit": 0.83564770926, "work": 0.327103701819, "pageresponsestatuscod": 7.2746685411000005, "num": 0.010394683048101, "pitfal": 15.551782332510001, "els": 5.083787162310001, "correspond": 2.40282912198, "curv": 2.40783363597, "for": 0.005039846326352001, "depend": 0.806969815, "dos": 14.334075753839999, "evil": 2.04307389751, "forc": 0.280652166524, "are": 0.2652072622443, "solut": 3.10692595254, "split": 1.24442043932, "better": 0.6964279406, "compani": 0.439777253097, "cpu": 8.77872017032, "current": 0.42695282784500005, "frequent": 1.4886422721700001, "preprocess": 28.430457825759998, "look": 1.2927733872, "option": 5.59384724644, "numpi": 7.2746685411000005, "veri": 0.230159793238, "extract": 28.58264126214, "higher": 0.752308398995, "brand": 1.7094517549200001, "writer": 1.0145657459, "unless": 1.69528182715, "public": 0.40464750097400004, "pars": 9.962431863339999, "flight": 4.485143970180001, "again": 0.822680463224, "compar": 0.6239191809269999, "medium": 1.94679237232, "doe": 0.5340417297169999, "even": 0.76194282417, "scraper": 23.04216323388, "could": 0.5578688168700001, "uncommon": 2.64347624975, "pagerespons": 29.098674164400002, "scrapi": 21.824005623300003, "avoid": 1.800216882582, "targetperformextract": 7.2746685411000005, "rental": 3.21422553056, "error": 3.5971708686, "investig": 1.13694148702, "move": 0.255615859253, "comment": 1.11826753454, "note": 0.353817568083, "robotstxt": 21.824005623300003, "analyz": 4.541444470319999, "machin": 2.78471916124, "into": 0.0447385896861, "automat": 3.8301700946399997, "auction": 2.95113811311, "won": 0.8404139079, "sure": 2.0086865552, "customis": 5.46787119451, "eretail": 7.2746685411000005, "where": 0.0649921387457, "figur": 0.7101721121600001, "buy": 3.2681035858599996, "local": 0.416867740206, "let": 4.995210269119999, "predetermin": 4.23448450498, "help": 1.008623164032, "main": 0.225571540588, "find": 3.834469312016, "wrong": 1.70078769102, "least": 0.96057116949, "relat": 0.639300904962, "forbidden": 2.4917326148599996, "fact": 0.5502899207949999, "hour": 0.815190981077, "who": 0.0609002329859, "decid": 0.655322871893, "univers": 0.444524211372, "hold": 1.007758234392, "post": 0.8057001527009999, "here": 2.6551145651100003, "lower": 1.484403859988, "speed": 4.060001625810001, "paramet": 2.8481901438599997, "apart": 2.2648713024, "much": 0.17749572930100002, "contain": 0.937690636472, "parallel": 10.650632077540001, "care": 2.7419829087329997, "about": 0.31421738737300003, "problem": 1.138281448546, "run": 0.885429951078, "have": 0.1034950163884, "just": 0.289531434109, "enabl": 1.26473915954, "crawl": 7.693127413039999, "requestsecond": 7.2746685411000005, "dure": 0.0982418133788, "beauti": 1.5672562984, "visit": 1.582566437666, "check": 14.98248396496, "recommend": 1.36461126863, "flag": 1.78385428972, "suspici": 3.0471714458899997, "destin": 1.8001086638400001, "tool": 1.60887117963, "start": 1.182216846455, "noth": 1.24245472939, "articl": 0.702131739574, "fluctuat": 3.0713336951700003, "singl": 0.475916769059, "them": 0.1883666538186, "request": 16.9250892516, "keep": 0.7141523446729999, "uibodi": 7.2746685411000005, "scrape": 93.64219505740002, "order": 0.44028076158600005, "wwwwebsitetoscrapecomrobotstxt": 7.2746685411000005, "whi": 1.18068843047, "autom": 2.9867028668299995, "usual": 0.545279017064, "abl": 0.599303982475, "intend": 1.707867169606, "uniqu": 1.1039173409, "the": 0.0, "build": 0.491137452091, "indefinit": 2.83415861306, "uihnum": 7.2746685411000005, "display": 1.07655944206, "framework": 2.10418454607, "too": 0.5965551547219999, "same": 0.224119299208, "thread": 3.18792857827, "bot": 15.41989296225, "there": 0.320783143404, "exist": 0.38165779408699996, "connect": 0.633605058682, "inform": 1.817814818648, "retriev": 2.321775060669, "best": 0.459227932947, "exampl": 0.8173653499979999, "pagebin": 14.549337082200001, "inspect": 9.85585390376, "take": 0.130691962197, "alert": 2.61037218162, "follow": 0.1360707332826, "data": 23.1195911112, "might": 5.378387535738001, "pagelist": 7.2746685411000005, "further": 0.308815895297, "name": 0.5833989983058001, "code": 9.49213367179, "profil": 1.5751375153100002, "becaus": 0.418029476475, "per": 1.345642048144, "high": 0.41347135962000003, "basic": 3.01310324685, "advis": 1.6123395734600001, "indic": 0.7336385419149999, "break": 0.88733019029, "honeypot": 14.37531432822, "tutori": 16.341260622, "also": 0.0879429468, "typic": 0.812774319158, "readi": 1.6405284994999998, "mani": 0.1732630324884, "everyth": 3.14541180634, "mean": 0.37092128352, "style": 0.866289529121, "gecko": 5.76054080847, "will": 2.4334384189800002, "notic": 1.47474978168, "bin": 6.211782768200001, "timeout": 22.44848321344, "ticket": 8.98405896604, "plan": 0.428982108147, "primer": 4.12348772901, "therefor": 0.847591848336, "area": 0.327954821122, "becom": 0.11771217648900001, "tri": 2.47036611664, "tag": 5.96592908944, "peopl": 0.579796735419, "spoof": 3.8465637065199996, "polit": 0.570142784146, "via": 0.831983625414, "want": 8.991275881314998, "percentag": 1.8054583135900002, "argument": 1.62793753414, "genuin": 2.44055348224, "reinvent": 3.8377530768400003, "howev": 0.4515755867375, "standalon": 4.26091776205, "share": 1.8562808992409998, "bio": 3.7456377879300002, "nonuniqu": 7.2746685411000005, "def": 8.80941130968, "which": 0.01553524153629, "databas": 4.21976513436, "random": 3.9454428130199997, "skill": 1.30805571015, "wwwwebsitecompagenumhtml": 29.098674164400002, "other": 0.02962424375928, "min": 3.5071459596699994, "techniqu": 1.31624384807, "one": 0.0312767582275, "show": 0.236682766013, "second": 0.32141929014, "fast": 1.5836950247400001, "hope": 0.919824304455, "avatar": 4.17539558861, "see": 0.240921585492, "some": 0.2770145634515, "https": 3.0339960247400004, "unnecessari": 2.8613194352999995, "get": 2.89884502891, "easier": 2.05923883436, "like": 0.83432145927, "implement": 1.27437940907, "through": 0.1367173837698, "crash": 1.8421381960799998, "file": 3.9820376616899997, "agent": 11.5914931048, "process": 3.16697519415, "both": 0.050842533389300004, "chromenum": 7.2746685411000005, "element": 7.728113302892999, "except": 2.16809804852, "structur": 2.1653150254050004, "rule": 2.219109694148, "travel": 0.675788018461, "straightforward": 3.3234248225200003, "access": 1.255611765432, "this": 0.026505143367499998, "beautifulsoup": 29.098674164400002, "count": 1.24748591139, "instant": 2.4427250357499997, "time": 0.0672691131756, "api": 8.87224370214, "part": 0.08479062196560001, "userag": 21.824005623300003, "chang": 0.49882687517400004, "requestssecond": 14.549337082200001, "project": 0.561601885907, "differ": 1.486247849184, "herodescript": 7.2746685411000005, "most": 0.08299158518239999, "between": 0.033953681165299995, "multiprocess": 13.256082752360001, "return": 0.9993806057760001, "server": 14.5579103684, "cooki": 8.38384978112, "domain": 2.24008000599, "detect": 5.0663482347899995, "along": 0.260344385917, "all": 0.04561052839119999, "readytous": 7.2746685411000005, "discount": 3.0713336951700003, "socal": 7.033506484289999, "performextract": 7.2746685411000005, "program": 2.8151423150599997, "someth": 1.18830712273, "almost": 0.42907884333400004, "passion": 2.0974921144, "case": 2.767843882223, "spider": 3.5456946297900003, "day": 0.33731741263400006, "discountedpric": 14.549337082200001, "built": 0.690379535065, "two": 0.0136988443582, "more": 0.05107479479999999, "and": 0.002834556392862, "list": 1.54922880753, "bidder": 4.5309002574, "automl": 7.2746685411000005, "smooth": 2.4057364663799996, "these": 0.1430672388016, "deliv": 1.22493508587, "yourself": 9.84194010153, "amount": 2.459696658597, "websit": 19.422774499926, "faster": 4.06007935934, "price": 9.98515218112, "befor": 0.191275543759, "librari": 4.934049904715, "none": 5.61020300652, "download": 2.6841506319, "can": 3.7338452170619996, "avail": 1.642363758867, "linux": 4.17539558861, "way": 0.9904575496750001, "search": 1.1798682540899998, "onli": 0.050648536658199995, "each": 1.042450135824, "wait": 1.51605358782, "concurr": 2.7352497326800003, "few": 0.275577913653, "cloth": 3.0944574543799996, "depth": 2.10936322154, "read": 2.51817804264, "packag": 14.404309144330004, "link": 4.597022441069999, "common": 0.338325805271, "alarm": 2.56186769092, "block": 4.65603126352, "directori": 2.6924878733399997, "reduc": 0.686617775143, "address": 7.359597227290001, "rather": 0.442714975539, "sinc": 0.0803681994577, "status": 0.9016590696060001, "arg": 5.74073818118, "wheel": 2.1926996827400003, "come": 0.28390990653000003, "origin": 0.128612437587, "send": 6.6094878669, "identifi": 1.667444000944, "import": 1.171273108264, "first": 0.0075872898121599995, "everi": 0.782970854842, "proceed": 1.23354840355, "when": 0.0411099777168, "date": 0.489080896097, "while": 0.08649996758760002, "listingspric": 7.2746685411000005, "easili": 1.3066587367, "target": 1.1690639496200002, "within": 0.21263272059799998, "answer": 3.0732620838, "learn": 4.213760323724999, "own": 0.492585232263, "pipelin": 3.47002829672, "user": 20.4258810688}, "logidf": {"after": 0.020490694648099998, "understand": 1.0880858756799998, "web": 1.6431309733200001, "vari": 0.915094672432, "onc": 0.403765872355, "page": 0.711326032411, "heavi": 1.0602422774, "proxi": 3.693678049, "delay": 1.44278606382, "assign": 1.3445959556, "http": 1.15437112215, "scrapersspid": 7.2746685411000005, "would": 0.0796176279647, "number": 0.0966085784186, "blend": 2.50090699113, "specif": 0.626980167541, "function": 0.914465741594, "class": 0.7497721899330001, "happi": 1.81237875643, "log": 2.61552683221, "python": 4.03065674296, "afterward": 1.5531652242899998, "disabl": 2.19044489035, "vpns": 7.2746685411000005, "sound": 1.13556799519, "tree": 1.41777488775, "requeststimeout": 7.2746685411000005, "permiss": 1.8373800586400002, "yield": 1.86708918863, "know": 0.952919694398, "their": 0.015360505122700001, "applewebkitnum": 7.2746685411000005, "bid": 2.1512458337, "notifi": 3.31819377311, "legal": 0.955536640608, "chose": 1.48637781968, "equal": 0.933027391343, "low": 0.7564602833490001, "creat": 0.222576818514, "kokatjuhha": 7.2746685411000005, "how": 0.47156695693000006, "programm": 1.6450872830399998, "repost": 6.83935046985, "present": 0.227546654799, "chunk": 4.394449154669999, "than": 0.0322608622182, "hundr": 0.904145087046, "due": 0.21341214386399998, "end": 0.101476798618, "found": 0.107841124048, "correl": 2.57915918803, "jekaterina": 7.2746685411000005, "interest": 0.47207177798199995, "slam": 3.28736941491, "deep": 1.2886734698, "color": 1.3417002006799998, "safarinum": 7.2746685411000005, "rotat": 2.1442320472, "consid": 0.214894723824, "site": 0.6791364434899999, "inum": 4.5788136131, "accompani": 1.21831042226, "snippet": 4.91038987911, "purpos": 0.803869037322, "new": 0.0177299468511, "but": 0.0161923720719, "definit": 1.1755733298, "need": 0.362740163442, "our": 0.8576392141820001, "longer": 0.7046772417749999, "background": 1.3931203261899998, "off": 0.41352852038800003, "overload": 4.1118821828900005, "mozillanum": 7.2746685411000005, "header": 4.425539741740001, "hidden": 2.0557880052, "straight": 1.82519197774, "uflextop": 7.2746685411000005, "generat": 0.719182341736, "hero": 1.61708867215, "crawler": 5.958991747200001, "action": 0.598043165069, "use": 0.0292080197316, "experi": 0.626272953933, "default": 3.0511581621399997, "nametxt": 7.2746685411000005, "generateuserag": 7.2746685411000005, "out": 0.0584263909193, "mistak": 2.1648737360799997, "say": 0.562154280552, "format": 0.9287132518729999, "mainpric": 7.2746685411000005, "specifi": 1.93451151621, "browser": 4.22152536034, "huge": 1.47916358195, "cpus": 5.16170430739, "around": 0.19387710578200001, "field": 0.5760642583510001, "reason": 0.544301552962, "ghost": 2.42121883053, "suggest": 0.563702610877, "xnum": 3.6761117252800006, "respect": 0.49733261904, "dont": 6.0349776541799995, "etc": 1.4366730879700003, "such": 0.059695977806, "not": 0.0155524130075, "store": 1.2374487335200002, "worker": 1.3041026762899999, "happen": 1.08640441802, "infinit": 2.7768811161599998, "handl": 1.36683266903, "stuff": 3.1490015077499995, "then": 0.08303386523089999, "customiz": 6.009002167769999, "they": 0.0297269947676, "design": 0.377239118022, "travers": 3.1272141535699998, "anoth": 0.127896361652, "descript": 1.38755491845, "allow": 0.24028061118900002, "scienc": 0.841436178891, "attract": 0.929510763678, "section": 0.755387177948, "away": 0.615957541869, "export": 1.90614691588, "filenam": 6.2068279111, "respons": 0.40991566230300003, "appear": 0.278735898493, "that": 0.00397614837964, "provid": 0.19517784432500002, "twitter": 3.50295308141, "respond": 1.15477080241, "offer": 0.431112446902, "either": 0.459327638815, "accord": 0.243650319127, "got": 1.2863908849299999, "sent": 0.844509277088, "should": 0.509419876758, "pattern": 1.33282404788, "spot": 1.5106180144299999, "sever": 0.06991112039689999, "those": 0.17854939087299998, "with": 0.00119749171339, "network": 0.9530830530519999, "lot": 1.4835969502500002, "box": 1.41751491115, "sourc": 0.529218310751, "small": 0.307101805059, "collect": 0.49536666052, "add": 1.52875583713, "may": 0.050709995284400004, "processor": 3.63708238138, "call": 0.0654627744488, "result": 0.136378908381, "document": 0.932547122383, "fetch": 4.07044499302, "bioinformatician": 7.2746685411000005, "content": 1.26473915954, "herotitl": 7.2746685411000005, "html": 4.26539204244, "less": 0.3846144626, "set": 0.171496011289, "caus": 0.325858567406, "ani": 0.125608358366, "question": 0.790310929014, "multi": 4.513508514690001, "write": 0.721512439877, "from": 0.000567054168866, "webpag": 3.9489787119499997, "abnorm": 3.3978017926599997, "visibl": 1.5445687581299998, "limit": 0.41782385463, "work": 0.109034567273, "pageresponsestatuscod": 7.2746685411000005, "num": 0.00031499039539700004, "pitfal": 5.18392744417, "els": 1.6945957207700002, "correspond": 1.20141456099, "curv": 2.40783363597, "for": 0.00031499039539700004, "depend": 0.806969815, "dos": 3.5835189384599997, "evil": 2.04307389751, "forc": 0.280652166524, "are": 0.0294674735827, "solut": 1.55346297627, "split": 1.24442043932, "better": 0.6964279406, "compani": 0.439777253097, "cpu": 4.38936008516, "current": 0.42695282784500005, "frequent": 0.7443211360850001, "preprocess": 7.1076144564399995, "look": 0.6463866936, "option": 1.39846181161, "numpi": 7.2746685411000005, "veri": 0.230159793238, "extract": 2.04161723301, "higher": 0.752308398995, "brand": 1.7094517549200001, "writer": 1.0145657459, "unless": 1.69528182715, "public": 0.20232375048700002, "pars": 4.9812159316699995, "flight": 1.4950479900600002, "again": 0.411340231612, "compar": 0.6239191809269999, "medium": 1.94679237232, "doe": 0.5340417297169999, "even": 0.152388564834, "scraper": 5.76054080847, "could": 0.18595627229000003, "uncommon": 2.64347624975, "pagerespons": 7.2746685411000005, "scrapi": 7.2746685411000005, "avoid": 0.900108441291, "targetperformextract": 7.2746685411000005, "rental": 3.21422553056, "error": 1.7985854343, "investig": 1.13694148702, "move": 0.255615859253, "comment": 1.11826753454, "note": 0.353817568083, "robotstxt": 7.2746685411000005, "analyz": 2.2707222351599996, "machin": 1.39235958062, "into": 0.0149128632287, "automat": 1.9150850473199998, "auction": 2.95113811311, "won": 0.8404139079, "sure": 2.0086865552, "customis": 5.46787119451, "eretail": 7.2746685411000005, "where": 0.0649921387457, "figur": 0.7101721121600001, "buy": 1.6340517929299998, "local": 0.416867740206, "let": 1.2488025672799998, "predetermin": 4.23448450498, "help": 0.336207721344, "main": 0.225571540588, "find": 0.547781330288, "wrong": 1.70078769102, "least": 0.480285584745, "relat": 0.21310030165399999, "forbidden": 2.4917326148599996, "fact": 0.5502899207949999, "hour": 0.815190981077, "who": 0.0609002329859, "decid": 0.655322871893, "univers": 0.222262105686, "hold": 0.503879117196, "post": 0.8057001527009999, "here": 0.8850381883700001, "lower": 0.742201929994, "speed": 1.3533338752700002, "paramet": 2.8481901438599997, "apart": 1.1324356512, "much": 0.17749572930100002, "contain": 0.468845318236, "parallel": 1.52151886822, "care": 0.9139943029109999, "about": 0.0628434774746, "problem": 0.569140724273, "run": 0.442714975539, "have": 0.0147850023412, "just": 0.289531434109, "enabl": 1.26473915954, "crawl": 3.8465637065199996, "requestsecond": 7.2746685411000005, "dure": 0.0491209066894, "beauti": 1.5672562984, "visit": 0.791283218833, "check": 1.87281049562, "recommend": 1.36461126863, "flag": 1.78385428972, "suspici": 3.0471714458899997, "destin": 1.8001086638400001, "tool": 1.60887117963, "start": 0.236443369291, "noth": 1.24245472939, "articl": 0.702131739574, "fluctuat": 3.0713336951700003, "singl": 0.475916769059, "them": 0.0941833269093, "request": 1.12833928344, "keep": 0.7141523446729999, "uibodi": 7.2746685411000005, "scrape": 4.2564634117, "order": 0.22014038079300002, "wwwwebsitetoscrapecomrobotstxt": 7.2746685411000005, "whi": 1.18068843047, "autom": 2.9867028668299995, "usual": 0.545279017064, "abl": 0.599303982475, "intend": 0.853933584803, "uniqu": 1.1039173409, "the": 0.0, "build": 0.491137452091, "indefinit": 2.83415861306, "uihnum": 7.2746685411000005, "display": 1.07655944206, "framework": 2.10418454607, "too": 0.5965551547219999, "same": 0.112059649604, "thread": 3.18792857827, "bot": 5.13996432075, "there": 0.0400978929255, "exist": 0.38165779408699996, "connect": 0.633605058682, "inform": 0.454453704662, "retriev": 0.773925020223, "best": 0.459227932947, "exampl": 0.40868267499899996, "pagebin": 7.2746685411000005, "inspect": 2.46396347594, "take": 0.130691962197, "alert": 2.61037218162, "follow": 0.045356911094199995, "data": 1.2168205848, "might": 0.7683410765340001, "pagelist": 7.2746685411000005, "further": 0.308815895297, "name": 0.09723316638430002, "code": 1.35601909597, "profil": 1.5751375153100002, "becaus": 0.139343158825, "per": 0.672821024072, "high": 0.13782378654000002, "basic": 1.00436774895, "advis": 1.6123395734600001, "indic": 0.7336385419149999, "break": 0.88733019029, "honeypot": 7.18765716411, "tutori": 4.0853151555, "also": 0.0146571578, "typic": 0.812774319158, "readi": 1.6405284994999998, "mani": 0.0433157581221, "everyth": 1.57270590317, "mean": 0.37092128352, "style": 0.866289529121, "gecko": 5.76054080847, "will": 0.202786534915, "notic": 1.47474978168, "bin": 3.1058913841000004, "timeout": 5.61212080336, "ticket": 2.24601474151, "plan": 0.428982108147, "primer": 4.12348772901, "therefor": 0.847591848336, "area": 0.327954821122, "becom": 0.11771217648900001, "tri": 0.61759152916, "tag": 2.98296454472, "peopl": 0.193265578473, "spoof": 3.8465637065199996, "polit": 0.570142784146, "via": 0.831983625414, "want": 0.6916366062549999, "percentag": 1.8054583135900002, "argument": 1.62793753414, "genuin": 2.44055348224, "reinvent": 3.8377530768400003, "howev": 0.0903151173475, "standalon": 4.26091776205, "share": 0.618760299747, "bio": 3.7456377879300002, "nonuniqu": 7.2746685411000005, "def": 4.40470565484, "which": 0.00517841384543, "databas": 2.10988256718, "random": 1.9727214065099998, "skill": 1.30805571015, "wwwwebsitecompagenumhtml": 7.2746685411000005, "other": 0.00987474791976, "min": 3.5071459596699994, "techniqu": 1.31624384807, "one": 0.0062553516455, "show": 0.236682766013, "second": 0.10713976337999999, "fast": 1.5836950247400001, "hope": 0.919824304455, "avatar": 4.17539558861, "see": 0.240921585492, "some": 0.0395735090645, "https": 3.0339960247400004, "unnecessari": 2.8613194352999995, "get": 0.579769005782, "easier": 2.05923883436, "like": 0.139053576545, "implement": 1.27437940907, "through": 0.0683586918849, "crash": 1.8421381960799998, "file": 1.32734588723, "agent": 1.4489366381, "process": 0.527829199025, "both": 0.050842533389300004, "chromenum": 7.2746685411000005, "element": 0.8586792558769999, "except": 0.54202451213, "structur": 0.7217716751350001, "rule": 0.554777423537, "travel": 0.675788018461, "straightforward": 3.3234248225200003, "access": 0.627805882716, "this": 0.0037864490525, "beautifulsoup": 7.2746685411000005, "count": 1.24748591139, "instant": 2.4427250357499997, "time": 0.0112115188626, "api": 4.43612185107, "part": 0.04239531098280001, "userag": 7.2746685411000005, "chang": 0.166275625058, "requestssecond": 7.2746685411000005, "project": 0.561601885907, "differ": 0.212321121312, "herodescript": 7.2746685411000005, "most": 0.020747896295599998, "between": 0.033953681165299995, "multiprocess": 6.6280413761800006, "return": 0.333126868592, "server": 3.6394775921, "cooki": 4.19192489056, "domain": 2.24008000599, "detect": 1.68878274493, "along": 0.260344385917, "all": 0.011402632097799998, "readytous": 7.2746685411000005, "discount": 3.0713336951700003, "socal": 7.033506484289999, "performextract": 7.2746685411000005, "program": 0.7037855787649999, "someth": 1.18830712273, "almost": 0.42907884333400004, "passion": 2.0974921144, "case": 0.395406268889, "spider": 3.5456946297900003, "day": 0.16865870631700003, "discountedpric": 7.2746685411000005, "built": 0.690379535065, "two": 0.0136988443582, "more": 0.017024931599999998, "and": 6.29901420636e-05, "list": 0.309845761506, "bidder": 4.5309002574, "automl": 7.2746685411000005, "smooth": 2.4057364663799996, "these": 0.0715336194008, "deliv": 1.22493508587, "yourself": 3.28064670051, "amount": 0.819898886199, "websit": 0.924894023806, "faster": 2.03003967967, "price": 1.24814402264, "befor": 0.0956377718795, "librari": 0.986809980943, "none": 1.40255075163, "download": 2.6841506319, "can": 0.162341096394, "avail": 0.547454586289, "linux": 4.17539558861, "way": 0.19809150993500002, "search": 1.1798682540899998, "onli": 0.025324268329099998, "each": 0.173741689304, "wait": 1.51605358782, "concurr": 2.7352497326800003, "few": 0.275577913653, "cloth": 1.5472287271899998, "depth": 2.10936322154, "read": 0.83939268088, "packag": 2.0577584491900005, "link": 0.7661704068449999, "common": 0.338325805271, "alarm": 2.56186769092, "block": 1.16400781588, "directori": 2.6924878733399997, "reduc": 0.686617775143, "address": 1.05137103247, "rather": 0.442714975539, "sinc": 0.0803681994577, "status": 0.9016590696060001, "arg": 5.74073818118, "wheel": 2.1926996827400003, "come": 0.28390990653000003, "origin": 0.128612437587, "send": 1.32189757338, "identifi": 0.833722000472, "import": 0.292818277066, "first": 0.0075872898121599995, "everi": 0.391485427421, "proceed": 1.23354840355, "when": 0.0205549888584, "date": 0.489080896097, "while": 0.04324998379380001, "listingspric": 7.2746685411000005, "easili": 1.3066587367, "target": 1.1690639496200002, "within": 0.21263272059799998, "answer": 1.5366310419, "learn": 0.842752064745, "own": 0.164195077421, "pipelin": 3.47002829672, "user": 2.04258810688}, "freq": {"after": 1, "understand": 1, "web": 10, "vari": 1, "onc": 2, "page": 13, "heavi": 1, "proxi": 4, "delay": 1, "assign": 3, "http": 1, "scrapersspid": 1, "would": 7, "number": 4, "blend": 1, "specif": 5, "function": 3, "class": 12, "happi": 1, "log": 1, "python": 4, "afterward": 1, "disabl": 1, "vpns": 1, "sound": 1, "tree": 1, "requeststimeout": 1, "permiss": 1, "yield": 1, "know": 2, "their": 2, "applewebkitnum": 1, "bid": 2, "notifi": 2, "legal": 1, "chose": 1, "equal": 1, "low": 2, "creat": 1, "kokatjuhha": 2, "how": 8, "programm": 1, "repost": 1, "present": 2, "chunk": 2, "than": 2, "hundr": 1, "due": 1, "end": 1, "found": 2, "correl": 1, "jekaterina": 2, "interest": 3, "slam": 1, "deep": 1, "color": 1, "safarinum": 1, "rotat": 1, "consid": 1, "site": 3, "inum": 1, "accompani": 1, "snippet": 1, "purpos": 1, "new": 1, "but": 4, "definit": 1, "need": 6, "our": 1, "longer": 1, "background": 1, "off": 1, "overload": 2, "mozillanum": 1, "header": 2, "hidden": 1, "straight": 1, "uflextop": 1, "generat": 4, "hero": 3, "crawler": 3, "action": 1, "use": 19, "experi": 1, "default": 1, "nametxt": 1, "generateuserag": 1, "out": 7, "mistak": 1, "say": 2, "format": 1, "mainpric": 5, "specifi": 1, "browser": 3, "huge": 2, "cpus": 3, "around": 1, "field": 1, "reason": 1, "ghost": 1, "suggest": 1, "xnum": 1, "respect": 1, "dont": 2, "etc": 2, "such": 4, "not": 10, "store": 1, "worker": 4, "happen": 1, "infinit": 1, "handl": 3, "stuff": 1, "then": 3, "customiz": 1, "they": 2, "design": 1, "travers": 1, "anoth": 3, "descript": 2, "allow": 3, "scienc": 2, "attract": 1, "section": 3, "away": 1, "export": 2, "filenam": 1, "respons": 1, "appear": 2, "that": 18, "provid": 4, "twitter": 1, "respond": 1, "offer": 2, "either": 3, "accord": 1, "got": 3, "sent": 1, "should": 2, "pattern": 1, "spot": 2, "sever": 3, "those": 1, "with": 17, "network": 1, "lot": 4, "box": 1, "sourc": 1, "small": 1, "collect": 2, "add": 1, "may": 1, "processor": 3, "call": 4, "result": 2, "document": 1, "fetch": 1, "bioinformatician": 1, "content": 3, "herotitl": 1, "html": 2, "less": 2, "set": 5, "caus": 1, "ani": 2, "question": 1, "multi": 1, "write": 4, "from": 12, "webpag": 1, "abnorm": 1, "visibl": 2, "limit": 2, "work": 3, "pageresponsestatuscod": 1, "num": 33, "pitfal": 3, "els": 3, "correspond": 2, "curv": 1, "for": 16, "depend": 1, "dos": 4, "evil": 1, "forc": 1, "are": 9, "solut": 2, "split": 1, "better": 1, "compani": 1, "cpu": 2, "current": 1, "frequent": 2, "preprocess": 4, "look": 2, "option": 4, "numpi": 1, "veri": 1, "extract": 14, "higher": 1, "brand": 1, "writer": 1, "unless": 1, "public": 2, "pars": 2, "flight": 3, "again": 2, "compar": 1, "medium": 1, "doe": 1, "even": 5, "scraper": 4, "could": 3, "uncommon": 1, "pagerespons": 4, "scrapi": 3, "avoid": 2, "targetperformextract": 1, "rental": 1, "error": 2, "investig": 1, "move": 1, "comment": 1, "note": 1, "robotstxt": 3, "analyz": 2, "machin": 2, "into": 3, "automat": 2, "auction": 1, "won": 1, "sure": 1, "customis": 1, "eretail": 1, "where": 1, "figur": 1, "buy": 2, "local": 1, "let": 4, "predetermin": 1, "help": 3, "main": 1, "find": 7, "wrong": 1, "least": 2, "relat": 3, "forbidden": 1, "fact": 1, "hour": 1, "who": 1, "decid": 1, "univers": 2, "hold": 2, "post": 1, "here": 3, "lower": 2, "speed": 3, "paramet": 1, "apart": 2, "much": 1, "contain": 2, "parallel": 7, "care": 3, "about": 5, "problem": 2, "run": 2, "have": 7, "just": 1, "enabl": 1, "crawl": 2, "requestsecond": 1, "dure": 2, "beauti": 1, "visit": 2, "check": 8, "recommend": 1, "flag": 1, "suspici": 1, "destin": 1, "tool": 1, "start": 5, "noth": 1, "articl": 1, "fluctuat": 1, "singl": 1, "them": 2, "request": 15, "keep": 1, "uibodi": 1, "scrape": 22, "order": 2, "wwwwebsitetoscrapecomrobotstxt": 1, "whi": 1, "autom": 1, "usual": 1, "abl": 1, "intend": 2, "uniqu": 1, "the": 106, "build": 1, "indefinit": 1, "uihnum": 1, "display": 1, "framework": 1, "too": 1, "same": 2, "thread": 1, "bot": 3, "there": 8, "exist": 1, "connect": 1, "inform": 4, "retriev": 3, "best": 1, "exampl": 2, "pagebin": 2, "inspect": 4, "take": 1, "alert": 1, "follow": 3, "data": 19, "might": 7, "pagelist": 1, "further": 1, "name": 6, "code": 7, "profil": 1, "becaus": 3, "per": 2, "high": 3, "basic": 3, "advis": 1, "indic": 1, "break": 1, "honeypot": 2, "tutori": 4, "also": 6, "typic": 1, "readi": 1, "mani": 4, "everyth": 2, "mean": 1, "style": 1, "gecko": 1, "will": 12, "notic": 1, "bin": 2, "timeout": 4, "ticket": 4, "plan": 1, "primer": 1, "therefor": 1, "area": 1, "becom": 1, "tri": 4, "tag": 2, "peopl": 3, "spoof": 1, "polit": 1, "via": 1, "want": 13, "percentag": 1, "argument": 1, "genuin": 1, "reinvent": 1, "howev": 5, "standalon": 1, "share": 3, "bio": 1, "nonuniqu": 1, "def": 2, "which": 3, "databas": 2, "random": 2, "skill": 1, "wwwwebsitecompagenumhtml": 4, "other": 3, "min": 1, "techniqu": 1, "one": 5, "show": 1, "second": 3, "fast": 1, "hope": 1, "avatar": 1, "see": 1, "some": 7, "https": 1, "unnecessari": 1, "get": 5, "easier": 1, "like": 6, "implement": 1, "through": 2, "crash": 1, "file": 3, "agent": 8, "process": 6, "both": 1, "chromenum": 1, "element": 9, "except": 4, "structur": 3, "rule": 4, "travel": 1, "straightforward": 1, "access": 2, "this": 7, "beautifulsoup": 4, "count": 1, "instant": 1, "time": 6, "api": 2, "part": 2, "userag": 3, "chang": 3, "requestssecond": 2, "project": 1, "differ": 7, "herodescript": 1, "most": 4, "between": 1, "multiprocess": 2, "return": 3, "server": 4, "cooki": 2, "domain": 1, "detect": 3, "along": 1, "all": 4, "readytous": 1, "discount": 1, "socal": 1, "performextract": 1, "program": 4, "someth": 1, "almost": 1, "passion": 1, "case": 7, "spider": 1, "day": 2, "discountedpric": 2, "built": 1, "two": 1, "more": 3, "and": 45, "list": 5, "bidder": 1, "automl": 1, "smooth": 1, "these": 2, "deliv": 1, "yourself": 3, "amount": 3, "websit": 21, "faster": 2, "price": 8, "befor": 2, "librari": 5, "none": 4, "download": 1, "can": 23, "avail": 3, "linux": 1, "way": 5, "search": 1, "onli": 2, "each": 6, "wait": 1, "concurr": 1, "few": 1, "cloth": 2, "depth": 1, "read": 3, "packag": 7, "link": 6, "common": 1, "alarm": 1, "block": 4, "directori": 1, "reduc": 1, "address": 7, "rather": 1, "sinc": 1, "status": 1, "arg": 1, "wheel": 1, "come": 1, "origin": 1, "send": 5, "identifi": 2, "import": 4, "first": 1, "everi": 2, "proceed": 1, "when": 2, "date": 1, "while": 2, "listingspric": 1, "easili": 1, "target": 1, "within": 1, "answer": 2, "learn": 5, "own": 3, "pipelin": 1, "user": 10}, "idf": {"after": 1.02070207021, "understand": 2.96858638743, "web": 5.17133550489, "vari": 2.4970116388799997, "onc": 1.4974533106999999, "page": 2.03669018602, "heavi": 2.88707037643, "proxi": 40.1924050633, "delay": 4.23247134098, "assign": 3.83663605607, "http": 3.17202797203, "scrapersspid": 1443.27272727, "would": 1.0828729281799998, "number": 1.10142916609, "blend": 12.1935483871, "specif": 1.8719490626099997, "function": 2.495441685, "class": 2.11651779763, "happi": 6.125, "log": 13.6744186047, "python": 56.2978723404, "afterward": 4.72640666865, "disabl": 8.93918918919, "vpns": 1443.27272727, "sound": 3.11294117647, "tree": 4.127925117, "requeststimeout": 1443.27272727, "permiss": 6.280063291139999, "yield": 6.46943765281, "know": 2.59327017315, "their": 1.01547908405, "applewebkitnum": 1443.27272727, "bid": 8.595560368160001, "notifi": 27.610434782600002, "legal": 2.6000655093400002, "chose": 4.42105263158, "equal": 2.542193755, "low": 2.13072070863, "creat": 1.2492917847, "kokatjuhha": 1443.27272727, "how": 1.60250328051, "programm": 5.181462140990001, "repost": 933.882352941, "present": 1.25551601423, "chunk": 81.0, "than": 1.03278688525, "hundr": 2.4698195395099996, "due": 1.23789473684, "end": 1.10680423871, "found": 1.11387076405, "correl": 13.1860465116, "jekaterina": 1443.27272727, "interest": 1.60331246213, "slam": 26.7723440135, "deep": 3.6279707495399998, "color": 3.8255421686699997, "safarinum": 1443.27272727, "rotat": 8.53548387097, "consid": 1.2397313759200002, "site": 1.9721739130400002, "inum": 97.3987730061, "accompani": 3.38146964856, "snippet": 135.692307692, "purpos": 2.23416830847, "new": 1.0178880554, "but": 1.01632417899, "definit": 3.24, "need": 1.4372623574099999, "our": 2.35758835759, "longer": 2.02319357716, "background": 4.02739726027, "off": 1.5121440137200002, "overload": 61.0615384615, "mozillanum": 1443.27272727, "header": 83.55789473680001, "hidden": 7.81299212598, "straight": 6.203985932, "uflextop": 1443.27272727, "generat": 2.05275407292, "hero": 5.0384005077800005, "crawler": 387.219512195, "action": 1.81855670103, "use": 1.0296387573799999, "experi": 1.87062566278, "default": 21.1398135819, "nametxt": 1443.27272727, "generateuserag": 1443.27272727, "out": 1.06016694491, "mistak": 8.71350164654, "say": 1.7544480053, "format": 2.53125, "mainpric": 1443.27272727, "specifi": 6.920662598080001, "browser": 68.1373390558, "huge": 4.38927287808, "cpus": 174.46153846200002, "around": 1.21394708671, "field": 1.7790228597, "reason": 1.72340425532, "ghost": 11.2595744681, "suggest": 1.7571665744299998, "xnum": 39.4925373134, "respect": 1.6443293630200002, "dont": 417.78947368400003, "etc": 4.2066772655, "such": 1.06151377374, "not": 1.01567398119, "store": 3.44680851064, "worker": 3.6843815270399998, "happen": 2.96359902931, "infinit": 16.0688259109, "handl": 3.9229058561900003, "stuff": 23.3127753304, "then": 1.08657860516, "customiz": 407.07692307699995, "they": 1.03017325287, "design": 1.45825296225, "travers": 22.8103448276, "anoth": 1.13643521832, "descript": 4.00504540868, "allow": 1.2716059271100002, "scienc": 2.31969608416, "attract": 2.53326950694, "section": 2.1284354471099998, "away": 1.85142857143, "export": 6.727118644069999, "filenam": 496.125, "respons": 1.5066907089300001, "appear": 1.3214582986499999, "that": 1.00398406375, "provid": 1.21552714187, "twitter": 33.213389121300004, "respond": 3.17329602239, "offer": 1.53896859248, "either": 1.5830092731099998, "accord": 1.27589809531, "got": 3.61969904241, "sent": 2.32683570277, "should": 1.6643254009900001, "pattern": 3.79173632673, "spot": 4.52952924394, "sever": 1.07241286139, "those": 1.19548192771, "with": 1.0011982089899998, "network": 2.59369384088, "lot": 4.40877534018, "box": 4.12685209254, "sourc": 1.69760479042, "small": 1.3594793629, "collect": 1.64109985528, "add": 4.61243463103, "may": 1.05201775893, "processor": 37.980861244, "call": 1.0676529926, "result": 1.14611608432, "document": 2.5409731114, "fetch": 58.583025830299995, "bioinformatician": 1443.27272727, "content": 3.5421686747, "herotitl": 1443.27272727, "html": 71.1928251121, "less": 1.46904783936, "set": 1.18707940781, "caus": 1.38521943984, "ani": 1.13383802314, "question": 2.20408163265, "multi": 91.2413793103, "write": 2.0575427682700003, "from": 1.00056721497, "webpag": 51.8823529412, "abnorm": 29.8983050847, "visibl": 4.68595041322, "limit": 1.5186531471200002, "work": 1.11520089913, "pageresponsestatuscod": 1443.27272727, "num": 1.00031504001, "pitfal": 178.38202247200002, "els": 5.44444444444, "correspond": 3.32481675393, "curv": 11.1098670399, "for": 1.00031504001, "depend": 2.2411067193700003, "dos": 36.0, "evil": 7.714285714289999, "forc": 1.32399299475, "are": 1.02990593578, "solut": 4.7278141751, "split": 3.4709226060300002, "better": 2.0065722952500002, "compani": 1.5523613963, "cpu": 80.5888324873, "current": 1.5325803649, "frequent": 2.10501193317, "preprocess": 1221.23076923, "look": 1.9086318826599997, "option": 4.04896710023, "numpi": 1443.27272727, "veri": 1.25880114177, "extract": 7.703056768560001, "higher": 2.1218925421, "brand": 5.5259310824900005, "writer": 2.75816539263, "unless": 5.44818119423, "public": 1.22424429365, "pars": 145.651376147, "flight": 4.4595505618, "again": 1.50883862384, "compar": 1.8662278123900002, "medium": 7.00617828773, "doe": 1.70581282905, "even": 1.16461267606, "scraper": 317.52, "could": 1.2043695949, "uncommon": 14.0620017715, "pagerespons": 1443.27272727, "scrapi": 1443.27272727, "avoid": 2.45986984816, "targetperformextract": 1443.27272727, "rental": 24.8840125392, "error": 6.04109589041, "investig": 3.11721971333, "move": 1.29125660838, "comment": 3.05954904606, "note": 1.42449528937, "robotstxt": 1443.27272727, "analyz": 9.68639414277, "machin": 4.02433460076, "into": 1.01502461479, "automat": 6.787516032490001, "auction": 19.1277108434, "won": 2.31732593782, "sure": 7.453521126760001, "customis": 236.955223881, "eretail": 1443.27272727, "where": 1.06715063521, "figur": 2.0343413634, "buy": 5.12459651388, "local": 1.51720183486, "let": 3.48616600791, "predetermin": 69.0260869565, "help": 1.39962972759, "main": 1.25303867403, "find": 1.7294117647099998, "wrong": 5.478260869570001, "least": 1.6165359943000002, "relat": 1.23750876919, "forbidden": 12.082191780799999, "fact": 1.73375559681, "hour": 2.25960717336, "who": 1.06279287723, "decid": 1.9257641921400002, "univers": 1.24889867841, "hold": 1.6551292744, "post": 2.23826307627, "here": 2.42307692308, "lower": 2.10055570257, "speed": 3.8703071672400005, "paramet": 17.256521739100002, "apart": 3.1032056294, "much": 1.1942229577299999, "contain": 1.59814777532, "parallel": 4.57917507932, "care": 2.49426551453, "about": 1.06486015159, "problem": 1.76674827509, "run": 1.55692850838, "have": 1.0148948411399998, "just": 1.33580143037, "enabl": 3.5421686747, "crawl": 46.8318584071, "requestsecond": 1443.27272727, "dure": 1.0503473370799998, "beauti": 4.79347826087, "visit": 2.20622568093, "check": 6.50655737705, "recommend": 3.9142011834300003, "flag": 5.95275590551, "suspici": 21.0557029178, "destin": 6.0503048780499995, "tool": 4.99716713881, "start": 1.26673581744, "noth": 3.46410648047, "articl": 2.01805008262, "fluctuat": 21.570652173899997, "singl": 1.60948905109, "them": 1.09876115994, "request": 3.09051975861, "keep": 2.04245465071, "uibodi": 1443.27272727, "scrape": 70.56, "order": 1.24625166811, "wwwwebsitetoscrapecomrobotstxt": 1443.27272727, "whi": 3.2566153846200003, "autom": 19.8202247191, "usual": 1.72508964468, "abl": 1.8208510150200001, "intend": 2.3488681757700003, "uniqu": 3.01595744681, "the": 1.0, "build": 1.6341739578, "indefinit": 17.0160771704, "uihnum": 1443.27272727, "display": 2.93456561922, "framework": 8.200413223139998, "too": 1.81585268215, "same": 1.11857958148, "thread": 24.2381679389, "bot": 170.709677419, "there": 1.04091266719, "exist": 1.4647107666799999, "connect": 1.8843916913900003, "inform": 1.5753125620200001, "retriev": 2.16826003824, "best": 1.5828514456600002, "exampl": 1.50483412322, "pagebin": 1443.27272727, "inspect": 11.7512953368, "take": 1.13961668222, "alert": 13.6041131105, "follow": 1.04640126549, "data": 3.37643555934, "might": 2.1561863370900003, "pagelist": 1443.27272727, "further": 1.3618116315, "name": 1.10211732037, "code": 3.8807137619199996, "profil": 4.8314059647, "becaus": 1.1495184997499999, "per": 1.9597580545599997, "high": 1.14777327935, "basic": 2.7301805675, "advis": 5.0145293746099995, "indic": 2.0826446281, "break": 2.42863698944, "honeypot": 1323.0, "tutori": 59.4606741573, "also": 1.01476510067, "typic": 2.2541530597799997, "readi": 5.15789473684, "mani": 1.04426757877, "everyth": 4.81967213115, "mean": 1.44906900329, "style": 2.37807070102, "gecko": 317.52, "will": 1.22481098596, "notic": 4.36994219653, "bin": 22.3291139241, "timeout": 273.724137931, "ticket": 9.45, "plan": 1.5356935577500002, "primer": 61.7743190661, "therefor": 2.33401940606, "area": 1.3881262568900001, "becom": 1.12492028626, "tri": 1.8544562551099997, "tag": 19.7462686567, "peopl": 1.21320495186, "spoof": 46.8318584071, "polit": 1.76851954996, "via": 2.2978723404299997, "want": 1.99698113208, "percentag": 6.08275862069, "argument": 5.09335899904, "genuin": 11.4793926247, "reinvent": 46.4210526316, "howev": 1.0945191313299998, "standalon": 70.875, "share": 1.8566249561500001, "bio": 42.336000000000006, "nonuniqu": 1443.27272727, "def": 81.83505154640001, "which": 1.005191845, "databas": 8.24727272727, "random": 7.1902173913, "skill": 3.6989748369099997, "wwwwebsitecompagenumhtml": 1443.27272727, "other": 1.00992366412, "min": 33.3529411765, "techniqu": 3.7293868921800004, "one": 1.00627495722, "show": 1.26703910615, "second": 1.1130898128, "fast": 4.8729281768, "hope": 2.50884955752, "avatar": 65.0655737705, "see": 1.27242125511, "some": 1.04036697248, "https": 20.780104712, "unnecessari": 17.4845814978, "get": 1.78562591385, "easier": 7.84, "like": 1.14918566775, "implement": 3.57648118946, "through": 1.07074930869, "crash": 6.3100158982500005, "file": 3.7710213776699995, "agent": 4.25858369099, "process": 1.69524826482, "both": 1.05215720061, "chromenum": 1443.27272727, "element": 2.36004162331, "except": 1.71948445792, "structur": 2.0580762250499998, "rule": 1.7415533128599998, "travel": 1.9655812801800001, "straightforward": 27.7552447552, "access": 1.8734953976900002, "this": 1.00379362671, "beautifulsoup": 1443.27272727, "count": 3.48157894737, "instant": 11.504347826099998, "time": 1.01127460348, "api": 84.44680851060001, "part": 1.04330682789, "userag": 1443.27272727, "chang": 1.1808985421, "requestssecond": 1443.27272727, "project": 1.7534791252500002, "differ": 1.23654490225, "herodescript": 1443.27272727, "most": 1.02096463023, "between": 1.03453668708, "multiprocess": 756.0, "return": 1.39532431007, "server": 38.071942446, "cooki": 66.15, "domain": 9.39408284024, "detect": 5.41288782816, "along": 1.2973768080399999, "all": 1.01146788991, "readytous": 1443.27272727, "discount": 21.570652173899997, "socal": 1134.0, "performextract": 1443.27272727, "program": 2.02139037433, "someth": 3.28152128979, "almost": 1.53584212054, "passion": 8.14571575167, "case": 1.48498737256, "spider": 34.6637554585, "day": 1.18371607516, "discountedpric": 1443.27272727, "built": 1.99447236181, "two": 1.01379310345, "more": 1.0171706817, "and": 1.00006299213, "list": 1.36321483771, "bidder": 92.8421052632, "automl": 1443.27272727, "smooth": 11.086592178800002, "these": 1.07415426252, "deliv": 3.4039451114899997, "yourself": 26.592964824099997, "amount": 2.27027027027, "websit": 2.52160101652, "faster": 7.61438848921, "price": 3.4838709677399997, "befor": 1.10036041031, "librari": 2.68266306185, "none": 4.06555697823, "download": 14.6457564576, "can": 1.17626139142, "avail": 1.7288467821, "linux": 65.0655737705, "way": 1.2190739461, "search": 3.2539454806299997, "onli": 1.0256476516600002, "each": 1.18974820144, "wait": 4.55421686747, "concurr": 15.413592233, "few": 1.31729173581, "cloth": 4.69843148861, "depth": 8.24299065421, "read": 2.3149606299200003, "packag": 7.828402366860001, "link": 2.15151104486, "common": 1.4025974025999999, "alarm": 12.96, "block": 3.20274359492, "directori": 14.768372093, "reduc": 1.98698372966, "address": 2.86157173756, "rather": 1.55692850838, "sinc": 1.08368600683, "status": 2.4636871508400002, "arg": 311.294117647, "wheel": 8.95936794582, "come": 1.32831325301, "origin": 1.13724928367, "send": 3.75053153792, "identifi": 2.30187037843, "import": 1.3401992233700002, "first": 1.00761614623, "everi": 1.47917637194, "proceed": 3.4333910034599997, "when": 1.02076769755, "date": 1.63081664099, "while": 1.0441988950299999, "listingspric": 1443.27272727, "easili": 3.6938110749199997, "target": 3.2189781021900004, "within": 1.2369302688, "answer": 4.64890190337, "learn": 2.32275054865, "own": 1.17844418052, "pipelin": 32.1376518219, "user": 7.71053909665}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Web Scraping Tutorial with Python: Tips and Tricks</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/02/web-scraping-tutorial-python.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Web Scraping Tutorial with Python: Tips and Tricks Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/02/agi-deep-learning-connection.html\" rel=\"prev\" title=\"The AGI/Deep Learning Connection\"/>\n<link href=\"https://www.kdnuggets.com/2018/02/datarobot-customer-360-models-automated-machine-learning.html\" rel=\"next\" title=\"Enhancing Customer 360 Models with Automated Machine Learning\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2018/02/web-scraping-tutorial-python.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=77333\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2018/02/web-scraping-tutorial-python.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-77333 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 1-Feb, 2018  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/02/index.html\">Feb</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/02/tutorials.html\">Tutorials, Overviews</a> \u00bb Web Scraping Tutorial with Python: Tips and Tricks (\u00a0<a href=\"/2018/n06.html\">18:n06</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\"><img align=\"right\" alt=\"Gold Blog\" src=\"/images/tkb-1802-g.png\" width=\"94\"/>Web Scraping Tutorial with Python: Tips and Tricks</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/02/agi-deep-learning-connection.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/02/datarobot-customer-360-models-automated-machine-learning.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/beautifulsoup\" rel=\"tag\">BeautifulSoup</a>, <a href=\"https://www.kdnuggets.com/tag/python\" rel=\"tag\">Python</a>, <a href=\"https://www.kdnuggets.com/tag/tips\" rel=\"tag\">Tips</a>, <a href=\"https://www.kdnuggets.com/tag/web-scraping\" rel=\"tag\">Web Scraping</a></div>\n<br/>\n<p class=\"excerpt\">\n     This post is intended for people who are interested to know about the common design patterns, pitfalls and rules related to the web scraping.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"https://hackernoon.com/@k.kokatjuhha\">Jekaterina Kokatjuhha</a></b>.</p>\n<p>I was searching for flight tickets and noticed that ticket prices fluctuate during the day. I tried to find out when the best time to buy tickets is, but there was nothing on the Web that helped. I built a small program to automatically collect the data from the web\u200a\u2014\u200aa so-called scraper. It extracted information for my specific flight destination on predetermined dates and notified me when the price got lower.</p>\n<blockquote><p>Web scraping is a technique used to extract data from websites through an automated process.</p></blockquote>\n<p>I learned a lot from this experience with Web scraping, and I want to share it.</p>\n<p>This post is intended for people who are interested to know about the common design patterns, pitfalls and rules related to the web scraping. The article presents several <strong>use cases</strong> and a collection of typical <strong>problems</strong>, such as how <strong>not to be detected</strong>, <strong>dos</strong> and <strong>don\u2019ts</strong>, and how to <strong>speed up (parallelization)</strong> your scraper.</p>\n<p>Everything will be accompanied by python snippets, so that you can start straight away. This document will also go through several useful python packages.</p>\n<p><strong>Use Cases</strong></p>\n<p>There are many reasons and use cases why you would want to scrape data. Let me list some of them:</p>\n<ul>\n<li>scrape pages of a e-retailer to spot if some of the clothes you want to buy got discounted</li>\n<li>compare prices of several clothes brands by scraping their pages</li>\n<li>price of the flight tickets can vary during the day. One could crawl the travel website and get alarmed once the price was lowered</li>\n<li>analyze the action websites to answer the question if starting bid should be low or high to attract more bidders or if the longer auction correlates with a higher end bid</li>\n</ul>\n<p><strong>Tutorial</strong><br>\nStructure of the tutorial:</br></p>\n<ol>\n<li>Available packages</li>\n<li>Basic code</li>\n<li>Pitfalls</li>\n<li>Dos and dont\u2019s</li>\n<li>Speed up\u200a\u2014\u200aparallelization</li>\n</ol>\n<p>Before we start: <strong>Be NICE to the servers; you DON\u2019T want to crash a website.</strong></p>\n<p><strong>1. Available packages and tools</strong></p>\n<p>There is no universal solution for web scraping because the way data is stored on each website is usually specific to that site. In fact, if you want to scrape the data, you need to understand the website\u2019s structure and either build your own solution or use a highly customizable one.</p>\n<p>However, you don\u2019t need to reinvent the wheel: there are many packages that do the most work for you. Depending on your programming skills and your intended use case, you might find different packages more or less useful.</p>\n<p><strong>1.1 Inspect option</strong></p>\n<p>Most of the time you will finding yourself inspecting the <a href=\"https://www.w3schools.com/html/html_intro.asp\" rel=\"noopener noreferrer\" target=\"_blank\">HTML</a> the website. You can easily do it with an \u201cinspect\u201d <a href=\"https://www.lifewire.com/get-inspect-element-tool-for-browser-756549\" rel=\"noopener noreferrer\" target=\"_blank\">option</a> of your browser.</p>\n<p>\u00a0</p>\n<p><img class=\"aligncenter size-full\" src=\"https://cdn-images-1.medium.com/max/2000/1*1nC5h4Qmq7vU9QCGTZPTTg.png\" width=\"90%\"/></p>\n<p>\u00a0</p>\n<p>The section of the website that holds my name, my avatar and my description is called <code>hero hero--profile u-flexTOP</code> (how interesting that Medium calls its writers \u2018heroes\u2019 :)). The &lt;h1&gt; class that holds my name is called<code class=\"markup--code markup--p-code\">ui-h2 hero-title</code> and the description is contained within the &lt;p&gt; class <code class=\"markup--code markup--p-code\">ui-body hero-description</code>.</p>\n<p>You can read more about <a href=\"https://www.w3schools.com/tags/\" rel=\"noopener noreferrer\" target=\"_blank\">HTML tags</a>, and differences between <a href=\"https://www.w3schools.com/html/html_classes.asp\" rel=\"noopener noreferrer\" target=\"_blank\">classes</a> and <a href=\"https://www.w3schools.com/tags/att_global_id.asp\" rel=\"noopener noreferrer\" target=\"_blank\">ids</a> <a href=\"https://www.quora.com/What-is-the-difference-between-class-and-id-in-HTML\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>.</p>\n<p><strong>1.2 Scrapy</strong></p>\n<p>There is a stand-alone ready-to-use data extracting framework called <a href=\"https://scrapy.org/\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>Scrapy</strong></a>. Apart from extracting HTML the package offers lots of functionalities like exporting data in formats, logging etc. It is also highly customisable: run different spiders on different processes, disable cookies<sup>1</sup> and set download delays<sup>2</sup>. It can also be used to extract data using API. However, the learning curve is not smooth for the new programmers: you need to read tutorials and examples to get started.</p>\n<p><sup>1</sup> Some sites use cookies to identify bots.<br>\n<sup>2</sup> The website can get overloaded due to a huge amount of crawling requests.</br></p>\n<p><em>For my use case it was too much \u2018out of the box\u2019: I just wanted to extract the links from all pages, access each link and extract information out of it.</em></p>\n<p><strong>1.3 BeautifulSoup with Requests</strong></p>\n<p><strong>BeautifulSoup</strong> is a library that allows you to parse the HTML source code in a beautiful way. Along with it you need a <strong>Request</strong> library that will fetch the content of the URL. However, you should take care of everything else like error handling, how to export data, how to parallelize the web scraper, etc.</p>\n<p><em>I chose BeautifulSoup as it would force me to figure out a lot of stuff that Scrapy handles on its own, and hopefully help me learn faster from my mistakes.</em></p>\n<p><strong>2. Basic code</strong></p>\n<p>It\u2019s very straightforward to start scraping a website. Most of the time you will find yourself inspecting <a href=\"https://www.w3schools.com/html/html_intro.asp\" rel=\"noopener noreferrer\" target=\"_blank\">HTML</a> of the website to access the classes and IDs you need. Lets say we have a following html structure and we want to extract the <code class=\"markup--code markup--p-code\">main_price</code> elements. Note: <code>discounted_price</code> element is optional.</p>\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-77335\" sizes=\"(max-width: 706px) 100vw, 706px\" src=\"https://www.kdnuggets.com/wp-content/uploads/webscraping-html-example.jpg\" srcset=\"https://www.kdnuggets.com/wp-content/uploads/webscraping-html-example.jpg 706w, https://www.kdnuggets.com/wp-content/uploads/webscraping-html-example-300x140.jpg 300w\" width=\"90%\"/></p>\n<p>The basic code would be to import the libraries, do the request, parse the html and then to find the <code class=\"markup--code markup--p-code\">class main_price</code>.</p>\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-77341\" sizes=\"(max-width: 702px) 100vw, 702px\" src=\"https://www.kdnuggets.com/wp-content/uploads/webscraping-code.jpg\" srcset=\"https://www.kdnuggets.com/wp-content/uploads/webscraping-code.jpg 702w, https://www.kdnuggets.com/wp-content/uploads/webscraping-code-300x152.jpg 300w\" width=\"90%\"/></p>\n<p>It can happen that the <code>class main_price</code> is present in another section of the website. To avoid extracting unnecessary <code>class main_price</code> from any other part of the webpage we could have first addressed the <code>id listings_prices</code>and only then find all elements with <code>class main_price</code>.</p>\n<p><strong>3. Pitfalls</strong></p>\n<p><strong>3.1 Check robots.txt</strong></p>\n<p>The scraping rules of the websites can be found in the <a data-href=\"https://www.robotstxt.org/robotstxt.html\" href=\"https://www.robotstxt.org/robotstxt.html\" rel=\"noopener noreferrer\" target=\"_blank\">robots.txt</a> file. You can find it by writing robots.txt after the main domain, e.g <a data-href=\"https://www.website_to_scrap.com/robots.txt\" href=\"https://www.website_to_scrap.com/robots.txt\" rel=\"noopener noreferrer\" target=\"_blank\">www.website_to_scrape.com/robots.txt</a>. These rules identify which parts of the websites are not allowed to be automatically extracted or how frequently a bot is allowed to request a page. Most people don\u2019t care about it, but try to be respectful and at least look at the rules even if you don\u2019t plan to follow them.</p>\n<p><strong>3.2 HTML can be evil</strong></p>\n<p>HTML tags can contain id, class or both. HTML id specifies a <em>unique</em> id and HTML class is non-unique. Changes in the class name or element could either break your code or deliver wrong results.</p>\n<p>There are two ways to avoid it or at least to be alerted about it:</p>\n<ul>\n<li>Use specific <code>id</code> rather than <code>class</code> since it is less likely to be changed</li>\n<li>Check if the element returns <code>None</code></li>\n</ul>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre>price = page_content.find(id='listings_prices')\r\n# check if the element with such id exists or not\r\nif price is None:\r\n    # NOTIFY! LOG IT, COUNT IT\r\nelse:\r\n    # do something\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p>However, because some fields can be optional (like <code class=\"markup--code markup--p-code\">discounted_price</code> in our HTML example), corresponding elements would not appear on each listing. In this case you can count the percentage of how many times this specific element returned None to the number of listings. If it is 100%, you might want to check if the element name was changed.</p>\n<p><strong>3.3 User agent spoofing</strong></p>\n<p>Every time you visit a website, it gets your <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://www.whoishostingthis.com/tools/user-agent/\" href=\"https://www.whoishostingthis.com/tools/user-agent/\" rel=\"noopener noreferrer\" target=\"_blank\">browser information</a> via <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://en.wikipedia.org/wiki/User_agent\" href=\"https://en.wikipedia.org/wiki/User_agent\" rel=\"noopener noreferrer\" target=\"_blank\">user agent</a>. Some websites won\u2019t show you any content unless you provide a user agent. Also, some sites offer different content to different browsers. Websites do not want to block genuine users but you would look suspicious if you send 200 requests/second with the same user agent. A way out might be either to generate (almost) random user agent or to set one yourself.</p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre># library to generate user agent\r\nfrom user_agent import generate_user_agent\r\n# generate a user agent\r\nheaders = {'User-Agent': generate_user_agent(device_type=\"desktop\", os=('mac', 'linux'))}\r\n#headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux i686 on x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.63 Safari/537.36'}\r\npage_response = requests.get(page_link, timeout=5, headers=headers)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p><strong>3.4 Timeout request</strong></p>\n<p><a class=\"markup--anchor markup--p-anchor\" data-href=\"https://docs.python-requests.org/en/master/user/quickstart/#timeouts\" href=\"https://docs.python-requests.org/en/master/user/quickstart/#timeouts\" rel=\"noopener noreferrer\" target=\"_blank\">By default, Request</a> will keep waiting for a response indefinitely. Therefore, it is advised to set the timeout parameter.</p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre># timeout is set to 5 seconds\r\npage_response = requests.get(page_link, timeout=5, headers=headers)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p><strong>3.5 Did I get blocked?</strong></p>\n<p>Frequent appearance of the <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\" href=\"https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\" rel=\"noopener noreferrer\" target=\"_blank\">status codes</a> like 404 (Not Found), 403 (Forbidden), 408 (Request Timeout) might indicate that you got blocked. You may want to check for those error codes and proceed accordingly.\u00a0Also, be ready to handle exceptions from the request.</p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre>try:\r\n    page_response = requests.get(page_link, timeout=5)\r\n    if page_response.status_code == 200:\r\n        # extract\r\n    else:\r\n        print(page_response.status_code)\r\n        # notify, try again\r\nexcept requests.Timeout as e:\r\n    print(\"It is time to timeout\")\r\n    print(str(e))\r\nexcept # other exception\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p><strong>3.6 IP Rotation</strong></p>\n<p>Even if you randomize your user agent, all your requests will be from the same IP address. That doesn\u2019t sound abnormal because libraries, universities, and also companies have only a few IP addresses. However, if there are uncommonly many requests coming from a single IP address, a server can detect it.<br>\nUsing shared <a data-href=\"https://www.privateinternetaccess.com/pages/tor-vpn-proxy\" href=\"https://www.privateinternetaccess.com/pages/tor-vpn-proxy\" rel=\"noopener noreferrer\" target=\"_blank\">proxies, VPNs or TOR</a> can help you become a ghost ;).</br></p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre>proxies = {'http' : 'http://10.10.0.0:0000',  \r\n          'https': 'http://120.10.0.0:0000'}\r\npage_response = requests.get(page_link, proxies=proxies, timeout=5)  \r\n</pre>\n</div>\n<p>\u00a0</p>\n<p>By using a shared proxy, the website will see the IP address of the proxy server and not yours. A VPN connects you to another network and the IP address of the VPN provider will be sent to the website.</p>\n<p><strong>3.7 Honeypots</strong></p>\n<p>Honeypots are means to detect crawlers or scrapers.</p>\n<p>These can be \u2018hidden\u2019 links that are not visible to the users but can be extracted by scrapers/spiders. Such links will have a CSS style set to display:none, they can be blended by having the color of the background, or even be moved off of the visible area of the page. Once your crawler visits such a link, your IP address can be flagged for further investigation, or even be instantly blocked.</p>\n<p>Another way to spot crawlers is to add links with infinitely deep directory trees. Then one would need to limit the number of retrieved pages or limit the traversal depth.</p>\n<p><strong>4. Dos and Don'ts</strong></p>\n<ul>\n<li>Before scraping, check if there is a public API available. Public APIs provide easier and faster (and legal) data retrieval than web scraping. Check out <a class=\"markup--anchor markup--li-anchor\" data-href=\"https://developer.twitter.com/en/docs\" href=\"https://developer.twitter.com/en/docs\" rel=\"noopener noreferrer\" target=\"_blank\">Twitter API</a> that provides APIs for different purposes.</li>\n<li>In case you scrape lots of data, you might want to consider using a database to be able to analyze or retrieve it fast. Follow <a class=\"markup--anchor markup--li-anchor\" data-href=\"https://zetcode.com/db/sqlitepythontutorial/\" href=\"https://zetcode.com/db/sqlitepythontutorial/\" rel=\"noopener noreferrer\" target=\"_blank\">this tutorial</a> on how to create a local database with python.</li>\n<li>Be polite. As <a class=\"markup--anchor markup--li-anchor\" data-href=\"https://webmasters.stackexchange.com/questions/6205/what-user-agent-should-i-set\" href=\"https://webmasters.stackexchange.com/questions/6205/what-user-agent-should-i-set\" rel=\"noopener noreferrer\" target=\"_blank\">this answer</a> suggests, it is recommended to let people know that you are scraping their website so they can better respond to the problems your bot might cause.</li>\n</ul>\n<p>Again, do not overload the website by sending hundreds of requests per second.</p>\n<p><strong>5. Speed up\u200a\u2014\u200aparallelization</strong></p>\n<p>If you decide to parallelize your program, be careful with your implementation so you don\u2019t slam the server. And be sure you read the <strong>Dos and Don\u2019ts</strong> section. Check out the the definitions of parallelization vs concurrency, processors and threads <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python\" href=\"https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python\" rel=\"noopener noreferrer\" target=\"_blank\">here</a> and <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://code.tutsplus.com/articles/introduction-to-parallel-and-concurrent-programming-in-python--cms-28612\" href=\"https://code.tutsplus.com/articles/introduction-to-parallel-and-concurrent-programming-in-python--cms-28612\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>.</p>\n<p>If you extract a huge amount of information from the page and do some preprocessing of the data while scraping, the number of requests per second you send to the page can be relatively low.</p>\n<p><em>For my other project where I scraped apartment rental prices, I did heavy preprocessing of the data while scraping, which resulted in 1 request/second. In order to scrape 4K ads, my program would run for about one hour.</em></p>\n<p>In order to send requests in parallel you might want to use a <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://docs.python.org/2/library/multiprocessing.html\" href=\"https://docs.python.org/2/library/multiprocessing.html\" rel=\"noopener noreferrer\" target=\"_blank\">multiprocessing</a> package.</p>\n<p>Let\u2019s say we have 100 pages and we want to assign every processor equal amount of pages to work with. If <code class=\"markup--code markup--p-code\">n</code> is the number of CPUs, you can evenly chunk all pages into the <code class=\"markup--code markup--p-code\">n</code> bins and assign each bin to a processor. Each process will have its own name, target function and the arguments to work with. The name of the process can be used afterwards to enable writing data to a specific file.</p>\n<p><em>I assigned 1K pages to each of my 4 CPUs which yielded 4 requests/second and reduced the scraping time to around 17 mins.</em></p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre>import numpy as np\r\nimport multiprocessing as multi\r\n\r\ndef chunks(n, page_list):\r\n    \"\"\"Splits the list into n chunks\"\"\"\r\n    return np.array_split(page_list,n)\r\n \r\ncpus = multi.cpu_count()\r\nworkers = []\r\npage_list = ['www.website.com/page1.html', 'www.website.com/page2.html'\r\n             'www.website.com/page3.html', 'www.website.com/page4.html']\r\n\r\npage_bins = chunks(cpus, page_list)\r\n\r\nfor cpu in range(cpus):\r\n    sys.stdout.write(\"CPU \" + str(cpu) + \"\\n\")\r\n    # Process that will send corresponding list of pages \r\n    # to the function perform_extraction\r\n    worker = multi.Process(name=str(cpu), \r\n                           target=perform_extraction, \r\n                           args=(page_bins[cpu],))\r\n    worker.start()\r\n    workers.append(worker)\r\n\r\nfor worker in workers:\r\n    worker.join()\r\n    \r\ndef perform_extraction(page_ranges):\r\n    \"\"\"Extracts data, does preprocessing, writes the data\"\"\"\r\n    # do requests and BeautifulSoup\r\n    # preprocess the data\r\n    file_name = multi.current_process().name+'.txt'\r\n    # write into current process file\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p>Happy scraping!</p>\n<p>Bio: <a href=\"https://medium.com/@k.kokatjuhha\">Jekaterina Kokatjuhha</a> is a passionate Bioinformatician with interest in Machine Learning and Data Science.</p>\n<p><a href=\"https://medium.com/@k.kokatjuhha/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071\">Original</a>. Reposted with permission.</p>\n<p><strong>Related</strong></p>\n<ul>\n<li><a href=\"https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-4.html\"><b>Using AutoML to Generate Machine Learning Pipelines with TPOT</b></a></li>\n<li><a href=\"https://www.kdnuggets.com/2018/01/primer-web-scraping-r.html\"><b>A Primer on Web Scraping in R</b></a></li>\n<li><a href=\"https://www.kdnuggets.com/2017/12/baesens-web-scraping-data-science-python.html\"><b>Web Scraping for Data Science with Python</b></a></li>\n</ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets'; \n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/02/agi-deep-learning-connection.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/02/datarobot-customer-360-models-automated-machine-learning.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/02/index.html\">Feb</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/02/tutorials.html\">Tutorials, Overviews</a> \u00bb Web Scraping Tutorial with Python: Tips and Tricks (\u00a0<a href=\"/2018/n06.html\">18:n06</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556328053\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 1.026 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 21:20:53 -->\n<!-- Compression = gzip -->", "content_tokenized": ["comment", "jekaterina", "kokatjuhha", "search", "for", "flight", "ticket", "and", "notic", "that", "ticket", "price", "fluctuat", "dure", "the", "day", "tri", "find", "out", "when", "the", "best", "time", "buy", "ticket", "but", "there", "noth", "the", "web", "that", "help", "built", "small", "program", "automat", "collect", "the", "data", "from", "the", "web", "socal", "scraper", "extract", "inform", "for", "specif", "flight", "destin", "predetermin", "date", "and", "notifi", "when", "the", "price", "got", "lower", "web", "scrape", "techniqu", "use", "extract", "data", "from", "websit", "through", "autom", "process", "learn", "lot", "from", "this", "experi", "with", "web", "scrape", "and", "want", "share", "this", "post", "intend", "for", "peopl", "who", "are", "interest", "know", "about", "the", "common", "design", "pattern", "pitfal", "and", "rule", "relat", "the", "web", "scrape", "the", "articl", "present", "sever", "use", "case", "and", "collect", "typic", "problem", "such", "how", "not", "detect", "dos", "and", "and", "how", "speed", "parallel", "scraper", "everyth", "will", "accompani", "python", "snippet", "that", "can", "start", "straight", "away", "this", "document", "will", "also", "through", "sever", "use", "python", "packag", "use", "case", "there", "are", "mani", "reason", "and", "use", "case", "whi", "would", "want", "scrape", "data", "let", "list", "some", "them", "scrape", "page", "eretail", "spot", "some", "the", "cloth", "want", "buy", "got", "discount", "compar", "price", "sever", "cloth", "brand", "scrape", "their", "page", "price", "the", "flight", "ticket", "can", "vari", "dure", "the", "day", "one", "could", "crawl", "the", "travel", "websit", "and", "get", "alarm", "onc", "the", "price", "lower", "analyz", "the", "action", "websit", "answer", "the", "question", "start", "bid", "should", "low", "high", "attract", "more", "bidder", "the", "longer", "auction", "correl", "with", "higher", "end", "bid", "tutori", "structur", "the", "tutori", "avail", "packag", "basic", "code", "pitfal", "dos", "and", "dont", "speed", "parallel", "befor", "start", "the", "server", "want", "crash", "websit", "num", "avail", "packag", "and", "tool", "there", "univers", "solut", "for", "web", "scrape", "becaus", "the", "way", "data", "store", "each", "websit", "usual", "specif", "that", "site", "fact", "want", "scrape", "the", "data", "need", "understand", "the", "websit", "structur", "and", "either", "build", "own", "solut", "use", "high", "customiz", "one", "howev", "need", "reinvent", "the", "wheel", "there", "are", "mani", "packag", "that", "the", "most", "work", "for", "depend", "program", "skill", "and", "intend", "use", "case", "might", "find", "differ", "packag", "more", "less", "use", "num", "inspect", "option", "most", "the", "time", "will", "find", "yourself", "inspect", "the", "the", "websit", "can", "easili", "with", "inspect", "option", "browser", "the", "section", "the", "websit", "that", "hold", "name", "avatar", "and", "descript", "call", "hero", "hero", "profil", "uflextop", "how", "interest", "that", "medium", "call", "writer", "hero", "the", "class", "that", "hold", "name", "call", "uihnum", "herotitl", "and", "the", "descript", "contain", "within", "the", "class", "uibodi", "herodescript", "can", "read", "more", "about", "tag", "and", "differ", "between", "class", "and", "here", "num", "scrapi", "there", "standalon", "readytous", "data", "extract", "framework", "call", "scrapi", "apart", "from", "extract", "the", "packag", "offer", "lot", "function", "like", "export", "data", "format", "log", "etc", "also", "high", "customis", "run", "differ", "spider", "differ", "process", "disabl", "cooki", "num", "and", "set", "download", "delay", "num", "can", "also", "use", "extract", "data", "use", "howev", "the", "learn", "curv", "not", "smooth", "for", "the", "new", "programm", "need", "read", "tutori", "and", "exampl", "get", "start", "num", "some", "site", "use", "cooki", "identifi", "bot", "num", "the", "websit", "can", "get", "overload", "due", "huge", "amount", "crawl", "request", "for", "use", "case", "too", "much", "out", "the", "box", "just", "want", "extract", "the", "link", "from", "all", "page", "access", "each", "link", "and", "extract", "inform", "out", "num", "beautifulsoup", "with", "request", "beautifulsoup", "librari", "that", "allow", "pars", "the", "sourc", "code", "beauti", "way", "along", "with", "need", "request", "librari", "that", "will", "fetch", "the", "content", "the", "howev", "should", "take", "care", "everyth", "els", "like", "error", "handl", "how", "export", "data", "how", "parallel", "the", "web", "scraper", "etc", "chose", "beautifulsoup", "would", "forc", "figur", "out", "lot", "stuff", "that", "scrapi", "handl", "own", "and", "hope", "help", "learn", "faster", "from", "mistak", "num", "basic", "code", "veri", "straightforward", "start", "scrape", "websit", "most", "the", "time", "will", "find", "yourself", "inspect", "the", "websit", "access", "the", "class", "and", "need", "let", "say", "have", "follow", "html", "structur", "and", "want", "extract", "the", "mainpric", "element", "note", "discountedpric", "element", "option", "the", "basic", "code", "would", "import", "the", "librari", "the", "request", "pars", "the", "html", "and", "then", "find", "the", "class", "mainpric", "can", "happen", "that", "the", "class", "mainpric", "present", "anoth", "section", "the", "websit", "avoid", "extract", "unnecessari", "class", "mainpric", "from", "ani", "other", "part", "the", "webpag", "could", "have", "first", "address", "the", "listingspric", "and", "onli", "then", "find", "all", "element", "with", "class", "mainpric", "num", "pitfal", "num", "check", "robotstxt", "the", "scrape", "rule", "the", "websit", "can", "found", "the", "robotstxt", "file", "can", "find", "write", "robotstxt", "after", "the", "main", "domain", "wwwwebsitetoscrapecomrobotstxt", "these", "rule", "identifi", "which", "part", "the", "websit", "are", "not", "allow", "automat", "extract", "how", "frequent", "bot", "allow", "request", "page", "most", "peopl", "care", "about", "but", "tri", "respect", "and", "least", "look", "the", "rule", "even", "plan", "follow", "them", "num", "can", "evil", "tag", "can", "contain", "class", "both", "specifi", "uniqu", "and", "class", "nonuniqu", "chang", "the", "class", "name", "element", "could", "either", "break", "code", "deliv", "wrong", "result", "there", "are", "two", "way", "avoid", "least", "alert", "about", "use", "specif", "rather", "than", "class", "sinc", "less", "like", "chang", "check", "the", "element", "return", "none", "price", "check", "the", "element", "with", "such", "exist", "not", "price", "none", "els", "someth", "howev", "becaus", "some", "field", "can", "option", "like", "discountedpric", "our", "exampl", "correspond", "element", "would", "not", "appear", "each", "list", "this", "case", "can", "count", "the", "percentag", "how", "mani", "time", "this", "specif", "element", "return", "none", "the", "number", "list", "num", "might", "want", "check", "the", "element", "name", "chang", "num", "user", "agent", "spoof", "everi", "time", "visit", "websit", "get", "browser", "inform", "via", "user", "agent", "some", "websit", "won", "show", "ani", "content", "unless", "provid", "user", "agent", "also", "some", "site", "offer", "differ", "content", "differ", "browser", "websit", "not", "want", "block", "genuin", "user", "but", "would", "look", "suspici", "send", "num", "requestssecond", "with", "the", "same", "user", "agent", "way", "out", "might", "either", "generat", "almost", "random", "user", "agent", "set", "one", "yourself", "librari", "generat", "user", "agent", "from", "userag", "import", "generateuserag", "generat", "user", "agent", "header", "userag", "header", "userag", "mozillanum", "linux", "inum", "xnum", "applewebkitnum", "like", "gecko", "chromenum", "safarinum", "pagerespons", "num", "timeout", "request", "default", "request", "will", "keep", "wait", "for", "respons", "indefinit", "therefor", "advis", "set", "the", "timeout", "paramet", "timeout", "set", "num", "second", "pagerespons", "num", "get", "block", "frequent", "appear", "the", "status", "code", "like", "num", "not", "found", "num", "forbidden", "num", "request", "timeout", "might", "indic", "that", "got", "block", "may", "want", "check", "for", "those", "error", "code", "and", "proceed", "accord", "also", "readi", "handl", "except", "from", "the", "request", "tri", "pagerespons", "pageresponsestatuscod", "num", "extract", "els", "notifi", "tri", "again", "except", "requeststimeout", "except", "other", "except", "num", "rotat", "even", "random", "user", "agent", "all", "request", "will", "from", "the", "same", "address", "that", "sound", "abnorm", "becaus", "librari", "univers", "and", "also", "compani", "have", "onli", "few", "address", "howev", "there", "are", "uncommon", "mani", "request", "come", "from", "singl", "address", "server", "can", "detect", "use", "share", "proxi", "vpns", "can", "help", "becom", "ghost", "proxi", "http", "num", "https", "num", "pagerespons", "use", "share", "proxi", "the", "websit", "will", "see", "the", "address", "the", "proxi", "server", "and", "not", "connect", "anoth", "network", "and", "the", "address", "the", "provid", "will", "sent", "the", "websit", "num", "honeypot", "honeypot", "are", "mean", "detect", "crawler", "scraper", "these", "can", "hidden", "link", "that", "are", "not", "visibl", "the", "user", "but", "can", "extract", "scrapersspid", "such", "link", "will", "have", "style", "set", "display", "none", "they", "can", "blend", "have", "the", "color", "the", "background", "even", "move", "off", "the", "visibl", "area", "the", "page", "onc", "crawler", "visit", "such", "link", "address", "can", "flag", "for", "further", "investig", "even", "instant", "block", "anoth", "way", "spot", "crawler", "add", "link", "with", "infinit", "deep", "directori", "tree", "then", "one", "would", "need", "limit", "the", "number", "retriev", "page", "limit", "the", "travers", "depth", "num", "dos", "and", "dont", "befor", "scrape", "check", "there", "public", "avail", "public", "api", "provid", "easier", "and", "faster", "and", "legal", "data", "retriev", "than", "web", "scrape", "check", "out", "twitter", "that", "provid", "api", "for", "differ", "purpos", "case", "scrape", "lot", "data", "might", "want", "consid", "use", "databas", "abl", "analyz", "retriev", "fast", "follow", "this", "tutori", "how", "creat", "local", "databas", "with", "python", "polit", "this", "answer", "suggest", "recommend", "let", "peopl", "know", "that", "are", "scrape", "their", "websit", "they", "can", "better", "respond", "the", "problem", "bot", "might", "caus", "again", "not", "overload", "the", "websit", "send", "hundr", "request", "per", "second", "num", "speed", "parallel", "decid", "parallel", "program", "care", "with", "implement", "slam", "the", "server", "and", "sure", "read", "the", "dos", "and", "section", "check", "out", "the", "the", "definit", "parallel", "concurr", "processor", "and", "thread", "here", "and", "here", "extract", "huge", "amount", "inform", "from", "the", "page", "and", "some", "preprocess", "the", "data", "while", "scrape", "the", "number", "request", "per", "second", "send", "the", "page", "can", "relat", "low", "for", "other", "project", "where", "scrape", "apart", "rental", "price", "heavi", "preprocess", "the", "data", "while", "scrape", "which", "result", "num", "requestsecond", "order", "scrape", "program", "would", "run", "for", "about", "one", "hour", "order", "send", "request", "parallel", "might", "want", "use", "multiprocess", "packag", "let", "say", "have", "num", "page", "and", "want", "assign", "everi", "processor", "equal", "amount", "page", "work", "with", "the", "number", "cpus", "can", "even", "chunk", "all", "page", "into", "the", "bin", "and", "assign", "each", "bin", "processor", "each", "process", "will", "have", "own", "name", "target", "function", "and", "the", "argument", "work", "with", "the", "name", "the", "process", "can", "use", "afterward", "enabl", "write", "data", "specif", "file", "assign", "page", "each", "num", "cpus", "which", "yield", "num", "requestssecond", "and", "reduc", "the", "scrape", "time", "around", "num", "min", "import", "numpi", "import", "multiprocess", "multi", "def", "split", "the", "list", "into", "chunk", "return", "cpus", "worker", "pagelist", "wwwwebsitecompagenumhtml", "wwwwebsitecompagenumhtml", "wwwwebsitecompagenumhtml", "wwwwebsitecompagenumhtml", "pagebin", "for", "cpu", "process", "that", "will", "send", "correspond", "list", "page", "the", "function", "performextract", "worker", "targetperformextract", "arg", "pagebin", "cpu", "for", "worker", "worker", "def", "extract", "data", "doe", "preprocess", "write", "the", "data", "request", "and", "beautifulsoup", "preprocess", "the", "data", "filenam", "nametxt", "write", "into", "current", "process", "file", "happi", "scrape", "bio", "jekaterina", "kokatjuhha", "passion", "bioinformatician", "with", "interest", "machin", "learn", "and", "data", "scienc", "origin", "repost", "with", "permiss", "relat", "use", "automl", "generat", "machin", "learn", "pipelin", "with", "primer", "web", "scrape", "web", "scrape", "for", "data", "scienc", "with", "python"], "timestamp_scraper": 1556363490.252088, "title": "Web Scraping Tutorial with Python: Tips and Tricks", "read_time": 618.0, "content_html": "<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"https://hackernoon.com/@k.kokatjuhha\">Jekaterina Kokatjuhha</a></b>.</p>\n<p>I was searching for flight tickets and noticed that ticket prices fluctuate during the day. I tried to find out when the best time to buy tickets is, but there was nothing on the Web that helped. I built a small program to automatically collect the data from the web\u200a\u2014\u200aa so-called scraper. It extracted information for my specific flight destination on predetermined dates and notified me when the price got lower.</p>\n<blockquote><p>Web scraping is a technique used to extract data from websites through an automated process.</p></blockquote>\n<p>I learned a lot from this experience with Web scraping, and I want to share it.</p>\n<p>This post is intended for people who are interested to know about the common design patterns, pitfalls and rules related to the web scraping. The article presents several <strong>use cases</strong> and a collection of typical <strong>problems</strong>, such as how <strong>not to be detected</strong>, <strong>dos</strong> and <strong>don\u2019ts</strong>, and how to <strong>speed up (parallelization)</strong> your scraper.</p>\n<p>Everything will be accompanied by python snippets, so that you can start straight away. This document will also go through several useful python packages.</p>\n<p><strong>Use Cases</strong></p>\n<p>There are many reasons and use cases why you would want to scrape data. Let me list some of them:</p>\n<ul>\n<li>scrape pages of a e-retailer to spot if some of the clothes you want to buy got discounted</li>\n<li>compare prices of several clothes brands by scraping their pages</li>\n<li>price of the flight tickets can vary during the day. One could crawl the travel website and get alarmed once the price was lowered</li>\n<li>analyze the action websites to answer the question if starting bid should be low or high to attract more bidders or if the longer auction correlates with a higher end bid</li>\n</ul>\n<p><strong>Tutorial</strong><br>\nStructure of the tutorial:</br></p>\n<ol>\n<li>Available packages</li>\n<li>Basic code</li>\n<li>Pitfalls</li>\n<li>Dos and dont\u2019s</li>\n<li>Speed up\u200a\u2014\u200aparallelization</li>\n</ol>\n<p>Before we start: <strong>Be NICE to the servers; you DON\u2019T want to crash a website.</strong></p>\n<p><strong>1. Available packages and tools</strong></p>\n<p>There is no universal solution for web scraping because the way data is stored on each website is usually specific to that site. In fact, if you want to scrape the data, you need to understand the website\u2019s structure and either build your own solution or use a highly customizable one.</p>\n<p>However, you don\u2019t need to reinvent the wheel: there are many packages that do the most work for you. Depending on your programming skills and your intended use case, you might find different packages more or less useful.</p>\n<p><strong>1.1 Inspect option</strong></p>\n<p>Most of the time you will finding yourself inspecting the <a href=\"https://www.w3schools.com/html/html_intro.asp\" rel=\"noopener noreferrer\" target=\"_blank\">HTML</a> the website. You can easily do it with an \u201cinspect\u201d <a href=\"https://www.lifewire.com/get-inspect-element-tool-for-browser-756549\" rel=\"noopener noreferrer\" target=\"_blank\">option</a> of your browser.</p>\n<p>\u00a0</p>\n<p><img class=\"aligncenter size-full\" src=\"https://cdn-images-1.medium.com/max/2000/1*1nC5h4Qmq7vU9QCGTZPTTg.png\" width=\"90%\"/></p>\n<p>\u00a0</p>\n<p>The section of the website that holds my name, my avatar and my description is called <code>hero hero--profile u-flexTOP</code> (how interesting that Medium calls its writers \u2018heroes\u2019 :)). The &lt;h1&gt; class that holds my name is called<code class=\"markup--code markup--p-code\">ui-h2 hero-title</code> and the description is contained within the &lt;p&gt; class <code class=\"markup--code markup--p-code\">ui-body hero-description</code>.</p>\n<p>You can read more about <a href=\"https://www.w3schools.com/tags/\" rel=\"noopener noreferrer\" target=\"_blank\">HTML tags</a>, and differences between <a href=\"https://www.w3schools.com/html/html_classes.asp\" rel=\"noopener noreferrer\" target=\"_blank\">classes</a> and <a href=\"https://www.w3schools.com/tags/att_global_id.asp\" rel=\"noopener noreferrer\" target=\"_blank\">ids</a> <a href=\"https://www.quora.com/What-is-the-difference-between-class-and-id-in-HTML\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>.</p>\n<p><strong>1.2 Scrapy</strong></p>\n<p>There is a stand-alone ready-to-use data extracting framework called <a href=\"https://scrapy.org/\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>Scrapy</strong></a>. Apart from extracting HTML the package offers lots of functionalities like exporting data in formats, logging etc. It is also highly customisable: run different spiders on different processes, disable cookies<sup>1</sup> and set download delays<sup>2</sup>. It can also be used to extract data using API. However, the learning curve is not smooth for the new programmers: you need to read tutorials and examples to get started.</p>\n<p><sup>1</sup> Some sites use cookies to identify bots.<br>\n<sup>2</sup> The website can get overloaded due to a huge amount of crawling requests.</br></p>\n<p><em>For my use case it was too much \u2018out of the box\u2019: I just wanted to extract the links from all pages, access each link and extract information out of it.</em></p>\n<p><strong>1.3 BeautifulSoup with Requests</strong></p>\n<p><strong>BeautifulSoup</strong> is a library that allows you to parse the HTML source code in a beautiful way. Along with it you need a <strong>Request</strong> library that will fetch the content of the URL. However, you should take care of everything else like error handling, how to export data, how to parallelize the web scraper, etc.</p>\n<p><em>I chose BeautifulSoup as it would force me to figure out a lot of stuff that Scrapy handles on its own, and hopefully help me learn faster from my mistakes.</em></p>\n<p><strong>2. Basic code</strong></p>\n<p>It\u2019s very straightforward to start scraping a website. Most of the time you will find yourself inspecting <a href=\"https://www.w3schools.com/html/html_intro.asp\" rel=\"noopener noreferrer\" target=\"_blank\">HTML</a> of the website to access the classes and IDs you need. Lets say we have a following html structure and we want to extract the <code class=\"markup--code markup--p-code\">main_price</code> elements. Note: <code>discounted_price</code> element is optional.</p>\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-77335\" sizes=\"(max-width: 706px) 100vw, 706px\" src=\"https://www.kdnuggets.com/wp-content/uploads/webscraping-html-example.jpg\" srcset=\"https://www.kdnuggets.com/wp-content/uploads/webscraping-html-example.jpg 706w, https://www.kdnuggets.com/wp-content/uploads/webscraping-html-example-300x140.jpg 300w\" width=\"90%\"/></p>\n<p>The basic code would be to import the libraries, do the request, parse the html and then to find the <code class=\"markup--code markup--p-code\">class main_price</code>.</p>\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-77341\" sizes=\"(max-width: 702px) 100vw, 702px\" src=\"https://www.kdnuggets.com/wp-content/uploads/webscraping-code.jpg\" srcset=\"https://www.kdnuggets.com/wp-content/uploads/webscraping-code.jpg 702w, https://www.kdnuggets.com/wp-content/uploads/webscraping-code-300x152.jpg 300w\" width=\"90%\"/></p>\n<p>It can happen that the <code>class main_price</code> is present in another section of the website. To avoid extracting unnecessary <code>class main_price</code> from any other part of the webpage we could have first addressed the <code>id listings_prices</code>and only then find all elements with <code>class main_price</code>.</p>\n<p><strong>3. Pitfalls</strong></p>\n<p><strong>3.1 Check robots.txt</strong></p>\n<p>The scraping rules of the websites can be found in the <a data-href=\"https://www.robotstxt.org/robotstxt.html\" href=\"https://www.robotstxt.org/robotstxt.html\" rel=\"noopener noreferrer\" target=\"_blank\">robots.txt</a> file. You can find it by writing robots.txt after the main domain, e.g <a data-href=\"https://www.website_to_scrap.com/robots.txt\" href=\"https://www.website_to_scrap.com/robots.txt\" rel=\"noopener noreferrer\" target=\"_blank\">www.website_to_scrape.com/robots.txt</a>. These rules identify which parts of the websites are not allowed to be automatically extracted or how frequently a bot is allowed to request a page. Most people don\u2019t care about it, but try to be respectful and at least look at the rules even if you don\u2019t plan to follow them.</p>\n<p><strong>3.2 HTML can be evil</strong></p>\n<p>HTML tags can contain id, class or both. HTML id specifies a <em>unique</em> id and HTML class is non-unique. Changes in the class name or element could either break your code or deliver wrong results.</p>\n<p>There are two ways to avoid it or at least to be alerted about it:</p>\n<ul>\n<li>Use specific <code>id</code> rather than <code>class</code> since it is less likely to be changed</li>\n<li>Check if the element returns <code>None</code></li>\n</ul>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre>price = page_content.find(id='listings_prices')\r\n# check if the element with such id exists or not\r\nif price is None:\r\n    # NOTIFY! LOG IT, COUNT IT\r\nelse:\r\n    # do something\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p>However, because some fields can be optional (like <code class=\"markup--code markup--p-code\">discounted_price</code> in our HTML example), corresponding elements would not appear on each listing. In this case you can count the percentage of how many times this specific element returned None to the number of listings. If it is 100%, you might want to check if the element name was changed.</p>\n<p><strong>3.3 User agent spoofing</strong></p>\n<p>Every time you visit a website, it gets your <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://www.whoishostingthis.com/tools/user-agent/\" href=\"https://www.whoishostingthis.com/tools/user-agent/\" rel=\"noopener noreferrer\" target=\"_blank\">browser information</a> via <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://en.wikipedia.org/wiki/User_agent\" href=\"https://en.wikipedia.org/wiki/User_agent\" rel=\"noopener noreferrer\" target=\"_blank\">user agent</a>. Some websites won\u2019t show you any content unless you provide a user agent. Also, some sites offer different content to different browsers. Websites do not want to block genuine users but you would look suspicious if you send 200 requests/second with the same user agent. A way out might be either to generate (almost) random user agent or to set one yourself.</p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre># library to generate user agent\r\nfrom user_agent import generate_user_agent\r\n# generate a user agent\r\nheaders = {'User-Agent': generate_user_agent(device_type=\"desktop\", os=('mac', 'linux'))}\r\n#headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux i686 on x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.63 Safari/537.36'}\r\npage_response = requests.get(page_link, timeout=5, headers=headers)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p><strong>3.4 Timeout request</strong></p>\n<p><a class=\"markup--anchor markup--p-anchor\" data-href=\"https://docs.python-requests.org/en/master/user/quickstart/#timeouts\" href=\"https://docs.python-requests.org/en/master/user/quickstart/#timeouts\" rel=\"noopener noreferrer\" target=\"_blank\">By default, Request</a> will keep waiting for a response indefinitely. Therefore, it is advised to set the timeout parameter.</p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre># timeout is set to 5 seconds\r\npage_response = requests.get(page_link, timeout=5, headers=headers)\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p><strong>3.5 Did I get blocked?</strong></p>\n<p>Frequent appearance of the <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\" href=\"https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\" rel=\"noopener noreferrer\" target=\"_blank\">status codes</a> like 404 (Not Found), 403 (Forbidden), 408 (Request Timeout) might indicate that you got blocked. You may want to check for those error codes and proceed accordingly.\u00a0Also, be ready to handle exceptions from the request.</p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre>try:\r\n    page_response = requests.get(page_link, timeout=5)\r\n    if page_response.status_code == 200:\r\n        # extract\r\n    else:\r\n        print(page_response.status_code)\r\n        # notify, try again\r\nexcept requests.Timeout as e:\r\n    print(\"It is time to timeout\")\r\n    print(str(e))\r\nexcept # other exception\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p><strong>3.6 IP Rotation</strong></p>\n<p>Even if you randomize your user agent, all your requests will be from the same IP address. That doesn\u2019t sound abnormal because libraries, universities, and also companies have only a few IP addresses. However, if there are uncommonly many requests coming from a single IP address, a server can detect it.<br>\nUsing shared <a data-href=\"https://www.privateinternetaccess.com/pages/tor-vpn-proxy\" href=\"https://www.privateinternetaccess.com/pages/tor-vpn-proxy\" rel=\"noopener noreferrer\" target=\"_blank\">proxies, VPNs or TOR</a> can help you become a ghost ;).</br></p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre>proxies = {'http' : 'http://10.10.0.0:0000',  \r\n          'https': 'http://120.10.0.0:0000'}\r\npage_response = requests.get(page_link, proxies=proxies, timeout=5)  \r\n</pre>\n</div>\n<p>\u00a0</p>\n<p>By using a shared proxy, the website will see the IP address of the proxy server and not yours. A VPN connects you to another network and the IP address of the VPN provider will be sent to the website.</p>\n<p><strong>3.7 Honeypots</strong></p>\n<p>Honeypots are means to detect crawlers or scrapers.</p>\n<p>These can be \u2018hidden\u2019 links that are not visible to the users but can be extracted by scrapers/spiders. Such links will have a CSS style set to display:none, they can be blended by having the color of the background, or even be moved off of the visible area of the page. Once your crawler visits such a link, your IP address can be flagged for further investigation, or even be instantly blocked.</p>\n<p>Another way to spot crawlers is to add links with infinitely deep directory trees. Then one would need to limit the number of retrieved pages or limit the traversal depth.</p>\n<p><strong>4. Dos and Don'ts</strong></p>\n<ul>\n<li>Before scraping, check if there is a public API available. Public APIs provide easier and faster (and legal) data retrieval than web scraping. Check out <a class=\"markup--anchor markup--li-anchor\" data-href=\"https://developer.twitter.com/en/docs\" href=\"https://developer.twitter.com/en/docs\" rel=\"noopener noreferrer\" target=\"_blank\">Twitter API</a> that provides APIs for different purposes.</li>\n<li>In case you scrape lots of data, you might want to consider using a database to be able to analyze or retrieve it fast. Follow <a class=\"markup--anchor markup--li-anchor\" data-href=\"https://zetcode.com/db/sqlitepythontutorial/\" href=\"https://zetcode.com/db/sqlitepythontutorial/\" rel=\"noopener noreferrer\" target=\"_blank\">this tutorial</a> on how to create a local database with python.</li>\n<li>Be polite. As <a class=\"markup--anchor markup--li-anchor\" data-href=\"https://webmasters.stackexchange.com/questions/6205/what-user-agent-should-i-set\" href=\"https://webmasters.stackexchange.com/questions/6205/what-user-agent-should-i-set\" rel=\"noopener noreferrer\" target=\"_blank\">this answer</a> suggests, it is recommended to let people know that you are scraping their website so they can better respond to the problems your bot might cause.</li>\n</ul>\n<p>Again, do not overload the website by sending hundreds of requests per second.</p>\n<p><strong>5. Speed up\u200a\u2014\u200aparallelization</strong></p>\n<p>If you decide to parallelize your program, be careful with your implementation so you don\u2019t slam the server. And be sure you read the <strong>Dos and Don\u2019ts</strong> section. Check out the the definitions of parallelization vs concurrency, processors and threads <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python\" href=\"https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python\" rel=\"noopener noreferrer\" target=\"_blank\">here</a> and <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://code.tutsplus.com/articles/introduction-to-parallel-and-concurrent-programming-in-python--cms-28612\" href=\"https://code.tutsplus.com/articles/introduction-to-parallel-and-concurrent-programming-in-python--cms-28612\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>.</p>\n<p>If you extract a huge amount of information from the page and do some preprocessing of the data while scraping, the number of requests per second you send to the page can be relatively low.</p>\n<p><em>For my other project where I scraped apartment rental prices, I did heavy preprocessing of the data while scraping, which resulted in 1 request/second. In order to scrape 4K ads, my program would run for about one hour.</em></p>\n<p>In order to send requests in parallel you might want to use a <a class=\"markup--anchor markup--p-anchor\" data-href=\"https://docs.python.org/2/library/multiprocessing.html\" href=\"https://docs.python.org/2/library/multiprocessing.html\" rel=\"noopener noreferrer\" target=\"_blank\">multiprocessing</a> package.</p>\n<p>Let\u2019s say we have 100 pages and we want to assign every processor equal amount of pages to work with. If <code class=\"markup--code markup--p-code\">n</code> is the number of CPUs, you can evenly chunk all pages into the <code class=\"markup--code markup--p-code\">n</code> bins and assign each bin to a processor. Each process will have its own name, target function and the arguments to work with. The name of the process can be used afterwards to enable writing data to a specific file.</p>\n<p><em>I assigned 1K pages to each of my 4 CPUs which yielded 4 requests/second and reduced the scraping time to around 17 mins.</em></p>\n<div style=\"width: 98%; border: 1px solid #ccc; overflow: auto; padding-left: 10px; padding-bottom: 10px; padding-top: 10px;\">\n<pre>import numpy as np\r\nimport multiprocessing as multi\r\n\r\ndef chunks(n, page_list):\r\n    \"\"\"Splits the list into n chunks\"\"\"\r\n    return np.array_split(page_list,n)\r\n \r\ncpus = multi.cpu_count()\r\nworkers = []\r\npage_list = ['www.website.com/page1.html', 'www.website.com/page2.html'\r\n             'www.website.com/page3.html', 'www.website.com/page4.html']\r\n\r\npage_bins = chunks(cpus, page_list)\r\n\r\nfor cpu in range(cpus):\r\n    sys.stdout.write(\"CPU \" + str(cpu) + \"\\n\")\r\n    # Process that will send corresponding list of pages \r\n    # to the function perform_extraction\r\n    worker = multi.Process(name=str(cpu), \r\n                           target=perform_extraction, \r\n                           args=(page_bins[cpu],))\r\n    worker.start()\r\n    workers.append(worker)\r\n\r\nfor worker in workers:\r\n    worker.join()\r\n    \r\ndef perform_extraction(page_ranges):\r\n    \"\"\"Extracts data, does preprocessing, writes the data\"\"\"\r\n    # do requests and BeautifulSoup\r\n    # preprocess the data\r\n    file_name = multi.current_process().name+'.txt'\r\n    # write into current process file\r\n</pre>\n</div>\n<p>\u00a0</p>\n<p>Happy scraping!</p>\n<p>Bio: <a href=\"https://medium.com/@k.kokatjuhha\">Jekaterina Kokatjuhha</a> is a passionate Bioinformatician with interest in Machine Learning and Data Science.</p>\n<p><a href=\"https://medium.com/@k.kokatjuhha/web-scraping-tutorial-with-python-tips-and-tricks-db070e70e071\">Original</a>. Reposted with permission.</p>\n<p><strong>Related</strong></p>\n<ul>\n<li><a href=\"https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-4.html\"><b>Using AutoML to Generate Machine Learning Pipelines with TPOT</b></a></li>\n<li><a href=\"https://www.kdnuggets.com/2018/01/primer-web-scraping-r.html\"><b>A Primer on Web Scraping in R</b></a></li>\n<li><a href=\"https://www.kdnuggets.com/2017/12/baesens-web-scraping-data-science-python.html\"><b>Web Scraping for Data Science with Python</b></a></li>\n</ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets'; \n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}