{"content": "comments By Emmanuel Ameisen , Head of AI at Insight Data Science Teaching computers to look at pictures the way we do Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York? \u00a0 Learn more about the\u00a0 Artificial Intelligence program at Insight. Are you a company working in AI and would like to get involved in the Insight AI Fellows Program? \u00a0Feel free to\u00a0 get in touch . For more content like this, follow\u00a0 Insight \u00a0and\u00a0 Emmanuel \u00a0on Twitter. \u00a0 Why similarity search? \u00a0 An image is worth a thousand words, and even more lines of code. Many products\u00a0 fundamentally appeal to our perception . When browsing through outfits on clothing sites, looking for a vacation rental on Airbnb, or choosing a pet to adopt, the way something looks is often an important factor in our decision. The way we perceive things is a strong predictor of what kind of items we will like, and therefore a valuable quality to measure. However, making computers understand images the way humans do has been a computer science challenge for quite some time. Since 2012, Deep Learning has slowly started overtaking classical methods such as\u00a0 Histograms of Oriented Gradients \u00a0(HOG) in perception tasks like image classification or object detection. One of the main reasons often credited for this shift is deep learning\u2019s ability to automatically\u00a0 extract meaningful representations \u00a0when trained on a large enough dataset. Visual search at Pinterest This is why many teams\u200a\u2014\u200alike at\u00a0 Pinterest ,\u00a0 StitchFix , and\u00a0 Flickr \u200a\u2014\u200astarted using Deep Learning to learn representations of their images, and\u00a0 provide recommendations\u00a0 based on the content users find visually pleasing. Similarly, Fellows at\u00a0 Insight \u00a0have used deep learning to build models for applications such as helping people find cats to adopt, recommending sunglasses to buy, and searching for art styles. Many recommendation systems are based on\u00a0 collaborative filtering : leveraging user correlations to make recommendations (\u201cusers that liked the items you have liked have also liked\u2026\u201d). However, these models require a\u00a0 significant amount of data \u00a0to be accurate, and\u00a0 struggle \u00a0to handle\u00a0 new items that have not yet been viewed by anyone. Item representation can be used in what\u2019s called\u00a0 content-based \u00a0recommendation systems, which do not suffer from the problem above. In addition, these representations allow consumers to\u00a0 efficiently search photo libraries for images that are similar to the selfie they just took (querying by image), or for photos of particular items such as cars (querying by text). Common examples of this include Google Reverse Image Search, as well as Google Image Search. Based on our experience providing technical mentorship for many semantic understanding projects, we wanted to write a tutorial on how you would go about\u00a0 building your own representations , both for image and text data, and\u00a0 efficiently do similarity search . By the end of this post, you should be able to build a quick semantic search model from scratch, no matter the size of your dataset. This post is accompanied by\u00a0 an annotated code notebook \u00a0using\u00a0 streamlit and a\u00a0 self-standing codebase \u00a0demonstrating and applying all these techniques. Feel free to run the code and follow along! \u00a0 What\u2019s our\u00a0plan? \u00a0 A break to chat about optimization In machine learning, just like in software engineering, there are many ways to tackle a problem, each with different tradeoffs. If we are doing research or local prototyping, we can get away with very inefficient solutions. But if we are building an image similarity search engine that needs to be maintainable and scalable, we have to consider both how we can\u00a0 adapt to data evolution , and\u00a0 how fast our model can run . Let\u2019s imagine a few approaches: Workflow for approach\u00a01 1/ We build an end-to-end model that is trained on all our images to take an image as an input, and output a similarity score over all of our images. Predictions happen quickly (one forward pass), but we would need to \u00a0train a new model \u00a0every time we add a new image. We would also quickly reach a state with so many classes that it would be extremely\u00a0 hard to optimize \u00a0it correctly. This approach is fast, but does not scale to large datasets. In addition, we would have to label our dataset by hand with image similarities, which could be extremely time consuming. Workflow for approach\u00a02 2/ Another approach is to build a model that takes in two images, and outputs a pairwise similarity score between 0 and 1 ( Siamese Networks [PDF], for example). These models are accurate for large datasets, but lead to another scalability issue. We usually want to find a similar image by\u00a0 looking through a vast collection of images , so we have to run our similarity model once for each image pair in our dataset. If our model is a CNN, and we have more than a dozen images, this becomes too slow to even be considered. In addition, this only works for image similarity, not text search. This method scales to large datasets, but is slow. Workflow for approach\u00a03 3/ There is a simpler method, which is similar to\u00a0 word embeddings . If we find an\u00a0 expressive vector representation, or embedding\u00a0 for images, we can then calculate their similarity by looking at\u00a0 how close their vectors are to each other . This type of search is a common problem that is well studied, and many libraries implement fast solutions (we will use\u00a0 Annoy \u00a0here). In addition, if we calculate these vectors for all images in our database ahead of time, this approach is both fast (one forward pass, and an efficient similarity search), and scalable. Finally, if we manage to find\u00a0 common embeddings \u00a0for our images and our words, we could use them to do text to image search! Because of its simplicity and efficiency, the third method will be the focus of this post. How do we get\u00a0there? So, how do we actually use\u00a0 deep learning representations \u00a0to create a\u00a0 search engine? Our final goal is to have a search engine that can take in images and output either similar images or tags, and take in text and output similar words, or images. To get there, we will go through three successive steps: Searching for\u00a0 similar images to an input image \u00a0(Image \u2192 Image) Searching for\u00a0 similar words to an input word \u00a0(Text \u2192 Text) Generating\u00a0 tags for images , and\u00a0 searching images using tex t (Image \u2194 Text) To do this, we will use\u00a0 embeddings , vector representations of images and text. Once we have embeddings, searching simply becomes a matter of finding vectors close to our input vector. The way we find these is by calculating the\u00a0 cosine similarity \u00a0between our image embedding, and embeddings for other images. Similar images will have similar embeddings, meaning a\u00a0 high cosine similarity between embeddings . Let\u2019s start with a dataset to experiment with. \u00a0 Dataset \u00a0 Images Our image dataset consists of a total of a\u00a0 1000 images , divided in 20 classes with 50 images for each. This dataset can be found\u00a0 here . Feel free to use the script in the linked code to automatically download all image files.\u00a0 Credit to Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier for the dataset. This dataset contains a category and a set of captions for every image. To make this problem harder, and to show how well our approach generalizes, we will\u00a0 only use the categories , and disregard the captions. We have a total of 20 classes, which I\u2019ve listed out below: aeroplane \u00a0 bicycle \u00a0 bird \u00a0 boat \u00a0 bottle \u00a0 bus \u00a0 car \u00a0 cat \u00a0 chair \u00a0 cow dining_table dog horse motorbike person potted_plant sheep sofa train tv_monitor Image examples. As we can see, the labels are quite\u00a0noisy. We can see our labels are pretty\u00a0 noisy : many photos contain multiple categories, and the label is not always from the most prominent one. For example, on the bottom right, the image is labeled\u00a0 chair \u00a0and not\u00a0 person even though 3 people stand in the center of the image, and the chair is barely visible. Text In addition, we load word embeddings that have been pre-trained on Wikipedia (this tutorial will use the ones from the\u00a0 GloVe \u00a0model). We will use these vectors to incorporate text to our semantic search. For more information on how these word vectors work, see\u00a0 Step 7 \u00a0of our NLP tutorial. \u00a0 Image ->\u00a0Image \u00a0 Starting simple We are now going to load a model that was\u00a0 pre-trained \u00a0on a large data set ( Imagenet ), and is freely available online. We use VGG16 here, but this approach would work with any recent CNN architecture. We use this model to generate\u00a0 embeddings \u00a0for our images. VGG16 (credit to Data Wow\u00a0Blog) What do we mean by generating embeddings? We will use our pre-trained model\u00a0 up to the penultimate layer , and store the value of the activations. In the image below, this is represented by the embedding layer highlighted in green, which is before the final classification layer. For our embeddings, we use the layer before the final classification layer. Once we\u2019ve used the model to generate image features, we can then store them to disk and re-use them\u00a0 without needing to do inference again ! This is one of the reasons that embeddings are so popular in practical applications, as they allow for huge efficiency gains. On top of storing them to disk, we will build a\u00a0 fast index \u00a0of the embeddings using\u00a0 Annoy , which will allow us to very quickly find the nearest embeddings to any given embedding. Below are our embeddings. Each image is now represented by a sparse vector of size 4096.\u00a0 Note: the reason the vector is sparse is that we have taken the values after the activation function, which zeros out negatives. Image Embeddings Using our embeddings to search through\u00a0images We can now simply take in an image, get its embedding, and look in our fast index to find similar embeddings, and thus similar images. This is especially useful since image labels are often\u00a0 noisy , and there is more to an image than its label. For example, in our dataset, we have both a class\u00a0 cat , and a class\u00a0 bottle . Which class do you think this image is labeled as? Cat or Bottle? (Image rescaled to 224x224, which is what the neural network\u00a0sees.) The correct answer is\u00a0 bottle \u00a0\u2026 This is an actual issue that comes up often in real datasets. Labeling images as unique categories is quite limiting, which is why we hope to use more nuanced representations. Luckily, this is exactly what\u00a0 deep learning is good at ! Let\u2019s see if our image search using embeddings does better than human labels. Searching for similar images to dataset/bottle/2008_000112.jpg \u2026 Great\u200a\u2014\u200awe mostly get more images of\u00a0 cats , which seems very reasonable! Our pre-trained network has been trained on a wide variety of images, including cats, and so it is able to accurately find similar images, even though it has never been trained on this particular dataset before. However, one image in the middle of the bottom row shows a shelf of bottles. This approach perform wells to find similar images, in general, but sometimes we are only interested in\u00a0 part of the image . For example, given an image of a cat and a bottle, we might be only interested in similar cats, not similar bottles. Semi-supervised search A common approach to solve this issue is to use an\u00a0 object detection \u00a0model first, detect our cat, and do image search on a cropped version of the original image. This adds a huge computing overhead, which we would like to avoid if possible. There is a simpler \u201chacky\u201d approach, which consists of\u00a0 re-weighing \u00a0the activations. We do this by loading the last layer of weights we had initially discarded, and only use the weights tied to the index of the class we are looking for to re-weigh our embedding. This cool trick was initially brought to my attention by\u00a0 Insight \u00a0Fellow\u00a0 Daweon Ryu . For example, in the image below, we use the weights of the\u00a0 Siamese cat \u00a0class to re-weigh the activations on our dataset (highlighted in green). Feel free to check out the attached notebook to see the implementation details. The hack to get weighted embeddings. The classification layer is shown for reference only. Let\u2019s examine how this works by weighing our activations according to class\u00a0 284 \u00a0in Imagenet,\u00a0 Siamese cat . Searching for similar images to dataset/bottle/2008_000112.jpg \u00a0using weighted features\u2026 We can see that the search has been biased to look for\u00a0 Siamese cat-like things . We no longer show any bottles, which is great. You might however notice that our last image is of a sheep! This is very interesting, as biasing our model has led to a\u00a0 different kind of error , which is more appropriate for our current domain. We have seen we can search for similar images in a\u00a0 broad \u00a0way, or by\u00a0 conditioning on a particular class \u00a0our model was trained on. This is a great step forward, but since we are using a model\u00a0 pre-trained on Imagenet , we are thus limited to the 1000\u00a0 Imagenet classes . These classes are far from all-encompassing (they lack a category for people, for example), so we would ideally like to find something more\u00a0 flexible.\u00a0 In addition, what if we simply wanted to search for cats\u00a0 without providing an input image? In order to do this, we are going to use more than simple tricks, and leverage a model that can understand the semantic power of words.", "title_html": "<h1 id=\"title\">Building an image search service from scratch</h1> ", "url": "https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html", "tfidf": {"tfidf": {"after": 1.02070207021, "hand": 1.6152202665600002, "real": 2.28103448276, "semant": 156.4137931036, "art": 1.9994962216599999, "visibl": 4.68595041322, "onc": 4.492359932099999, "too": 1.81585268215, "ahead": 5.625797306869999, "sheep": 35.716535433000004, "wide": 1.5598349381, "kind": 5.1612483745199995, "addit": 7.47809703252, "would": 9.745856353619999, "what": 8.77404073896, "datasetbottlenumjpg": 3175.2, "taken": 1.6012102874399998, "success": 1.32002993265, "dataset": 3291.365853666, "wikipedia": 40.5, "function": 2.495441685, "class": 25.39821357156, "dog": 6.26272189349, "crop": 9.71009174312, "pet": 18.0614334471, "bare": 8.815102720710001, "product": 1.62264922322, "work": 5.57600449565, "never": 1.55769230769, "teach": 3.98594024605, "green": 5.26130903066, "approach": 24.906785200679998, "technic": 3.1400316455699997, "tackl": 19.8698372966, "their": 3.0464372521500005, "highlight": 11.66066838046, "automat": 13.575032064980002, "particular": 4.144448311860001, "measur": 2.41093394077, "cosin": 387.219512196, "creat": 1.2492917847, "pairwis": 387.219512195, "multipl": 2.74813917258, "selfstand": 1587.6, "how": 14.422529524589999, "perceiv": 4.92279069767, "maintain": 1.77306231852, "had": 1.0475750577399998, "especi": 1.66712170534, "initi": 2.7, "end": 1.10680423871, "found": 1.11387076405, "given": 2.70852170946, "uniqu": 3.01595744681, "interest": 4.80993738639, "deep": 21.76782449724, "will": 14.69773183152, "weigh": 10.9944598338, "numxnum": 52.0524590164, "consid": 2.4794627518400003, "site": 1.9721739130400002, "write": 2.0575427682700003, "accompani": 3.38146964856, "disk": 49.535101403999995, "peter": 2.34090238868, "incorpor": 2.62847682119, "new": 4.0715522216, "below": 9.02430012788, "perform": 1.5313977042500002, "but": 8.13059343192, "demonstr": 2.64997496244, "need": 4.31178707223, "our": 91.94594594601, "classif": 32.268292682920006, "annot": 22.8760806916, "final": 5.360344391, "feel": 12.542761208760002, "sunglass": 162.0, "applic": 6.85344269372, "lead": 1.2664326739, "struggl": 3.36, "discard": 19.1277108434, "enough": 2.2319696330700003, "collabor": 4.45454545455, "pass": 3.23636734278, "use": 29.859523964019996, "experi": 3.74125132556, "appeal": 3.6614391143900002, "streamlit": 1587.6, "annoy": 53.6351351352, "out": 3.18050083473, "model": 41.811956808000005, "onlin": 2.6051854282900004, "suffer": 2.16117615029, "good": 1.51981619759, "alway": 2.06745670009, "slow": 8.09586945436, "comment": 3.05954904606, "glove": 31.3754940711, "huge": 8.77854575616, "categori": 19.9097065463, "julia": 20.9722589168, "forward": 10.99699838376, "reason": 6.89361702128, "bus": 6.993832599119999, "view": 1.6407606448899998, "not": 7.10971786833, "far": 1.71022298826, "involv": 1.4498630137000001, "emmanuel": 74.013986014, "revers": 4.29894394801, "store": 10.34042553192, "happen": 2.96359902931, "quick": 8.82, "pleas": 9.12938470385, "howev": 4.378076525319999, "young": 1.8886509636000002, "adopt": 4.0885912954000005, "shown": 2.76923076923, "handl": 3.9229058561900003, "solv": 7.26923076923, "than": 4.131147541, "then": 2.17315721032, "index": 20.9907448215, "they": 3.09051975861, "rental": 24.8840125392, "anoth": 2.27287043664, "reach": 1.49801849406, "last": 2.4234468020200004, "let": 13.94466403164, "scienc": 4.63939216832, "general": 2.2436404748400003, "blog": 14.1876675603, "away": 1.85142857143, "featur": 1.52712581762, "item": 25.3934740883, "practic": 1.70434782609, "lack": 1.9271667880599999, "simpl": 6.7962328767199995, "cow": 19.6972704715, "might": 4.312372674180001, "calcul": 18.38918918919, "twitter": 33.213389121300004, "valley": 4.21673306773, "valu": 4.555523672880001, "either": 1.5830092731099998, "accord": 1.27589809531, "tvmonitor": 1587.6, "should": 1.6643254009900001, "pinterest": 2886.54545454, "such": 3.18454132122, "thousand": 2.4767550702000003, "network": 7.781081522639999, "three": 1.06621893889, "fast": 29.237569060800002, "some": 1.04036697248, "collect": 1.64109985528, "both": 4.20862880244, "version": 2.0083491461099996, "fellow": 13.0918086861, "classic": 2.4087391898, "filter": 16.8893617021, "tex": 85.3548387097, "slowli": 6.080428954419999, "set": 2.37415881562, "third": 1.4195278969999998, "ani": 3.40151406942, "two": 1.01379310345, "divid": 2.3169877408099997, "from": 6.00340328982, "stitchfix": 1587.6, "answer": 4.64890190337, "allow": 3.8148177813300004, "limit": 3.0373062942400004, "detail": 2.26186066391, "num": 18.00567072018, "veri": 5.03520456708, "brows": 103.090909091, "free": 6.87272727272, "aeroplan": 64.2753036437, "for": 45.01417680045, "car": 7.07486631016, "predict": 5.18484650555, "output": 30.707930367520003, "bottom": 12.54523903596, "word": 16.168835577689997, "seen": 1.61079545455, "with": 8.009585671919998, "seem": 2.29123971713, "zero": 8.75192943771, "are": 20.598118715600002, "solut": 9.4556283502, "recommend": 19.571005917150003, "better": 2.0065722952500002, "compani": 1.5523613963, "caption": 111.020979021, "current": 1.5325803649, "meaning": 21.8076923077, "team": 2.2748244734200003, "look": 15.269055061279998, "popular": 1.50769230769, "promin": 2.39746300211, "bicycl": 18.290322580599998, "harder": 17.1262135922, "think": 2.90715986083, "negat": 3.75852272727, "googl": 22.777618364400002, "amount": 2.27027027027, "focus": 2.01012914662, "though": 2.72152224222, "artifici": 16.63279203772, "hacki": 1587.6, "disregard": 21.9889196676, "doe": 3.4116256581, "even": 4.65845070424, "hodosh": 1587.6, "infer": 21.1398135819, "simpler": 35.8374717832, "extract": 7.703056768560001, "mentorship": 337.787234043, "train": 13.5559892657, "great": 3.79778327088, "avoid": 2.45986984816, "goal": 3.28152128979, "motorbik": 184.604651163, "error": 6.04109589041, "liked\u2026": 1587.6, "librari": 5.3653261237, "diningt": 1587.6, "note": 1.42449528937, "shift": 3.3317943336799996, "row": 5.549108703250001, "scale": 7.493981590739999, "boat": 6.466802443990001, "notebook": 80.3848101266, "again": 1.50883862384, "ameisen": 1587.6, "bias": 27.4671280276, "abov": 1.90382539873, "requir": 1.52844902282, "strong": 1.6439888163999998, "buy": 5.12459651388, "local": 1.51720183486, "choos": 4.17899447223, "head": 1.57781753131, "flexibl": 9.68639414277, "help": 1.39962972759, "main": 1.25303867403, "larg": 5.928747479249999, "find": 20.752941176519997, "bird": 6.46416938111, "photo": 19.25919935301, "line": 1.4182597820299998, "nuanc": 48.2553191489, "own": 1.17844418052, "base": 3.43884476535, "label": 44.7715736041, "pictur": 3.4953764861300005, "allencompass": 1587.6, "pottedpl": 1587.6, "post": 6.71478922881, "here": 7.26923076924, "ideal": 4.65571847507, "factor": 2.89127663449, "techniqu": 3.7293868921800004, "cyrus": 57.1079136691, "attent": 2.81040892193, "through": 4.28299723476, "human": 3.7930952096599997, "layer": 56.990769230779996, "fundament": 5.32930513595, "siames": 654.680412372, "contain": 3.19629555064, "noisi": 186.7764705882, "about": 3.19458045477, "problem": 7.06699310036, "bottl": 123.54863813200001, "neural": 59.4606741573, "thing": 4.813096862219999, "rashtchian": 1587.6, "just": 2.67160286074, "adapt": 3.32272917539, "correct": 7.3262574988399995, "simpli": 7.557600761670001, "chair": 21.338709677430003, "represent": 53.354742345000005, "attach": 4.4885496183199995, "check": 6.50655737705, "vacat": 12.96, "led": 1.33782758911, "percept": 16.69400630914, "overhead": 24.054545454499998, "start": 5.06694326976, "outfit": 23.042089985500002, "took": 1.4009883515700001, "thus": 3.2927512185000003, "run": 4.6707855251399995, "them": 4.39504463976, "insight": 70.8223048326, "load": 20.41491641664, "studi": 1.53184098804, "vector": 258.98858075, "spars": 42.0, "order": 1.24625166811, "nearest": 15.148854961800001, "whi": 9.769846153860001, "quit": 8.65491550065, "usual": 1.72508964468, "abl": 3.6417020300400003, "mani": 8.35414063016, "refer": 1.30024570025, "over": 1.02525024217, "step": 8.48379052368, "generat": 8.21101629168, "features\u2026": 1587.6, "scratch": 25.8146341463, "provid": 3.64658142561, "total": 3.0920245398799997, "broad": 4.27693965517, "consum": 9.86086956522, "chat": 36.164009111599995, "exact": 3.46864758575, "catlik": 1587.6, "penultim": 64.0161290323, "endtoend": 1587.6, "inform": 1.5753125620200001, "extrem": 4.73204172876, "selfi": 1587.6, "shelf": 47.391044776099996, "possibl": 1.4173734488, "semisupervis": 1587.6, "origin": 1.13724928367, "right": 1.4054532577899999, "wow": 91.2413793103, "correl": 13.1860465116, "softwar": 10.2624434389, "take": 5.6980834111, "visual": 10.45505432994, "follow": 2.09280253098, "now": 3.4823426189999998, "matter": 4.89546716004, "code": 15.522855047679998, "hard": 2.73253012048, "simplic": 25.4423076923, "high": 1.14777327935, "power": 1.3396337861799998, "leverag": 71.5135135136, "break": 2.42863698944, "issu": 4.31765025837, "tutori": 178.3820224719, "center": 1.7423178226499998, "also": 2.02953020134, "trick": 29.4545454546, "person": 2.81040892194, "mean": 2.89813800658, "style": 2.37807070102, "brought": 1.8387769284200002, "one": 7.04392470054, "evolut": 6.360576923080001, "notic": 4.36994219653, "task": 3.88641370869, "user": 23.13161728995, "research": 1.9420183486200002, "plan": 1.5356935577500002, "script": 8.299006795610001, "signific": 1.4529147982100001, "therefor": 2.33401940606, "the": 71.0, "sofa": 160.363636364, "becom": 2.24984057252, "recent": 1.54405757635, "tag": 39.4925373134, "peopl": 3.6396148555800005, "stand": 2.0845588235299997, "hockenmai": 1587.6, "dozen": 5.95275590551, "challeng": 2.55816951337, "machin": 4.02433460076, "want": 7.98792452832, "sinc": 3.2510580204900004, "system": 2.77479681902, "build": 11.4392177046, "rescal": 441.0, "overtak": 50.2405063291, "actual": 3.74964572508, "silicon": 31.8795180723, "which": 15.077877674999998, "databas": 8.24727272727, "intellig": 8.38668779714, "imagenet": 6350.4, "workflow": 1107.627906978, "activ": 7.32017705645, "daweon": 1587.6, "other": 2.01984732824, "appli": 4.5944147012, "reweigh": 4762.799999999999, "consist": 2.9802890932999997, "show": 3.80111731845, "hope": 2.50884955752, "see": 8.90694878577, "codebas": 793.8, "becaus": 1.1495184997499999, "imag": 213.40888208259003, "ineffici": 26.154859967100002, "get": 14.2850073108, "like": 11.491856677500001, "implement": 7.15296237892, "examin": 3.8505942275, "manag": 1.6448404475799998, "flickr": 206.181818182, "imagin": 6.598503740650001, "file": 3.7710213776699995, "detect": 16.23866348448, "part": 1.04330682789, "add": 9.22486926206, "touch": 5.45567010309, "yet": 2.1258703802900003, "includ": 2.0381282495599997, "cool": 6.8578833693300005, "anyon": 5.37440758294, "this": 36.13657056156, "pretti": 15.75, "time": 4.04509841392, "text": 34.41103448277, "engin": 9.88542963884, "predictor": 100.481012658, "similar": 42.62936336067, "express": 1.9120799710900003, "project": 1.7534791252500002, "differ": 2.4730898045, "embed": 437.72640509, "been": 6.143566591439999, "profession": 2.6389627659599997, "decis": 2.16, "valuabl": 7.46754468485, "most": 2.04192926046, "between": 3.1036100612399995, "ryu": 453.6, "hack": 43.3770491803, "domain": 9.39408284024, "credit": 9.129384703860001, "along": 1.2973768080399999, "all": 5.05733944955, "top": 3.6775538568400004, "input": 61.0146041505, "varieti": 2.2972073506, "program": 4.04278074866, "someth": 6.56304257958, "there": 6.24547600314, "gradient": 41.889182058, "condit": 1.92483026188, "appropri": 4.31413043478, "longer": 2.02319357716, "middl": 2.04245465071, "content": 7.0843373494, "that": 17.06772908375, "pair": 4.36873968079, "weight": 24.394591272299998, "architectur": 5.12790697674, "tradeoff": 208.89473684200001, "call": 1.0676529926, "object": 4.697736351540001, "more": 11.188877498699998, "and": 49.00308661437, "histogram": 1058.4, "list": 1.36321483771, "type": 2.0281042411900003, "these": 9.66738836268, "hors": 5.073825503359999, "cat": 126.4180491036, "gain": 1.84819557625, "contentbas": 1587.6, "could": 2.4087391898, "reus": 29.7861163227, "befor": 3.30108123093, "micah": 260.262295082, "state": 1.0477133240899998, "download": 14.6457564576, "close": 2.5697636775599997, "orient": 4.994023277759999, "can": 16.46765947988, "avail": 1.7288467821, "tie": 3.31786833856, "comput": 15.711034141520003, "make": 3.2287980475800007, "queri": 112.5957446808, "way": 8.5335176227, "search": 94.36441893826999, "onli": 6.153885909960001, "each": 5.948741007200001, "size": 4.9877474081, "few": 1.31729173581, "well": 4.2622994832, "cloth": 4.69843148861, "accur": 17.306686046520003, "pretrain": 7938.0, "link": 2.15151104486, "common": 5.6103896103999995, "freeli": 11.3643521832, "data": 20.25861335604, "qualiti": 2.9329392204, "method": 10.285714285720001, "york": 1.53361669243, "repres": 2.93945565636, "sometim": 1.7126213592200001, "airbnb": 1587.6, "vast": 4.05620848237, "come": 1.32831325301, "exampl": 12.03867298576, "worth": 5.210370856580001, "abil": 2.70875277256, "understand": 8.90575916229, "scalabl": 560.329411764, "import": 1.3401992233700002, "first": 1.00761614623, "everi": 2.95835274388, "luckili": 191.277108434, "when": 2.0415353951, "have": 16.238317458239997, "score": 8.576985413300001, "without": 2.59094247246, "often": 5.1780821918, "prototyp": 11.7426035503, "learn": 23.2275054865, "optim": 23.0755813954, "effici": 25.4667949952, "has": 6.261898501199999}, "logtfidf": {"after": 0.020490694648099998, "hand": 0.479471335336, "real": 0.824629060574, "semant": 14.6648426172, "art": 0.6928952596619999, "visibl": 1.5445687581299998, "onc": 1.211297617065, "too": 0.5965551547219999, "ahead": 1.7273626814900003, "sheep": 5.76493315678, "wide": 0.44458000675399995, "kind": 1.896062605434, "addit": 1.3213132978320001, "would": 0.7165586516822999, "what": 1.581211077789, "datasetbottlenumjpg": 14.739957441820001, "taken": 0.470759772949, "success": 0.27765441259199997, "dataset": 89.51935763288, "wikipedia": 3.70130197411, "function": 0.914465741594, "class": 8.997266279196001, "dog": 1.8346148978799999, "crop": 2.27316573057, "pet": 2.8937789162199996, "bare": 2.17646646873, "product": 0.484060136536, "work": 0.545172836365, "never": 0.443205436091, "teach": 1.38277323072, "green": 1.9344653607420002, "approach": 8.762803374972002, "technic": 1.14423287808, "tackl": 2.98920286814, "their": 0.046081515368100005, "highlight": 3.52614864246, "automat": 3.8301700946399997, "particular": 0.969472181412, "measur": 0.880014199726, "cosin": 10.53168913328, "creat": 0.222576818514, "pairwis": 5.958991747200001, "multipl": 1.01092401812, "selfstand": 7.369978720910001, "how": 4.244102612370001, "perceiv": 1.5938755846700001, "maintain": 0.572708175102, "had": 0.0464780244111, "especi": 0.511098609709, "initi": 0.6002091849, "end": 0.101476798618, "found": 0.107841124048, "given": 0.606511621662, "uniqu": 1.1039173409, "interest": 1.416215333946, "deep": 7.7320408188, "will": 2.4334384189800002, "weigh": 2.39739149445, "numxnum": 3.9522520373, "consid": 0.429789447648, "site": 0.6791364434899999, "write": 0.721512439877, "accompani": 1.21831042226, "disk": 6.419068713960001, "peter": 0.850536491217, "incorpor": 0.9664045229739999, "new": 0.0709197874044, "below": 3.254506367744, "perform": 0.42618085058, "but": 0.1295389765752, "demonstr": 0.9745501918189999, "need": 1.088220490326, "our": 33.447929353098004, "classif": 8.35116294516, "annot": 3.1300918533999997, "final": 1.170935455792, "feel": 4.5713973677199995, "sunglass": 5.08759633523, "applic": 2.46320785698, "lead": 0.23620402986699998, "struggl": 1.2119409739799998, "discard": 2.95113811311, "enough": 0.802884439169, "collabor": 1.4939250253100003, "pass": 0.96260865948, "use": 0.8470325722164, "experi": 1.252545907866, "appeal": 1.2978562707799999, "streamlit": 7.369978720910001, "annoy": 6.5781143580400006, "out": 0.1752791727579, "model": 14.74900146222, "onlin": 0.957503854357, "suffer": 0.7706525875229999, "good": 0.418589404907, "alway": 0.726319204572, "slow": 2.7964136143, "comment": 1.11826753454, "glove": 3.4460271446199995, "huge": 2.9583271639, "categori": 6.9088473326, "julia": 3.04320056047, "forward": 3.8970302180700003, "reason": 2.177206211848, "bus": 1.94502870343, "view": 0.49515994217299997, "not": 0.1088668910525, "far": 0.536623764503, "involv": 0.371469078658, "emmanuel": 7.22221378994, "revers": 1.45836939905, "store": 3.7123462005600008, "happen": 1.08640441802, "quick": 3.162910035596, "pleas": 2.21149829955, "howev": 0.36126046939, "young": 0.6358627983380001, "adopt": 1.4301066072220001, "shown": 1.01856958099, "handl": 1.36683266903, "solv": 1.9836504770400003, "than": 0.1290434488728, "then": 0.16606773046179998, "index": 5.83640798736, "they": 0.0891809843028, "rental": 3.21422553056, "anoth": 0.255792723304, "reach": 0.40414323085000003, "last": 0.38408728922200003, "let": 4.995210269119999, "scienc": 1.682872357782, "general": 0.229905156126, "blog": 2.65237310559, "away": 0.615957541869, "featur": 0.423387418142, "item": 8.1252715146, "practic": 0.533182530867, "lack": 0.656050938907, "simpl": 2.4464425787799997, "cow": 2.9804800713999997, "might": 1.5366821530680002, "calcul": 5.439451977629999, "twitter": 3.50295308141, "valley": 1.4390606736700002, "valu": 1.646386620296, "either": 0.459327638815, "accord": 0.243650319127, "tvmonitor": 7.369978720910001, "should": 0.509419876758, "pinterest": 14.549337082200001, "such": 0.179087933418, "thousand": 0.906949263988, "network": 2.8592491591559996, "three": 0.06411868822490001, "fast": 9.502170148440001, "some": 0.0395735090645, "collect": 0.49536666052, "both": 0.20337013355720002, "version": 0.697313064259, "fellow": 4.420123364459999, "classic": 0.8791034528499999, "filter": 2.82668393864, "tex": 4.44681714019, "slowli": 1.8050752452, "set": 0.342992022578, "third": 0.35032434942900004, "ani": 0.376825075098, "two": 0.0136988443582, "divid": 0.8402679544589999, "from": 0.0034023250131959997, "stitchfix": 7.369978720910001, "answer": 1.5366310419, "allow": 0.720841833567, "limit": 0.83564770926, "detail": 0.816187777173, "num": 0.005669827117146001, "veri": 0.920639172952, "brows": 4.63561121149, "free": 2.1650665970680003, "aeroplan": 4.16317547727, "for": 0.014174567792865002, "car": 2.5268027334, "predict": 1.6457402376899999, "output": 8.15290631308, "bottom": 3.6723881067199997, "word": 5.272749741465, "seen": 0.47672812813, "with": 0.00957993370712, "seem": 0.829093032276, "zero": 2.1692741832299998, "are": 0.589349471654, "solut": 3.10692595254, "recommend": 6.82305634315, "better": 0.6964279406, "compani": 0.439777253097, "caption": 8.03314400616, "current": 0.42695282784500005, "meaning": 3.08226276571, "team": 0.821902894886, "look": 5.1710935488, "popular": 0.41058020877499996, "promin": 0.8744110957960001, "bicycl": 2.9063720992400004, "harder": 2.84061024834, "think": 1.06717661175, "negat": 1.32402598852, "googl": 4.86526244516, "amount": 0.819898886199, "focus": 0.6981989720559999, "though": 0.616088382158, "artifici": 4.23645798036, "hacki": 7.369978720910001, "disregard": 3.09053867501, "doe": 1.0680834594339998, "even": 0.609554259336, "hodosh": 7.369978720910001, "infer": 3.0511581621399997, "simpler": 5.7716937266, "extract": 2.04161723301, "mentorship": 5.822416212189999, "train": 4.626428189873001, "great": 0.707415774237, "avoid": 0.900108441291, "goal": 1.18830712273, "motorbik": 5.21821651765, "error": 1.7985854343, "liked\u2026": 7.369978720910001, "librari": 1.973619961886, "diningt": 7.369978720910001, "note": 0.353817568083, "shift": 1.20351099781, "row": 1.71363732085, "scale": 2.64190612656, "boat": 1.86668177367, "notebook": 7.387356098, "again": 0.411340231612, "ameisen": 7.369978720910001, "bias": 5.23968552934, "abov": 0.643865229816, "requir": 0.424253510675, "strong": 0.49712549393600003, "buy": 1.6340517929299998, "local": 0.416867740206, "choos": 1.43007066072, "head": 0.456042582852, "flexibl": 2.2707222351599996, "help": 0.336207721344, "main": 0.225571540588, "larg": 0.8518753030300001, "find": 6.573375963456, "bird": 1.86627452464, "photo": 5.578129639139999, "line": 0.349430614452, "nuanc": 3.87650606314, "own": 0.164195077421, "base": 0.40956990686100003, "label": 14.9898832727, "pictur": 1.25144109124, "allencompass": 7.369978720910001, "pottedpl": 7.369978720910001, "post": 2.4171004581029996, "here": 2.6551145651100003, "ideal": 1.53809624363, "factor": 1.06169814662, "techniqu": 1.31624384807, "cyrus": 4.04494270021, "attent": 1.03332999658, "through": 0.2734347675396, "human": 1.280070366486, "layer": 14.678854136450003, "fundament": 1.67322086119, "siames": 20.3914113416, "contain": 0.937690636472, "noisi": 12.393900806220001, "about": 0.18853043242380002, "problem": 2.276562897092, "bottl": 21.897546943120002, "neural": 4.0853151555, "thing": 1.7563870693599999, "rashtchian": 7.369978720910001, "just": 0.579062868218, "adapt": 1.2007864860200002, "correct": 2.59663526362, "simpli": 2.771824474758, "chair": 5.8857314712, "represent": 16.017644588849997, "attach": 1.5015296247, "check": 1.87281049562, "vacat": 2.56186769092, "led": 0.29104709623799996, "percept": 4.2438051416, "overhead": 3.18032397888, "start": 0.945773477164, "outfit": 3.1373225428900002, "took": 0.337177952953, "thus": 0.9971525427860001, "run": 1.328144926617, "them": 0.3767333076372, "insight": 14.81048713122, "load": 5.75296062564, "studi": 0.426470272221, "vector": 32.5419887797, "spars": 6.08904487544, "order": 0.22014038079300002, "nearest": 2.71792494902, "whi": 3.54206529141, "quit": 3.17854541052, "usual": 0.545279017064, "abl": 1.19860796495, "mani": 0.3465260649768, "refer": 0.262553246798, "over": 0.0249367214957, "step": 3.11863517094, "generat": 2.876729366944, "features\u2026": 7.369978720910001, "scratch": 3.2509415461, "provid": 0.585533532975, "total": 0.8713577734100001, "broad": 1.45323772, "consum": 3.1908543507200005, "chat": 3.58806440083, "exact": 1.2437647732500001, "catlik": 7.369978720910001, "penultim": 4.15913506774, "endtoend": 7.369978720910001, "inform": 0.454453704662, "extrem": 1.7224191678740002, "selfi": 7.369978720910001, "shelf": 3.85843328208, "possibl": 0.348805474891, "semisupervis": 7.369978720910001, "origin": 0.128612437587, "right": 0.34035985417, "wow": 4.513508514690001, "correl": 2.57915918803, "softwar": 2.32849096333, "take": 0.653459810985, "visual": 3.3078766977200003, "follow": 0.09071382218839999, "now": 0.44727883506300004, "matter": 1.7903250540720002, "code": 5.42407638388, "hard": 1.00522796406, "simplic": 3.2364134455299998, "high": 0.13782378654000002, "power": 0.292396282715, "leverag": 7.153478502939999, "break": 0.88733019029, "issu": 1.092297131802, "tutori": 12.2559454665, "center": 0.555216308776, "also": 0.0293143156, "trick": 5.3794021248599995, "person": 0.6803656320360001, "mean": 0.74184256704, "style": 0.866289529121, "brought": 0.609100637788, "one": 0.0437874615185, "evolut": 1.85011908441, "notic": 1.47474978168, "task": 1.35748680661, "user": 6.127764320639999, "research": 0.663727818138, "plan": 0.428982108147, "script": 2.1161358444599996, "signific": 0.373571744332, "therefor": 0.847591848336, "the": 0.0, "sofa": 5.0774439637699995, "becom": 0.23542435297800002, "recent": 0.434413741288, "tag": 5.96592908944, "peopl": 0.579796735419, "stand": 0.7345572374320001, "hockenmai": 7.369978720910001, "dozen": 1.78385428972, "challeng": 0.9392919688950001, "machin": 1.39235958062, "want": 2.7665464250199996, "sinc": 0.2411045983731, "system": 0.65486069117, "build": 3.437962164637, "rescal": 6.08904487545, "overtak": 3.9168216003199996, "actual": 1.257028363296, "silicon": 3.46196373688, "which": 0.07767620768145, "databas": 2.10988256718, "intellig": 2.86699696426, "imagenet": 29.479914883640003, "workflow": 17.73409109463, "activ": 1.90598301642, "daweon": 7.369978720910001, "other": 0.01974949583952, "appli": 1.6633883796239999, "reweigh": 22.10993616273, "consist": 0.797746252852, "show": 0.710048298039, "hope": 0.919824304455, "see": 1.686451098444, "codebas": 6.676831540349999, "becaus": 0.139343158825, "imag": 78.50720647591, "ineffici": 3.2640350228400004, "get": 4.638152046256, "like": 1.3905357654500001, "implement": 2.54875881814, "examin": 1.3482274812000001, "manag": 0.497643387158, "flickr": 5.32875839205, "imagin": 1.88684291737, "file": 1.32734588723, "detect": 5.0663482347899995, "part": 0.04239531098280001, "add": 3.05751167426, "touch": 1.6966554537399998, "yet": 0.754181309241, "includ": 0.037769362781, "cool": 1.9253988473800001, "anyon": 1.68164835081, "this": 0.13631216589, "pretti": 2.75684036527, "time": 0.0448460754504, "text": 12.545302109889999, "engin": 3.619070233104, "predictor": 4.609968780880001, "similar": 9.875238855534, "express": 0.648191639641, "project": 0.561601885907, "differ": 0.424642242624, "embed": 73.41093581302, "been": 0.14187589421040003, "profession": 0.970385948273, "decis": 0.7701082216959999, "valuabl": 2.010566255, "most": 0.041495792591199995, "between": 0.10186104349589999, "ryu": 6.117215752409999, "hack": 3.7699304805000002, "domain": 2.24008000599, "credit": 3.33865803264, "along": 0.260344385917, "all": 0.057013160488999994, "top": 1.218201275576, "input": 12.50837667695, "varieti": 0.8316941898119999, "program": 1.4075711575299998, "someth": 2.37661424546, "there": 0.24058735755299998, "gradient": 3.73502760882, "condit": 0.654837788206, "appropri": 1.4618957827399999, "longer": 0.7046772417749999, "middl": 0.7141523446729999, "content": 2.52947831908, "that": 0.06759452245388, "pair": 1.47447456495, "weight": 7.9246176306, "architectur": 1.63469757919, "tradeoff": 5.34183047362, "call": 0.0654627744488, "object": 1.707867169606, "more": 0.18727424759999997, "and": 0.0030865169611164, "histogram": 6.964513612799999, "list": 0.309845761506, "type": 0.707101485387, "these": 0.6438025746072, "hors": 1.62409507023, "cat": 28.256251383360002, "gain": 0.6142097989249999, "contentbas": 7.369978720910001, "could": 0.37191254458000006, "reus": 3.3940423897400005, "befor": 0.2869133156385, "micah": 5.561689949730001, "state": 0.0466100027668, "download": 2.6841506319, "close": 0.501333519728, "orient": 1.60824185299, "can": 2.272775349516, "avail": 0.547454586289, "tie": 1.19932251002, "comput": 5.47227566376, "make": 0.22049297346869998, "queri": 8.06131348592, "way": 1.3866405695450001, "search": 34.21617936860999, "onli": 0.1519456099746, "each": 0.86870844652, "size": 1.8276744121219999, "few": 0.275577913653, "well": 0.2540577532624, "cloth": 1.5472287271899998, "accur": 5.25744184455, "pretrain": 36.849893604550005, "link": 0.7661704068449999, "common": 1.353303221084, "freeli": 2.43048145465, "data": 7.3009235088, "qualiti": 1.07600506711, "method": 3.777846435364, "york": 0.42762879716200003, "repres": 0.7701544655, "sometim": 0.538025155343, "airbnb": 7.369978720910001, "vast": 1.40024866595, "come": 0.28390990653000003, "exampl": 3.2694613999919997, "worth": 1.65065103492, "abil": 0.996488297427, "understand": 3.264257627039999, "scalabl": 15.689737672229999, "import": 0.292818277066, "first": 0.0075872898121599995, "everi": 0.782970854842, "luckili": 5.25372320611, "when": 0.0411099777168, "have": 0.2365600374592, "score": 2.9118706415400006, "without": 0.517749035882, "often": 1.032561573404, "prototyp": 2.4632235573, "learn": 8.427520647449999, "optim": 4.891255590819999, "effici": 8.139687670699999, "has": 0.2563436691288}, "logidf": {"after": 0.020490694648099998, "hand": 0.479471335336, "real": 0.824629060574, "semant": 3.6662106543, "art": 0.6928952596619999, "visibl": 1.5445687581299998, "onc": 0.403765872355, "too": 0.5965551547219999, "ahead": 1.7273626814900003, "sheep": 2.88246657839, "wide": 0.44458000675399995, "kind": 0.948031302717, "addit": 0.220218882972, "would": 0.0796176279647, "what": 0.225887296827, "datasetbottlenumjpg": 7.369978720910001, "taken": 0.470759772949, "success": 0.27765441259199997, "dataset": 5.26584456664, "wikipedia": 3.70130197411, "function": 0.914465741594, "class": 0.7497721899330001, "dog": 1.8346148978799999, "crop": 2.27316573057, "pet": 2.8937789162199996, "bare": 2.17646646873, "product": 0.484060136536, "work": 0.109034567273, "never": 0.443205436091, "teach": 1.38277323072, "green": 0.9672326803710001, "approach": 0.7302336145810001, "technic": 1.14423287808, "tackl": 2.98920286814, "their": 0.015360505122700001, "highlight": 1.76307432123, "automat": 1.9150850473199998, "particular": 0.323157393804, "measur": 0.880014199726, "cosin": 5.26584456664, "creat": 0.222576818514, "pairwis": 5.958991747200001, "multipl": 1.01092401812, "selfstand": 7.369978720910001, "how": 0.47156695693000006, "perceiv": 1.5938755846700001, "maintain": 0.572708175102, "had": 0.0464780244111, "especi": 0.511098609709, "initi": 0.30010459245, "end": 0.101476798618, "found": 0.107841124048, "given": 0.303255810831, "uniqu": 1.1039173409, "interest": 0.47207177798199995, "deep": 1.2886734698, "will": 0.202786534915, "weigh": 2.39739149445, "numxnum": 3.9522520373, "consid": 0.214894723824, "site": 0.6791364434899999, "write": 0.721512439877, "accompani": 1.21831042226, "disk": 3.2095343569800003, "peter": 0.850536491217, "incorpor": 0.9664045229739999, "new": 0.0177299468511, "below": 0.813626591936, "perform": 0.42618085058, "but": 0.0161923720719, "demonstr": 0.9745501918189999, "need": 0.362740163442, "our": 0.8576392141820001, "classif": 2.08779073629, "annot": 3.1300918533999997, "final": 0.292733863948, "feel": 1.1428493419299999, "sunglass": 5.08759633523, "applic": 1.23160392849, "lead": 0.23620402986699998, "struggl": 1.2119409739799998, "discard": 2.95113811311, "enough": 0.802884439169, "collabor": 1.4939250253100003, "pass": 0.48130432974, "use": 0.0292080197316, "experi": 0.626272953933, "appeal": 1.2978562707799999, "streamlit": 7.369978720910001, "annoy": 3.2890571790200003, "out": 0.0584263909193, "model": 0.7374500731110001, "onlin": 0.957503854357, "suffer": 0.7706525875229999, "good": 0.418589404907, "alway": 0.726319204572, "slow": 1.39820680715, "comment": 1.11826753454, "glove": 3.4460271446199995, "huge": 1.47916358195, "categori": 1.38176946652, "julia": 3.04320056047, "forward": 1.29901007269, "reason": 0.544301552962, "bus": 1.94502870343, "view": 0.49515994217299997, "not": 0.0155524130075, "far": 0.536623764503, "involv": 0.371469078658, "emmanuel": 3.61110689497, "revers": 1.45836939905, "store": 1.2374487335200002, "happen": 1.08640441802, "quick": 0.790727508899, "pleas": 2.21149829955, "howev": 0.0903151173475, "young": 0.6358627983380001, "adopt": 0.7150533036110001, "shown": 1.01856958099, "handl": 1.36683266903, "solv": 1.9836504770400003, "than": 0.0322608622182, "then": 0.08303386523089999, "index": 1.94546932912, "they": 0.0297269947676, "rental": 3.21422553056, "anoth": 0.127896361652, "reach": 0.40414323085000003, "last": 0.19204364461100001, "let": 1.2488025672799998, "scienc": 0.841436178891, "general": 0.114952578063, "blog": 2.65237310559, "away": 0.615957541869, "featur": 0.423387418142, "item": 1.62505430292, "practic": 0.533182530867, "lack": 0.656050938907, "simpl": 1.2232212893899999, "cow": 2.9804800713999997, "might": 0.7683410765340001, "calcul": 1.8131506592099997, "twitter": 3.50295308141, "valley": 1.4390606736700002, "valu": 0.823193310148, "either": 0.459327638815, "accord": 0.243650319127, "tvmonitor": 7.369978720910001, "should": 0.509419876758, "pinterest": 7.2746685411000005, "such": 0.059695977806, "thousand": 0.906949263988, "network": 0.9530830530519999, "three": 0.06411868822490001, "fast": 1.5836950247400001, "some": 0.0395735090645, "collect": 0.49536666052, "both": 0.050842533389300004, "version": 0.697313064259, "fellow": 1.4733744548199998, "classic": 0.8791034528499999, "filter": 2.82668393864, "tex": 4.44681714019, "slowli": 1.8050752452, "set": 0.171496011289, "third": 0.35032434942900004, "ani": 0.125608358366, "two": 0.0136988443582, "divid": 0.8402679544589999, "from": 0.000567054168866, "stitchfix": 7.369978720910001, "answer": 1.5366310419, "allow": 0.24028061118900002, "limit": 0.41782385463, "detail": 0.816187777173, "num": 0.00031499039539700004, "veri": 0.230159793238, "brows": 4.63561121149, "free": 0.5412666492670001, "aeroplan": 4.16317547727, "for": 0.00031499039539700004, "car": 1.2634013667, "predict": 1.6457402376899999, "output": 2.03822657827, "bottom": 1.8361940533599999, "word": 0.585861082385, "seen": 0.47672812813, "with": 0.00119749171339, "seem": 0.829093032276, "zero": 2.1692741832299998, "are": 0.0294674735827, "solut": 1.55346297627, "recommend": 1.36461126863, "better": 0.6964279406, "compani": 0.439777253097, "caption": 4.01657200308, "current": 0.42695282784500005, "meaning": 3.08226276571, "team": 0.821902894886, "look": 0.6463866936, "popular": 0.41058020877499996, "promin": 0.8744110957960001, "bicycl": 2.9063720992400004, "harder": 2.84061024834, "think": 1.06717661175, "negat": 1.32402598852, "googl": 2.43263122258, "amount": 0.819898886199, "focus": 0.6981989720559999, "though": 0.308044191079, "artifici": 2.11822899018, "hacki": 7.369978720910001, "disregard": 3.09053867501, "doe": 0.5340417297169999, "even": 0.152388564834, "hodosh": 7.369978720910001, "infer": 3.0511581621399997, "simpler": 2.8858468633, "extract": 2.04161723301, "mentorship": 5.822416212189999, "train": 0.660918312839, "great": 0.235805258079, "avoid": 0.900108441291, "goal": 1.18830712273, "motorbik": 5.21821651765, "error": 1.7985854343, "liked\u2026": 7.369978720910001, "librari": 0.986809980943, "diningt": 7.369978720910001, "note": 0.353817568083, "shift": 1.20351099781, "row": 1.71363732085, "scale": 1.32095306328, "boat": 1.86668177367, "notebook": 3.693678049, "again": 0.411340231612, "ameisen": 7.369978720910001, "bias": 2.61984276467, "abov": 0.643865229816, "requir": 0.424253510675, "strong": 0.49712549393600003, "buy": 1.6340517929299998, "local": 0.416867740206, "choos": 1.43007066072, "head": 0.456042582852, "flexibl": 2.2707222351599996, "help": 0.336207721344, "main": 0.225571540588, "larg": 0.17037506060600002, "find": 0.547781330288, "bird": 1.86627452464, "photo": 1.8593765463799998, "line": 0.349430614452, "nuanc": 3.87650606314, "own": 0.164195077421, "base": 0.13652330228700002, "label": 1.49898832727, "pictur": 1.25144109124, "allencompass": 7.369978720910001, "pottedpl": 7.369978720910001, "post": 0.8057001527009999, "here": 0.8850381883700001, "ideal": 1.53809624363, "factor": 1.06169814662, "techniqu": 1.31624384807, "cyrus": 4.04494270021, "attent": 1.03332999658, "through": 0.0683586918849, "human": 0.640035183243, "layer": 2.0969791623500003, "fundament": 1.67322086119, "siames": 5.0978528354, "contain": 0.468845318236, "noisi": 4.1313002687400004, "about": 0.0628434774746, "problem": 0.569140724273, "bottl": 2.7371933678900002, "neural": 4.0853151555, "thing": 0.8781935346799999, "rashtchian": 7.369978720910001, "just": 0.289531434109, "adapt": 1.2007864860200002, "correct": 1.29831763181, "simpli": 0.923941491586, "chair": 1.9619104904, "represent": 1.7797382876499999, "attach": 1.5015296247, "check": 1.87281049562, "vacat": 2.56186769092, "led": 0.29104709623799996, "percept": 2.1219025708, "overhead": 3.18032397888, "start": 0.236443369291, "outfit": 3.1373225428900002, "took": 0.337177952953, "thus": 0.49857627139300004, "run": 0.442714975539, "them": 0.0941833269093, "insight": 2.46841452187, "load": 1.91765354188, "studi": 0.426470272221, "vector": 3.25419887797, "spars": 3.04452243772, "order": 0.22014038079300002, "nearest": 2.71792494902, "whi": 1.18068843047, "quit": 1.05951513684, "usual": 0.545279017064, "abl": 0.599303982475, "mani": 0.0433157581221, "refer": 0.262553246798, "over": 0.0249367214957, "step": 1.03954505698, "generat": 0.719182341736, "features\u2026": 7.369978720910001, "scratch": 3.2509415461, "provid": 0.19517784432500002, "total": 0.43567888670500005, "broad": 1.45323772, "consum": 1.5954271753600002, "chat": 3.58806440083, "exact": 1.2437647732500001, "catlik": 7.369978720910001, "penultim": 4.15913506774, "endtoend": 7.369978720910001, "inform": 0.454453704662, "extrem": 0.8612095839370001, "selfi": 7.369978720910001, "shelf": 3.85843328208, "possibl": 0.348805474891, "semisupervis": 7.369978720910001, "origin": 0.128612437587, "right": 0.34035985417, "wow": 4.513508514690001, "correl": 2.57915918803, "softwar": 2.32849096333, "take": 0.130691962197, "visual": 1.6539383488600001, "follow": 0.045356911094199995, "now": 0.149092945021, "matter": 0.8951625270360001, "code": 1.35601909597, "hard": 1.00522796406, "simplic": 3.2364134455299998, "high": 0.13782378654000002, "power": 0.292396282715, "leverag": 3.5767392514699994, "break": 0.88733019029, "issu": 0.364099043934, "tutori": 4.0853151555, "center": 0.555216308776, "also": 0.0146571578, "trick": 2.6897010624299997, "person": 0.34018281601800004, "mean": 0.37092128352, "style": 0.866289529121, "brought": 0.609100637788, "one": 0.0062553516455, "evolut": 1.85011908441, "notic": 1.47474978168, "task": 1.35748680661, "user": 2.04258810688, "research": 0.663727818138, "plan": 0.428982108147, "script": 2.1161358444599996, "signific": 0.373571744332, "therefor": 0.847591848336, "the": 0.0, "sofa": 5.0774439637699995, "becom": 0.11771217648900001, "recent": 0.434413741288, "tag": 2.98296454472, "peopl": 0.193265578473, "stand": 0.7345572374320001, "hockenmai": 7.369978720910001, "dozen": 1.78385428972, "challeng": 0.9392919688950001, "machin": 1.39235958062, "want": 0.6916366062549999, "sinc": 0.0803681994577, "system": 0.327430345585, "build": 0.491137452091, "rescal": 6.08904487545, "overtak": 3.9168216003199996, "actual": 0.628514181648, "silicon": 3.46196373688, "which": 0.00517841384543, "databas": 2.10988256718, "intellig": 1.43349848213, "imagenet": 7.369978720910001, "workflow": 5.91136369821, "activ": 0.381196603284, "daweon": 7.369978720910001, "other": 0.00987474791976, "appli": 0.8316941898119999, "reweigh": 7.369978720910001, "consist": 0.398873126426, "show": 0.236682766013, "hope": 0.919824304455, "see": 0.240921585492, "codebas": 6.676831540349999, "becaus": 0.139343158825, "imag": 0.99376210729, "ineffici": 3.2640350228400004, "get": 0.579769005782, "like": 0.139053576545, "implement": 1.27437940907, "examin": 1.3482274812000001, "manag": 0.497643387158, "flickr": 5.32875839205, "imagin": 1.88684291737, "file": 1.32734588723, "detect": 1.68878274493, "part": 0.04239531098280001, "add": 1.52875583713, "touch": 1.6966554537399998, "yet": 0.754181309241, "includ": 0.0188846813905, "cool": 1.9253988473800001, "anyon": 1.68164835081, "this": 0.0037864490525, "pretti": 2.75684036527, "time": 0.0112115188626, "text": 1.14048200999, "engin": 0.904767558276, "predictor": 4.609968780880001, "similar": 0.318556092114, "express": 0.648191639641, "project": 0.561601885907, "differ": 0.212321121312, "embed": 2.82349753127, "been": 0.023645982368400004, "profession": 0.970385948273, "decis": 0.7701082216959999, "valuabl": 2.010566255, "most": 0.020747896295599998, "between": 0.033953681165299995, "ryu": 6.117215752409999, "hack": 3.7699304805000002, "domain": 2.24008000599, "credit": 1.11288601088, "along": 0.260344385917, "all": 0.011402632097799998, "top": 0.609100637788, "input": 2.50167533539, "varieti": 0.8316941898119999, "program": 0.7037855787649999, "someth": 1.18830712273, "there": 0.0400978929255, "gradient": 3.73502760882, "condit": 0.654837788206, "appropri": 1.4618957827399999, "longer": 0.7046772417749999, "middl": 0.7141523446729999, "content": 1.26473915954, "that": 0.00397614837964, "pair": 1.47447456495, "weight": 1.58492352612, "architectur": 1.63469757919, "tradeoff": 5.34183047362, "call": 0.0654627744488, "object": 0.853933584803, "more": 0.017024931599999998, "and": 6.29901420636e-05, "histogram": 6.964513612799999, "list": 0.309845761506, "type": 0.707101485387, "these": 0.0715336194008, "hors": 1.62409507023, "cat": 2.35468761528, "gain": 0.6142097989249999, "contentbas": 7.369978720910001, "could": 0.18595627229000003, "reus": 3.3940423897400005, "befor": 0.0956377718795, "micah": 5.561689949730001, "state": 0.0466100027668, "download": 2.6841506319, "close": 0.250666759864, "orient": 1.60824185299, "can": 0.162341096394, "avail": 0.547454586289, "tie": 1.19932251002, "comput": 1.36806891594, "make": 0.07349765782289999, "queri": 4.03065674296, "way": 0.19809150993500002, "search": 1.1798682540899998, "onli": 0.025324268329099998, "each": 0.173741689304, "size": 0.9138372060609999, "few": 0.275577913653, "well": 0.0635144383156, "cloth": 1.5472287271899998, "accur": 1.75248061485, "pretrain": 7.369978720910001, "link": 0.7661704068449999, "common": 0.338325805271, "freeli": 2.43048145465, "data": 1.2168205848, "qualiti": 1.07600506711, "method": 0.944461608841, "york": 0.42762879716200003, "repres": 0.38507723275, "sometim": 0.538025155343, "airbnb": 7.369978720910001, "vast": 1.40024866595, "come": 0.28390990653000003, "exampl": 0.40868267499899996, "worth": 1.65065103492, "abil": 0.996488297427, "understand": 1.0880858756799998, "scalabl": 5.22991255741, "import": 0.292818277066, "first": 0.0075872898121599995, "everi": 0.391485427421, "luckili": 5.25372320611, "when": 0.0205549888584, "have": 0.0147850023412, "score": 1.4559353207700003, "without": 0.258874517941, "often": 0.258140393351, "prototyp": 2.4632235573, "learn": 0.842752064745, "optim": 2.4456277954099996, "effici": 1.62793753414, "has": 0.0427239448548}, "freq": {"after": 1, "hand": 1, "real": 1, "semant": 4, "art": 1, "visibl": 1, "onc": 3, "too": 1, "ahead": 1, "sheep": 2, "wide": 1, "kind": 2, "addit": 6, "would": 9, "what": 7, "datasetbottlenumjpg": 2, "taken": 1, "success": 1, "dataset": 17, "wikipedia": 1, "function": 1, "class": 12, "dog": 1, "crop": 1, "pet": 1, "bare": 1, "product": 1, "work": 5, "never": 1, "teach": 1, "green": 2, "approach": 12, "technic": 1, "tackl": 1, "their": 3, "highlight": 2, "automat": 2, "particular": 3, "measur": 1, "cosin": 2, "creat": 1, "pairwis": 1, "multipl": 1, "selfstand": 1, "how": 9, "perceiv": 1, "maintain": 1, "had": 1, "especi": 1, "initi": 2, "end": 1, "found": 1, "given": 2, "uniqu": 1, "interest": 3, "deep": 6, "will": 12, "weigh": 1, "numxnum": 1, "consid": 2, "site": 1, "write": 1, "accompani": 1, "disk": 2, "peter": 1, "incorpor": 1, "new": 4, "below": 4, "perform": 1, "but": 8, "demonstr": 1, "need": 3, "our": 39, "classif": 4, "annot": 1, "final": 4, "feel": 4, "sunglass": 1, "applic": 2, "lead": 1, "struggl": 1, "discard": 1, "enough": 1, "collabor": 1, "pass": 2, "use": 29, "experi": 2, "appeal": 1, "streamlit": 1, "annoy": 2, "out": 3, "model": 20, "onlin": 1, "suffer": 1, "good": 1, "alway": 1, "slow": 2, "comment": 1, "glove": 1, "huge": 2, "categori": 5, "julia": 1, "forward": 3, "reason": 4, "bus": 1, "view": 1, "not": 7, "far": 1, "involv": 1, "emmanuel": 2, "revers": 1, "store": 3, "happen": 1, "quick": 4, "pleas": 1, "howev": 4, "young": 1, "adopt": 2, "shown": 1, "handl": 1, "solv": 1, "than": 4, "then": 2, "index": 3, "they": 3, "rental": 1, "anoth": 2, "reach": 1, "last": 2, "let": 4, "scienc": 2, "general": 2, "blog": 1, "away": 1, "featur": 1, "item": 5, "practic": 1, "lack": 1, "simpl": 2, "cow": 1, "might": 2, "calcul": 3, "twitter": 1, "valley": 1, "valu": 2, "either": 1, "accord": 1, "tvmonitor": 1, "should": 1, "pinterest": 2, "such": 3, "thousand": 1, "network": 3, "three": 1, "fast": 6, "some": 1, "collect": 1, "both": 4, "version": 1, "fellow": 3, "classic": 1, "filter": 1, "tex": 1, "slowli": 1, "set": 2, "third": 1, "ani": 3, "two": 1, "divid": 1, "from": 6, "stitchfix": 1, "answer": 1, "allow": 3, "limit": 2, "detail": 1, "num": 18, "veri": 4, "brows": 1, "free": 4, "aeroplan": 1, "for": 45, "car": 2, "predict": 1, "output": 4, "bottom": 2, "word": 9, "seen": 1, "with": 8, "seem": 1, "zero": 1, "are": 20, "solut": 2, "recommend": 5, "better": 1, "compani": 1, "caption": 2, "current": 1, "meaning": 1, "team": 1, "look": 8, "popular": 1, "promin": 1, "bicycl": 1, "harder": 1, "think": 1, "negat": 1, "googl": 2, "amount": 1, "focus": 1, "though": 2, "artifici": 2, "hacki": 1, "disregard": 1, "doe": 2, "even": 4, "hodosh": 1, "infer": 1, "simpler": 2, "extract": 1, "mentorship": 1, "train": 7, "great": 3, "avoid": 1, "goal": 1, "motorbik": 1, "error": 1, "liked\u2026": 1, "librari": 2, "diningt": 1, "note": 1, "shift": 1, "row": 1, "scale": 2, "boat": 1, "notebook": 2, "again": 1, "ameisen": 1, "bias": 2, "abov": 1, "requir": 1, "strong": 1, "buy": 1, "local": 1, "choos": 1, "head": 1, "flexibl": 1, "help": 1, "main": 1, "larg": 5, "find": 12, "bird": 1, "photo": 3, "line": 1, "nuanc": 1, "own": 1, "base": 3, "label": 10, "pictur": 1, "allencompass": 1, "pottedpl": 1, "post": 3, "here": 3, "ideal": 1, "factor": 1, "techniqu": 1, "cyrus": 1, "attent": 1, "through": 4, "human": 2, "layer": 7, "fundament": 1, "siames": 4, "contain": 2, "noisi": 3, "about": 3, "problem": 4, "bottl": 8, "neural": 1, "thing": 2, "rashtchian": 1, "just": 2, "adapt": 1, "correct": 2, "simpli": 3, "chair": 3, "represent": 9, "attach": 1, "check": 1, "vacat": 1, "led": 1, "percept": 2, "overhead": 1, "start": 4, "outfit": 1, "took": 1, "thus": 2, "run": 3, "them": 4, "insight": 6, "load": 3, "studi": 1, "vector": 10, "spars": 2, "order": 1, "nearest": 1, "whi": 3, "quit": 3, "usual": 1, "abl": 2, "mani": 8, "refer": 1, "over": 1, "step": 3, "generat": 4, "features\u2026": 1, "scratch": 1, "provid": 3, "total": 2, "broad": 1, "consum": 2, "chat": 1, "exact": 1, "catlik": 1, "penultim": 1, "endtoend": 1, "inform": 1, "extrem": 2, "selfi": 1, "shelf": 1, "possibl": 1, "semisupervis": 1, "origin": 1, "right": 1, "wow": 1, "correl": 1, "softwar": 1, "take": 5, "visual": 2, "follow": 2, "now": 3, "matter": 2, "code": 4, "hard": 1, "simplic": 1, "high": 1, "power": 1, "leverag": 2, "break": 1, "issu": 3, "tutori": 3, "center": 1, "also": 2, "trick": 2, "person": 2, "mean": 2, "style": 1, "brought": 1, "one": 7, "evolut": 1, "notic": 1, "task": 1, "user": 3, "research": 1, "plan": 1, "script": 1, "signific": 1, "therefor": 1, "the": 71, "sofa": 1, "becom": 2, "recent": 1, "tag": 2, "peopl": 3, "stand": 1, "hockenmai": 1, "dozen": 1, "challeng": 1, "machin": 1, "want": 4, "sinc": 3, "system": 2, "build": 7, "rescal": 1, "overtak": 1, "actual": 2, "silicon": 1, "which": 15, "databas": 1, "intellig": 2, "imagenet": 4, "workflow": 3, "activ": 5, "daweon": 1, "other": 2, "appli": 2, "reweigh": 3, "consist": 2, "show": 3, "hope": 1, "see": 7, "codebas": 1, "becaus": 1, "imag": 79, "ineffici": 1, "get": 8, "like": 10, "implement": 2, "examin": 1, "manag": 1, "flickr": 1, "imagin": 1, "file": 1, "detect": 3, "part": 1, "add": 2, "touch": 1, "yet": 1, "includ": 2, "cool": 1, "anyon": 1, "this": 36, "pretti": 1, "time": 4, "text": 11, "engin": 4, "predictor": 1, "similar": 31, "express": 1, "project": 1, "differ": 2, "embed": 26, "been": 6, "profession": 1, "decis": 1, "valuabl": 1, "most": 2, "between": 3, "ryu": 1, "hack": 1, "domain": 1, "credit": 3, "along": 1, "all": 5, "top": 2, "input": 5, "varieti": 1, "program": 2, "someth": 2, "there": 6, "gradient": 1, "condit": 1, "appropri": 1, "longer": 1, "middl": 1, "content": 2, "that": 17, "pair": 1, "weight": 5, "architectur": 1, "tradeoff": 1, "call": 1, "object": 2, "more": 11, "and": 49, "histogram": 1, "list": 1, "type": 1, "these": 9, "hors": 1, "cat": 12, "gain": 1, "contentbas": 1, "could": 2, "reus": 1, "befor": 3, "micah": 1, "state": 1, "download": 1, "close": 2, "orient": 1, "can": 14, "avail": 1, "tie": 1, "comput": 4, "make": 3, "queri": 2, "way": 7, "search": 29, "onli": 6, "each": 5, "size": 2, "few": 1, "well": 4, "cloth": 1, "accur": 3, "pretrain": 5, "link": 1, "common": 4, "freeli": 1, "data": 6, "qualiti": 1, "method": 4, "york": 1, "repres": 2, "sometim": 1, "airbnb": 1, "vast": 1, "come": 1, "exampl": 8, "worth": 1, "abil": 1, "understand": 3, "scalabl": 3, "import": 1, "first": 1, "everi": 2, "luckili": 1, "when": 2, "have": 16, "score": 2, "without": 2, "often": 4, "prototyp": 1, "learn": 10, "optim": 2, "effici": 5, "has": 6}, "idf": {"after": 1.02070207021, "hand": 1.6152202665600002, "real": 2.28103448276, "semant": 39.1034482759, "art": 1.9994962216599999, "visibl": 4.68595041322, "onc": 1.4974533106999999, "too": 1.81585268215, "ahead": 5.625797306869999, "sheep": 17.858267716500002, "wide": 1.5598349381, "kind": 2.5806241872599998, "addit": 1.24634950542, "would": 1.0828729281799998, "what": 1.25343439128, "datasetbottlenumjpg": 1587.6, "taken": 1.6012102874399998, "success": 1.32002993265, "dataset": 193.609756098, "wikipedia": 40.5, "function": 2.495441685, "class": 2.11651779763, "dog": 6.26272189349, "crop": 9.71009174312, "pet": 18.0614334471, "bare": 8.815102720710001, "product": 1.62264922322, "work": 1.11520089913, "never": 1.55769230769, "teach": 3.98594024605, "green": 2.63065451533, "approach": 2.07556543339, "technic": 3.1400316455699997, "tackl": 19.8698372966, "their": 1.01547908405, "highlight": 5.83033419023, "automat": 6.787516032490001, "particular": 1.3814827706200001, "measur": 2.41093394077, "cosin": 193.609756098, "creat": 1.2492917847, "pairwis": 387.219512195, "multipl": 2.74813917258, "selfstand": 1587.6, "how": 1.60250328051, "perceiv": 4.92279069767, "maintain": 1.77306231852, "had": 1.0475750577399998, "especi": 1.66712170534, "initi": 1.35, "end": 1.10680423871, "found": 1.11387076405, "given": 1.35426085473, "uniqu": 3.01595744681, "interest": 1.60331246213, "deep": 3.6279707495399998, "will": 1.22481098596, "weigh": 10.9944598338, "numxnum": 52.0524590164, "consid": 1.2397313759200002, "site": 1.9721739130400002, "write": 2.0575427682700003, "accompani": 3.38146964856, "disk": 24.767550701999998, "peter": 2.34090238868, "incorpor": 2.62847682119, "new": 1.0178880554, "below": 2.25607503197, "perform": 1.5313977042500002, "but": 1.01632417899, "demonstr": 2.64997496244, "need": 1.4372623574099999, "our": 2.35758835759, "classif": 8.067073170730001, "annot": 22.8760806916, "final": 1.34008609775, "feel": 3.1356903021900004, "sunglass": 162.0, "applic": 3.42672134686, "lead": 1.2664326739, "struggl": 3.36, "discard": 19.1277108434, "enough": 2.2319696330700003, "collabor": 4.45454545455, "pass": 1.61818367139, "use": 1.0296387573799999, "experi": 1.87062566278, "appeal": 3.6614391143900002, "streamlit": 1587.6, "annoy": 26.8175675676, "out": 1.06016694491, "model": 2.0905978404, "onlin": 2.6051854282900004, "suffer": 2.16117615029, "good": 1.51981619759, "alway": 2.06745670009, "slow": 4.04793472718, "comment": 3.05954904606, "glove": 31.3754940711, "huge": 4.38927287808, "categori": 3.98194130926, "julia": 20.9722589168, "forward": 3.66566612792, "reason": 1.72340425532, "bus": 6.993832599119999, "view": 1.6407606448899998, "not": 1.01567398119, "far": 1.71022298826, "involv": 1.4498630137000001, "emmanuel": 37.006993007, "revers": 4.29894394801, "store": 3.44680851064, "happen": 2.96359902931, "quick": 2.205, "pleas": 9.12938470385, "howev": 1.0945191313299998, "young": 1.8886509636000002, "adopt": 2.0442956477000003, "shown": 2.76923076923, "handl": 3.9229058561900003, "solv": 7.26923076923, "than": 1.03278688525, "then": 1.08657860516, "index": 6.9969149405, "they": 1.03017325287, "rental": 24.8840125392, "anoth": 1.13643521832, "reach": 1.49801849406, "last": 1.2117234010100002, "let": 3.48616600791, "scienc": 2.31969608416, "general": 1.1218202374200001, "blog": 14.1876675603, "away": 1.85142857143, "featur": 1.52712581762, "item": 5.07869481766, "practic": 1.70434782609, "lack": 1.9271667880599999, "simpl": 3.3981164383599998, "cow": 19.6972704715, "might": 2.1561863370900003, "calcul": 6.12972972973, "twitter": 33.213389121300004, "valley": 4.21673306773, "valu": 2.2777618364400003, "either": 1.5830092731099998, "accord": 1.27589809531, "tvmonitor": 1587.6, "should": 1.6643254009900001, "pinterest": 1443.27272727, "such": 1.06151377374, "thousand": 2.4767550702000003, "network": 2.59369384088, "three": 1.06621893889, "fast": 4.8729281768, "some": 1.04036697248, "collect": 1.64109985528, "both": 1.05215720061, "version": 2.0083491461099996, "fellow": 4.3639362287, "classic": 2.4087391898, "filter": 16.8893617021, "tex": 85.3548387097, "slowli": 6.080428954419999, "set": 1.18707940781, "third": 1.4195278969999998, "ani": 1.13383802314, "two": 1.01379310345, "divid": 2.3169877408099997, "from": 1.00056721497, "stitchfix": 1587.6, "answer": 4.64890190337, "allow": 1.2716059271100002, "limit": 1.5186531471200002, "detail": 2.26186066391, "num": 1.00031504001, "veri": 1.25880114177, "brows": 103.090909091, "free": 1.71818181818, "aeroplan": 64.2753036437, "for": 1.00031504001, "car": 3.53743315508, "predict": 5.18484650555, "output": 7.676982591880001, "bottom": 6.27261951798, "word": 1.7965372864099998, "seen": 1.61079545455, "with": 1.0011982089899998, "seem": 2.29123971713, "zero": 8.75192943771, "are": 1.02990593578, "solut": 4.7278141751, "recommend": 3.9142011834300003, "better": 2.0065722952500002, "compani": 1.5523613963, "caption": 55.5104895105, "current": 1.5325803649, "meaning": 21.8076923077, "team": 2.2748244734200003, "look": 1.9086318826599997, "popular": 1.50769230769, "promin": 2.39746300211, "bicycl": 18.290322580599998, "harder": 17.1262135922, "think": 2.90715986083, "negat": 3.75852272727, "googl": 11.388809182200001, "amount": 2.27027027027, "focus": 2.01012914662, "though": 1.36076112111, "artifici": 8.31639601886, "hacki": 1587.6, "disregard": 21.9889196676, "doe": 1.70581282905, "even": 1.16461267606, "hodosh": 1587.6, "infer": 21.1398135819, "simpler": 17.9187358916, "extract": 7.703056768560001, "mentorship": 337.787234043, "train": 1.9365698950999999, "great": 1.26592775696, "avoid": 2.45986984816, "goal": 3.28152128979, "motorbik": 184.604651163, "error": 6.04109589041, "liked\u2026": 1587.6, "librari": 2.68266306185, "diningt": 1587.6, "note": 1.42449528937, "shift": 3.3317943336799996, "row": 5.549108703250001, "scale": 3.7469907953699995, "boat": 6.466802443990001, "notebook": 40.1924050633, "again": 1.50883862384, "ameisen": 1587.6, "bias": 13.7335640138, "abov": 1.90382539873, "requir": 1.52844902282, "strong": 1.6439888163999998, "buy": 5.12459651388, "local": 1.51720183486, "choos": 4.17899447223, "head": 1.57781753131, "flexibl": 9.68639414277, "help": 1.39962972759, "main": 1.25303867403, "larg": 1.18574949585, "find": 1.7294117647099998, "bird": 6.46416938111, "photo": 6.41973311767, "line": 1.4182597820299998, "nuanc": 48.2553191489, "own": 1.17844418052, "base": 1.14628158845, "label": 4.47715736041, "pictur": 3.4953764861300005, "allencompass": 1587.6, "pottedpl": 1587.6, "post": 2.23826307627, "here": 2.42307692308, "ideal": 4.65571847507, "factor": 2.89127663449, "techniqu": 3.7293868921800004, "cyrus": 57.1079136691, "attent": 2.81040892193, "through": 1.07074930869, "human": 1.8965476048299998, "layer": 8.14153846154, "fundament": 5.32930513595, "siames": 163.670103093, "contain": 1.59814777532, "noisi": 62.2588235294, "about": 1.06486015159, "problem": 1.76674827509, "bottl": 15.443579766500001, "neural": 59.4606741573, "thing": 2.4065484311099996, "rashtchian": 1587.6, "just": 1.33580143037, "adapt": 3.32272917539, "correct": 3.6631287494199998, "simpli": 2.5192002538900002, "chair": 7.112903225810001, "represent": 5.928304705, "attach": 4.4885496183199995, "check": 6.50655737705, "vacat": 12.96, "led": 1.33782758911, "percept": 8.34700315457, "overhead": 24.054545454499998, "start": 1.26673581744, "outfit": 23.042089985500002, "took": 1.4009883515700001, "thus": 1.6463756092500001, "run": 1.55692850838, "them": 1.09876115994, "insight": 11.8037174721, "load": 6.80497213888, "studi": 1.53184098804, "vector": 25.898858075, "spars": 21.0, "order": 1.24625166811, "nearest": 15.148854961800001, "whi": 3.2566153846200003, "quit": 2.8849718335500003, "usual": 1.72508964468, "abl": 1.8208510150200001, "mani": 1.04426757877, "refer": 1.30024570025, "over": 1.02525024217, "step": 2.8279301745599996, "generat": 2.05275407292, "features\u2026": 1587.6, "scratch": 25.8146341463, "provid": 1.21552714187, "total": 1.5460122699399999, "broad": 4.27693965517, "consum": 4.93043478261, "chat": 36.164009111599995, "exact": 3.46864758575, "catlik": 1587.6, "penultim": 64.0161290323, "endtoend": 1587.6, "inform": 1.5753125620200001, "extrem": 2.36602086438, "selfi": 1587.6, "shelf": 47.391044776099996, "possibl": 1.4173734488, "semisupervis": 1587.6, "origin": 1.13724928367, "right": 1.4054532577899999, "wow": 91.2413793103, "correl": 13.1860465116, "softwar": 10.2624434389, "take": 1.13961668222, "visual": 5.22752716497, "follow": 1.04640126549, "now": 1.160780873, "matter": 2.44773358002, "code": 3.8807137619199996, "hard": 2.73253012048, "simplic": 25.4423076923, "high": 1.14777327935, "power": 1.3396337861799998, "leverag": 35.7567567568, "break": 2.42863698944, "issu": 1.43921675279, "tutori": 59.4606741573, "center": 1.7423178226499998, "also": 1.01476510067, "trick": 14.7272727273, "person": 1.40520446097, "mean": 1.44906900329, "style": 2.37807070102, "brought": 1.8387769284200002, "one": 1.00627495722, "evolut": 6.360576923080001, "notic": 4.36994219653, "task": 3.88641370869, "user": 7.71053909665, "research": 1.9420183486200002, "plan": 1.5356935577500002, "script": 8.299006795610001, "signific": 1.4529147982100001, "therefor": 2.33401940606, "the": 1.0, "sofa": 160.363636364, "becom": 1.12492028626, "recent": 1.54405757635, "tag": 19.7462686567, "peopl": 1.21320495186, "stand": 2.0845588235299997, "hockenmai": 1587.6, "dozen": 5.95275590551, "challeng": 2.55816951337, "machin": 4.02433460076, "want": 1.99698113208, "sinc": 1.08368600683, "system": 1.38739840951, "build": 1.6341739578, "rescal": 441.0, "overtak": 50.2405063291, "actual": 1.87482286254, "silicon": 31.8795180723, "which": 1.005191845, "databas": 8.24727272727, "intellig": 4.19334389857, "imagenet": 1587.6, "workflow": 369.209302326, "activ": 1.46403541129, "daweon": 1587.6, "other": 1.00992366412, "appli": 2.2972073506, "reweigh": 1587.6, "consist": 1.4901445466499998, "show": 1.26703910615, "hope": 2.50884955752, "see": 1.27242125511, "codebas": 793.8, "becaus": 1.1495184997499999, "imag": 2.70137825421, "ineffici": 26.154859967100002, "get": 1.78562591385, "like": 1.14918566775, "implement": 3.57648118946, "examin": 3.8505942275, "manag": 1.6448404475799998, "flickr": 206.181818182, "imagin": 6.598503740650001, "file": 3.7710213776699995, "detect": 5.41288782816, "part": 1.04330682789, "add": 4.61243463103, "touch": 5.45567010309, "yet": 2.1258703802900003, "includ": 1.0190641247799999, "cool": 6.8578833693300005, "anyon": 5.37440758294, "this": 1.00379362671, "pretti": 15.75, "time": 1.01127460348, "text": 3.12827586207, "engin": 2.47135740971, "predictor": 100.481012658, "similar": 1.37514075357, "express": 1.9120799710900003, "project": 1.7534791252500002, "differ": 1.23654490225, "embed": 16.835630965, "been": 1.0239277652399998, "profession": 2.6389627659599997, "decis": 2.16, "valuabl": 7.46754468485, "most": 1.02096463023, "between": 1.03453668708, "ryu": 453.6, "hack": 43.3770491803, "domain": 9.39408284024, "credit": 3.04312823462, "along": 1.2973768080399999, "all": 1.01146788991, "top": 1.8387769284200002, "input": 12.2029208301, "varieti": 2.2972073506, "program": 2.02139037433, "someth": 3.28152128979, "there": 1.04091266719, "gradient": 41.889182058, "condit": 1.92483026188, "appropri": 4.31413043478, "longer": 2.02319357716, "middl": 2.04245465071, "content": 3.5421686747, "that": 1.00398406375, "pair": 4.36873968079, "weight": 4.878918254459999, "architectur": 5.12790697674, "tradeoff": 208.89473684200001, "call": 1.0676529926, "object": 2.3488681757700003, "more": 1.0171706817, "and": 1.00006299213, "histogram": 1058.4, "list": 1.36321483771, "type": 2.0281042411900003, "these": 1.07415426252, "hors": 5.073825503359999, "cat": 10.5348374253, "gain": 1.84819557625, "contentbas": 1587.6, "could": 1.2043695949, "reus": 29.7861163227, "befor": 1.10036041031, "micah": 260.262295082, "state": 1.0477133240899998, "download": 14.6457564576, "close": 1.2848818387799998, "orient": 4.994023277759999, "can": 1.17626139142, "avail": 1.7288467821, "tie": 3.31786833856, "comput": 3.9277585353800006, "make": 1.0762660158600001, "queri": 56.2978723404, "way": 1.2190739461, "search": 3.2539454806299997, "onli": 1.0256476516600002, "each": 1.18974820144, "size": 2.49387370405, "few": 1.31729173581, "well": 1.0655748708, "cloth": 4.69843148861, "accur": 5.768895348840001, "pretrain": 1587.6, "link": 2.15151104486, "common": 1.4025974025999999, "freeli": 11.3643521832, "data": 3.37643555934, "qualiti": 2.9329392204, "method": 2.5714285714300003, "york": 1.53361669243, "repres": 1.46972782818, "sometim": 1.7126213592200001, "airbnb": 1587.6, "vast": 4.05620848237, "come": 1.32831325301, "exampl": 1.50483412322, "worth": 5.210370856580001, "abil": 2.70875277256, "understand": 2.96858638743, "scalabl": 186.776470588, "import": 1.3401992233700002, "first": 1.00761614623, "everi": 1.47917637194, "luckili": 191.277108434, "when": 1.02076769755, "have": 1.0148948411399998, "score": 4.2884927066500005, "without": 1.29547123623, "often": 1.29452054795, "prototyp": 11.7426035503, "learn": 2.32275054865, "optim": 11.5377906977, "effici": 5.09335899904, "has": 1.0436497502}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Building an image search service from scratch</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Building an image search service from scratch Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/n05.html\" rel=\"prev\" title=\"KDnuggets\u2122 News 19:n05, Jan 30: Your AI skills are worth less than you think; 7 Steps to Mastering Basic Machine Learning\"/>\n<link href=\"https://www.kdnuggets.com/2019/01/random-forests-explained-intuitively.html\" rel=\"next\" title=\"Random forests explained intuitively\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=89919\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-89919 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 30-Jan, 2019  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/01/index.html\">Jan</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/01/tutorials.html\">Tutorials, Overviews</a> \u00bb Building an image search service from scratch (\u00a0<a href=\"/2019/n06.html\">19:n06</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Building an image search service from scratch</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2019/n05.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/01/random-forests-explained-intuitively.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/computer-vision\" rel=\"tag\">Computer Vision</a>, <a href=\"https://www.kdnuggets.com/tag/image-recognition\" rel=\"tag\">Image Recognition</a>, <a href=\"https://www.kdnuggets.com/tag/nlp\" rel=\"tag\">NLP</a>, <a href=\"https://www.kdnuggets.com/tag/search-engine\" rel=\"tag\">Search Engine</a>, <a href=\"https://www.kdnuggets.com/tag/word-embeddings\" rel=\"tag\">Word Embeddings</a></div>\n<br/>\n<p class=\"excerpt\">\n     By the end of this post, you should be able to build a quick semantic search model from scratch, no matter the size of your dataset.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"/2019/01/building-image-search-service-from-scratch.html?page=2#comments\">comments</a></div>\n<p><b>By <a href=\"https://www.linkedin.com/in/ameisen/\" rel=\"noopener noreferrer\" target=\"_blank\">Emmanuel Ameisen</a>, Head of AI at Insight Data Science</b></p>\n<p><center><img alt=\"Figure\" src=\"https://cdn-images-1.medium.com/max/2560/1*GVCnE4-SPEHzeqdNBqnymA.jpeg\" width=\"99%\"><br>\n<font size=\"-1\">Teaching computers to look at pictures the way we do</font></br></img></center></p>\n<p><strong><em>Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York?</em></strong><em>\u00a0</em>Learn more about the\u00a0<a href=\"http://insightdata.ai/?utm_source=representations&amp;utm_medium=blog&amp;utm_content=top\" rel=\"noopener noreferrer\" target=\"_blank\">Artificial Intelligence</a> program at Insight.</p>\n<p><strong><em>Are you a company working in AI and would like to get involved in the Insight AI Fellows Program?</em></strong><em>\u00a0Feel free to\u00a0</em><a href=\"http://insightdatascience.com/partnerships?utm_source=representations&amp;utm_medium=blog&amp;utm_content=top\" rel=\"noopener noreferrer\" target=\"_blank\"><em>get in touch</em></a><em>.</em></p>\n<p><em>For more content like this, follow\u00a0</em><a href=\"https://twitter.com/InsightDataSci\" rel=\"noopener noreferrer\" target=\"_blank\"><em>Insight</em></a><em>\u00a0and\u00a0</em><a href=\"https://twitter.com/EmmanuelAmeisen\" rel=\"noopener noreferrer\" target=\"_blank\"><em>Emmanuel</em></a><em>\u00a0on Twitter.</em></p>\n<p>\u00a0</p>\n<h3>Why similarity search?</h3>\n<p>\u00a0<br>\n<em>An image is worth a thousand words, and even more lines of code.</em></br></p>\n<p>Many products\u00a0<strong>fundamentally appeal to our perception</strong>. When browsing through outfits on clothing sites, looking for a vacation rental on Airbnb, or choosing a pet to adopt, the way something looks is often an important factor in our decision. The way we perceive things is a strong predictor of what kind of items we will like, and therefore a valuable quality to measure.</p>\n<p>However, making computers understand images the way humans do has been a computer science challenge for quite some time. Since 2012, Deep Learning has slowly started overtaking classical methods such as\u00a0<a href=\"https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\" rel=\"noopener noreferrer\" target=\"_blank\">Histograms of Oriented Gradients</a>\u00a0(HOG) in perception tasks like image classification or object detection. One of the main reasons often credited for this shift is deep learning\u2019s ability to automatically\u00a0<strong>extract meaningful representations</strong>\u00a0when trained on a large enough dataset.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*1SoitQHdT7q2BZ2pszvqTQ.gif\" width=\"99%\"/><br>\n<font size=\"-1\">Visual search at Pinterest</font></br></center></p>\n<p>This is why many teams\u200a\u2014\u200alike at\u00a0<a href=\"https://medium.com/@Pinterest_Engineering/introducing-a-new-way-to-visually-search-on-pinterest-67c8284b3684\" rel=\"noopener noreferrer\" target=\"_blank\">Pinterest</a>,\u00a0<a href=\"https://multithreaded.stitchfix.com/blog/2015/09/17/deep-style/\" rel=\"noopener noreferrer\" target=\"_blank\">StitchFix</a>, and\u00a0<a href=\"http://code.flickr.net/2017/03/07/introducing-similarity-search-at-flickr/\" rel=\"noopener noreferrer\" target=\"_blank\">Flickr</a>\u200a\u2014\u200astarted using Deep Learning to learn representations of their images, and\u00a0<strong>provide recommendations\u00a0</strong>based on the content users find visually pleasing. Similarly, Fellows at\u00a0<a href=\"http://insightdatascience.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Insight</a>\u00a0have used deep learning to build models for applications such as helping people find cats to adopt, recommending sunglasses to buy, and searching for art styles.</p>\n<p>Many recommendation systems are based on\u00a0<a href=\"https://en.wikipedia.org/wiki/Collaborative_filtering\" rel=\"noopener noreferrer\" target=\"_blank\">collaborative filtering</a>: leveraging user correlations to make recommendations (\u201cusers that liked the items you have liked have also liked\u2026\u201d). However, these models require a\u00a0<strong>significant amount of data</strong>\u00a0to be accurate, and\u00a0<a href=\"https://en.wikipedia.org/wiki/Cold_start_%28computing%29\" rel=\"noopener noreferrer\" target=\"_blank\">struggle</a>\u00a0to handle\u00a0<strong>new items</strong>that have not yet been viewed by anyone. Item representation can be used in what\u2019s called\u00a0<a href=\"https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering\" rel=\"noopener noreferrer\" target=\"_blank\">content-based</a>\u00a0recommendation systems, which do not suffer from the problem above.</p>\n<p>In addition, these representations allow consumers to\u00a0<strong>efficiently search</strong> photo libraries for images that are similar to the selfie they just took (querying by image), or for photos of particular items such as cars (querying by text). Common examples of this include Google Reverse Image Search, as well as Google Image Search.</p>\n<p>Based on our experience providing technical mentorship for many semantic understanding projects, we wanted to write a tutorial on how you would go about\u00a0<strong>building your own representations</strong>, both for image and text data, and\u00a0<strong>efficiently do similarity search</strong>. By the end of this post, you should be able to build a quick semantic search model from scratch, no matter the size of your dataset.</p>\n<p><strong><em>This post is accompanied by\u00a0</em></strong><a href=\"http://insight.streamlit.io/0.13.3-8ErS/index.html?id=QAKzY9mLjr4WbTCgxz3XBX\" rel=\"noopener noreferrer\" target=\"_blank\"><strong><em>an annotated code notebook</em></strong></a><strong><em>\u00a0using\u00a0</em></strong><a href=\"http://streamlit.io/\" rel=\"noopener noreferrer\" target=\"_blank\"><strong><em>streamlit</em></strong></a> <strong><em>and a\u00a0</em></strong><a href=\"https://github.com/hundredblocks/semantic-search\" rel=\"noopener noreferrer\" target=\"_blank\"><strong><em>self-standing codebase</em></strong></a><strong><em>\u00a0demonstrating and applying all these techniques. Feel free to run the code and follow along!</em></strong></p>\n<p>\u00a0</p>\n<h3>What\u2019s our\u00a0plan?</h3>\n<p>\u00a0<br>\n<b>A break to chat about optimization</b></br></p>\n<p>In machine learning, just like in software engineering, there are many ways to tackle a problem, each with different tradeoffs. If we are doing research or local prototyping, we can get away with very inefficient solutions. But if we are building an image similarity search engine that needs to be maintainable and scalable, we have to consider both how we can\u00a0<strong>adapt to data evolution</strong>, and\u00a0<strong>how fast our model can run</strong>.</p>\n<p>Let\u2019s imagine a few approaches:</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*fCDUAwVmCn1EJuhBR7kMYQ.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Workflow for approach\u00a01</font></center></p>\n<p>1/ We build an end-to-end model that is trained on all our images to take an image as an input, and output a similarity score over all of our images. Predictions happen quickly (one forward pass), but we would need to<strong>\u00a0train a new model</strong>\u00a0every time we add a new image. We would also quickly reach a state with so many classes that it would be extremely\u00a0<strong>hard to optimize</strong>\u00a0it correctly. This approach is fast, but does not scale to large datasets. In addition, we would have to label our dataset by hand with image similarities, which could be extremely time consuming.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*dVzkAN44aulHdgTm7ZVyww.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Workflow for approach\u00a02</font></center></p>\n<p>2/ Another approach is to build a model that takes in two images, and outputs a pairwise similarity score between 0 and 1 (<a href=\"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Siamese Networks</a>[PDF], for example). These models are accurate for large datasets, but lead to another scalability issue. We usually want to find a similar image by\u00a0<strong>looking through a vast collection of images</strong>, so we have to run our similarity model once for each image pair in our dataset. If our model is a CNN, and we have more than a dozen images, this becomes too slow to even be considered. In addition, this only works for image similarity, not text search. This method scales to large datasets, but is slow.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*zYN9ME-2mgw6lQNhAFGVDg.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Workflow for approach\u00a03</font></center></p>\n<p>3/ There is a simpler method, which is similar to\u00a0<a href=\"https://en.wikipedia.org/wiki/Word_embedding\" rel=\"noopener noreferrer\" target=\"_blank\">word embeddings</a>. If we find an\u00a0<strong>expressive vector representation, or embedding\u00a0</strong>for images, we can then calculate their similarity by looking at\u00a0<strong>how close their vectors are to each other</strong>. This type of search is a common problem that is well studied, and many libraries implement fast solutions (we will use\u00a0<a href=\"https://github.com/spotify/annoy\" rel=\"noopener noreferrer\" target=\"_blank\">Annoy</a>\u00a0here). In addition, if we calculate these vectors for all images in our database ahead of time, this approach is both fast (one forward pass, and an efficient similarity search), and scalable. Finally, if we manage to find\u00a0<strong>common embeddings</strong>\u00a0for our images and our words, we could use them to do text to image search!</p>\n<p>Because of its simplicity and efficiency, the third method will be the focus of this post.</p>\n<p><b>How do we get\u00a0there?</b></p>\n<p>So, how do we actually use\u00a0<strong>deep learning representations</strong>\u00a0to create a\u00a0<strong>search engine?</strong></p>\n<p>Our final goal is to have a search engine that can take in images and output either similar images or tags, and take in text and output similar words, or images. To get there, we will go through three successive steps:</p>\n<ul>\n<li>Searching for\u00a0<strong>similar images to an input image</strong>\u00a0(Image \u2192 Image)\n<li>Searching for\u00a0<strong>similar words to an input word</strong>\u00a0(Text \u2192 Text)\n<li>Generating\u00a0<strong>tags for images</strong>, and\u00a0<strong>searching images using tex</strong>t (Image \u2194 Text)\n</li></li></li></ul>\n<p>To do this, we will use\u00a0<strong>embeddings</strong>, vector representations of images and text. Once we have embeddings, searching simply becomes a matter of finding vectors close to our input vector.</p>\n<p>The way we find these is by calculating the\u00a0<a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noopener noreferrer\" target=\"_blank\">cosine similarity</a>\u00a0between our image embedding, and embeddings for other images. Similar images will have similar embeddings, meaning a\u00a0<strong>high cosine similarity between embeddings</strong>.</p>\n<p>Let\u2019s start with a dataset to experiment with.</p>\n<p>\u00a0</p>\n<h3>Dataset</h3>\n<p>\u00a0<br/>\n<b>Images</b></p>\n<p>Our image dataset consists of a total of a\u00a0<strong>1000 images</strong>, divided in 20 classes with 50 images for each. This dataset can be found\u00a0<a href=\"http://vision.cs.uiuc.edu/pascal-sentences/\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>. Feel free to use the script in the linked code to automatically download all image files.\u00a0<em>Credit to Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier for the dataset.</em></p>\n<p>This dataset contains a category and a set of captions for every image. To make this problem harder, and to show how well our approach generalizes, we will\u00a0<strong>only use the categories</strong>, and disregard the captions. We have a total of 20 classes, which I\u2019ve listed out below:</p>\n<p><code>aeroplane</code>\u00a0<code>bicycle</code>\u00a0<code>bird</code>\u00a0<code>boat</code>\u00a0<code>bottle</code>\u00a0<code>bus</code>\u00a0<code>car</code>\u00a0<code>cat</code>\u00a0<code>chair</code>\u00a0<code>cow</code> <code>dining_table</code> <code>dog</code> <code>horse</code> <code>motorbike</code> <code>person</code> <code>potted_plant</code> <code>sheep</code> <code>sofa</code> <code>train</code> <code>tv_monitor</code></p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*PBvv1EkQ67RJtLgmzspP5w.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Image examples. As we can see, the labels are quite\u00a0noisy.</font></center></p>\n<p>We can see our labels are pretty\u00a0<strong>noisy</strong>: many photos contain multiple categories, and the label is not always from the most prominent one. For example, on the bottom right, the image is labeled\u00a0<code>chair</code>\u00a0and not\u00a0<code>person</code>even though 3 people stand in the center of the image, and the chair is barely visible.</p>\n<p><b>Text</b></p>\n<p>In addition, we load word embeddings that have been pre-trained on Wikipedia (this tutorial will use the ones from the\u00a0<a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"noopener noreferrer\" target=\"_blank\">GloVe</a>\u00a0model). We will use these vectors to incorporate text to our semantic search. For more information on how these word vectors work, see\u00a0<a href=\"https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\" rel=\"noopener noreferrer\" target=\"_blank\">Step 7</a>\u00a0of our NLP tutorial.</p>\n<p>\u00a0</p>\n<h3>Image -&gt;\u00a0Image</h3>\n<p>\u00a0<br/>\n<em>Starting simple</em><br/>\nWe are now going to load a model that was\u00a0<strong>pre-trained</strong>\u00a0on a large data set (<a href=\"http://www.image-net.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Imagenet</a>), and is freely available online. We use VGG16 here, but this approach would work with any recent CNN architecture. We use this model to generate\u00a0<strong>embeddings</strong>\u00a0for our images.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*LdB1mhrN6hT-EznkkFE8Cg.jpeg\" width=\"99%\"/><br/>\n<font size=\"-1\">VGG16 (credit to Data Wow\u00a0Blog)</font></center></p>\n<p>What do we mean by generating embeddings? We will use our pre-trained model\u00a0<strong>up to the penultimate layer</strong>, and store the value of the activations. In the image below, this is represented by the embedding layer highlighted in green, which is before the final classification layer.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*A-kdSGUtBL1kcJ2B8TKSmg.png\" width=\"99%\"/><br/>\n<font size=\"-1\">For our embeddings, we use the layer before the final classification layer.</font></center></p>\n<p>Once we\u2019ve used the model to generate image features, we can then store them to disk and re-use them\u00a0<strong>without needing to do inference again</strong>! This is one of the reasons that embeddings are so popular in practical applications, as they allow for huge efficiency gains. On top of storing them to disk, we will build a\u00a0<strong>fast index</strong>\u00a0of the embeddings using\u00a0<a href=\"https://github.com/spotify/annoy\" rel=\"noopener noreferrer\" target=\"_blank\">Annoy</a>, which will allow us to very quickly find the nearest embeddings to any given embedding.</p>\n<p>Below are our embeddings. Each image is now represented by a sparse vector of size 4096.\u00a0<em>Note: the reason the vector is sparse is that we have taken the values after the activation function, which zeros out negatives.</em></p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*5A-M_7imJHRjOAlDwqiu5Q.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Image Embeddings</font></center></p>\n<p><b><strong>Using our embeddings to search through\u00a0images</strong></b></p>\n<p>We can now simply take in an image, get its embedding, and look in our fast index to find similar embeddings, and thus similar images.</p>\n<p>This is especially useful since image labels are often\u00a0<strong>noisy</strong>, and there is more to an image than its label.</p>\n<p>For example, in our dataset, we have both a class\u00a0<code>cat</code>, and a class\u00a0<code>bottle</code>.</p>\n<p>Which class do you think this image is labeled as?</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*RPHlEkGIyqih7e3cnjKtGQ.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Cat or Bottle? (Image rescaled to 224x224, which is what the neural network\u00a0sees.)</font></center></p>\n<p>The correct answer is\u00a0<strong>bottle</strong>\u00a0\u2026 This is an actual issue that comes up often in real datasets. Labeling images as unique categories is quite limiting, which is why we hope to use more nuanced representations. Luckily, this is exactly what\u00a0<strong>deep learning is good at</strong>!</p>\n<p>Let\u2019s see if our image search using embeddings does better than human labels.</p>\n<p>Searching for similar images to<code>dataset/bottle/2008_000112.jpg</code>\u2026</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/1600/1*PamdW6zfGIbZeD1gaHAAmA.png\" width=\"99%\"/></p>\n<p>Great\u200a\u2014\u200awe mostly get more images of\u00a0<strong>cats</strong>, which seems very reasonable! Our pre-trained network has been trained on a wide variety of images, including cats, and so it is able to accurately find similar images, even though it has never been trained on this particular dataset before.</p>\n<p>However, one image in the middle of the bottom row shows a shelf of bottles. This approach perform wells to find similar images, in general, but sometimes we are only interested in\u00a0<strong>part of the image</strong>.</p>\n<p>For example, given an image of a cat and a bottle, we might be only interested in similar cats, not similar bottles.</p>\n<p><b>Semi-supervised search</b></p>\n<p>A common approach to solve this issue is to use an\u00a0<strong>object detection</strong>\u00a0model first, detect our cat, and do image search on a cropped version of the original image.</p>\n<p>This adds a huge computing overhead, which we would like to avoid if possible.</p>\n<p>There is a simpler \u201chacky\u201d approach, which consists of\u00a0<strong>re-weighing</strong>\u00a0the activations. We do this by loading the last layer of weights we had initially discarded, and only use the weights tied to the index of the class we are looking for to re-weigh our embedding. This cool trick was initially brought to my attention by\u00a0<a href=\"http://insightdatascience.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Insight</a>\u00a0Fellow\u00a0<a href=\"https://www.linkedin.com/in/daweonryu/\" rel=\"noopener noreferrer\" target=\"_blank\">Daweon Ryu</a>. For example, in the image below, we use the weights of the\u00a0<code>Siamese cat</code>\u00a0class to re-weigh the activations on our dataset (highlighted in green). Feel free to check out the attached notebook to see the implementation details.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*ef3jietHYrBhfw26qGLZ8g.png\" width=\"99%\"/><br/>\n<font size=\"-1\">The hack to get weighted embeddings. The classification layer is shown for reference only.</font></center></p>\n<p>Let\u2019s examine how this works by weighing our activations according to class\u00a0<code>284</code>\u00a0in Imagenet,\u00a0<code>Siamese cat</code>.</p>\n<p>Searching for similar images to<code>dataset/bottle/2008_000112.jpg</code>\u00a0using weighted features\u2026</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/1600/1*qMNxg8XlxcNIAImu4A-n4g.png\" width=\"99%\"/></p>\n<p>We can see that the search has been biased to look for\u00a0<strong>Siamese cat-like things</strong>. We no longer show any bottles, which is great. You might however notice that our last image is of a sheep! This is very interesting, as biasing our model has led to a\u00a0<strong>different kind of error</strong>, which is more appropriate for our current domain.</p>\n<p>We have seen we can search for similar images in a\u00a0<strong>broad</strong>\u00a0way, or by\u00a0<strong>conditioning on a particular class</strong>\u00a0our model was trained on.</p>\n<p>This is a great step forward, but since we are using a model\u00a0<strong>pre-trained on Imagenet</strong>, we are thus limited to the 1000\u00a0<strong>Imagenet classes</strong>. These classes are far from all-encompassing (they lack a category for people, for example), so we would ideally like to find something more\u00a0<strong>flexible.\u00a0</strong>In addition, what if we simply wanted to search for cats\u00a0<strong>without providing an input image?</strong></p>\n<p>In order to do this, we are going to use more than simple tricks, and leverage a model that can understand the semantic power of words.</p>\n</div>\n<div class=\"page-link\"><p>Pages: 1 <a href=\"https://www.kdnuggets.com/2019/01/building-image-search-service-from-scratch.html/2\">2</a></p></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2019/n05.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/01/random-forests-explained-intuitively.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/kdnuggets-editor.html\">Looking for a KDnuggets Editor</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning Experts</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/01/index.html\">Jan</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/01/tutorials.html\">Tutorials, Overviews</a> \u00bb Building an image search service from scratch (\u00a0<a href=\"/2019/n06.html\">19:n06</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<div>\n<br/><span style=\"font-size:9px\">By subscribing, you agree to KDnuggets <a href=\"https://www.kdnuggets.com/news/privacy-policy.html\">privacy policy</a></span>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556419141\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.753 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 22:39:01 -->\n<!-- Compression = gzip -->", "content_tokenized": ["comment", "emmanuel", "ameisen", "head", "insight", "data", "scienc", "teach", "comput", "look", "pictur", "the", "way", "want", "learn", "appli", "artifici", "intellig", "from", "top", "profession", "silicon", "valley", "new", "york", "learn", "more", "about", "the", "artifici", "intellig", "program", "insight", "are", "compani", "work", "and", "would", "like", "get", "involv", "the", "insight", "fellow", "program", "feel", "free", "get", "touch", "for", "more", "content", "like", "this", "follow", "insight", "and", "emmanuel", "twitter", "whi", "similar", "search", "imag", "worth", "thousand", "word", "and", "even", "more", "line", "code", "mani", "product", "fundament", "appeal", "our", "percept", "when", "brows", "through", "outfit", "cloth", "site", "look", "for", "vacat", "rental", "airbnb", "choos", "pet", "adopt", "the", "way", "someth", "look", "often", "import", "factor", "our", "decis", "the", "way", "perceiv", "thing", "strong", "predictor", "what", "kind", "item", "will", "like", "and", "therefor", "valuabl", "qualiti", "measur", "howev", "make", "comput", "understand", "imag", "the", "way", "human", "has", "been", "comput", "scienc", "challeng", "for", "quit", "some", "time", "sinc", "num", "deep", "learn", "has", "slowli", "start", "overtak", "classic", "method", "such", "histogram", "orient", "gradient", "percept", "task", "like", "imag", "classif", "object", "detect", "one", "the", "main", "reason", "often", "credit", "for", "this", "shift", "deep", "learn", "abil", "automat", "extract", "meaning", "represent", "when", "train", "larg", "enough", "dataset", "visual", "search", "pinterest", "this", "whi", "mani", "team", "like", "pinterest", "stitchfix", "and", "flickr", "start", "use", "deep", "learn", "learn", "represent", "their", "imag", "and", "provid", "recommend", "base", "the", "content", "user", "find", "visual", "pleas", "similar", "fellow", "insight", "have", "use", "deep", "learn", "build", "model", "for", "applic", "such", "help", "peopl", "find", "cat", "adopt", "recommend", "sunglass", "buy", "and", "search", "for", "art", "style", "mani", "recommend", "system", "are", "base", "collabor", "filter", "leverag", "user", "correl", "make", "recommend", "user", "that", "like", "the", "item", "have", "like", "have", "also", "liked\u2026", "howev", "these", "model", "requir", "signific", "amount", "data", "accur", "and", "struggl", "handl", "new", "item", "that", "have", "not", "yet", "been", "view", "anyon", "item", "represent", "can", "use", "what", "call", "contentbas", "recommend", "system", "which", "not", "suffer", "from", "the", "problem", "abov", "addit", "these", "represent", "allow", "consum", "effici", "search", "photo", "librari", "for", "imag", "that", "are", "similar", "the", "selfi", "they", "just", "took", "queri", "imag", "for", "photo", "particular", "item", "such", "car", "queri", "text", "common", "exampl", "this", "includ", "googl", "revers", "imag", "search", "well", "googl", "imag", "search", "base", "our", "experi", "provid", "technic", "mentorship", "for", "mani", "semant", "understand", "project", "want", "write", "tutori", "how", "would", "about", "build", "own", "represent", "both", "for", "imag", "and", "text", "data", "and", "effici", "similar", "search", "the", "end", "this", "post", "should", "abl", "build", "quick", "semant", "search", "model", "from", "scratch", "matter", "the", "size", "dataset", "this", "post", "accompani", "annot", "code", "notebook", "use", "streamlit", "and", "selfstand", "codebas", "demonstr", "and", "appli", "all", "these", "techniqu", "feel", "free", "run", "the", "code", "and", "follow", "along", "what", "our", "plan", "break", "chat", "about", "optim", "machin", "learn", "just", "like", "softwar", "engin", "there", "are", "mani", "way", "tackl", "problem", "each", "with", "differ", "tradeoff", "are", "research", "local", "prototyp", "can", "get", "away", "with", "veri", "ineffici", "solut", "but", "are", "build", "imag", "similar", "search", "engin", "that", "need", "maintain", "and", "scalabl", "have", "consid", "both", "how", "can", "adapt", "data", "evolut", "and", "how", "fast", "our", "model", "can", "run", "let", "imagin", "few", "approach", "workflow", "for", "approach", "num", "num", "build", "endtoend", "model", "that", "train", "all", "our", "imag", "take", "imag", "input", "and", "output", "similar", "score", "over", "all", "our", "imag", "predict", "happen", "quick", "one", "forward", "pass", "but", "would", "need", "train", "new", "model", "everi", "time", "add", "new", "imag", "would", "also", "quick", "reach", "state", "with", "mani", "class", "that", "would", "extrem", "hard", "optim", "correct", "this", "approach", "fast", "but", "doe", "not", "scale", "larg", "dataset", "addit", "would", "have", "label", "our", "dataset", "hand", "with", "imag", "similar", "which", "could", "extrem", "time", "consum", "workflow", "for", "approach", "num", "num", "anoth", "approach", "build", "model", "that", "take", "two", "imag", "and", "output", "pairwis", "similar", "score", "between", "num", "and", "num", "siames", "network", "for", "exampl", "these", "model", "are", "accur", "for", "larg", "dataset", "but", "lead", "anoth", "scalabl", "issu", "usual", "want", "find", "similar", "imag", "look", "through", "vast", "collect", "imag", "have", "run", "our", "similar", "model", "onc", "for", "each", "imag", "pair", "our", "dataset", "our", "model", "and", "have", "more", "than", "dozen", "imag", "this", "becom", "too", "slow", "even", "consid", "addit", "this", "onli", "work", "for", "imag", "similar", "not", "text", "search", "this", "method", "scale", "larg", "dataset", "but", "slow", "workflow", "for", "approach", "num", "num", "there", "simpler", "method", "which", "similar", "word", "embed", "find", "express", "vector", "represent", "embed", "for", "imag", "can", "then", "calcul", "their", "similar", "look", "how", "close", "their", "vector", "are", "each", "other", "this", "type", "search", "common", "problem", "that", "well", "studi", "and", "mani", "librari", "implement", "fast", "solut", "will", "use", "annoy", "here", "addit", "calcul", "these", "vector", "for", "all", "imag", "our", "databas", "ahead", "time", "this", "approach", "both", "fast", "one", "forward", "pass", "and", "effici", "similar", "search", "and", "scalabl", "final", "manag", "find", "common", "embed", "for", "our", "imag", "and", "our", "word", "could", "use", "them", "text", "imag", "search", "becaus", "simplic", "and", "effici", "the", "third", "method", "will", "the", "focus", "this", "post", "how", "get", "there", "how", "actual", "use", "deep", "learn", "represent", "creat", "search", "engin", "our", "final", "goal", "have", "search", "engin", "that", "can", "take", "imag", "and", "output", "either", "similar", "imag", "tag", "and", "take", "text", "and", "output", "similar", "word", "imag", "get", "there", "will", "through", "three", "success", "step", "search", "for", "similar", "imag", "input", "imag", "imag", "imag", "search", "for", "similar", "word", "input", "word", "text", "text", "generat", "tag", "for", "imag", "and", "search", "imag", "use", "tex", "imag", "text", "this", "will", "use", "embed", "vector", "represent", "imag", "and", "text", "onc", "have", "embed", "search", "simpli", "becom", "matter", "find", "vector", "close", "our", "input", "vector", "the", "way", "find", "these", "calcul", "the", "cosin", "similar", "between", "our", "imag", "embed", "and", "embed", "for", "other", "imag", "similar", "imag", "will", "have", "similar", "embed", "mean", "high", "cosin", "similar", "between", "embed", "let", "start", "with", "dataset", "experi", "with", "dataset", "imag", "our", "imag", "dataset", "consist", "total", "num", "imag", "divid", "num", "class", "with", "num", "imag", "for", "each", "this", "dataset", "can", "found", "here", "feel", "free", "use", "the", "script", "the", "link", "code", "automat", "download", "all", "imag", "file", "credit", "cyrus", "rashtchian", "peter", "young", "micah", "hodosh", "and", "julia", "hockenmai", "for", "the", "dataset", "this", "dataset", "contain", "categori", "and", "set", "caption", "for", "everi", "imag", "make", "this", "problem", "harder", "and", "show", "how", "well", "our", "approach", "general", "will", "onli", "use", "the", "categori", "and", "disregard", "the", "caption", "have", "total", "num", "class", "which", "list", "out", "below", "aeroplan", "bicycl", "bird", "boat", "bottl", "bus", "car", "cat", "chair", "cow", "diningt", "dog", "hors", "motorbik", "person", "pottedpl", "sheep", "sofa", "train", "tvmonitor", "imag", "exampl", "can", "see", "the", "label", "are", "quit", "noisi", "can", "see", "our", "label", "are", "pretti", "noisi", "mani", "photo", "contain", "multipl", "categori", "and", "the", "label", "not", "alway", "from", "the", "most", "promin", "one", "for", "exampl", "the", "bottom", "right", "the", "imag", "label", "chair", "and", "not", "person", "even", "though", "num", "peopl", "stand", "the", "center", "the", "imag", "and", "the", "chair", "bare", "visibl", "text", "addit", "load", "word", "embed", "that", "have", "been", "pretrain", "wikipedia", "this", "tutori", "will", "use", "the", "one", "from", "the", "glove", "model", "will", "use", "these", "vector", "incorpor", "text", "our", "semant", "search", "for", "more", "inform", "how", "these", "word", "vector", "work", "see", "step", "num", "our", "tutori", "imag", "imag", "start", "simpl", "are", "now", "load", "model", "that", "pretrain", "larg", "data", "set", "imagenet", "and", "freeli", "avail", "onlin", "use", "here", "but", "this", "approach", "would", "work", "with", "ani", "recent", "architectur", "use", "this", "model", "generat", "embed", "for", "our", "imag", "credit", "data", "wow", "blog", "what", "mean", "generat", "embed", "will", "use", "our", "pretrain", "model", "the", "penultim", "layer", "and", "store", "the", "valu", "the", "activ", "the", "imag", "below", "this", "repres", "the", "embed", "layer", "highlight", "green", "which", "befor", "the", "final", "classif", "layer", "for", "our", "embed", "use", "the", "layer", "befor", "the", "final", "classif", "layer", "onc", "use", "the", "model", "generat", "imag", "featur", "can", "then", "store", "them", "disk", "and", "reus", "them", "without", "need", "infer", "again", "this", "one", "the", "reason", "that", "embed", "are", "popular", "practic", "applic", "they", "allow", "for", "huge", "effici", "gain", "top", "store", "them", "disk", "will", "build", "fast", "index", "the", "embed", "use", "annoy", "which", "will", "allow", "veri", "quick", "find", "the", "nearest", "embed", "ani", "given", "embed", "below", "are", "our", "embed", "each", "imag", "now", "repres", "spars", "vector", "size", "num", "note", "the", "reason", "the", "vector", "spars", "that", "have", "taken", "the", "valu", "after", "the", "activ", "function", "which", "zero", "out", "negat", "imag", "embed", "use", "our", "embed", "search", "through", "imag", "can", "now", "simpli", "take", "imag", "get", "embed", "and", "look", "our", "fast", "index", "find", "similar", "embed", "and", "thus", "similar", "imag", "this", "especi", "use", "sinc", "imag", "label", "are", "often", "noisi", "and", "there", "more", "imag", "than", "label", "for", "exampl", "our", "dataset", "have", "both", "class", "cat", "and", "class", "bottl", "which", "class", "think", "this", "imag", "label", "cat", "bottl", "imag", "rescal", "numxnum", "which", "what", "the", "neural", "network", "see", "the", "correct", "answer", "bottl", "this", "actual", "issu", "that", "come", "often", "real", "dataset", "label", "imag", "uniqu", "categori", "quit", "limit", "which", "whi", "hope", "use", "more", "nuanc", "represent", "luckili", "this", "exact", "what", "deep", "learn", "good", "let", "see", "our", "imag", "search", "use", "embed", "doe", "better", "than", "human", "label", "search", "for", "similar", "imag", "datasetbottlenumjpg", "great", "most", "get", "more", "imag", "cat", "which", "seem", "veri", "reason", "our", "pretrain", "network", "has", "been", "train", "wide", "varieti", "imag", "includ", "cat", "and", "abl", "accur", "find", "similar", "imag", "even", "though", "has", "never", "been", "train", "this", "particular", "dataset", "befor", "howev", "one", "imag", "the", "middl", "the", "bottom", "row", "show", "shelf", "bottl", "this", "approach", "perform", "well", "find", "similar", "imag", "general", "but", "sometim", "are", "onli", "interest", "part", "the", "imag", "for", "exampl", "given", "imag", "cat", "and", "bottl", "might", "onli", "interest", "similar", "cat", "not", "similar", "bottl", "semisupervis", "search", "common", "approach", "solv", "this", "issu", "use", "object", "detect", "model", "first", "detect", "our", "cat", "and", "imag", "search", "crop", "version", "the", "origin", "imag", "this", "add", "huge", "comput", "overhead", "which", "would", "like", "avoid", "possibl", "there", "simpler", "hacki", "approach", "which", "consist", "reweigh", "the", "activ", "this", "load", "the", "last", "layer", "weight", "had", "initi", "discard", "and", "onli", "use", "the", "weight", "tie", "the", "index", "the", "class", "are", "look", "for", "reweigh", "our", "embed", "this", "cool", "trick", "initi", "brought", "attent", "insight", "fellow", "daweon", "ryu", "for", "exampl", "the", "imag", "below", "use", "the", "weight", "the", "siames", "cat", "class", "reweigh", "the", "activ", "our", "dataset", "highlight", "green", "feel", "free", "check", "out", "the", "attach", "notebook", "see", "the", "implement", "detail", "the", "hack", "get", "weight", "embed", "the", "classif", "layer", "shown", "for", "refer", "onli", "let", "examin", "how", "this", "work", "weigh", "our", "activ", "accord", "class", "num", "imagenet", "siames", "cat", "search", "for", "similar", "imag", "datasetbottlenumjpg", "use", "weight", "features\u2026", "can", "see", "that", "the", "search", "has", "been", "bias", "look", "for", "siames", "catlik", "thing", "longer", "show", "ani", "bottl", "which", "great", "might", "howev", "notic", "that", "our", "last", "imag", "sheep", "this", "veri", "interest", "bias", "our", "model", "has", "led", "differ", "kind", "error", "which", "more", "appropri", "for", "our", "current", "domain", "have", "seen", "can", "search", "for", "similar", "imag", "broad", "way", "condit", "particular", "class", "our", "model", "train", "this", "great", "step", "forward", "but", "sinc", "are", "use", "model", "pretrain", "imagenet", "are", "thus", "limit", "the", "num", "imagenet", "class", "these", "class", "are", "far", "from", "allencompass", "they", "lack", "categori", "for", "peopl", "for", "exampl", "would", "ideal", "like", "find", "someth", "more", "flexibl", "addit", "what", "simpli", "want", "search", "for", "cat", "without", "provid", "input", "imag", "order", "this", "are", "use", "more", "than", "simpl", "trick", "and", "leverag", "model", "that", "can", "understand", "the", "semant", "power", "word"], "timestamp_scraper": 1556480827.34454, "title": "Building an image search service from scratch", "read_time": 682.5, "content_html": "<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"/2019/01/building-image-search-service-from-scratch.html?page=2#comments\">comments</a></div>\n<p><b>By <a href=\"https://www.linkedin.com/in/ameisen/\" rel=\"noopener noreferrer\" target=\"_blank\">Emmanuel Ameisen</a>, Head of AI at Insight Data Science</b></p>\n<p><center><img alt=\"Figure\" src=\"https://cdn-images-1.medium.com/max/2560/1*GVCnE4-SPEHzeqdNBqnymA.jpeg\" width=\"99%\"><br>\n<font size=\"-1\">Teaching computers to look at pictures the way we do</font></br></img></center></p>\n<p><strong><em>Want to learn applied Artificial Intelligence from top professionals in Silicon Valley or New York?</em></strong><em>\u00a0</em>Learn more about the\u00a0<a href=\"http://insightdata.ai/?utm_source=representations&amp;utm_medium=blog&amp;utm_content=top\" rel=\"noopener noreferrer\" target=\"_blank\">Artificial Intelligence</a> program at Insight.</p>\n<p><strong><em>Are you a company working in AI and would like to get involved in the Insight AI Fellows Program?</em></strong><em>\u00a0Feel free to\u00a0</em><a href=\"http://insightdatascience.com/partnerships?utm_source=representations&amp;utm_medium=blog&amp;utm_content=top\" rel=\"noopener noreferrer\" target=\"_blank\"><em>get in touch</em></a><em>.</em></p>\n<p><em>For more content like this, follow\u00a0</em><a href=\"https://twitter.com/InsightDataSci\" rel=\"noopener noreferrer\" target=\"_blank\"><em>Insight</em></a><em>\u00a0and\u00a0</em><a href=\"https://twitter.com/EmmanuelAmeisen\" rel=\"noopener noreferrer\" target=\"_blank\"><em>Emmanuel</em></a><em>\u00a0on Twitter.</em></p>\n<p>\u00a0</p>\n<h3>Why similarity search?</h3>\n<p>\u00a0<br>\n<em>An image is worth a thousand words, and even more lines of code.</em></br></p>\n<p>Many products\u00a0<strong>fundamentally appeal to our perception</strong>. When browsing through outfits on clothing sites, looking for a vacation rental on Airbnb, or choosing a pet to adopt, the way something looks is often an important factor in our decision. The way we perceive things is a strong predictor of what kind of items we will like, and therefore a valuable quality to measure.</p>\n<p>However, making computers understand images the way humans do has been a computer science challenge for quite some time. Since 2012, Deep Learning has slowly started overtaking classical methods such as\u00a0<a href=\"https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\" rel=\"noopener noreferrer\" target=\"_blank\">Histograms of Oriented Gradients</a>\u00a0(HOG) in perception tasks like image classification or object detection. One of the main reasons often credited for this shift is deep learning\u2019s ability to automatically\u00a0<strong>extract meaningful representations</strong>\u00a0when trained on a large enough dataset.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*1SoitQHdT7q2BZ2pszvqTQ.gif\" width=\"99%\"/><br>\n<font size=\"-1\">Visual search at Pinterest</font></br></center></p>\n<p>This is why many teams\u200a\u2014\u200alike at\u00a0<a href=\"https://medium.com/@Pinterest_Engineering/introducing-a-new-way-to-visually-search-on-pinterest-67c8284b3684\" rel=\"noopener noreferrer\" target=\"_blank\">Pinterest</a>,\u00a0<a href=\"https://multithreaded.stitchfix.com/blog/2015/09/17/deep-style/\" rel=\"noopener noreferrer\" target=\"_blank\">StitchFix</a>, and\u00a0<a href=\"http://code.flickr.net/2017/03/07/introducing-similarity-search-at-flickr/\" rel=\"noopener noreferrer\" target=\"_blank\">Flickr</a>\u200a\u2014\u200astarted using Deep Learning to learn representations of their images, and\u00a0<strong>provide recommendations\u00a0</strong>based on the content users find visually pleasing. Similarly, Fellows at\u00a0<a href=\"http://insightdatascience.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Insight</a>\u00a0have used deep learning to build models for applications such as helping people find cats to adopt, recommending sunglasses to buy, and searching for art styles.</p>\n<p>Many recommendation systems are based on\u00a0<a href=\"https://en.wikipedia.org/wiki/Collaborative_filtering\" rel=\"noopener noreferrer\" target=\"_blank\">collaborative filtering</a>: leveraging user correlations to make recommendations (\u201cusers that liked the items you have liked have also liked\u2026\u201d). However, these models require a\u00a0<strong>significant amount of data</strong>\u00a0to be accurate, and\u00a0<a href=\"https://en.wikipedia.org/wiki/Cold_start_%28computing%29\" rel=\"noopener noreferrer\" target=\"_blank\">struggle</a>\u00a0to handle\u00a0<strong>new items</strong>that have not yet been viewed by anyone. Item representation can be used in what\u2019s called\u00a0<a href=\"https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering\" rel=\"noopener noreferrer\" target=\"_blank\">content-based</a>\u00a0recommendation systems, which do not suffer from the problem above.</p>\n<p>In addition, these representations allow consumers to\u00a0<strong>efficiently search</strong> photo libraries for images that are similar to the selfie they just took (querying by image), or for photos of particular items such as cars (querying by text). Common examples of this include Google Reverse Image Search, as well as Google Image Search.</p>\n<p>Based on our experience providing technical mentorship for many semantic understanding projects, we wanted to write a tutorial on how you would go about\u00a0<strong>building your own representations</strong>, both for image and text data, and\u00a0<strong>efficiently do similarity search</strong>. By the end of this post, you should be able to build a quick semantic search model from scratch, no matter the size of your dataset.</p>\n<p><strong><em>This post is accompanied by\u00a0</em></strong><a href=\"http://insight.streamlit.io/0.13.3-8ErS/index.html?id=QAKzY9mLjr4WbTCgxz3XBX\" rel=\"noopener noreferrer\" target=\"_blank\"><strong><em>an annotated code notebook</em></strong></a><strong><em>\u00a0using\u00a0</em></strong><a href=\"http://streamlit.io/\" rel=\"noopener noreferrer\" target=\"_blank\"><strong><em>streamlit</em></strong></a> <strong><em>and a\u00a0</em></strong><a href=\"https://github.com/hundredblocks/semantic-search\" rel=\"noopener noreferrer\" target=\"_blank\"><strong><em>self-standing codebase</em></strong></a><strong><em>\u00a0demonstrating and applying all these techniques. Feel free to run the code and follow along!</em></strong></p>\n<p>\u00a0</p>\n<h3>What\u2019s our\u00a0plan?</h3>\n<p>\u00a0<br>\n<b>A break to chat about optimization</b></br></p>\n<p>In machine learning, just like in software engineering, there are many ways to tackle a problem, each with different tradeoffs. If we are doing research or local prototyping, we can get away with very inefficient solutions. But if we are building an image similarity search engine that needs to be maintainable and scalable, we have to consider both how we can\u00a0<strong>adapt to data evolution</strong>, and\u00a0<strong>how fast our model can run</strong>.</p>\n<p>Let\u2019s imagine a few approaches:</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*fCDUAwVmCn1EJuhBR7kMYQ.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Workflow for approach\u00a01</font></center></p>\n<p>1/ We build an end-to-end model that is trained on all our images to take an image as an input, and output a similarity score over all of our images. Predictions happen quickly (one forward pass), but we would need to<strong>\u00a0train a new model</strong>\u00a0every time we add a new image. We would also quickly reach a state with so many classes that it would be extremely\u00a0<strong>hard to optimize</strong>\u00a0it correctly. This approach is fast, but does not scale to large datasets. In addition, we would have to label our dataset by hand with image similarities, which could be extremely time consuming.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*dVzkAN44aulHdgTm7ZVyww.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Workflow for approach\u00a02</font></center></p>\n<p>2/ Another approach is to build a model that takes in two images, and outputs a pairwise similarity score between 0 and 1 (<a href=\"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Siamese Networks</a>[PDF], for example). These models are accurate for large datasets, but lead to another scalability issue. We usually want to find a similar image by\u00a0<strong>looking through a vast collection of images</strong>, so we have to run our similarity model once for each image pair in our dataset. If our model is a CNN, and we have more than a dozen images, this becomes too slow to even be considered. In addition, this only works for image similarity, not text search. This method scales to large datasets, but is slow.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*zYN9ME-2mgw6lQNhAFGVDg.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Workflow for approach\u00a03</font></center></p>\n<p>3/ There is a simpler method, which is similar to\u00a0<a href=\"https://en.wikipedia.org/wiki/Word_embedding\" rel=\"noopener noreferrer\" target=\"_blank\">word embeddings</a>. If we find an\u00a0<strong>expressive vector representation, or embedding\u00a0</strong>for images, we can then calculate their similarity by looking at\u00a0<strong>how close their vectors are to each other</strong>. This type of search is a common problem that is well studied, and many libraries implement fast solutions (we will use\u00a0<a href=\"https://github.com/spotify/annoy\" rel=\"noopener noreferrer\" target=\"_blank\">Annoy</a>\u00a0here). In addition, if we calculate these vectors for all images in our database ahead of time, this approach is both fast (one forward pass, and an efficient similarity search), and scalable. Finally, if we manage to find\u00a0<strong>common embeddings</strong>\u00a0for our images and our words, we could use them to do text to image search!</p>\n<p>Because of its simplicity and efficiency, the third method will be the focus of this post.</p>\n<p><b>How do we get\u00a0there?</b></p>\n<p>So, how do we actually use\u00a0<strong>deep learning representations</strong>\u00a0to create a\u00a0<strong>search engine?</strong></p>\n<p>Our final goal is to have a search engine that can take in images and output either similar images or tags, and take in text and output similar words, or images. To get there, we will go through three successive steps:</p>\n<ul>\n<li>Searching for\u00a0<strong>similar images to an input image</strong>\u00a0(Image \u2192 Image)\n<li>Searching for\u00a0<strong>similar words to an input word</strong>\u00a0(Text \u2192 Text)\n<li>Generating\u00a0<strong>tags for images</strong>, and\u00a0<strong>searching images using tex</strong>t (Image \u2194 Text)\n</li></li></li></ul>\n<p>To do this, we will use\u00a0<strong>embeddings</strong>, vector representations of images and text. Once we have embeddings, searching simply becomes a matter of finding vectors close to our input vector.</p>\n<p>The way we find these is by calculating the\u00a0<a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\" rel=\"noopener noreferrer\" target=\"_blank\">cosine similarity</a>\u00a0between our image embedding, and embeddings for other images. Similar images will have similar embeddings, meaning a\u00a0<strong>high cosine similarity between embeddings</strong>.</p>\n<p>Let\u2019s start with a dataset to experiment with.</p>\n<p>\u00a0</p>\n<h3>Dataset</h3>\n<p>\u00a0<br/>\n<b>Images</b></p>\n<p>Our image dataset consists of a total of a\u00a0<strong>1000 images</strong>, divided in 20 classes with 50 images for each. This dataset can be found\u00a0<a href=\"http://vision.cs.uiuc.edu/pascal-sentences/\" rel=\"noopener noreferrer\" target=\"_blank\">here</a>. Feel free to use the script in the linked code to automatically download all image files.\u00a0<em>Credit to Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier for the dataset.</em></p>\n<p>This dataset contains a category and a set of captions for every image. To make this problem harder, and to show how well our approach generalizes, we will\u00a0<strong>only use the categories</strong>, and disregard the captions. We have a total of 20 classes, which I\u2019ve listed out below:</p>\n<p><code>aeroplane</code>\u00a0<code>bicycle</code>\u00a0<code>bird</code>\u00a0<code>boat</code>\u00a0<code>bottle</code>\u00a0<code>bus</code>\u00a0<code>car</code>\u00a0<code>cat</code>\u00a0<code>chair</code>\u00a0<code>cow</code> <code>dining_table</code> <code>dog</code> <code>horse</code> <code>motorbike</code> <code>person</code> <code>potted_plant</code> <code>sheep</code> <code>sofa</code> <code>train</code> <code>tv_monitor</code></p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*PBvv1EkQ67RJtLgmzspP5w.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Image examples. As we can see, the labels are quite\u00a0noisy.</font></center></p>\n<p>We can see our labels are pretty\u00a0<strong>noisy</strong>: many photos contain multiple categories, and the label is not always from the most prominent one. For example, on the bottom right, the image is labeled\u00a0<code>chair</code>\u00a0and not\u00a0<code>person</code>even though 3 people stand in the center of the image, and the chair is barely visible.</p>\n<p><b>Text</b></p>\n<p>In addition, we load word embeddings that have been pre-trained on Wikipedia (this tutorial will use the ones from the\u00a0<a href=\"https://nlp.stanford.edu/projects/glove/\" rel=\"noopener noreferrer\" target=\"_blank\">GloVe</a>\u00a0model). We will use these vectors to incorporate text to our semantic search. For more information on how these word vectors work, see\u00a0<a href=\"https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e\" rel=\"noopener noreferrer\" target=\"_blank\">Step 7</a>\u00a0of our NLP tutorial.</p>\n<p>\u00a0</p>\n<h3>Image -&gt;\u00a0Image</h3>\n<p>\u00a0<br/>\n<em>Starting simple</em><br/>\nWe are now going to load a model that was\u00a0<strong>pre-trained</strong>\u00a0on a large data set (<a href=\"http://www.image-net.org/\" rel=\"noopener noreferrer\" target=\"_blank\">Imagenet</a>), and is freely available online. We use VGG16 here, but this approach would work with any recent CNN architecture. We use this model to generate\u00a0<strong>embeddings</strong>\u00a0for our images.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*LdB1mhrN6hT-EznkkFE8Cg.jpeg\" width=\"99%\"/><br/>\n<font size=\"-1\">VGG16 (credit to Data Wow\u00a0Blog)</font></center></p>\n<p>What do we mean by generating embeddings? We will use our pre-trained model\u00a0<strong>up to the penultimate layer</strong>, and store the value of the activations. In the image below, this is represented by the embedding layer highlighted in green, which is before the final classification layer.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*A-kdSGUtBL1kcJ2B8TKSmg.png\" width=\"99%\"/><br/>\n<font size=\"-1\">For our embeddings, we use the layer before the final classification layer.</font></center></p>\n<p>Once we\u2019ve used the model to generate image features, we can then store them to disk and re-use them\u00a0<strong>without needing to do inference again</strong>! This is one of the reasons that embeddings are so popular in practical applications, as they allow for huge efficiency gains. On top of storing them to disk, we will build a\u00a0<strong>fast index</strong>\u00a0of the embeddings using\u00a0<a href=\"https://github.com/spotify/annoy\" rel=\"noopener noreferrer\" target=\"_blank\">Annoy</a>, which will allow us to very quickly find the nearest embeddings to any given embedding.</p>\n<p>Below are our embeddings. Each image is now represented by a sparse vector of size 4096.\u00a0<em>Note: the reason the vector is sparse is that we have taken the values after the activation function, which zeros out negatives.</em></p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*5A-M_7imJHRjOAlDwqiu5Q.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Image Embeddings</font></center></p>\n<p><b><strong>Using our embeddings to search through\u00a0images</strong></b></p>\n<p>We can now simply take in an image, get its embedding, and look in our fast index to find similar embeddings, and thus similar images.</p>\n<p>This is especially useful since image labels are often\u00a0<strong>noisy</strong>, and there is more to an image than its label.</p>\n<p>For example, in our dataset, we have both a class\u00a0<code>cat</code>, and a class\u00a0<code>bottle</code>.</p>\n<p>Which class do you think this image is labeled as?</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*RPHlEkGIyqih7e3cnjKtGQ.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Cat or Bottle? (Image rescaled to 224x224, which is what the neural network\u00a0sees.)</font></center></p>\n<p>The correct answer is\u00a0<strong>bottle</strong>\u00a0\u2026 This is an actual issue that comes up often in real datasets. Labeling images as unique categories is quite limiting, which is why we hope to use more nuanced representations. Luckily, this is exactly what\u00a0<strong>deep learning is good at</strong>!</p>\n<p>Let\u2019s see if our image search using embeddings does better than human labels.</p>\n<p>Searching for similar images to<code>dataset/bottle/2008_000112.jpg</code>\u2026</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/1600/1*PamdW6zfGIbZeD1gaHAAmA.png\" width=\"99%\"/></p>\n<p>Great\u200a\u2014\u200awe mostly get more images of\u00a0<strong>cats</strong>, which seems very reasonable! Our pre-trained network has been trained on a wide variety of images, including cats, and so it is able to accurately find similar images, even though it has never been trained on this particular dataset before.</p>\n<p>However, one image in the middle of the bottom row shows a shelf of bottles. This approach perform wells to find similar images, in general, but sometimes we are only interested in\u00a0<strong>part of the image</strong>.</p>\n<p>For example, given an image of a cat and a bottle, we might be only interested in similar cats, not similar bottles.</p>\n<p><b>Semi-supervised search</b></p>\n<p>A common approach to solve this issue is to use an\u00a0<strong>object detection</strong>\u00a0model first, detect our cat, and do image search on a cropped version of the original image.</p>\n<p>This adds a huge computing overhead, which we would like to avoid if possible.</p>\n<p>There is a simpler \u201chacky\u201d approach, which consists of\u00a0<strong>re-weighing</strong>\u00a0the activations. We do this by loading the last layer of weights we had initially discarded, and only use the weights tied to the index of the class we are looking for to re-weigh our embedding. This cool trick was initially brought to my attention by\u00a0<a href=\"http://insightdatascience.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Insight</a>\u00a0Fellow\u00a0<a href=\"https://www.linkedin.com/in/daweonryu/\" rel=\"noopener noreferrer\" target=\"_blank\">Daweon Ryu</a>. For example, in the image below, we use the weights of the\u00a0<code>Siamese cat</code>\u00a0class to re-weigh the activations on our dataset (highlighted in green). Feel free to check out the attached notebook to see the implementation details.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/1600/1*ef3jietHYrBhfw26qGLZ8g.png\" width=\"99%\"/><br/>\n<font size=\"-1\">The hack to get weighted embeddings. The classification layer is shown for reference only.</font></center></p>\n<p>Let\u2019s examine how this works by weighing our activations according to class\u00a0<code>284</code>\u00a0in Imagenet,\u00a0<code>Siamese cat</code>.</p>\n<p>Searching for similar images to<code>dataset/bottle/2008_000112.jpg</code>\u00a0using weighted features\u2026</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/1600/1*qMNxg8XlxcNIAImu4A-n4g.png\" width=\"99%\"/></p>\n<p>We can see that the search has been biased to look for\u00a0<strong>Siamese cat-like things</strong>. We no longer show any bottles, which is great. You might however notice that our last image is of a sheep! This is very interesting, as biasing our model has led to a\u00a0<strong>different kind of error</strong>, which is more appropriate for our current domain.</p>\n<p>We have seen we can search for similar images in a\u00a0<strong>broad</strong>\u00a0way, or by\u00a0<strong>conditioning on a particular class</strong>\u00a0our model was trained on.</p>\n<p>This is a great step forward, but since we are using a model\u00a0<strong>pre-trained on Imagenet</strong>, we are thus limited to the 1000\u00a0<strong>Imagenet classes</strong>. These classes are far from all-encompassing (they lack a category for people, for example), so we would ideally like to find something more\u00a0<strong>flexible.\u00a0</strong>In addition, what if we simply wanted to search for cats\u00a0<strong>without providing an input image?</strong></p>\n<p>In order to do this, we are going to use more than simple tricks, and leverage a model that can understand the semantic power of words.</p>\n</div> ", "website": "kdnuggets"}