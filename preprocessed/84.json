{"content": "By Taposh Roy, Kaiser Permanente. Analyzing images and videos, and using them in various applications such as self driven cars, drones etc. with underlying deep learning techniques has been the new research frontier. The recent research papers such as \u201c A Neural Algorithm of Artistic Style \u201d, show how a styles can be transferred from an artist and applied to an image, to create a new image. Other papers such as \u201c Generative Adversarial Networks \u201d (GAN) and \u201c Wasserstein GAN \u201d have paved the path to develop models that can learn to create data that is similar to data that we give them. Thus opening up the world to semi-supervised learning and paving the path to a future of unsupervised learning. While these research areas are still on the generic images, our goal is to use these research into medical images to help healthcare. We need to start with some basics. In this article, I start with basics of image processing, basics of medical image format data and visualize some medical data. In the next article I will deep dive into some convolutional neural nets and use them with Keras for predicting lung cancer. Basic Image Processing (using\u00a0python) \u00a0 There are a variety of image processing libraries, however OpenCV (open computer vision) has become mainstream due to its large community support and availability in C++, java and python. I prefer using opencv using jupyter notebook. Install OpenCV using: pip install opencv-python or install directly from the source from opencv.org Installing opencv. Now open your Jupyter notebook and confirm you can import cv2. You will also need numpy and matplotlib to view your plots inside the notebook. Now, lets check if you can open an image and view it on your notebook using the code below. Example image load through\u00a0OpenCV. Basic Face Detection Lets, do something fun such as detecting a face. To detect face we will use an open source xml stump-based 20x20 gentle adaboost frontal face detector originally created by Rainer Lienhart. A good post with details on Haar-cascade detection is here . Face detection using\u00a0OpenCV. There are a lot of examples for image processing using opencv in the docs section. /trunk/d6/d00/tutorial_py_root.html . I leave it up to the reader to play with more examples. Now that we know the basics of image processing, lets move to the next level of understanding medical image format. Medical Image Data\u00a0Format \u00a0 Medical images follow Digital Imaging and Communications (DICOM) as a standard solution for storing and exchanging medical image-data. The first version of this standard was released in 1985. Since then there are several changes made. This standard uses a file format and a communications protocol. File Format \u200a\u2014\u200aAll patient medical images are saved in the DICOM file format. This format has PHI (protected health information) about the patient such as\u200a\u2014\u200aname, sex, age in addition to other image related data such as equipment used to capture the image and some context to the medical treatment. Medical Imaging Equipments create DICOM files. Doctors use DICOM Viewers, computer software applications that can display DICOM images, read and to diagnose the findings in the images. Communications Protocol \u200a\u2014\u200aThe DICOM communication protocol is used to search for imaging studies in the archive and restore imaging studies to the workstation in order to display it. All medical imaging applications that are connected to the hospital network use the DICOM protocol to exchange information, mainly DICOM images but also patient and procedure information. There are also more advanced network commands that are used to control and follow the treatment, schedule procedures, report statuses and share the workload between doctors and imaging devices. A very good blog that goes into details of the DICOM standard is here Analyze DICOM\u00a0Images \u00a0 A very good python package used for analyzing DICOM images is pydicom. In this section, we will see how to render a DICOM image on a Jupyter notebook. Install OpenCV using: pip install pydicom After you install pydicom package, go back to the jupyter notebook. In the notebook, import the dicom package and other packages as shown below. We also use other packages such as pandas, scipy, skimage, mpl_toolkit for data processing and analysis. There\u2019s a wealth of freely available DICOM datasets online but here\u2019s a few that should help you get started: Kaggle Competitions and Datasets : This is my personal favorite. Check out the data for lung cancer competition and diabetes retinopathy. Dicom Library \u00a0: DICOM Library is a free online medical DICOM image or video file sharing service for educational and scientific purposes. Osirix Datasets : Provides a large range of human datasets acquired through a variety of imaging modalities. Visible Human Datasets : Parts of the Visible Human project are somehow freely distributed here which is weird cause getting that data is neither free nor hassle-free. The Zubal Phantom : This website offers multiple datasets of two human males in CT and MRI which are freely distributed. Download the dicom files and load them on your jupyter notebook. Now, load the DICOM images into a list. Step 1\u00a0: Basic Viewing of DICOM Image in Jupyter In the first line we load the 1st DICOM file, which we\u2019re gonna use as a reference named RefDs , to extract metadata and whose filename is first in the lstFilesDCM list. We then calculate the total dimensions of the 3D NumPy array which are equal to (Number of pixel rows in a slice) x (Number of pixel columns in a slice) x (Number of slices) along the x, y, and z cartesian axes. Lastly, we use the PixelSpacing and SliceThickness attributes to calculate the spacing between pixels in the three axes. We store the array dimensions in ConstPixelDims and the spacing in ConstPixelSpacing [1]. Step 2: Looking into details of DICOM format The unit of measurement in CT scans is the Hounsfield Unit (HU) , which is a measure of radiodensity. CT scanners are carefully calibrated to accurately measure this. A detailed understanding on this can be found here . Each pixel is assigned a numerical value (CT number), which is the average of all the attenuation values contained within the corresponding voxel. This number is compared to the attenuation value of water and displayed on a scale of arbitrary units named Hounsfield units (HU) after Sir Godfrey Hounsfield. This scale assigns water as an attenuation value (HU) of zero. The range of CT numbers is 2000 HU wide although some modern scanners have a greater range of HU up to 4000. Each number represents a shade of grey with +1000 (white) and \u20131000 (black) at either end of the spectrum. Hounsfield Scale [credits: \u201cIntroduction to CT physics\u201d (PDF). elsevierhealth.com.] Some scanners have cylindrical scanning bounds, but the output image is square. The pixels that fall outside of these bounds get the fixed value -2000 . CT Scanner Image [credits\u00a0: \u201cIntroduction to CT physics\u201d (PDF). elsevierhealth.com.] The first step usually is setting these values to 0. Next, let\u2019s go back to HU units, by multiplying with the rescale slope and adding the intercept (which are conveniently stored in the metadata of the scans!). In the next part, we will use Kaggle\u2019s lung cancer data-set and Convolution Neural Nets using Keras. We will build upon the information provided by this article to go to the next one. Acknowledgements: /2014/09/08/dicom-in-python-importing-medical-image-data-into-numpy-with-pydicom-and-vtk/ /resources/dicom-image-library/ /paper_of_week/viola01rapid.pdf /posts/2016/06/introduction-deep-learning-python-caffe/ / /c/data-science-bowl-2017 /trunk/d6/d00/tutorial_py_root.html Kaggle community for all the different scripts and support Bio: Taposh Roy leads innovation team in Kaiser Permanente's Decision Support group. He works with research, technology and business leaders to derive insights from data. Original . Reposted with permission. Related: Deep Learning, Generative Adversarial Networks\u200a & Boxing \u2013 Toward a Fundamental Understanding I\u2019m a data scientist \u2013 mind if I do surgery on your heart? arXiv Paper Spotlight: Sampled Image Tagging and Retrieval Methods on User Generated Content", "title_html": "<h1 id=\"title\">Medical Image Analysis with Deep Learning\u200a</h1> ", "url": "https://www.kdnuggets.com/2017/03/medical-image-analysis-deep-learning.html", "tfidf": {"tfidf": {"after": 2.04140414042, "fall": 1.6945244956799999, "dive": 16.085106383, "plot": 5.383519837230001, "pip": 271.384615384, "wide": 1.5598349381, "new": 2.0357761108, "assign": 7.67327211214, "space": 4.79637462236, "addit": 1.24634950542, "veri": 2.51760228354, "panda": 111.802816901, "number": 7.71000416263, "been": 1.0239277652399998, "etc": 4.2066772655, "dataset": 1355.268292686, "godfrey": 54.9342560554, "acknowledg": 4.5476940704699995, "unsupervis": 345.13043478300006, "work": 1.11520089913, "elsevierhealthcom": 2886.54545454, "path": 9.284210526319999, "resourcesdicomimagelibrari": 1443.27272727, "name": 3.3063519611100003, "know": 2.59327017315, "opencvpython": 1443.27272727, "osirix": 1443.27272727, "wealth": 6.727118644069999, "scale": 11.240972386109998, "kaiser": 73.1612903226, "equal": 2.542193755, "measur": 7.23280182231, "bound": 10.81471389646, "creat": 4.9971671388, "radiodens": 1443.27272727, "slicethick": 1443.27272727, "how": 3.20500656102, "repost": 933.882352941, "origin": 2.27449856734, "websit": 2.52160101652, "distribut": 5.479206212259999, "due": 1.23789473684, "end": 1.10680423871, "found": 1.11387076405, "spotlight": 31.625498008, "driven": 5.601976005650001, "deep": 10.88391224862, "will": 7.34886591576, "numxnum": 52.0524590164, "numdicominpythonimportingmedicalimagedataintonumpywithpydicomandvtk": 1443.27272727, "credit": 6.08625646924, "purpos": 2.23416830847, "somehow": 24.8450704225, "next": 7.4752801582, "below": 4.51215006394, "similar": 1.37514075357, "but": 3.04897253697, "adaboost": 1443.27272727, "need": 2.8745247148199997, "our": 2.35758835759, "metadata": 423.36, "pave": 24.5, "dicom": 4329.81818181, "applic": 10.28016404058, "level": 1.6544393497299998, "various": 1.3323262839899999, "has": 3.1309492505999996, "generat": 6.15826221876, "use": 24.711330177119997, "usual": 1.72508964468, "onlin": 5.210370856580001, "out": 1.06016694491, "model": 2.0905978404, "\u2013num": 1443.27272727, "format": 20.25, "good": 4.55944859277, "mainstream": 7.450023463160001, "under": 1.0781663837, "numer": 1.83325635104, "xml": 191.277108434, "cartesian": 109.48965517200001, "view": 4.92228193467, "someth": 3.28152128979, "arbitrari": 17.8181818182, "lienhart": 1443.27272727, "kaggl": 4329.81818181, "treatment": 7.74250182882, "solut": 4.7278141751, "axe": 42.6201342282, "jupyt": 8659.63636362, "shown": 2.76923076923, "constpixeldim": 1443.27272727, "then": 2.17315721032, "grey": 10.6981132075, "frontal": 36.4965517241, "instal": 26.51049618322, "cancer": 24.03027245208, "let": 13.94466403164, "world": 1.11340206186, "pixel": 431.41304347849996, "blog": 14.1876675603, "section": 4.2568708942199995, "group": 1.20996875238, "filenam": 496.125, "that": 11.04382470125, "algorithm": 27.9507042254, "array": 20.2888178914, "offer": 1.53896859248, "softwar": 10.2624434389, "develop": 1.1955719557200002, "such": 7.43059641618, "sever": 1.07241286139, "scanner": 362.88, "network": 10.37477536352, "python": 168.8936170212, "deriv": 2.78379800105, "lot": 4.40877534018, "some": 6.2422018348799995, "sourc": 3.39520958084, "should": 1.6643254009900001, "permanent": 675.574468086, "step": 8.48379052368, "slice": 106.0757238306, "provid": 2.43105428374, "version": 2.0083491461099996, "varieti": 4.5944147012, "content": 3.5421686747, "becom": 1.12492028626, "protect": 1.96460834055, "set": 1.18707940781, "caus": 1.38521943984, "compar": 1.8662278123900002, "drone": 52.92, "lstfilesdcm": 1443.27272727, "gentl": 17.1447084233, "workload": 74.5352112676, "visibl": 9.37190082644, "patient": 28.639807576680003, "detail": 9.04744265564, "num": 9.00283536009, "multipli": 20.4061696658, "correspond": 3.32481675393, "free": 3.43636363636, "acquir": 3.10563380282, "for": 9.00283536009, "car": 3.53743315508, "predict": 5.18484650555, "output": 7.676982591880001, "releas": 1.8377126982299998, "with": 10.011982089899998, "competit": 6.1392111369, "zero": 8.75192943771, "are": 13.38877716514, "cvnum": 835.5789473680001, "look": 1.9086318826599997, "insid": 2.7396031061299997, "conveni": 9.85474860335, "video": 6.59439252336, "futur": 1.8577112099200002, "numpi": 2886.54545454, "refer": 1.30024570025, "extract": 7.703056768560001, "restor": 2.97526236882, "modal": 54.1843003413, "net": 13.92631578948, "column": 7.078020508250001, "toward": 1.6303142329, "retinopathi": 1443.27272727, "haarcascad": 1443.27272727, "and": 40.0025196852, "skimag": 1443.27272727, "shade": 16.979679144400002, "give": 1.3653250774, "pixelspac": 1443.27272727, "part": 2.08661365578, "goal": 3.28152128979, "educ": 2.00733341763, "move": 1.29125660838, "lead": 1.2664326739, "opencv": 11546.18181816, "heart": 3.00340522134, "row": 5.549108703250001, "analyz": 29.059182428310002, "scientist": 4.69426374926, "notebook": 321.5392405064, "into": 5.0751230739499995, "paper": 7.988594431410001, "medic": 39.30513719832, "lung": 55.575262543799994, "dimens": 16.51170046802, "through": 2.14149861738, "scan": 56.4312796209, "servic": 1.51300867245, "doctor": 7.8789081885800005, "last": 1.2117234010100002, "face": 9.01635620175, "hounsfield": 5773.09090908, "techniqu": 3.7293868921800004, "voxel": 1443.27272727, "cdatasciencebowlnum": 1443.27272727, "help": 2.79925945518, "main": 1.25303867403, "larg": 2.3714989917, "find": 1.7294117647099998, "line": 1.4182597820299998, "paperofweekviolanumrapidpdf": 1443.27272727, "librari": 8.04798918555, "relat": 2.47501753838, "squar": 3.26666666667, "water": 4.26315789474, "permiss": 6.280063291139999, "post": 2.23826307627, "here": 12.1153846154, "report": 1.3634489866, "multipl": 2.74813917258, "black": 1.94917127072, "averag": 2.60390355913, "valu": 13.666571018640003, "human": 7.586190419319999, "stumpbas": 1443.27272727, "greater": 2.14801785956, "contain": 1.59814777532, "care": 2.49426551453, "about": 1.06486015159, "communiti": 3.92242124768, "neural": 178.3820224719, "mind": 3.5918552036199998, "context": 4.25972632144, "rainer": 79.7788944724, "devic": 5.00820189274, "standard": 7.566305254360001, "artist": 5.73347778982, "check": 13.0131147541, "scipi": 1443.27272727, "render": 5.97740963855, "save": 2.8178913737999998, "communic": 11.358254337319998, "nor": 3.3479544496, "start": 3.8002074523200005, "articl": 6.054150247859999, "load": 27.21988855552, "them": 4.39504463976, "insight": 11.8037174721, "roy": 17.16324324324, "studi": 3.06368197608, "order": 1.24625166811, "digit": 4.416133518780001, "health": 2.71570304482, "play": 1.46390041494, "appli": 2.2972073506, "the": 68.0, "age": 1.48623853211, "fun": 12.8863636364, "build": 1.6341739578, "analysi": 3.47852760736, "protocol": 58.9090909092, "display": 8.80369685766, "total": 1.5460122699399999, "adversari": 52.656716418, "team": 2.2748244734200003, "gon": 39.2, "there": 5.20456333595, "command": 2.66689064337, "connect": 1.8843916913900003, "inform": 6.301250248080001, "diabet": 39.297029703, "diagnos": 16.283076923099998, "semisupervis": 1443.27272727, "repres": 1.46972782818, "either": 1.5830092731099998, "visual": 5.22752716497, "follow": 2.09280253098, "now": 4.643123492, "reader": 6.437956204380001, "code": 3.8807137619199996, "support": 3.8056731921600004, "slope": 12.7620578778, "basic": 19.1112639725, "self": 11.972850678699999, "unit": 5.76973397295, "goe": 4.251740760580001, "fix": 4.4346368715099995, "also": 4.05906040268, "person": 1.40520446097, "modern": 1.5319888063299998, "style": 4.75614140204, "pydicom": 4329.81818181, "white": 1.86930413282, "two": 1.01379310345, "research": 9.710091743100001, "script": 8.299006795610001, "area": 1.3881262568900001, "doc": 34.8157894737, "calibr": 39.2, "rescal": 441.0, "tag": 19.7462686567, "open": 6.227836183899999, "hospit": 3.4633507853400003, "captur": 2.88026124819, "trunkdnumdnumtutorialpyroothtml": 2886.54545454, "generic": 17.9592760181, "howev": 1.0945191313299998, "viewer": 12.690647482000001, "share": 3.7132499123000002, "although": 1.14968498805, "attribut": 3.4156626506, "archiv": 3.3451327433599998, "calcul": 12.25945945946, "wasserstein": 992.25, "numst": 2.6297830047999997, "which": 7.036342915, "upon": 1.60331246213, "thus": 1.6463756092500001, "other": 4.03969465648, "introduct": 5.561744613759999, "three": 1.06621893889, "procedur": 11.738262476900001, "one": 1.00627495722, "show": 1.26703910615, "workstat": 121.190839695, "prefer": 3.0216977540900003, "status": 2.4636871508400002, "see": 1.27242125511, "surgeri": 12.2689335394, "box": 4.12685209254, "imag": 102.65237365998001, "refd": 1443.27272727, "get": 5.35687774155, "still": 1.1866357724799999, "spectrum": 12.6401273885, "outsid": 1.67450690855, "control": 1.46959178006, "technolog": 2.6034765496900003, "file": 26.397149643689996, "detect": 27.064439140799998, "process": 10.17148958892, "neither": 3.6622837370199997, "recent": 1.54405757635, "cylindr": 49.3043478261, "kera": 1671.1578947360001, "frontier": 8.7422907489, "this": 12.04552352052, "schedul": 3.6648199445999996, "sampl": 7.23280182232, "rang": 5.354468802689999, "sir": 4.55030094583, "imagedata": 1443.27272727, "back": 2.52140077822, "chang": 1.1808985421, "zubal": 1443.27272727, "project": 1.7534791252500002, "differ": 1.23654490225, "healthcar": 18.7659574468, "decis": 2.16, "weird": 41.2363636364, "between": 2.06907337416, "equip": 6.49059689288, "opencvorg": 1443.27272727, "java": 31.625498008, "vision": 4.88041807562, "along": 1.2973768080399999, "all": 4.04587155964, "attenu": 270.6136363635, "store": 10.34042553192, "physic": 4.78264798916, "taposh": 2886.54545454, "intercept": 15.8285144566, "phantom": 28.6054054054, "exchang": 6.11674051242, "whose": 1.73508196721, "confirm": 3.0827184466000004, "more": 2.0343413634, "mpltoolkit": 1443.27272727, "list": 2.72642967542, "male": 3.4709226060300002, "these": 4.29661705008, "made": 1.07038834951, "retriev": 2.16826003824, "download": 14.6457564576, "detector": 45.6206896552, "can": 7.05756834852, "avail": 3.4576935642, "comput": 7.855517070760001, "fundament": 5.32930513595, "search": 3.2539454806299997, "each": 2.37949640288, "constpixelspac": 1443.27272727, "few": 1.31729173581, "accur": 5.768895348840001, "read": 2.3149606299200003, "packag": 39.14201183430001, "transfer": 2.72549356223, "advanc": 1.9997480791, "freeli": 34.0930565496, "data": 37.14079115274, "method": 2.5714285714300003, "hasslefre": 1443.27272727, "from": 4.00226885988, "favorit": 8.116564417180001, "sinc": 1.08368600683, "convolut": 202.242038216, "bio": 42.336000000000006, "matplotlib": 1443.27272727, "exampl": 4.51450236966, "scientif": 4.15275961287, "leader": 2.0994445913799997, "understand": 8.90575916229, "sex": 6.432739059969999, "direct": 1.22226499346, "import": 2.6803984467400004, "first": 4.03046458492, "leav": 1.6615384615399997, "busi": 2.05541170378, "have": 3.0446845234199995, "while": 1.0441988950299999, "innov": 4.74051955808, "within": 1.2369302688, "learn": 11.61375274325, "postsnumintroductiondeeplearningpythoncaff": 1443.27272727, "arxiv": 441.0, "user": 7.71053909665}, "logtfidf": {"after": 0.040981389296199995, "fall": 0.527402167952, "dive": 2.7778937744700003, "plot": 1.68334240509, "pip": 9.82077975822, "wide": 0.44458000675399995, "new": 0.0354598937022, "assign": 2.6891919112, "space": 1.749426329944, "addit": 0.220218882972, "veri": 0.460319586476, "panda": 4.7167367562999996, "number": 0.6762600489302, "been": 0.023645982368400004, "etc": 1.4366730879700003, "dataset": 36.86091196648, "godfrey": 4.00613712579, "acknowledg": 1.5146203068000001, "unsupervis": 5.843922417409999, "work": 0.109034567273, "elsevierhealthcom": 14.549337082200001, "path": 3.0703359676999997, "resourcesdicomimagelibrari": 7.2746685411000005, "name": 0.29169949915290005, "know": 0.952919694398, "opencvpython": 7.2746685411000005, "osirix": 7.2746685411000005, "wealth": 1.90614691588, "scale": 3.9628591898399996, "kaiser": 7.1990385596, "equal": 0.933027391343, "measur": 2.640042599178, "bound": 3.37552084834, "creat": 0.890307274056, "radiodens": 7.2746685411000005, "slicethick": 7.2746685411000005, "how": 0.9431339138600001, "repost": 6.83935046985, "origin": 0.257224875174, "websit": 0.924894023806, "distribut": 2.01562611626, "due": 0.21341214386399998, "end": 0.101476798618, "found": 0.107841124048, "spotlight": 3.45396369421, "driven": 1.72311939365, "deep": 3.8660204094, "will": 1.2167192094900001, "numxnum": 3.9522520373, "numdicominpythonimportingmedicalimagedataintonumpywithpydicomandvtk": 7.2746685411000005, "credit": 2.22577202176, "purpos": 0.803869037322, "somehow": 3.21265935953, "next": 2.010818427495, "below": 1.627253183872, "similar": 0.318556092114, "but": 0.0485771162157, "adaboost": 7.2746685411000005, "need": 0.725480326884, "our": 0.8576392141820001, "metadata": 10.71015140074, "pave": 5.01105187398, "dicom": 21.824005623300003, "applic": 3.6948117854699998, "level": 0.503462189943, "various": 0.28692650007, "has": 0.1281718345644, "generat": 2.1575470252080002, "use": 0.7009924735584, "usual": 0.545279017064, "onlin": 1.915007708714, "out": 0.0584263909193, "model": 0.7374500731110001, "\u2013num": 7.2746685411000005, "format": 7.429706014983999, "good": 1.2557682147209999, "mainstream": 2.0082171818, "under": 0.07526180538319999, "numer": 0.606093812346, "xml": 5.25372320611, "cartesian": 4.69583007148, "view": 1.485479826519, "someth": 1.18830712273, "arbitrari": 2.88021938643, "lienhart": 7.2746685411000005, "kaggl": 21.824005623300003, "treatment": 2.7071553770200003, "solut": 1.55346297627, "axe": 6.11835919104, "jupyt": 43.64801124660001, "shown": 1.01856958099, "constpixeldim": 7.2746685411000005, "then": 0.16606773046179998, "grey": 2.37006739018, "frontal": 3.5972177828099996, "instal": 9.321414115300001, "cancer": 6.242106297029999, "let": 4.995210269119999, "world": 0.107420248621, "pixel": 22.288140281449998, "blog": 2.65237310559, "section": 1.510774355896, "group": 0.190594534797, "filenam": 6.2068279111, "that": 0.043737632176039994, "algorithm": 3.33044239518, "array": 4.63384542186, "offer": 0.431112446902, "softwar": 2.32849096333, "develop": 0.178624694913, "such": 0.417871844642, "sever": 0.06991112039689999, "scanner": 18.03111135992, "network": 3.8123322122079997, "python": 12.09197022888, "deriv": 1.02381618275, "lot": 1.4835969502500002, "some": 0.23744105438700003, "sourc": 1.058436621502, "should": 0.509419876758, "permanent": 11.644832424379999, "step": 3.11863517094, "slice": 10.69662277848, "provid": 0.39035568865000003, "version": 0.697313064259, "varieti": 1.6633883796239999, "content": 1.26473915954, "becom": 0.11771217648900001, "protect": 0.67529290767, "set": 0.171496011289, "caus": 0.325858567406, "compar": 0.6239191809269999, "drone": 3.96878133925, "lstfilesdcm": 7.2746685411000005, "gentl": 2.84168957926, "workload": 4.31127164819, "visibl": 3.0891375162599997, "patient": 6.76855600413, "detail": 3.264751108692, "num": 0.0028349135585730005, "multipli": 3.01583728972, "correspond": 1.20141456099, "free": 1.0825332985340002, "acquir": 1.1332178178499999, "for": 0.0028349135585730005, "car": 1.2634013667, "predict": 1.6457402376899999, "output": 2.03822657827, "releas": 0.608521699544, "with": 0.0119749171339, "competit": 2.24309814802, "zero": 2.1692741832299998, "are": 0.3830771565751, "cvnum": 6.72812483474, "look": 0.6463866936, "insid": 1.00781305813, "conveni": 2.28795343073, "video": 2.38614497934, "futur": 0.619345197699, "numpi": 14.549337082200001, "refer": 0.262553246798, "extract": 2.04161723301, "restor": 1.09033222631, "modal": 3.99239120489, "net": 3.8812661838999998, "column": 1.95699427938, "toward": 0.48877277716000006, "retinopathi": 7.2746685411000005, "haarcascad": 7.2746685411000005, "and": 0.002519605682544, "skimag": 7.2746685411000005, "shade": 2.8320172846099996, "give": 0.311392552224, "pixelspac": 7.2746685411000005, "part": 0.08479062196560001, "goal": 1.18830712273, "educ": 0.696807183384, "move": 0.255615859253, "lead": 0.23620402986699998, "opencv": 58.197348328800004, "heart": 1.09974671874, "row": 1.71363732085, "analyz": 6.812166705479999, "scientist": 1.54634128444, "notebook": 29.549424392, "into": 0.0745643161435, "paper": 2.938207618995, "medic": 14.23738293672, "lung": 8.757377685929999, "dimens": 4.22184413662, "through": 0.1367173837698, "scan": 8.80323395793, "servic": 0.41410016674500005, "doctor": 2.74208431792, "last": 0.19204364461100001, "face": 2.948011856285, "hounsfield": 29.098674164400002, "techniqu": 1.31624384807, "voxel": 7.2746685411000005, "cdatasciencebowlnum": 7.2746685411000005, "help": 0.672415442688, "main": 0.225571540588, "larg": 0.34075012121200005, "find": 0.547781330288, "line": 0.349430614452, "paperofweekviolanumrapidpdf": 7.2746685411000005, "librari": 2.960429942829, "relat": 0.42620060330799997, "squar": 1.18377009701, "water": 1.513725989892, "permiss": 1.8373800586400002, "post": 0.8057001527009999, "here": 4.42519094185, "report": 0.31001750903700004, "multipl": 1.01092401812, "black": 0.667404292867, "averag": 0.957011687995, "valu": 4.939159860888, "human": 2.560140732972, "stumpbas": 7.2746685411000005, "greater": 0.764545491118, "contain": 0.468845318236, "care": 0.9139943029109999, "about": 0.0628434774746, "communiti": 1.347123895582, "neural": 12.2559454665, "mind": 1.2786688388299998, "context": 1.44920491442, "rainer": 4.37925898918, "devic": 1.6110769470299997, "standard": 2.54964203928, "artist": 2.10635022034, "check": 3.74562099124, "scipi": 7.2746685411000005, "render": 1.7879873033099998, "save": 1.03598886547, "communic": 4.17460149152, "nor": 1.2083495472799999, "start": 0.709330107873, "articl": 2.106395218722, "load": 7.67061416752, "them": 0.3767333076372, "insight": 2.46841452187, "roy": 4.29924579166, "studi": 0.852940544442, "order": 0.22014038079300002, "digit": 1.48526454375, "health": 0.9990508682320001, "play": 0.38110439064199997, "appli": 0.8316941898119999, "the": 0.0, "age": 0.39624845300300005, "fun": 2.5561696698099996, "build": 0.491137452091, "analysi": 1.2466091029200002, "protocol": 10.758804249719999, "display": 3.22967832618, "total": 0.43567888670500005, "adversari": 6.54129323436, "team": 0.821902894886, "gon": 3.6686767468, "there": 0.2004894646275, "command": 0.9809132407500001, "connect": 0.633605058682, "inform": 1.817814818648, "diabet": 3.6711489359400002, "diagnos": 2.7901263429100003, "semisupervis": 7.2746685411000005, "repres": 0.38507723275, "either": 0.459327638815, "visual": 1.6539383488600001, "follow": 0.09071382218839999, "now": 0.596371780084, "reader": 1.8622111301800002, "code": 1.35601909597, "support": 0.713641830111, "slope": 2.5464765406, "basic": 7.03057424265, "self": 2.48264164316, "unit": 0.7159403090850001, "goe": 1.4473284897999998, "fix": 1.48944573451, "also": 0.0586286312, "person": 0.34018281601800004, "modern": 0.426566764719, "style": 1.732579058242, "pydicom": 21.824005623300003, "white": 0.625566240123, "two": 0.0136988443582, "research": 3.3186390906899996, "script": 2.1161358444599996, "area": 0.327954821122, "doc": 3.55007100439, "calibr": 3.6686767468, "rescal": 6.08904487545, "tag": 2.98296454472, "open": 1.097955190145, "hospit": 1.24223655551, "captur": 1.0578810012100002, "trunkdnumdnumtutorialpyroothtml": 14.549337082200001, "generic": 2.8881067512700005, "howev": 0.0903151173475, "viewer": 2.54086530344, "share": 1.237520599494, "although": 0.139487981418, "attribut": 1.2283715153700001, "archiv": 1.20750637691, "calcul": 3.6263013184199995, "wasserstein": 6.89997509166, "numst": 0.966901335107, "which": 0.036248896918010004, "upon": 0.47207177798199995, "thus": 0.49857627139300004, "other": 0.03949899167904, "introduct": 2.04552931588, "three": 0.06411868822490001, "procedur": 3.53941324524, "one": 0.0062553516455, "show": 0.236682766013, "workstat": 4.7973664907, "prefer": 1.10581884366, "status": 0.9016590696060001, "see": 0.240921585492, "surgeri": 2.50707033884, "box": 1.41751491115, "imag": 37.762960077020004, "refd": 7.2746685411000005, "get": 1.739307017346, "still": 0.17112222142900002, "spectrum": 2.53687646687, "outsid": 0.515518738985, "control": 0.38498466158600003, "technolog": 0.956847686355, "file": 9.29142121061, "detect": 8.443913724649999, "process": 3.16697519415, "neither": 1.29808692469, "recent": 0.434413741288, "cylindr": 3.8980122683599996, "kera": 13.45624966948, "frontier": 2.16817225474, "this": 0.04543738863, "schedul": 1.2987792057799998, "sampl": 1.9786264883900002, "rang": 1.7379576414089999, "sir": 1.5151933727200002, "imagedata": 7.2746685411000005, "back": 0.46333486179399996, "chang": 0.166275625058, "zubal": 7.2746685411000005, "project": 0.561601885907, "differ": 0.212321121312, "healthcar": 2.9320444543, "decis": 0.7701082216959999, "weird": 3.7193204796199995, "between": 0.06790736233059999, "equip": 2.3544146342, "opencvorg": 7.2746685411000005, "java": 3.45396369421, "vision": 1.58523088743, "along": 0.260344385917, "all": 0.04561052839119999, "attenu": 13.506239456579998, "store": 3.7123462005600008, "physic": 1.743694370368, "taposh": 14.549337082200001, "intercept": 2.7618130259400004, "phantom": 3.3535957001599996, "exchang": 2.23576435616, "whose": 0.5510546556329999, "confirm": 1.12581182025, "more": 0.034049863199999995, "mpltoolkit": 7.2746685411000005, "list": 0.619691523012, "male": 1.24442043932, "these": 0.2861344776032, "made": 0.0680215260973, "retriev": 0.773925020223, "download": 2.6841506319, "detector": 3.8203613341300007, "can": 0.974046578364, "avail": 1.094909172578, "comput": 2.73613783188, "fundament": 1.67322086119, "search": 1.1798682540899998, "each": 0.347483378608, "constpixelspac": 7.2746685411000005, "few": 0.275577913653, "accur": 1.75248061485, "read": 0.83939268088, "packag": 10.288792245950003, "transfer": 1.00264953547, "advanc": 0.6930212121780001, "freeli": 7.291444363950001, "data": 13.3850264328, "method": 0.944461608841, "hasslefre": 7.2746685411000005, "from": 0.002268216675464, "favorit": 2.09390696331, "sinc": 0.0803681994577, "convolut": 9.2326360171, "bio": 3.7456377879300002, "matplotlib": 7.2746685411000005, "exampl": 1.2260480249969998, "scientif": 1.42377308021, "leader": 0.741672829452, "understand": 3.264257627039999, "sex": 1.86140042888, "direct": 0.200705689496, "import": 0.585636554132, "first": 0.030349159248639998, "leav": 0.507743957229, "busi": 0.720476170355, "have": 0.0443550070236, "while": 0.04324998379380001, "innov": 1.55614674111, "within": 0.21263272059799998, "learn": 4.213760323724999, "postsnumintroductiondeeplearningpythoncaff": 7.2746685411000005, "arxiv": 6.08904487545, "user": 2.04258810688}, "logidf": {"after": 0.020490694648099998, "fall": 0.527402167952, "dive": 2.7778937744700003, "plot": 1.68334240509, "pip": 4.91038987911, "wide": 0.44458000675399995, "new": 0.0177299468511, "assign": 1.3445959556, "space": 0.874713164972, "addit": 0.220218882972, "veri": 0.230159793238, "panda": 4.7167367562999996, "number": 0.0966085784186, "been": 0.023645982368400004, "etc": 1.4366730879700003, "dataset": 5.26584456664, "godfrey": 4.00613712579, "acknowledg": 1.5146203068000001, "unsupervis": 5.843922417409999, "work": 0.109034567273, "elsevierhealthcom": 7.2746685411000005, "path": 1.5351679838499999, "resourcesdicomimagelibrari": 7.2746685411000005, "name": 0.09723316638430002, "know": 0.952919694398, "opencvpython": 7.2746685411000005, "osirix": 7.2746685411000005, "wealth": 1.90614691588, "scale": 1.32095306328, "kaiser": 3.5995192798, "equal": 0.933027391343, "measur": 0.880014199726, "bound": 1.68776042417, "creat": 0.222576818514, "radiodens": 7.2746685411000005, "slicethick": 7.2746685411000005, "how": 0.47156695693000006, "repost": 6.83935046985, "origin": 0.128612437587, "websit": 0.924894023806, "distribut": 1.00781305813, "due": 0.21341214386399998, "end": 0.101476798618, "found": 0.107841124048, "spotlight": 3.45396369421, "driven": 1.72311939365, "deep": 1.2886734698, "will": 0.202786534915, "numxnum": 3.9522520373, "numdicominpythonimportingmedicalimagedataintonumpywithpydicomandvtk": 7.2746685411000005, "credit": 1.11288601088, "purpos": 0.803869037322, "somehow": 3.21265935953, "next": 0.402163685499, "below": 0.813626591936, "similar": 0.318556092114, "but": 0.0161923720719, "adaboost": 7.2746685411000005, "need": 0.362740163442, "our": 0.8576392141820001, "metadata": 5.35507570037, "pave": 2.50552593699, "dicom": 7.2746685411000005, "applic": 1.23160392849, "level": 0.503462189943, "various": 0.28692650007, "has": 0.0427239448548, "generat": 0.719182341736, "use": 0.0292080197316, "usual": 0.545279017064, "onlin": 0.957503854357, "out": 0.0584263909193, "model": 0.7374500731110001, "\u2013num": 7.2746685411000005, "format": 0.9287132518729999, "good": 0.418589404907, "mainstream": 2.0082171818, "under": 0.07526180538319999, "numer": 0.606093812346, "xml": 5.25372320611, "cartesian": 4.69583007148, "view": 0.49515994217299997, "someth": 1.18830712273, "arbitrari": 2.88021938643, "lienhart": 7.2746685411000005, "kaggl": 7.2746685411000005, "treatment": 1.3535776885100002, "solut": 1.55346297627, "axe": 3.05917959552, "jupyt": 7.2746685411000005, "shown": 1.01856958099, "constpixeldim": 7.2746685411000005, "then": 0.08303386523089999, "grey": 2.37006739018, "frontal": 3.5972177828099996, "instal": 1.3316305879, "cancer": 2.08070209901, "let": 1.2488025672799998, "world": 0.107420248621, "pixel": 4.45762805629, "blog": 2.65237310559, "section": 0.755387177948, "group": 0.190594534797, "filenam": 6.2068279111, "that": 0.00397614837964, "algorithm": 3.33044239518, "array": 2.31692271093, "offer": 0.431112446902, "softwar": 2.32849096333, "develop": 0.178624694913, "such": 0.059695977806, "sever": 0.06991112039689999, "scanner": 4.50777783998, "network": 0.9530830530519999, "python": 4.03065674296, "deriv": 1.02381618275, "lot": 1.4835969502500002, "some": 0.0395735090645, "sourc": 0.529218310751, "should": 0.509419876758, "permanent": 5.822416212189999, "step": 1.03954505698, "slice": 3.56554092616, "provid": 0.19517784432500002, "version": 0.697313064259, "varieti": 0.8316941898119999, "content": 1.26473915954, "becom": 0.11771217648900001, "protect": 0.67529290767, "set": 0.171496011289, "caus": 0.325858567406, "compar": 0.6239191809269999, "drone": 3.96878133925, "lstfilesdcm": 7.2746685411000005, "gentl": 2.84168957926, "workload": 4.31127164819, "visibl": 1.5445687581299998, "patient": 2.25618533471, "detail": 0.816187777173, "num": 0.00031499039539700004, "multipli": 3.01583728972, "correspond": 1.20141456099, "free": 0.5412666492670001, "acquir": 1.1332178178499999, "for": 0.00031499039539700004, "car": 1.2634013667, "predict": 1.6457402376899999, "output": 2.03822657827, "releas": 0.608521699544, "with": 0.00119749171339, "competit": 1.12154907401, "zero": 2.1692741832299998, "are": 0.0294674735827, "cvnum": 6.72812483474, "look": 0.6463866936, "insid": 1.00781305813, "conveni": 2.28795343073, "video": 1.19307248967, "futur": 0.619345197699, "numpi": 7.2746685411000005, "refer": 0.262553246798, "extract": 2.04161723301, "restor": 1.09033222631, "modal": 3.99239120489, "net": 1.9406330919499999, "column": 1.95699427938, "toward": 0.48877277716000006, "retinopathi": 7.2746685411000005, "haarcascad": 7.2746685411000005, "and": 6.29901420636e-05, "skimag": 7.2746685411000005, "shade": 2.8320172846099996, "give": 0.311392552224, "pixelspac": 7.2746685411000005, "part": 0.04239531098280001, "goal": 1.18830712273, "educ": 0.696807183384, "move": 0.255615859253, "lead": 0.23620402986699998, "opencv": 7.2746685411000005, "heart": 1.09974671874, "row": 1.71363732085, "analyz": 2.2707222351599996, "scientist": 1.54634128444, "notebook": 3.693678049, "into": 0.0149128632287, "paper": 0.979402539665, "medic": 1.18644857806, "lung": 2.9191258953099997, "dimens": 2.11092206831, "through": 0.0683586918849, "scan": 2.93441131931, "servic": 0.41410016674500005, "doctor": 1.37104215896, "last": 0.19204364461100001, "face": 0.589602371257, "hounsfield": 7.2746685411000005, "techniqu": 1.31624384807, "voxel": 7.2746685411000005, "cdatasciencebowlnum": 7.2746685411000005, "help": 0.336207721344, "main": 0.225571540588, "larg": 0.17037506060600002, "find": 0.547781330288, "line": 0.349430614452, "paperofweekviolanumrapidpdf": 7.2746685411000005, "librari": 0.986809980943, "relat": 0.21310030165399999, "squar": 1.18377009701, "water": 0.756862994946, "permiss": 1.8373800586400002, "post": 0.8057001527009999, "here": 0.8850381883700001, "report": 0.31001750903700004, "multipl": 1.01092401812, "black": 0.667404292867, "averag": 0.957011687995, "valu": 0.823193310148, "human": 0.640035183243, "stumpbas": 7.2746685411000005, "greater": 0.764545491118, "contain": 0.468845318236, "care": 0.9139943029109999, "about": 0.0628434774746, "communiti": 0.673561947791, "neural": 4.0853151555, "mind": 1.2786688388299998, "context": 1.44920491442, "rainer": 4.37925898918, "devic": 1.6110769470299997, "standard": 0.63741050982, "artist": 1.05317511017, "check": 1.87281049562, "scipi": 7.2746685411000005, "render": 1.7879873033099998, "save": 1.03598886547, "communic": 1.04365037288, "nor": 1.2083495472799999, "start": 0.236443369291, "articl": 0.702131739574, "load": 1.91765354188, "them": 0.0941833269093, "insight": 2.46841452187, "roy": 2.14962289583, "studi": 0.426470272221, "order": 0.22014038079300002, "digit": 1.48526454375, "health": 0.9990508682320001, "play": 0.38110439064199997, "appli": 0.8316941898119999, "the": 0.0, "age": 0.39624845300300005, "fun": 2.5561696698099996, "build": 0.491137452091, "analysi": 1.2466091029200002, "protocol": 2.6897010624299997, "display": 1.07655944206, "total": 0.43567888670500005, "adversari": 3.27064661718, "team": 0.821902894886, "gon": 3.6686767468, "there": 0.0400978929255, "command": 0.9809132407500001, "connect": 0.633605058682, "inform": 0.454453704662, "diabet": 3.6711489359400002, "diagnos": 2.7901263429100003, "semisupervis": 7.2746685411000005, "repres": 0.38507723275, "either": 0.459327638815, "visual": 1.6539383488600001, "follow": 0.045356911094199995, "now": 0.149092945021, "reader": 1.8622111301800002, "code": 1.35601909597, "support": 0.237880610037, "slope": 2.5464765406, "basic": 1.00436774895, "self": 2.48264164316, "unit": 0.143188061817, "goe": 1.4473284897999998, "fix": 1.48944573451, "also": 0.0146571578, "person": 0.34018281601800004, "modern": 0.426566764719, "style": 0.866289529121, "pydicom": 7.2746685411000005, "white": 0.625566240123, "two": 0.0136988443582, "research": 0.663727818138, "script": 2.1161358444599996, "area": 0.327954821122, "doc": 3.55007100439, "calibr": 3.6686767468, "rescal": 6.08904487545, "tag": 2.98296454472, "open": 0.219591038029, "hospit": 1.24223655551, "captur": 1.0578810012100002, "trunkdnumdnumtutorialpyroothtml": 7.2746685411000005, "generic": 2.8881067512700005, "howev": 0.0903151173475, "viewer": 2.54086530344, "share": 0.618760299747, "although": 0.139487981418, "attribut": 1.2283715153700001, "archiv": 1.20750637691, "calcul": 1.8131506592099997, "wasserstein": 6.89997509166, "numst": 0.966901335107, "which": 0.00517841384543, "upon": 0.47207177798199995, "thus": 0.49857627139300004, "other": 0.00987474791976, "introduct": 1.02276465794, "three": 0.06411868822490001, "procedur": 1.76970662262, "one": 0.0062553516455, "show": 0.236682766013, "workstat": 4.7973664907, "prefer": 1.10581884366, "status": 0.9016590696060001, "see": 0.240921585492, "surgeri": 2.50707033884, "box": 1.41751491115, "imag": 0.99376210729, "refd": 7.2746685411000005, "get": 0.579769005782, "still": 0.17112222142900002, "spectrum": 2.53687646687, "outsid": 0.515518738985, "control": 0.38498466158600003, "technolog": 0.956847686355, "file": 1.32734588723, "detect": 1.68878274493, "process": 0.527829199025, "neither": 1.29808692469, "recent": 0.434413741288, "cylindr": 3.8980122683599996, "kera": 6.72812483474, "frontier": 2.16817225474, "this": 0.0037864490525, "schedul": 1.2987792057799998, "sampl": 1.9786264883900002, "rang": 0.579319213803, "sir": 1.5151933727200002, "imagedata": 7.2746685411000005, "back": 0.23166743089699998, "chang": 0.166275625058, "zubal": 7.2746685411000005, "project": 0.561601885907, "differ": 0.212321121312, "healthcar": 2.9320444543, "decis": 0.7701082216959999, "weird": 3.7193204796199995, "between": 0.033953681165299995, "equip": 1.1772073171, "opencvorg": 7.2746685411000005, "java": 3.45396369421, "vision": 1.58523088743, "along": 0.260344385917, "all": 0.011402632097799998, "attenu": 4.5020798188599995, "store": 1.2374487335200002, "physic": 0.871847185184, "taposh": 7.2746685411000005, "intercept": 2.7618130259400004, "phantom": 3.3535957001599996, "exchang": 1.11788217808, "whose": 0.5510546556329999, "confirm": 1.12581182025, "more": 0.017024931599999998, "mpltoolkit": 7.2746685411000005, "list": 0.309845761506, "male": 1.24442043932, "these": 0.0715336194008, "made": 0.0680215260973, "retriev": 0.773925020223, "download": 2.6841506319, "detector": 3.8203613341300007, "can": 0.162341096394, "avail": 0.547454586289, "comput": 1.36806891594, "fundament": 1.67322086119, "search": 1.1798682540899998, "each": 0.173741689304, "constpixelspac": 7.2746685411000005, "few": 0.275577913653, "accur": 1.75248061485, "read": 0.83939268088, "packag": 2.0577584491900005, "transfer": 1.00264953547, "advanc": 0.6930212121780001, "freeli": 2.43048145465, "data": 1.2168205848, "method": 0.944461608841, "hasslefre": 7.2746685411000005, "from": 0.000567054168866, "favorit": 2.09390696331, "sinc": 0.0803681994577, "convolut": 4.61631800855, "bio": 3.7456377879300002, "matplotlib": 7.2746685411000005, "exampl": 0.40868267499899996, "scientif": 1.42377308021, "leader": 0.741672829452, "understand": 1.0880858756799998, "sex": 1.86140042888, "direct": 0.200705689496, "import": 0.292818277066, "first": 0.0075872898121599995, "leav": 0.507743957229, "busi": 0.720476170355, "have": 0.0147850023412, "while": 0.04324998379380001, "innov": 1.55614674111, "within": 0.21263272059799998, "learn": 0.842752064745, "postsnumintroductiondeeplearningpythoncaff": 7.2746685411000005, "arxiv": 6.08904487545, "user": 2.04258810688}, "freq": {"after": 2, "fall": 1, "dive": 1, "plot": 1, "pip": 2, "wide": 1, "new": 2, "assign": 2, "space": 2, "addit": 1, "veri": 2, "panda": 1, "number": 7, "been": 1, "etc": 1, "dataset": 7, "godfrey": 1, "acknowledg": 1, "unsupervis": 1, "work": 1, "elsevierhealthcom": 2, "path": 2, "resourcesdicomimagelibrari": 1, "name": 3, "know": 1, "opencvpython": 1, "osirix": 1, "wealth": 1, "scale": 3, "kaiser": 2, "equal": 1, "measur": 3, "bound": 2, "creat": 4, "radiodens": 1, "slicethick": 1, "how": 2, "repost": 1, "origin": 2, "websit": 1, "distribut": 2, "due": 1, "end": 1, "found": 1, "spotlight": 1, "driven": 1, "deep": 3, "will": 6, "numxnum": 1, "numdicominpythonimportingmedicalimagedataintonumpywithpydicomandvtk": 1, "credit": 2, "purpos": 1, "somehow": 1, "next": 5, "below": 2, "similar": 1, "but": 3, "adaboost": 1, "need": 2, "our": 1, "metadata": 2, "pave": 2, "dicom": 3, "applic": 3, "level": 1, "various": 1, "has": 3, "generat": 3, "use": 24, "usual": 1, "onlin": 2, "out": 1, "model": 1, "\u2013num": 1, "format": 8, "good": 3, "mainstream": 1, "under": 1, "numer": 1, "xml": 1, "cartesian": 1, "view": 3, "someth": 1, "arbitrari": 1, "lienhart": 1, "kaggl": 3, "treatment": 2, "solut": 1, "axe": 2, "jupyt": 6, "shown": 1, "constpixeldim": 1, "then": 2, "grey": 1, "frontal": 1, "instal": 7, "cancer": 3, "let": 4, "world": 1, "pixel": 5, "blog": 1, "section": 2, "group": 1, "filenam": 1, "that": 11, "algorithm": 1, "array": 2, "offer": 1, "softwar": 1, "develop": 1, "such": 7, "sever": 1, "scanner": 4, "network": 4, "python": 3, "deriv": 1, "lot": 1, "some": 6, "sourc": 2, "should": 1, "permanent": 2, "step": 3, "slice": 3, "provid": 2, "version": 1, "varieti": 2, "content": 1, "becom": 1, "protect": 1, "set": 1, "caus": 1, "compar": 1, "drone": 1, "lstfilesdcm": 1, "gentl": 1, "workload": 1, "visibl": 2, "patient": 3, "detail": 4, "num": 9, "multipli": 1, "correspond": 1, "free": 2, "acquir": 1, "for": 9, "car": 1, "predict": 1, "output": 1, "releas": 1, "with": 10, "competit": 2, "zero": 1, "are": 13, "cvnum": 1, "look": 1, "insid": 1, "conveni": 1, "video": 2, "futur": 1, "numpi": 2, "refer": 1, "extract": 1, "restor": 1, "modal": 1, "net": 2, "column": 1, "toward": 1, "retinopathi": 1, "haarcascad": 1, "and": 40, "skimag": 1, "shade": 1, "give": 1, "pixelspac": 1, "part": 2, "goal": 1, "educ": 1, "move": 1, "lead": 1, "opencv": 8, "heart": 1, "row": 1, "analyz": 3, "scientist": 1, "notebook": 8, "into": 5, "paper": 3, "medic": 12, "lung": 3, "dimens": 2, "through": 2, "scan": 3, "servic": 1, "doctor": 2, "last": 1, "face": 5, "hounsfield": 4, "techniqu": 1, "voxel": 1, "cdatasciencebowlnum": 1, "help": 2, "main": 1, "larg": 2, "find": 1, "line": 1, "paperofweekviolanumrapidpdf": 1, "librari": 3, "relat": 2, "squar": 1, "water": 2, "permiss": 1, "post": 1, "here": 5, "report": 1, "multipl": 1, "black": 1, "averag": 1, "valu": 6, "human": 4, "stumpbas": 1, "greater": 1, "contain": 1, "care": 1, "about": 1, "communiti": 2, "neural": 3, "mind": 1, "context": 1, "rainer": 1, "devic": 1, "standard": 4, "artist": 2, "check": 2, "scipi": 1, "render": 1, "save": 1, "communic": 4, "nor": 1, "start": 3, "articl": 3, "load": 4, "them": 4, "insight": 1, "roy": 2, "studi": 2, "order": 1, "digit": 1, "health": 1, "play": 1, "appli": 1, "the": 68, "age": 1, "fun": 1, "build": 1, "analysi": 1, "protocol": 4, "display": 3, "total": 1, "adversari": 2, "team": 1, "gon": 1, "there": 5, "command": 1, "connect": 1, "inform": 4, "diabet": 1, "diagnos": 1, "semisupervis": 1, "repres": 1, "either": 1, "visual": 1, "follow": 2, "now": 4, "reader": 1, "code": 1, "support": 3, "slope": 1, "basic": 7, "self": 1, "unit": 5, "goe": 1, "fix": 1, "also": 4, "person": 1, "modern": 1, "style": 2, "pydicom": 3, "white": 1, "two": 1, "research": 5, "script": 1, "area": 1, "doc": 1, "calibr": 1, "rescal": 1, "tag": 1, "open": 5, "hospit": 1, "captur": 1, "trunkdnumdnumtutorialpyroothtml": 2, "generic": 1, "howev": 1, "viewer": 1, "share": 2, "although": 1, "attribut": 1, "archiv": 1, "calcul": 2, "wasserstein": 1, "numst": 1, "which": 7, "upon": 1, "thus": 1, "other": 4, "introduct": 2, "three": 1, "procedur": 2, "one": 1, "show": 1, "workstat": 1, "prefer": 1, "status": 1, "see": 1, "surgeri": 1, "box": 1, "imag": 38, "refd": 1, "get": 3, "still": 1, "spectrum": 1, "outsid": 1, "control": 1, "technolog": 1, "file": 7, "detect": 5, "process": 6, "neither": 1, "recent": 1, "cylindr": 1, "kera": 2, "frontier": 1, "this": 12, "schedul": 1, "sampl": 1, "rang": 3, "sir": 1, "imagedata": 1, "back": 2, "chang": 1, "zubal": 1, "project": 1, "differ": 1, "healthcar": 1, "decis": 1, "weird": 1, "between": 2, "equip": 2, "opencvorg": 1, "java": 1, "vision": 1, "along": 1, "all": 4, "attenu": 3, "store": 3, "physic": 2, "taposh": 2, "intercept": 1, "phantom": 1, "exchang": 2, "whose": 1, "confirm": 1, "more": 2, "mpltoolkit": 1, "list": 2, "male": 1, "these": 4, "made": 1, "retriev": 1, "download": 1, "detector": 1, "can": 6, "avail": 2, "comput": 2, "fundament": 1, "search": 1, "each": 2, "constpixelspac": 1, "few": 1, "accur": 1, "read": 1, "packag": 5, "transfer": 1, "advanc": 1, "freeli": 3, "data": 11, "method": 1, "hasslefre": 1, "from": 4, "favorit": 1, "sinc": 1, "convolut": 2, "bio": 1, "matplotlib": 1, "exampl": 3, "scientif": 1, "leader": 1, "understand": 3, "sex": 1, "direct": 1, "import": 2, "first": 4, "leav": 1, "busi": 1, "have": 3, "while": 1, "innov": 1, "within": 1, "learn": 5, "postsnumintroductiondeeplearningpythoncaff": 1, "arxiv": 1, "user": 1}, "idf": {"after": 1.02070207021, "fall": 1.6945244956799999, "dive": 16.085106383, "plot": 5.383519837230001, "pip": 135.692307692, "wide": 1.5598349381, "new": 1.0178880554, "assign": 3.83663605607, "space": 2.39818731118, "addit": 1.24634950542, "veri": 1.25880114177, "panda": 111.802816901, "number": 1.10142916609, "been": 1.0239277652399998, "etc": 4.2066772655, "dataset": 193.609756098, "godfrey": 54.9342560554, "acknowledg": 4.5476940704699995, "unsupervis": 345.13043478300006, "work": 1.11520089913, "elsevierhealthcom": 1443.27272727, "path": 4.6421052631599995, "resourcesdicomimagelibrari": 1443.27272727, "name": 1.10211732037, "know": 2.59327017315, "opencvpython": 1443.27272727, "osirix": 1443.27272727, "wealth": 6.727118644069999, "scale": 3.7469907953699995, "kaiser": 36.5806451613, "equal": 2.542193755, "measur": 2.41093394077, "bound": 5.40735694823, "creat": 1.2492917847, "radiodens": 1443.27272727, "slicethick": 1443.27272727, "how": 1.60250328051, "repost": 933.882352941, "origin": 1.13724928367, "websit": 2.52160101652, "distribut": 2.7396031061299997, "due": 1.23789473684, "end": 1.10680423871, "found": 1.11387076405, "spotlight": 31.625498008, "driven": 5.601976005650001, "deep": 3.6279707495399998, "will": 1.22481098596, "numxnum": 52.0524590164, "numdicominpythonimportingmedicalimagedataintonumpywithpydicomandvtk": 1443.27272727, "credit": 3.04312823462, "purpos": 2.23416830847, "somehow": 24.8450704225, "next": 1.4950560316400001, "below": 2.25607503197, "similar": 1.37514075357, "but": 1.01632417899, "adaboost": 1443.27272727, "need": 1.4372623574099999, "our": 2.35758835759, "metadata": 211.68, "pave": 12.25, "dicom": 1443.27272727, "applic": 3.42672134686, "level": 1.6544393497299998, "various": 1.3323262839899999, "has": 1.0436497502, "generat": 2.05275407292, "use": 1.0296387573799999, "usual": 1.72508964468, "onlin": 2.6051854282900004, "out": 1.06016694491, "model": 2.0905978404, "\u2013num": 1443.27272727, "format": 2.53125, "good": 1.51981619759, "mainstream": 7.450023463160001, "under": 1.0781663837, "numer": 1.83325635104, "xml": 191.277108434, "cartesian": 109.48965517200001, "view": 1.6407606448899998, "someth": 3.28152128979, "arbitrari": 17.8181818182, "lienhart": 1443.27272727, "kaggl": 1443.27272727, "treatment": 3.87125091441, "solut": 4.7278141751, "axe": 21.3100671141, "jupyt": 1443.27272727, "shown": 2.76923076923, "constpixeldim": 1443.27272727, "then": 1.08657860516, "grey": 10.6981132075, "frontal": 36.4965517241, "instal": 3.78721374046, "cancer": 8.01009081736, "let": 3.48616600791, "world": 1.11340206186, "pixel": 86.28260869569999, "blog": 14.1876675603, "section": 2.1284354471099998, "group": 1.20996875238, "filenam": 496.125, "that": 1.00398406375, "algorithm": 27.9507042254, "array": 10.1444089457, "offer": 1.53896859248, "softwar": 10.2624434389, "develop": 1.1955719557200002, "such": 1.06151377374, "sever": 1.07241286139, "scanner": 90.72, "network": 2.59369384088, "python": 56.2978723404, "deriv": 2.78379800105, "lot": 4.40877534018, "some": 1.04036697248, "sourc": 1.69760479042, "should": 1.6643254009900001, "permanent": 337.787234043, "step": 2.8279301745599996, "slice": 35.3585746102, "provid": 1.21552714187, "version": 2.0083491461099996, "varieti": 2.2972073506, "content": 3.5421686747, "becom": 1.12492028626, "protect": 1.96460834055, "set": 1.18707940781, "caus": 1.38521943984, "compar": 1.8662278123900002, "drone": 52.92, "lstfilesdcm": 1443.27272727, "gentl": 17.1447084233, "workload": 74.5352112676, "visibl": 4.68595041322, "patient": 9.54660252556, "detail": 2.26186066391, "num": 1.00031504001, "multipli": 20.4061696658, "correspond": 3.32481675393, "free": 1.71818181818, "acquir": 3.10563380282, "for": 1.00031504001, "car": 3.53743315508, "predict": 5.18484650555, "output": 7.676982591880001, "releas": 1.8377126982299998, "with": 1.0011982089899998, "competit": 3.06960556845, "zero": 8.75192943771, "are": 1.02990593578, "cvnum": 835.5789473680001, "look": 1.9086318826599997, "insid": 2.7396031061299997, "conveni": 9.85474860335, "video": 3.29719626168, "futur": 1.8577112099200002, "numpi": 1443.27272727, "refer": 1.30024570025, "extract": 7.703056768560001, "restor": 2.97526236882, "modal": 54.1843003413, "net": 6.96315789474, "column": 7.078020508250001, "toward": 1.6303142329, "retinopathi": 1443.27272727, "haarcascad": 1443.27272727, "and": 1.00006299213, "skimag": 1443.27272727, "shade": 16.979679144400002, "give": 1.3653250774, "pixelspac": 1443.27272727, "part": 1.04330682789, "goal": 3.28152128979, "educ": 2.00733341763, "move": 1.29125660838, "lead": 1.2664326739, "opencv": 1443.27272727, "heart": 3.00340522134, "row": 5.549108703250001, "analyz": 9.68639414277, "scientist": 4.69426374926, "notebook": 40.1924050633, "into": 1.01502461479, "paper": 2.6628648104700003, "medic": 3.27542809986, "lung": 18.5250875146, "dimens": 8.25585023401, "through": 1.07074930869, "scan": 18.8104265403, "servic": 1.51300867245, "doctor": 3.9394540942900003, "last": 1.2117234010100002, "face": 1.80327124035, "hounsfield": 1443.27272727, "techniqu": 3.7293868921800004, "voxel": 1443.27272727, "cdatasciencebowlnum": 1443.27272727, "help": 1.39962972759, "main": 1.25303867403, "larg": 1.18574949585, "find": 1.7294117647099998, "line": 1.4182597820299998, "paperofweekviolanumrapidpdf": 1443.27272727, "librari": 2.68266306185, "relat": 1.23750876919, "squar": 3.26666666667, "water": 2.13157894737, "permiss": 6.280063291139999, "post": 2.23826307627, "here": 2.42307692308, "report": 1.3634489866, "multipl": 2.74813917258, "black": 1.94917127072, "averag": 2.60390355913, "valu": 2.2777618364400003, "human": 1.8965476048299998, "stumpbas": 1443.27272727, "greater": 2.14801785956, "contain": 1.59814777532, "care": 2.49426551453, "about": 1.06486015159, "communiti": 1.96121062384, "neural": 59.4606741573, "mind": 3.5918552036199998, "context": 4.25972632144, "rainer": 79.7788944724, "devic": 5.00820189274, "standard": 1.8915763135900003, "artist": 2.86673889491, "check": 6.50655737705, "scipi": 1443.27272727, "render": 5.97740963855, "save": 2.8178913737999998, "communic": 2.8395635843299996, "nor": 3.3479544496, "start": 1.26673581744, "articl": 2.01805008262, "load": 6.80497213888, "them": 1.09876115994, "insight": 11.8037174721, "roy": 8.58162162162, "studi": 1.53184098804, "order": 1.24625166811, "digit": 4.416133518780001, "health": 2.71570304482, "play": 1.46390041494, "appli": 2.2972073506, "the": 1.0, "age": 1.48623853211, "fun": 12.8863636364, "build": 1.6341739578, "analysi": 3.47852760736, "protocol": 14.7272727273, "display": 2.93456561922, "total": 1.5460122699399999, "adversari": 26.328358209, "team": 2.2748244734200003, "gon": 39.2, "there": 1.04091266719, "command": 2.66689064337, "connect": 1.8843916913900003, "inform": 1.5753125620200001, "diabet": 39.297029703, "diagnos": 16.283076923099998, "semisupervis": 1443.27272727, "repres": 1.46972782818, "either": 1.5830092731099998, "visual": 5.22752716497, "follow": 1.04640126549, "now": 1.160780873, "reader": 6.437956204380001, "code": 3.8807137619199996, "support": 1.2685577307200002, "slope": 12.7620578778, "basic": 2.7301805675, "self": 11.972850678699999, "unit": 1.15394679459, "goe": 4.251740760580001, "fix": 4.4346368715099995, "also": 1.01476510067, "person": 1.40520446097, "modern": 1.5319888063299998, "style": 2.37807070102, "pydicom": 1443.27272727, "white": 1.86930413282, "two": 1.01379310345, "research": 1.9420183486200002, "script": 8.299006795610001, "area": 1.3881262568900001, "doc": 34.8157894737, "calibr": 39.2, "rescal": 441.0, "tag": 19.7462686567, "open": 1.24556723678, "hospit": 3.4633507853400003, "captur": 2.88026124819, "trunkdnumdnumtutorialpyroothtml": 1443.27272727, "generic": 17.9592760181, "howev": 1.0945191313299998, "viewer": 12.690647482000001, "share": 1.8566249561500001, "although": 1.14968498805, "attribut": 3.4156626506, "archiv": 3.3451327433599998, "calcul": 6.12972972973, "wasserstein": 992.25, "numst": 2.6297830047999997, "which": 1.005191845, "upon": 1.60331246213, "thus": 1.6463756092500001, "other": 1.00992366412, "introduct": 2.7808723068799996, "three": 1.06621893889, "procedur": 5.8691312384500005, "one": 1.00627495722, "show": 1.26703910615, "workstat": 121.190839695, "prefer": 3.0216977540900003, "status": 2.4636871508400002, "see": 1.27242125511, "surgeri": 12.2689335394, "box": 4.12685209254, "imag": 2.70137825421, "refd": 1443.27272727, "get": 1.78562591385, "still": 1.1866357724799999, "spectrum": 12.6401273885, "outsid": 1.67450690855, "control": 1.46959178006, "technolog": 2.6034765496900003, "file": 3.7710213776699995, "detect": 5.41288782816, "process": 1.69524826482, "neither": 3.6622837370199997, "recent": 1.54405757635, "cylindr": 49.3043478261, "kera": 835.5789473680001, "frontier": 8.7422907489, "this": 1.00379362671, "schedul": 3.6648199445999996, "sampl": 7.23280182232, "rang": 1.7848229342299997, "sir": 4.55030094583, "imagedata": 1443.27272727, "back": 1.26070038911, "chang": 1.1808985421, "zubal": 1443.27272727, "project": 1.7534791252500002, "differ": 1.23654490225, "healthcar": 18.7659574468, "decis": 2.16, "weird": 41.2363636364, "between": 1.03453668708, "equip": 3.24529844644, "opencvorg": 1443.27272727, "java": 31.625498008, "vision": 4.88041807562, "along": 1.2973768080399999, "all": 1.01146788991, "attenu": 90.2045454545, "store": 3.44680851064, "physic": 2.39132399458, "taposh": 1443.27272727, "intercept": 15.8285144566, "phantom": 28.6054054054, "exchang": 3.05837025621, "whose": 1.73508196721, "confirm": 3.0827184466000004, "more": 1.0171706817, "mpltoolkit": 1443.27272727, "list": 1.36321483771, "male": 3.4709226060300002, "these": 1.07415426252, "made": 1.07038834951, "retriev": 2.16826003824, "download": 14.6457564576, "detector": 45.6206896552, "can": 1.17626139142, "avail": 1.7288467821, "comput": 3.9277585353800006, "fundament": 5.32930513595, "search": 3.2539454806299997, "each": 1.18974820144, "constpixelspac": 1443.27272727, "few": 1.31729173581, "accur": 5.768895348840001, "read": 2.3149606299200003, "packag": 7.828402366860001, "transfer": 2.72549356223, "advanc": 1.9997480791, "freeli": 11.3643521832, "data": 3.37643555934, "method": 2.5714285714300003, "hasslefre": 1443.27272727, "from": 1.00056721497, "favorit": 8.116564417180001, "sinc": 1.08368600683, "convolut": 101.121019108, "bio": 42.336000000000006, "matplotlib": 1443.27272727, "exampl": 1.50483412322, "scientif": 4.15275961287, "leader": 2.0994445913799997, "understand": 2.96858638743, "sex": 6.432739059969999, "direct": 1.22226499346, "import": 1.3401992233700002, "first": 1.00761614623, "leav": 1.6615384615399997, "busi": 2.05541170378, "have": 1.0148948411399998, "while": 1.0441988950299999, "innov": 4.74051955808, "within": 1.2369302688, "learn": 2.32275054865, "postsnumintroductiondeeplearningpythoncaff": 1443.27272727, "arxiv": 441.0, "user": 7.71053909665}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Medical Image Analysis with Deep Learning\u200a</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/03/medical-image-analysis-deep-learning.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Medical Image Analysis with Deep Learning\u200a Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/04/ie-madrid-big-data-cybersecurity.html\" rel=\"prev\" title=\"Are You a Big Data Genius or a Cybersecurity Hero?\"/>\n<link href=\"https://www.kdnuggets.com/2017/04/avoid-analytic-rabbit-holes-investigation-loops.html\" rel=\"next\" title=\"How to stay out of analytic rabbit holes: avoiding investigation loops and their traps\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2017/03/medical-image-analysis-deep-learning.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=64690\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2017/03/medical-image-analysis-deep-learning.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-64690 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 6-Apr, 2017  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/03/index.html\">Mar</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/03/tutorials.html\">Tutorials, Overviews</a> \u00bb Medical Image Analysis with Deep Learning\u200a (\u00a0<a href=\"/2017/n14.html\">17:n14</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Medical Image Analysis with Deep Learning\u200a</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/04/ie-madrid-big-data-cybersecurity.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/04/avoid-analytic-rabbit-holes-investigation-loops.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <span class=\"http-likes\" style=\"float: left; font-size:14px\">http likes 637</span> <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/image-recognition\" rel=\"tag\">Image Recognition</a>, <a href=\"https://www.kdnuggets.com/tag/medical\" rel=\"tag\">Medical</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a></div>\n<br/>\n<p class=\"excerpt\">\n     In this article, I start with basics of image processing, basics of medical image format data and visualize some medical data.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><b>By Taposh Roy, Kaiser Permanente.</b></p>\n<p><img alt=\"\" class=\"aligncenter\" src=\"/wp-content/uploads/medical-image-analysis-header.jpeg\" width=\"85%\"/></p>\n<p>Analyzing images and videos, and using them in various applications such as self driven cars, drones etc. with underlying deep learning techniques has been the new research frontier. The recent research papers such as \u201c<a href=\"https://arxiv.org/abs/1508.06576\" target=\"_blank\">A Neural Algorithm of Artistic Style</a>\u201d, show how a styles can be transferred from an artist and applied to an image, to create a new image. Other papers such as \u201c<a href=\"https://arxiv.org/abs/1406.2661\" target=\"_blank\">Generative Adversarial Networks</a>\u201d (GAN) and \u201c<a href=\"https://arxiv.org/pdf/1701.07875\" target=\"_blank\">Wasserstein GAN</a>\u201d have paved the path to develop models that can learn to create data that is similar to data that we give them. Thus opening up the world to semi-supervised learning and paving the path to a future of unsupervised learning.</p>\n<p>While these research areas are still on the generic images, our goal is to use these research into medical images to help healthcare. We need to start with some basics. In this article, I start with basics of image processing, basics of medical image format data and visualize some medical data. In the next article I will deep dive into some convolutional neural nets and use them with <a href=\"https://keras.io/\" target=\"_blank\">Keras</a> for predicting lung cancer.</p>\n<h3><strong>Basic Image Processing (using\u00a0python)</strong></h3>\n<p>\u00a0<br>\nThere are a variety of image processing libraries, however <a href=\"http://opencv.org/\" target=\"_blank\">OpenCV</a> (open computer vision) has become mainstream due to its large community support and availability in C++, java and python. I prefer using opencv using jupyter notebook.</br></p>\n<p>Install OpenCV using: <strong><em>pip install opencv-python</em></strong> or install directly from the source from opencv.org</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*8gb6h21kw1xpV5YjeBsLoA.png\" width=\"99%\"/><br>\n<font size=\"-1\">Installing opencv.</font></br></center></p>\n<p>Now open your Jupyter notebook and confirm you can import cv2. You will also need numpy and matplotlib to view your plots inside the notebook.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*tW_9HZ6IQrLTbnPA3t-_CA.png\" width=\"99%\"/></p>\n<p>Now, lets check if you can open an image and view it on your notebook using the code below.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*xRLhjAAqu6VFOV-SRtlOHQ.png\" width=\"99%\"/><br>\n<font size=\"-1\">Example image load through\u00a0OpenCV.</font></br></center></p>\n<p><strong>Basic Face Detection</strong></p>\n<p>Lets, do something fun such as detecting a face. To detect face we will use an open source xml stump-based 20x20 gentle adaboost frontal face detector originally created by Rainer Lienhart. A good post with details on Haar-cascade detection is <a href=\"http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html\" target=\"_blank\">here</a>.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*-Fu-eV90aMfkqTIOgoKHYQ.png\" width=\"99%\"/><br>\n<font size=\"-1\">Face detection using\u00a0OpenCV.</font></br></center></p>\n<p>There are a lot of examples for image processing using opencv in the docs section. <a href=\"http://docs.opencv.org/trunk/d6/d00/tutorial_py_root.html\" target=\"_blank\">http://docs.opencv.org/trunk/d6/d00/tutorial_py_root.html</a>. I leave it up to the reader to play with more examples. Now that we know the basics of image processing, lets move to the next level of understanding medical image format.</p>\n<h3><strong>Medical Image Data\u00a0Format</strong></h3>\n<p>\u00a0<br/>\nMedical images follow <a href=\"http://en.wikipedia.org/wiki/DICOM\" target=\"_blank\">Digital Imaging and Communications (DICOM) </a>as a standard solution for storing and exchanging medical image-data. The first version of this standard was released in 1985. Since then there are several changes made. This standard uses a file format and a communications protocol.</p>\n<ul>\n<li><strong>File Format</strong>\u200a\u2014\u200aAll patient medical images are saved in the DICOM file format. This format has <a href=\"https://en.wikipedia.org/wiki/Protected_health_information\" target=\"_blank\">PHI (protected health information)</a> about the patient such as\u200a\u2014\u200aname, sex, age in addition to other image related data such as equipment used to capture the image and some context to the medical treatment. Medical Imaging Equipments create DICOM files. Doctors use DICOM Viewers, computer software applications that can display DICOM images, read and to diagnose the findings in the images.\n<li><strong>Communications Protocol</strong>\u200a\u2014\u200aThe DICOM communication protocol is used to search for imaging studies in the archive and restore imaging studies to the workstation in order to display it. All medical imaging applications that are connected to the hospital network use the DICOM protocol to exchange information, mainly DICOM images but also patient and procedure information. There are also more advanced network commands that are used to control and follow the treatment, schedule procedures, report statuses and share the workload between doctors and imaging devices.\n</li></li></ul>\n<p>A very good blog that goes into details of the DICOM standard is <a href=\"http://dicomiseasy.blogspot.com/\" target=\"_blank\">here</a></p>\n<h3><strong>Analyze DICOM\u00a0Images</strong></h3>\n<p>\u00a0<br/>\nA very good python package used for analyzing DICOM images is pydicom. In this section, we will see how to render a DICOM image on a Jupyter notebook.</p>\n<p>Install OpenCV using: <strong><em>pip install pydicom</em></strong></p>\n<p>After you install pydicom package, go back to the jupyter notebook. In the notebook, import the dicom package and other packages as shown below.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*i53SlB3laMgXl5WqJZ-rJA.png\" width=\"99%\"/></p>\n<p>We also use other packages such as pandas, scipy, skimage, mpl_toolkit for data processing and analysis.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*dxp_yfg6cqS4ThlcEPRjQw.png\" width=\"99%\"/></p>\n<p>There\u2019s a wealth of freely available DICOM datasets online but here\u2019s a few that should help you get started:</p>\n<ul>\n<li><a href=\"http://www.kaggle.com/\" target=\"_blank\">Kaggle Competitions and Datasets</a>: This is my personal favorite. Check out the data for <a href=\"https://www.kaggle.com/c/data-science-bowl-2017/data\" target=\"_blank\">lung cancer competition</a> and diabetes retinopathy.\n<li><a href=\"http://www.dicomlibrary.com/\" target=\"_blank\">Dicom Library</a>\u00a0: DICOM Library is a free online medical DICOM image or video file sharing service for educational and scientific purposes.\n<li><a href=\"http://www.osirix-viewer.com/datasets/\" target=\"_blank\">Osirix Datasets</a>: Provides a large range of human datasets acquired through a variety of imaging modalities.\n<li><a href=\"https://mri.radiology.uiowa.edu/visible_human_datasets.html\" target=\"_blank\">Visible Human Datasets</a>: Parts of the <a href=\"http://www.nlm.nih.gov/research/visible/visible_human.html\" target=\"_blank\">Visible Human project</a> are somehow freely distributed here which is weird cause <a href=\"http://www.nlm.nih.gov/research/visible/getting_data.html\" target=\"_blank\">getting that data</a> is neither free nor hassle-free.\n<li><a href=\"http://noodle.med.yale.edu/zubal/\" target=\"_blank\">The Zubal Phantom</a>: This website offers multiple datasets of two human males in CT and MRI which are freely distributed.\n</li></li></li></li></li></ul>\n<p>Download the dicom files and load them on your jupyter notebook.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*7qcA2uS6F7d6DhJ7aDdM3A.png\" width=\"99%\"/></p>\n<p>Now, load the DICOM images into a list.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*cmCmmsXOY_JI5j68yh7knA.png\" width=\"99%\"/><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/1600/1*cmCmmsXOY_JI5j68yh7knA.png\" width=\"99%\"/></p>\n<p><strong>Step 1\u00a0: Basic Viewing of DICOM Image in Jupyter</strong></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*Xo7zQWzK_slqM4xu0Yy95A.png\" width=\"99%\"/></p>\n<p>In the first line we load the 1st DICOM file, which we\u2019re gonna use as a reference named <code>RefDs</code>, to extract metadata and whose filename is first in the <code>lstFilesDCM</code> list.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*C9gmJeO1sTYYkpy7AO1b8Q.png\" width=\"99%\"/></p>\n<p>We then calculate the total dimensions of the 3D NumPy array which are equal to (Number of pixel rows in a slice) x (Number of pixel columns in a slice) x (Number of slices) along the x, y, and z cartesian axes. Lastly, we use the <code>PixelSpacing</code> and <code>SliceThickness</code> attributes to calculate the spacing between pixels in the three axes. We store the array dimensions in <code>ConstPixelDims</code> and the spacing in <code>ConstPixelSpacing [1].</code></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*y0cDTlZL_0cnkM3CrcMZ8A.png\" width=\"99%\"/></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*y2fdN31UKpKTW59kBiER1g.png\" width=\"99%\"/></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*LsJa3-_iotHkAO_uHhswJg.png\" width=\"99%\"/></p>\n<p><strong>Step 2: Looking into details of DICOM format</strong></p>\n<p>The unit of measurement in CT scans is the <strong>Hounsfield Unit (HU)</strong>, which is a measure of radiodensity. CT scanners are carefully calibrated to accurately measure this. A detailed understanding on this can be found <a href=\"https://web.archive.org/web/20070926231241/http://www.intl.elsevierhealth.com/e-books/pdf/940.pdf\" target=\"_blank\">here</a>.</p>\n<p>Each pixel is assigned a numerical value (CT number), which is the average of all the attenuation values contained within the corresponding voxel. This number is compared to the attenuation value of water and displayed on a scale of arbitrary units named Hounsfield units (HU) after Sir Godfrey Hounsfield.</p>\n<p>This scale assigns water as an attenuation value (HU) of zero. The range of CT numbers is <strong>2000 HU </strong>wide although some modern scanners have a greater range of HU up to 4000. Each number represents a shade of grey with +1000 (white) and \u20131000 (black) at either end of the spectrum.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*HNn99mQnjnkgmbolXYZNVg.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Hounsfield Scale [credits: <a href=\"https://web.archive.org/web/20070926231241/http://www.intl.elsevierhealth.com/e-books/pdf/940.pdf\" target=\"_blank\">\u201cIntroduction to CT physics\u201d</a> (PDF). elsevierhealth.com.]</font></center></p>\n<p>Some scanners have cylindrical scanning bounds, but the output image is square. The pixels that fall outside of these bounds get the fixed value <strong>-2000</strong>.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*KeJXyh0vPFOj_DxKBc1prA.jpeg\" width=\"99%\"/><br/>\n<font size=\"-1\">CT Scanner Image [credits\u00a0: <a href=\"https://web.archive.org/web/20070926231241/http://www.intl.elsevierhealth.com/e-books/pdf/940.pdf\" target=\"_blank\">\u201cIntroduction to CT physics\u201d</a> (PDF). elsevierhealth.com.]</font></center></p>\n<p>The first step usually is setting these values to 0. Next, let\u2019s go back to HU units, by multiplying with the rescale slope and adding the intercept (which are conveniently stored in the metadata of the scans!).</p>\n<p>In the next part, we will use Kaggle\u2019s lung cancer data-set and Convolution Neural Nets using Keras. We will build upon the information provided by this article to go to the next one.</p>\n<p>Acknowledgements:</p>\n<ol>\n<li><a href=\"https://pyscience.wordpress.com/2014/09/08/dicom-in-python-importing-medical-image-data-into-numpy-with-pydicom-and-vtk/\" target=\"_blank\">https://pyscience.wordpress.com/2014/09/08/dicom-in-python-importing-medical-image-data-into-numpy-with-pydicom-and-vtk/</a>\n<li><a href=\"http://www.osirix-viewer.com/resources/dicom-image-library/\" target=\"_blank\">http://www.osirix-viewer.com/resources/dicom-image-library/</a>\n<li><a href=\"http://wearables.cc.gatech.edu/paper_of_week/viola01rapid.pdf\" target=\"_blank\">http://wearables.cc.gatech.edu/paper_of_week/viola01rapid.pdf</a>\n<li><a href=\"http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/\" target=\"_blank\">http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/</a>\n<li><a href=\"http://dicomiseasy.blogspot.com/2012/08/chapter-12-pixel-data.html\" target=\"_blank\">http://dicomiseasy.blogspot.com/</a>\n<li><a href=\"https://www.kaggle.com/c/data-science-bowl-2017\" target=\"_blank\">https://www.kaggle.com/c/data-science-bowl-2017</a>\n<li><a href=\"http://docs.opencv.org/trunk/d6/d00/tutorial_py_root.html\" target=\"_blank\">http://docs.opencv.org/trunk/d6/d00/tutorial_py_root.html</a>\n<li>Kaggle community for all the different scripts and support\n</li></li></li></li></li></li></li></li></ol>\n<p><b>Bio: <a href=\"https://www.linkedin.com/in/taposh/\" target=\"_blank\">Taposh Roy</a></b> leads innovation team in Kaiser Permanente's Decision Support group. He works with research, technology and business leaders to derive insights from data.</p>\n<p><a href=\"https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-i-23d518abf531#.axcbxvzex\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/03/deep-learning-gans-boxing-fundamental-understanding.html\">Deep Learning, Generative Adversarial Networks\u200a &amp; Boxing \u2013 Toward a Fundamental Understanding</a>\n<li><a href=\"/2015/06/data-scientist-heart-surgery.html\">I\u2019m a data scientist \u2013 mind if I do surgery on your heart?</a>\n<li><a href=\"/2017/01/arxiv-spotlight-sampled-image-tagging-retrieval-methods-user-generated-content.html\">arXiv Paper Spotlight: Sampled Image Tagging and Retrieval Methods on User Generated Content</a>\n</li></li></li></ul>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/04/ie-madrid-big-data-cybersecurity.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/04/avoid-analytic-rabbit-holes-investigation-loops.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/03/index.html\">Mar</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/03/tutorials.html\">Tutorials, Overviews</a> \u00bb Medical Image Analysis with Deep Learning\u200a (\u00a0<a href=\"/2017/n14.html\">17:n14</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556326523\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.808 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 20:55:23 -->\n<!-- Compression = gzip -->", "content_tokenized": ["taposh", "roy", "kaiser", "permanent", "analyz", "imag", "and", "video", "and", "use", "them", "various", "applic", "such", "self", "driven", "car", "drone", "etc", "with", "under", "deep", "learn", "techniqu", "has", "been", "the", "new", "research", "frontier", "the", "recent", "research", "paper", "such", "neural", "algorithm", "artist", "style", "show", "how", "style", "can", "transfer", "from", "artist", "and", "appli", "imag", "creat", "new", "imag", "other", "paper", "such", "generat", "adversari", "network", "and", "wasserstein", "have", "pave", "the", "path", "develop", "model", "that", "can", "learn", "creat", "data", "that", "similar", "data", "that", "give", "them", "thus", "open", "the", "world", "semisupervis", "learn", "and", "pave", "the", "path", "futur", "unsupervis", "learn", "while", "these", "research", "area", "are", "still", "the", "generic", "imag", "our", "goal", "use", "these", "research", "into", "medic", "imag", "help", "healthcar", "need", "start", "with", "some", "basic", "this", "articl", "start", "with", "basic", "imag", "process", "basic", "medic", "imag", "format", "data", "and", "visual", "some", "medic", "data", "the", "next", "articl", "will", "deep", "dive", "into", "some", "convolut", "neural", "net", "and", "use", "them", "with", "kera", "for", "predict", "lung", "cancer", "basic", "imag", "process", "use", "python", "there", "are", "varieti", "imag", "process", "librari", "howev", "opencv", "open", "comput", "vision", "has", "becom", "mainstream", "due", "larg", "communiti", "support", "and", "avail", "java", "and", "python", "prefer", "use", "opencv", "use", "jupyt", "notebook", "instal", "opencv", "use", "pip", "instal", "opencvpython", "instal", "direct", "from", "the", "sourc", "from", "opencvorg", "instal", "opencv", "now", "open", "jupyt", "notebook", "and", "confirm", "can", "import", "cvnum", "will", "also", "need", "numpi", "and", "matplotlib", "view", "plot", "insid", "the", "notebook", "now", "let", "check", "can", "open", "imag", "and", "view", "notebook", "use", "the", "code", "below", "exampl", "imag", "load", "through", "opencv", "basic", "face", "detect", "let", "someth", "fun", "such", "detect", "face", "detect", "face", "will", "use", "open", "sourc", "xml", "stumpbas", "numxnum", "gentl", "adaboost", "frontal", "face", "detector", "origin", "creat", "rainer", "lienhart", "good", "post", "with", "detail", "haarcascad", "detect", "here", "face", "detect", "use", "opencv", "there", "are", "lot", "exampl", "for", "imag", "process", "use", "opencv", "the", "doc", "section", "trunkdnumdnumtutorialpyroothtml", "leav", "the", "reader", "play", "with", "more", "exampl", "now", "that", "know", "the", "basic", "imag", "process", "let", "move", "the", "next", "level", "understand", "medic", "imag", "format", "medic", "imag", "data", "format", "medic", "imag", "follow", "digit", "imag", "and", "communic", "standard", "solut", "for", "store", "and", "exchang", "medic", "imagedata", "the", "first", "version", "this", "standard", "releas", "num", "sinc", "then", "there", "are", "sever", "chang", "made", "this", "standard", "use", "file", "format", "and", "communic", "protocol", "file", "format", "all", "patient", "medic", "imag", "are", "save", "the", "file", "format", "this", "format", "has", "protect", "health", "inform", "about", "the", "patient", "such", "name", "sex", "age", "addit", "other", "imag", "relat", "data", "such", "equip", "use", "captur", "the", "imag", "and", "some", "context", "the", "medic", "treatment", "medic", "imag", "equip", "creat", "file", "doctor", "use", "viewer", "comput", "softwar", "applic", "that", "can", "display", "imag", "read", "and", "diagnos", "the", "find", "the", "imag", "communic", "protocol", "the", "communic", "protocol", "use", "search", "for", "imag", "studi", "the", "archiv", "and", "restor", "imag", "studi", "the", "workstat", "order", "display", "all", "medic", "imag", "applic", "that", "are", "connect", "the", "hospit", "network", "use", "the", "protocol", "exchang", "inform", "main", "imag", "but", "also", "patient", "and", "procedur", "inform", "there", "are", "also", "more", "advanc", "network", "command", "that", "are", "use", "control", "and", "follow", "the", "treatment", "schedul", "procedur", "report", "status", "and", "share", "the", "workload", "between", "doctor", "and", "imag", "devic", "veri", "good", "blog", "that", "goe", "into", "detail", "the", "standard", "here", "analyz", "imag", "veri", "good", "python", "packag", "use", "for", "analyz", "imag", "pydicom", "this", "section", "will", "see", "how", "render", "imag", "jupyt", "notebook", "instal", "opencv", "use", "pip", "instal", "pydicom", "after", "instal", "pydicom", "packag", "back", "the", "jupyt", "notebook", "the", "notebook", "import", "the", "dicom", "packag", "and", "other", "packag", "shown", "below", "also", "use", "other", "packag", "such", "panda", "scipi", "skimag", "mpltoolkit", "for", "data", "process", "and", "analysi", "there", "wealth", "freeli", "avail", "dataset", "onlin", "but", "here", "few", "that", "should", "help", "get", "start", "kaggl", "competit", "and", "dataset", "this", "person", "favorit", "check", "out", "the", "data", "for", "lung", "cancer", "competit", "and", "diabet", "retinopathi", "dicom", "librari", "librari", "free", "onlin", "medic", "imag", "video", "file", "share", "servic", "for", "educ", "and", "scientif", "purpos", "osirix", "dataset", "provid", "larg", "rang", "human", "dataset", "acquir", "through", "varieti", "imag", "modal", "visibl", "human", "dataset", "part", "the", "visibl", "human", "project", "are", "somehow", "freeli", "distribut", "here", "which", "weird", "caus", "get", "that", "data", "neither", "free", "nor", "hasslefre", "the", "zubal", "phantom", "this", "websit", "offer", "multipl", "dataset", "two", "human", "male", "and", "which", "are", "freeli", "distribut", "download", "the", "dicom", "file", "and", "load", "them", "jupyt", "notebook", "now", "load", "the", "imag", "into", "list", "step", "num", "basic", "view", "imag", "jupyt", "the", "first", "line", "load", "the", "numst", "file", "which", "gon", "use", "refer", "name", "refd", "extract", "metadata", "and", "whose", "filenam", "first", "the", "lstfilesdcm", "list", "then", "calcul", "the", "total", "dimens", "the", "numpi", "array", "which", "are", "equal", "number", "pixel", "row", "slice", "number", "pixel", "column", "slice", "number", "slice", "along", "the", "and", "cartesian", "axe", "last", "use", "the", "pixelspac", "and", "slicethick", "attribut", "calcul", "the", "space", "between", "pixel", "the", "three", "axe", "store", "the", "array", "dimens", "constpixeldim", "and", "the", "space", "constpixelspac", "num", "step", "num", "look", "into", "detail", "format", "the", "unit", "measur", "scan", "the", "hounsfield", "unit", "which", "measur", "radiodens", "scanner", "are", "care", "calibr", "accur", "measur", "this", "detail", "understand", "this", "can", "found", "here", "each", "pixel", "assign", "numer", "valu", "number", "which", "the", "averag", "all", "the", "attenu", "valu", "contain", "within", "the", "correspond", "voxel", "this", "number", "compar", "the", "attenu", "valu", "water", "and", "display", "scale", "arbitrari", "unit", "name", "hounsfield", "unit", "after", "sir", "godfrey", "hounsfield", "this", "scale", "assign", "water", "attenu", "valu", "zero", "the", "rang", "number", "num", "wide", "although", "some", "modern", "scanner", "have", "greater", "rang", "num", "each", "number", "repres", "shade", "grey", "with", "num", "white", "and", "\u2013num", "black", "either", "end", "the", "spectrum", "hounsfield", "scale", "credit", "introduct", "physic", "elsevierhealthcom", "some", "scanner", "have", "cylindr", "scan", "bound", "but", "the", "output", "imag", "squar", "the", "pixel", "that", "fall", "outsid", "these", "bound", "get", "the", "fix", "valu", "num", "scanner", "imag", "credit", "introduct", "physic", "elsevierhealthcom", "the", "first", "step", "usual", "set", "these", "valu", "num", "next", "let", "back", "unit", "multipli", "with", "the", "rescal", "slope", "and", "the", "intercept", "which", "are", "conveni", "store", "the", "metadata", "the", "scan", "the", "next", "part", "will", "use", "kaggl", "lung", "cancer", "dataset", "and", "convolut", "neural", "net", "use", "kera", "will", "build", "upon", "the", "inform", "provid", "this", "articl", "the", "next", "one", "acknowledg", "numdicominpythonimportingmedicalimagedataintonumpywithpydicomandvtk", "resourcesdicomimagelibrari", "paperofweekviolanumrapidpdf", "postsnumintroductiondeeplearningpythoncaff", "cdatasciencebowlnum", "trunkdnumdnumtutorialpyroothtml", "kaggl", "communiti", "for", "all", "the", "differ", "script", "and", "support", "bio", "taposh", "roy", "lead", "innov", "team", "kaiser", "permanent", "decis", "support", "group", "work", "with", "research", "technolog", "and", "busi", "leader", "deriv", "insight", "from", "data", "origin", "repost", "with", "permiss", "relat", "deep", "learn", "generat", "adversari", "network", "box", "toward", "fundament", "understand", "data", "scientist", "mind", "surgeri", "heart", "arxiv", "paper", "spotlight", "sampl", "imag", "tag", "and", "retriev", "method", "user", "generat", "content"], "timestamp_scraper": 1556379411.27867, "title": "Medical Image Analysis with Deep Learning", "read_time": 391.5, "content_html": "<div class=\"post\" id=\"post-\">\n<p><b>By Taposh Roy, Kaiser Permanente.</b></p>\n<p><img alt=\"\" class=\"aligncenter\" src=\"/wp-content/uploads/medical-image-analysis-header.jpeg\" width=\"85%\"/></p>\n<p>Analyzing images and videos, and using them in various applications such as self driven cars, drones etc. with underlying deep learning techniques has been the new research frontier. The recent research papers such as \u201c<a href=\"https://arxiv.org/abs/1508.06576\" target=\"_blank\">A Neural Algorithm of Artistic Style</a>\u201d, show how a styles can be transferred from an artist and applied to an image, to create a new image. Other papers such as \u201c<a href=\"https://arxiv.org/abs/1406.2661\" target=\"_blank\">Generative Adversarial Networks</a>\u201d (GAN) and \u201c<a href=\"https://arxiv.org/pdf/1701.07875\" target=\"_blank\">Wasserstein GAN</a>\u201d have paved the path to develop models that can learn to create data that is similar to data that we give them. Thus opening up the world to semi-supervised learning and paving the path to a future of unsupervised learning.</p>\n<p>While these research areas are still on the generic images, our goal is to use these research into medical images to help healthcare. We need to start with some basics. In this article, I start with basics of image processing, basics of medical image format data and visualize some medical data. In the next article I will deep dive into some convolutional neural nets and use them with <a href=\"https://keras.io/\" target=\"_blank\">Keras</a> for predicting lung cancer.</p>\n<h3><strong>Basic Image Processing (using\u00a0python)</strong></h3>\n<p>\u00a0<br>\nThere are a variety of image processing libraries, however <a href=\"http://opencv.org/\" target=\"_blank\">OpenCV</a> (open computer vision) has become mainstream due to its large community support and availability in C++, java and python. I prefer using opencv using jupyter notebook.</br></p>\n<p>Install OpenCV using: <strong><em>pip install opencv-python</em></strong> or install directly from the source from opencv.org</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*8gb6h21kw1xpV5YjeBsLoA.png\" width=\"99%\"/><br>\n<font size=\"-1\">Installing opencv.</font></br></center></p>\n<p>Now open your Jupyter notebook and confirm you can import cv2. You will also need numpy and matplotlib to view your plots inside the notebook.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*tW_9HZ6IQrLTbnPA3t-_CA.png\" width=\"99%\"/></p>\n<p>Now, lets check if you can open an image and view it on your notebook using the code below.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*xRLhjAAqu6VFOV-SRtlOHQ.png\" width=\"99%\"/><br>\n<font size=\"-1\">Example image load through\u00a0OpenCV.</font></br></center></p>\n<p><strong>Basic Face Detection</strong></p>\n<p>Lets, do something fun such as detecting a face. To detect face we will use an open source xml stump-based 20x20 gentle adaboost frontal face detector originally created by Rainer Lienhart. A good post with details on Haar-cascade detection is <a href=\"http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html\" target=\"_blank\">here</a>.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*-Fu-eV90aMfkqTIOgoKHYQ.png\" width=\"99%\"/><br>\n<font size=\"-1\">Face detection using\u00a0OpenCV.</font></br></center></p>\n<p>There are a lot of examples for image processing using opencv in the docs section. <a href=\"http://docs.opencv.org/trunk/d6/d00/tutorial_py_root.html\" target=\"_blank\">http://docs.opencv.org/trunk/d6/d00/tutorial_py_root.html</a>. I leave it up to the reader to play with more examples. Now that we know the basics of image processing, lets move to the next level of understanding medical image format.</p>\n<h3><strong>Medical Image Data\u00a0Format</strong></h3>\n<p>\u00a0<br/>\nMedical images follow <a href=\"http://en.wikipedia.org/wiki/DICOM\" target=\"_blank\">Digital Imaging and Communications (DICOM) </a>as a standard solution for storing and exchanging medical image-data. The first version of this standard was released in 1985. Since then there are several changes made. This standard uses a file format and a communications protocol.</p>\n<ul>\n<li><strong>File Format</strong>\u200a\u2014\u200aAll patient medical images are saved in the DICOM file format. This format has <a href=\"https://en.wikipedia.org/wiki/Protected_health_information\" target=\"_blank\">PHI (protected health information)</a> about the patient such as\u200a\u2014\u200aname, sex, age in addition to other image related data such as equipment used to capture the image and some context to the medical treatment. Medical Imaging Equipments create DICOM files. Doctors use DICOM Viewers, computer software applications that can display DICOM images, read and to diagnose the findings in the images.\n<li><strong>Communications Protocol</strong>\u200a\u2014\u200aThe DICOM communication protocol is used to search for imaging studies in the archive and restore imaging studies to the workstation in order to display it. All medical imaging applications that are connected to the hospital network use the DICOM protocol to exchange information, mainly DICOM images but also patient and procedure information. There are also more advanced network commands that are used to control and follow the treatment, schedule procedures, report statuses and share the workload between doctors and imaging devices.\n</li></li></ul>\n<p>A very good blog that goes into details of the DICOM standard is <a href=\"http://dicomiseasy.blogspot.com/\" target=\"_blank\">here</a></p>\n<h3><strong>Analyze DICOM\u00a0Images</strong></h3>\n<p>\u00a0<br/>\nA very good python package used for analyzing DICOM images is pydicom. In this section, we will see how to render a DICOM image on a Jupyter notebook.</p>\n<p>Install OpenCV using: <strong><em>pip install pydicom</em></strong></p>\n<p>After you install pydicom package, go back to the jupyter notebook. In the notebook, import the dicom package and other packages as shown below.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*i53SlB3laMgXl5WqJZ-rJA.png\" width=\"99%\"/></p>\n<p>We also use other packages such as pandas, scipy, skimage, mpl_toolkit for data processing and analysis.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*dxp_yfg6cqS4ThlcEPRjQw.png\" width=\"99%\"/></p>\n<p>There\u2019s a wealth of freely available DICOM datasets online but here\u2019s a few that should help you get started:</p>\n<ul>\n<li><a href=\"http://www.kaggle.com/\" target=\"_blank\">Kaggle Competitions and Datasets</a>: This is my personal favorite. Check out the data for <a href=\"https://www.kaggle.com/c/data-science-bowl-2017/data\" target=\"_blank\">lung cancer competition</a> and diabetes retinopathy.\n<li><a href=\"http://www.dicomlibrary.com/\" target=\"_blank\">Dicom Library</a>\u00a0: DICOM Library is a free online medical DICOM image or video file sharing service for educational and scientific purposes.\n<li><a href=\"http://www.osirix-viewer.com/datasets/\" target=\"_blank\">Osirix Datasets</a>: Provides a large range of human datasets acquired through a variety of imaging modalities.\n<li><a href=\"https://mri.radiology.uiowa.edu/visible_human_datasets.html\" target=\"_blank\">Visible Human Datasets</a>: Parts of the <a href=\"http://www.nlm.nih.gov/research/visible/visible_human.html\" target=\"_blank\">Visible Human project</a> are somehow freely distributed here which is weird cause <a href=\"http://www.nlm.nih.gov/research/visible/getting_data.html\" target=\"_blank\">getting that data</a> is neither free nor hassle-free.\n<li><a href=\"http://noodle.med.yale.edu/zubal/\" target=\"_blank\">The Zubal Phantom</a>: This website offers multiple datasets of two human males in CT and MRI which are freely distributed.\n</li></li></li></li></li></ul>\n<p>Download the dicom files and load them on your jupyter notebook.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*7qcA2uS6F7d6DhJ7aDdM3A.png\" width=\"99%\"/></p>\n<p>Now, load the DICOM images into a list.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*cmCmmsXOY_JI5j68yh7knA.png\" width=\"99%\"/><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/1600/1*cmCmmsXOY_JI5j68yh7knA.png\" width=\"99%\"/></p>\n<p><strong>Step 1\u00a0: Basic Viewing of DICOM Image in Jupyter</strong></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*Xo7zQWzK_slqM4xu0Yy95A.png\" width=\"99%\"/></p>\n<p>In the first line we load the 1st DICOM file, which we\u2019re gonna use as a reference named <code>RefDs</code>, to extract metadata and whose filename is first in the <code>lstFilesDCM</code> list.</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*C9gmJeO1sTYYkpy7AO1b8Q.png\" width=\"99%\"/></p>\n<p>We then calculate the total dimensions of the 3D NumPy array which are equal to (Number of pixel rows in a slice) x (Number of pixel columns in a slice) x (Number of slices) along the x, y, and z cartesian axes. Lastly, we use the <code>PixelSpacing</code> and <code>SliceThickness</code> attributes to calculate the spacing between pixels in the three axes. We store the array dimensions in <code>ConstPixelDims</code> and the spacing in <code>ConstPixelSpacing [1].</code></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*y0cDTlZL_0cnkM3CrcMZ8A.png\" width=\"99%\"/></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*y2fdN31UKpKTW59kBiER1g.png\" width=\"99%\"/></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*LsJa3-_iotHkAO_uHhswJg.png\" width=\"99%\"/></p>\n<p><strong>Step 2: Looking into details of DICOM format</strong></p>\n<p>The unit of measurement in CT scans is the <strong>Hounsfield Unit (HU)</strong>, which is a measure of radiodensity. CT scanners are carefully calibrated to accurately measure this. A detailed understanding on this can be found <a href=\"https://web.archive.org/web/20070926231241/http://www.intl.elsevierhealth.com/e-books/pdf/940.pdf\" target=\"_blank\">here</a>.</p>\n<p>Each pixel is assigned a numerical value (CT number), which is the average of all the attenuation values contained within the corresponding voxel. This number is compared to the attenuation value of water and displayed on a scale of arbitrary units named Hounsfield units (HU) after Sir Godfrey Hounsfield.</p>\n<p>This scale assigns water as an attenuation value (HU) of zero. The range of CT numbers is <strong>2000 HU </strong>wide although some modern scanners have a greater range of HU up to 4000. Each number represents a shade of grey with +1000 (white) and \u20131000 (black) at either end of the spectrum.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*HNn99mQnjnkgmbolXYZNVg.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Hounsfield Scale [credits: <a href=\"https://web.archive.org/web/20070926231241/http://www.intl.elsevierhealth.com/e-books/pdf/940.pdf\" target=\"_blank\">\u201cIntroduction to CT physics\u201d</a> (PDF). elsevierhealth.com.]</font></center></p>\n<p>Some scanners have cylindrical scanning bounds, but the output image is square. The pixels that fall outside of these bounds get the fixed value <strong>-2000</strong>.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*KeJXyh0vPFOj_DxKBc1prA.jpeg\" width=\"99%\"/><br/>\n<font size=\"-1\">CT Scanner Image [credits\u00a0: <a href=\"https://web.archive.org/web/20070926231241/http://www.intl.elsevierhealth.com/e-books/pdf/940.pdf\" target=\"_blank\">\u201cIntroduction to CT physics\u201d</a> (PDF). elsevierhealth.com.]</font></center></p>\n<p>The first step usually is setting these values to 0. Next, let\u2019s go back to HU units, by multiplying with the rescale slope and adding the intercept (which are conveniently stored in the metadata of the scans!).</p>\n<p>In the next part, we will use Kaggle\u2019s lung cancer data-set and Convolution Neural Nets using Keras. We will build upon the information provided by this article to go to the next one.</p>\n<p>Acknowledgements:</p>\n<ol>\n<li><a href=\"https://pyscience.wordpress.com/2014/09/08/dicom-in-python-importing-medical-image-data-into-numpy-with-pydicom-and-vtk/\" target=\"_blank\">https://pyscience.wordpress.com/2014/09/08/dicom-in-python-importing-medical-image-data-into-numpy-with-pydicom-and-vtk/</a>\n<li><a href=\"http://www.osirix-viewer.com/resources/dicom-image-library/\" target=\"_blank\">http://www.osirix-viewer.com/resources/dicom-image-library/</a>\n<li><a href=\"http://wearables.cc.gatech.edu/paper_of_week/viola01rapid.pdf\" target=\"_blank\">http://wearables.cc.gatech.edu/paper_of_week/viola01rapid.pdf</a>\n<li><a href=\"http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/\" target=\"_blank\">http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/</a>\n<li><a href=\"http://dicomiseasy.blogspot.com/2012/08/chapter-12-pixel-data.html\" target=\"_blank\">http://dicomiseasy.blogspot.com/</a>\n<li><a href=\"https://www.kaggle.com/c/data-science-bowl-2017\" target=\"_blank\">https://www.kaggle.com/c/data-science-bowl-2017</a>\n<li><a href=\"http://docs.opencv.org/trunk/d6/d00/tutorial_py_root.html\" target=\"_blank\">http://docs.opencv.org/trunk/d6/d00/tutorial_py_root.html</a>\n<li>Kaggle community for all the different scripts and support\n</li></li></li></li></li></li></li></li></ol>\n<p><b>Bio: <a href=\"https://www.linkedin.com/in/taposh/\" target=\"_blank\">Taposh Roy</a></b> leads innovation team in Kaiser Permanente's Decision Support group. He works with research, technology and business leaders to derive insights from data.</p>\n<p><a href=\"https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-i-23d518abf531#.axcbxvzex\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/03/deep-learning-gans-boxing-fundamental-understanding.html\">Deep Learning, Generative Adversarial Networks\u200a &amp; Boxing \u2013 Toward a Fundamental Understanding</a>\n<li><a href=\"/2015/06/data-scientist-heart-surgery.html\">I\u2019m a data scientist \u2013 mind if I do surgery on your heart?</a>\n<li><a href=\"/2017/01/arxiv-spotlight-sampled-image-tagging-retrieval-methods-user-generated-content.html\">arXiv Paper Spotlight: Sampled Image Tagging and Retrieval Methods on User Generated Content</a>\n</li></li></li></ul>\n</div> ", "website": "kdnuggets"}