{"content": "comments By Dima Shulga , Data Scientist at HiredScore I started my way in the Data Science world a few years back. I was a Software Engineer back then and I started to learn online first (before starting my Master\u2019s degree). I remember that as I searched for online resources I saw only names of learning algorithms\u200a\u2014\u200aLinear Regression, Support Vector Machine, Decision Tree, Random Forest, Neural Networks and so on. It was very hard to understand where I should start.\u00a0Today I know that the most important thing to learn to become a Data Scientist is the pipeline, i.e, the process of getting and processing data, understanding the data, building the model, evaluating the results (both of the model and the data processing phase) and deployment.\u00a0So as a\u00a0 TL;DR \u00a0for this post: Learn Logistic Regression first to become familiar with the pipeline and not being overwhelmed with fancy algorithms. You can read more about my experience of moving from Software Engineering into Data Science in this\u00a0 post . So here\u2019s my 5 reasons why today I think that we should start with Logistic Regression first to become a Data Scientist. This is only my opinion of course, for other people it might be easier to do things in a different way. \u00a0 Because the learning algorithm is just a part of the pipeline As I said in the beginning, the Data Science work is not just model building. It includes these steps: You can see that \u201cModeling\u201d is one part of this repetitive process. When building a data product, it is a good practice to build your whole pipeline first, keep it simple as possible, understand what exactly you\u2019re trying to achieve, how can you measure yourself and what is your baseline. After that, you can do fancy Machine Learning and be able to know if you\u2019re getting better. By the way, Logistic Regression (or any ML algorithm) may be used not only in the \u201cModeling\u201d part but also in \u201cData Understanding\u201d and \u201cData Preparation\u201d, imputing is one example for this. \u00a0 Because you\u2019ll better understand Machine Learning I think that the first question people ask themselves when reading this post title is why \u201cLogistic\u201d and not \u201cLinear\u201d regression. And the truth is that it doesn\u2019t matter. This question alone brings to notion 2 types of supervised learning algorithms\u200a\u2014\u200aClassification (Logistic Regression) and Regression (Linear Regression). When you build your pipeline with Logistic or Linear Regression you\u2019re becoming familiar with most of the Machine Learning concepts while keeping things simple. Concepts like Supervised and Unsupervised Learning, Classification vs Regression, Linear vs Non-Linear problems and many more. Also you\u2019re getting an idea about how to prepare your data, what challenges might be there (like imputing and feature selection), how do you measure your model, should you use \u201cAccuracy\u201d, \u201cPrecision-Recall\u201d, \u201cROC AUC\u201d? or maybe \u201cMean Squared Error\u201d and \u201cPearson Correlation\u201d?. All those concepts are the most important part of the Data Science process. After you\u2019re familiar with them, you\u2019ll be able to replace your simple model with much more complex one onces you\u2019ve mastered them. \u00a0 Because \u201cLogistic Regression\u201d is (sometimes) enough Logistic regression is a very powerful algorithm, even for very complex problems it may do a good job. Take MNIST for example, you can achieve 95% accuracy using Logistic Regression only, it\u2019s not a great result, but its more than good enough to make sure you pipeline works. Actually, with the right representation of the features, it can do a fantastic job. When dealing with non-linear problems, we sometimes try to represent the original data in a way that may be explained linearly. Here\u2019s a small example of this idea: We want to perform a simple classification task on the following data: X1 x2 | Y\r ==================\r -2 0 1\r 2 0 1\r -1 0 0\r 1 0 0 If we plot this data, we\u2019ll be able to see that there is no single line that can separate it: \r  In this case, Logistic Regression without doing something with the data won\u2019t help us, but if we drop our\u00a0 x2 \u00a0feature and use\u00a0 x1\u00b2 \u00a0instead, it will look like this: X1 x1^2 | Y\r ==================\r -2 4 1\r 2 4 1\r -1 1 0\r 1 1 0 Now, there is a simple line that can separate the data. Of course, this toy example is nothing like real life, and in real life, it will be very hard to tell how exactly you need to change your data so a linear classier will help you, but, if you invest some time in feature engineering and feature selection your Logistic Regression might do a very good job. \u00a0 Because it is an important tool in Statistics Linear Regression is good not only for prediction, once you have a fitted Linear Regression model you can learn things about relationships between the depended and the independent variables, or in more \u201cML\u201d language, you can learn the relations between your features and you target value. Consider a simple example where we have data about house pricing, we have a bunch of features and the actual price. We fit a Linear Regression model and get good results. We can look at the actual weights the model learned for each feature, and if those are significant, we can say that some feature is more important than others, moreover, we can say that the house size, for example, responsible for 50% of the change in the house price and increase in 1 square meter will lead to increase in 10K in house price. Linear Regression is a powerful tool to learn relationships from data and statisticians use it quite often. \u00a0 Because its a great start to learning Neural Networks For me, studying Logistic regression first helped a lot when I started to learn Neural Networks. You can think of each neuron in the network as a Logistic Regression, it has the input, the weights, the bias you do a dot product to all of that, then apply some non linear function. Moreover, the final layer of a neural network is a simple linear model (most of the time). Take a look at this very basic neural network: Let\u2019s look closer at the \u201coutput layer\u201d, you can see that this is a simple linear (or logistic) regression, we have the input (hidden layer 2), we have the weighs, we do a dot product and then add a non linear function (depends on the task). A nice way to think about neural networks is dividing the NN into two parts, the representation part, and the classification/regression part: The first part (on the left) is trying to learn a good representation of the data that will help the second part (on the right) to perform a linear classification/regression. You can read more about that idea in this great\u00a0 post . \u00a0 Conclusion There\u2019s a lot to know if you want to become a Data Scientist, and at first glance, it looks like the learning algorithms are the most important part. The reality is that the learning algorithms are very complicated in most cases and require a lot of time and effort to understand, but are only a small part of the Data Science pipeline. \u00a0 Bio: Dima Shulga is a Data Scientist at HiredScore. Original . Reposted with permission. Related: A Tour of The Top 10 Algorithms for Machine Learning Newbies Top 10 Machine Learning Algorithms for Beginners Logistic Regression: A Concise Technical Overview", "title_html": "<h1 id=\"title\"><img align=\"right\" alt=\"Silver Blog\" src=\"/images/tkb-1805-s.png\" width=\"94\"/>5 Reasons Logistic Regression should be the first thing you learn when becoming a Data Scientist</h1> ", "url": "https://www.kdnuggets.com/2018/05/5-reasons-logistic-regression-first-data-scientist.html", "tfidf": {"tfidf": {"after": 2.04140414042, "real": 4.56206896552, "deal": 2.18346857379, "squar": 6.53333333334, "unsupervis": 345.13043478300006, "onc": 2.9949066213999997, "relat": 2.47501753838, "fit": 6.74140127388, "here": 4.84615384616, "pearson": 23.5898959881, "weigh": 10.9944598338, "neuron": 64.2753036437, "truth": 5.745928338759999, "onlin": 5.210370856580001, "layer": 24.424615384619997, "experi": 1.87062566278, "evalu": 6.9509632224199995, "than": 2.0655737705, "repres": 1.46972782818, "about": 6.38916090954, "function": 4.99088337, "deploy": 7.41869158879, "job": 9.761836441889999, "say": 3.5088960106, "neural": 356.7640449438, "thing": 9.626193724439998, "precisionrecal": 933.882352941, "just": 2.67160286074, "saw": 1.94845360825, "product": 4.86794766966, "tri": 5.563368765329999, "tree": 4.127925117, "price": 13.935483870959999, "technic": 3.1400316455699997, "toy": 15.383720930199999, "permiss": 6.280063291139999, "xnum\u00b2": 933.882352941, "know": 7.779810519450001, "most": 6.12578778138, "should": 4.99297620297, "won": 2.31732593782, "where": 2.13430127042, "rememb": 4.88793103448, "complex": 4.6804245283, "instead": 1.59461631177, "post": 8.95305230508, "tool": 9.99433427762, "few": 1.31729173581, "noth": 3.46410648047, "how": 6.41001312204, "repost": 933.882352941, "them": 2.19752231988, "bunch": 40.5, "studi": 1.53184098804, "keep": 4.08490930142, "quit": 2.8849718335500003, "increas": 2.6404989605, "explain": 2.60049140049, "singl": 1.60948905109, "mayb": 21.0557029178, "correl": 13.1860465116, "support": 1.2685577307200002, "abl": 5.46255304506, "mani": 1.04426757877, "beginn": 53.4545454545, "overview": 12.6805111821, "regress": 1177.8967741934, "consid": 1.2397313759200002, "build": 8.170869789000001, "want": 3.99396226416, "second": 1.1130898128, "power": 2.6792675723599997, "linear": 222.0419580416, "perform": 3.0627954085000004, "but": 5.0816208949499995, "represent": 17.784914115, "need": 1.4372623574099999, "exact": 6.9372951715, "classif": 24.201219512190004, "imput": 356.76404494400003, "scienc": 11.5984804208, "final": 1.34008609775, "possibl": 1.4173734488, "titl": 1.87261146497, "num": 32.01008128032, "lead": 1.2664326739, "phase": 4.3012733676499995, "exampl": 9.02900473932, "right": 2.8109065155799997, "has": 1.0436497502, "enough": 4.463939266140001, "replac": 1.5602948402899999, "classier": 933.882352941, "take": 2.27923336444, "use": 5.148193786899999, "follow": 1.04640126549, "now": 1.160780873, "might": 6.468559011270001, "resourc": 2.9487369985100003, "name": 1.10211732037, "hard": 5.46506024096, "model": 22.9965762444, "good": 10.63871338313, "sure": 7.453521126760001, "basic": 2.7301805675, "concept": 7.97121338913, "output": 7.676982591880001, "much": 1.1942229577299999, "cours": 4.30185611706, "also": 2.02953020134, "hidden": 7.81299212598, "start": 8.86715072208, "target": 3.2189781021900004, "mean": 1.44906900329, "vector": 25.898858075, "process": 8.4762413241, "not": 6.09404388714, "will": 6.1240549298, "becaus": 5.74759249875, "xnum": 118.4776119402, "fanci": 61.1791907514, "classificationregress": 1867.764705882, "tour": 3.4876977152900004, "overwhelm": 6.86381322957, "signific": 1.4529147982100001, "the": 60.0, "may": 3.15605327679, "becom": 5.6246014313, "whole": 2.29488291414, "read": 6.944881889760001, "logist": 211.4920071045, "peopl": 2.42640990372, "whi": 6.513230769240001, "challeng": 2.55816951337, "those": 2.39096385542, "said": 1.54751925139, "problem": 5.30024482527, "complic": 5.6478121664900005, "separ": 3.2024205748799996, "great": 3.79778327088, "themselv": 2.05967825636, "chang": 2.3617970842, "let": 3.48616600791, "world": 1.11340206186, "meter": 9.61016949153, "actual": 5.62446858762, "network": 18.15585688616, "comment": 3.05954904606, "weight": 9.757836508919999, "random": 7.1902173913, "featur": 13.74413235858, "non": 33.959358288800004, "practic": 1.70434782609, "respons": 1.5066907089300001, "simpl": 27.184931506879998, "other": 2.01984732824, "appli": 2.2972073506, "algorithm": 279.507042254, "one": 3.01882487166, "valu": 2.2777618364400003, "softwar": 20.5248868778, "begin": 1.3305397251100002, "see": 3.81726376533, "moreov": 15.12, "accuraci": 25.5241157556, "shulga": 1867.764705882, "invest": 4.16146788991, "get": 7.1425036554, "easier": 7.84, "like": 5.745928338750001, "small": 2.7189587258, "without": 1.29547123623, "lot": 13.22632602054, "some": 3.1211009174399997, "hiredscor": 1867.764705882, "dot": 37.7550535078, "drop": 2.4594887684, "both": 1.05215720061, "opinion": 3.8044572250199997, "tell": 3.36142282448, "part": 11.47637510679, "alon": 2.99716820842, "independ": 1.58950740889, "that": 18.0717131475, "supervis": 15.48122866894, "baselin": 57.7309090909, "understand": 17.81151832458, "think": 11.62863944332, "languag": 2.29488291414, "ani": 1.13383802314, "question": 4.4081632653, "divid": 2.3169877408099997, "time": 3.03382381044, "from": 2.00113442994, "engin": 7.414072229129999, "back": 2.52140077822, "newbi": 417.78947368400003, "work": 2.23040179826, "differ": 1.23654490225, "variabl": 8.747107438019999, "statist": 4.24265098878, "first": 8.06092916984, "idea": 6.279235332900001, "between": 2.06907337416, "left": 1.4398693996, "add": 4.61243463103, "for": 13.00409552013, "depend": 4.4822134387400006, "predict": 5.18484650555, "all": 2.02293577982, "top": 3.6775538568400004, "closer": 5.5666199158500005, "input": 24.4058416602, "today": 3.49922856514, "our": 2.35758835759, "with": 11.013180298889997, "fantast": 24.3496932515, "reason": 1.72340425532, "someth": 3.28152128979, "there": 4.16365066876, "are": 5.1495296789000005, "includ": 1.0190641247799999, "better": 4.0131445905000005, "degre": 2.4852849092, "case": 2.96997474512, "effort": 1.89247824532, "result": 3.43834825296, "look": 9.543159413299998, "plot": 5.383519837230001, "more": 7.1201947719, "and": 31.001952756029997, "dima": 1024.258064516, "veri": 8.81160799239, "type": 2.0281042411900003, "task": 7.77282741738, "achiev": 3.74433962264, "these": 1.07415426252, "prepar": 4.86024797184, "life": 2.74102209944, "statistician": 83.55789473680001, "yourself": 26.592964824099997, "forest": 4.89546716004, "master": 6.30250099246, "befor": 1.10036041031, "familiar": 20.59143968871, "then": 3.25973581548, "select": 4.04690288044, "can": 18.82018226272, "glanc": 46.285714285699996, "make": 1.0762660158600001, "way": 6.0953697305, "search": 3.2539454806299997, "onli": 6.153885909960001, "each": 2.37949640288, "size": 2.49387370405, "bring": 2.03616775683, "this": 15.056904400650001, "step": 2.8279301745599996, "error": 6.04109589041, "two": 1.01379310345, "move": 1.29125660838, "ask": 2.1744966443, "hous": 5.849668386159999, "data": 87.78732454284, "matter": 2.44773358002, "scientist": 23.4713187463, "machin": 24.146007604559998, "into": 2.03004922958, "sometim": 3.4252427184400003, "bio": 42.336000000000006, "bias": 13.7335640138, "origin": 2.27449856734, "realiti": 4.563380281690001, "learn": 48.77776152165001, "requir": 1.52844902282, "concis": 22.647646219699997, "import": 6.700996116850001, "decis": 2.16, "what": 3.7603031738399997, "nonlinear": 198.45, "pipelin": 224.9635627533, "nice": 17.7583892617, "have": 5.0744742057, "relationship": 4.78264798916, "while": 1.0441988950299999, "help": 5.59851891036, "year": 1.0485436893200002, "when": 5.10383848775, "measur": 4.82186788154, "often": 1.29452054795, "repetit": 20.4324324324, "notion": 7.356811862839999, "line": 2.8365195640599996, "conclus": 4.84615384615, "even": 1.16461267606}, "logtfidf": {"after": 0.040981389296199995, "real": 1.649258121148, "deal": 0.780914701253, "squar": 2.36754019402, "unsupervis": 5.843922417409999, "onc": 0.80753174471, "relat": 0.42620060330799997, "fit": 2.4302412537799998, "here": 1.7700763767400003, "pearson": 3.16081848426, "weigh": 2.39739149445, "neuron": 4.16317547727, "truth": 1.74849148898, "onlin": 1.915007708714, "layer": 6.290937487050001, "experi": 0.626272953933, "evalu": 1.9388802431299998, "than": 0.0645217244364, "repres": 0.38507723275, "about": 0.37706086484760004, "function": 1.828931483188, "deploy": 2.00400270589, "job": 3.5396047622699998, "say": 1.124308561104, "neural": 24.511890933, "thing": 3.5127741387199998, "precisionrecal": 6.83935046985, "just": 0.579062868218, "saw": 0.667036036556, "product": 1.452180409608, "tri": 1.8527745874800001, "tree": 1.41777488775, "price": 4.99257609056, "technic": 1.14423287808, "toy": 2.73330986786, "permiss": 1.8373800586400002, "xnum\u00b2": 6.83935046985, "know": 2.8587590831939997, "most": 0.12448737777359999, "should": 1.5282596302740001, "won": 0.8404139079, "where": 0.1299842774914, "rememb": 1.5867691126199999, "complex": 1.7004832728619999, "instead": 0.46663315041500003, "post": 3.2228006108039997, "tool": 3.21774235926, "few": 0.275577913653, "noth": 1.24245472939, "how": 1.8862678277200002, "repost": 6.83935046985, "them": 0.1883666538186, "bunch": 3.70130197411, "studi": 0.426470272221, "keep": 1.4283046893459999, "quit": 1.05951513684, "increas": 0.555641437858, "explain": 0.955700427358, "singl": 0.475916769059, "mayb": 3.0471714458899997, "correl": 2.57915918803, "support": 0.237880610037, "abl": 1.7979119474250003, "mani": 0.0433157581221, "beginn": 3.9788316751, "overview": 2.54006626224, "regress": 90.52780487765999, "consid": 0.214894723824, "build": 2.455687260455, "want": 1.3832732125099998, "second": 0.10713976337999999, "power": 0.58479256543, "linear": 42.08444227136, "perform": 0.85236170116, "but": 0.0809618603595, "represent": 5.33921486295, "need": 0.362740163442, "exact": 2.4875295465000002, "classif": 6.263372208870001, "imput": 10.36785488834, "scienc": 4.207180894455, "final": 0.292733863948, "possibl": 0.348805474891, "titl": 0.6273339619899999, "num": 0.010079692652704001, "lead": 0.23620402986699998, "phase": 1.4589111108700001, "exampl": 2.4520960499939997, "right": 0.68071970834, "has": 0.0427239448548, "enough": 1.605768878338, "replac": 0.444874803592, "classier": 6.83935046985, "take": 0.261383924394, "use": 0.146040098658, "follow": 0.045356911094199995, "now": 0.149092945021, "might": 2.3050232296020003, "resourc": 1.08137694258, "name": 0.09723316638430002, "hard": 2.01045592812, "model": 8.111950804221001, "good": 2.930125834349, "sure": 2.0086865552, "basic": 1.00436774895, "concept": 2.931673311309, "output": 2.03822657827, "much": 0.17749572930100002, "cours": 1.531798808266, "also": 0.0293143156, "hidden": 2.0557880052, "start": 1.655103585037, "target": 1.1690639496200002, "mean": 0.37092128352, "vector": 3.25419887797, "process": 2.6391459951250003, "not": 0.093314478045, "will": 1.0139326745750001, "becaus": 0.6967157941250001, "xnum": 11.028335175840002, "fanci": 6.84131986148, "classificationregress": 13.6787009397, "tour": 1.2492418381000001, "overwhelm": 1.92626315167, "signific": 0.373571744332, "the": 0.0, "may": 0.1521299858532, "becom": 0.588560882445, "whole": 0.8306818244059999, "read": 2.51817804264, "logist": 39.692055077999996, "peopl": 0.386531156946, "whi": 2.36137686094, "challeng": 0.9392919688950001, "those": 0.35709878174599996, "said": 0.436653165815, "problem": 1.707422172819, "complic": 1.7312682430000002, "separ": 0.941519545898, "great": 0.707415774237, "themselv": 0.7225497843690001, "chang": 0.332551250116, "let": 1.2488025672799998, "world": 0.107420248621, "meter": 2.26282185982, "actual": 1.885542544944, "network": 6.671581371364, "comment": 1.11826753454, "weight": 3.16984705224, "random": 1.9727214065099998, "featur": 3.810486763278, "non": 5.664034569219999, "practic": 0.533182530867, "respons": 0.40991566230300003, "simpl": 9.785770315119999, "other": 0.01974949583952, "appli": 0.8316941898119999, "algorithm": 33.3044239518, "one": 0.0187660549365, "valu": 0.823193310148, "softwar": 4.65698192666, "begin": 0.285584668268, "see": 0.722764756476, "moreov": 4.04574238038, "accuraci": 5.0929530812, "shulga": 13.6787009397, "invest": 1.42586787018, "get": 2.319076023128, "easier": 2.05923883436, "like": 0.6952678827250001, "small": 0.614203610118, "without": 0.258874517941, "lot": 4.450790850750001, "some": 0.11872052719350001, "hiredscor": 13.6787009397, "dot": 5.87594430786, "drop": 0.8999535106219999, "both": 0.050842533389300004, "opinion": 1.33617333331, "tell": 1.21236434401, "part": 0.4663484208108001, "alon": 1.09766791236, "independ": 0.463424162503, "that": 0.07157067083351999, "supervis": 4.09296211166, "baselin": 4.05579271624, "understand": 6.528515254079998, "think": 4.268706447, "languag": 0.8306818244059999, "ani": 0.125608358366, "question": 1.580621858028, "divid": 0.8402679544589999, "time": 0.0336345565878, "from": 0.001134108337732, "engin": 2.7143026748279997, "back": 0.46333486179399996, "newbi": 6.0349776541799995, "work": 0.218069134546, "differ": 0.212321121312, "variabl": 2.1687230672, "statist": 1.4451883070700002, "first": 0.060698318497279996, "idea": 2.21590776636, "between": 0.06790736233059999, "left": 0.364552414753, "add": 1.52875583713, "for": 0.004094875140161, "depend": 1.61393963, "predict": 1.6457402376899999, "all": 0.022805264195599997, "top": 1.218201275576, "closer": 1.7167880323700002, "input": 5.00335067078, "today": 1.118790707358, "our": 0.8576392141820001, "with": 0.01317240884729, "fantast": 3.1925192519800003, "reason": 0.544301552962, "someth": 1.18830712273, "there": 0.160391571702, "are": 0.1473373679135, "includ": 0.0188846813905, "better": 1.3928558812, "degre": 0.910387304568, "case": 0.790812537778, "effort": 0.637887211057, "result": 0.40913672514300004, "look": 3.2319334680000003, "plot": 1.68334240509, "more": 0.11917452119999998, "and": 0.0019526944039716, "dima": 12.477153218839998, "veri": 1.6111185526660001, "type": 0.707101485387, "task": 2.71497361322, "achiev": 1.2541961702339999, "these": 0.0715336194008, "prepar": 1.7758845581240001, "life": 0.630367398554, "statistician": 4.425539741740001, "yourself": 3.28064670051, "forest": 1.5883097076, "master": 2.29559871398, "befor": 0.0956377718795, "familiar": 5.77878945501, "then": 0.24910159569269996, "select": 1.409609374266, "can": 2.597457542304, "glanc": 3.8348333667400003, "make": 0.07349765782289999, "way": 0.9904575496750001, "search": 1.1798682540899998, "onli": 0.1519456099746, "each": 0.347483378608, "size": 0.9138372060609999, "bring": 0.7110694905930001, "this": 0.0567967357875, "step": 1.03954505698, "error": 1.7985854343, "two": 0.0136988443582, "move": 0.255615859253, "ask": 0.776797209847, "hous": 1.5203624495519998, "data": 31.6373352048, "matter": 0.8951625270360001, "scientist": 7.7317064222, "machin": 8.35415748372, "into": 0.0298257264574, "sometim": 1.076050310686, "bio": 3.7456377879300002, "bias": 2.61984276467, "origin": 0.257224875174, "realiti": 1.51806363875, "learn": 17.697793359645, "requir": 0.424253510675, "concis": 3.1200559268700006, "import": 1.46409138533, "decis": 0.7701082216959999, "what": 0.677661890481, "nonlinear": 9.19477999734, "pipelin": 24.29019807704, "nice": 2.8768580387299996, "have": 0.07392501170600001, "relationship": 1.743694370368, "while": 0.04324998379380001, "help": 1.344830885376, "year": 0.047402238894600005, "when": 0.102774944292, "measur": 1.760028399452, "often": 0.258140393351, "repetit": 3.0171234635400004, "notion": 1.9956266680799999, "line": 0.698861228904, "conclus": 1.57818536893, "even": 0.152388564834}, "logidf": {"after": 0.020490694648099998, "real": 0.824629060574, "deal": 0.780914701253, "squar": 1.18377009701, "unsupervis": 5.843922417409999, "onc": 0.403765872355, "relat": 0.21310030165399999, "fit": 1.2151206268899999, "here": 0.8850381883700001, "pearson": 3.16081848426, "weigh": 2.39739149445, "neuron": 4.16317547727, "truth": 1.74849148898, "onlin": 0.957503854357, "layer": 2.0969791623500003, "experi": 0.626272953933, "evalu": 1.9388802431299998, "than": 0.0322608622182, "repres": 0.38507723275, "about": 0.0628434774746, "function": 0.914465741594, "deploy": 2.00400270589, "job": 1.1798682540899998, "say": 0.562154280552, "neural": 4.0853151555, "thing": 0.8781935346799999, "precisionrecal": 6.83935046985, "just": 0.289531434109, "saw": 0.667036036556, "product": 0.484060136536, "tri": 0.61759152916, "tree": 1.41777488775, "price": 1.24814402264, "technic": 1.14423287808, "toy": 2.73330986786, "permiss": 1.8373800586400002, "xnum\u00b2": 6.83935046985, "know": 0.952919694398, "most": 0.020747896295599998, "should": 0.509419876758, "won": 0.8404139079, "where": 0.0649921387457, "rememb": 1.5867691126199999, "complex": 0.8502416364309999, "instead": 0.46663315041500003, "post": 0.8057001527009999, "tool": 1.60887117963, "few": 0.275577913653, "noth": 1.24245472939, "how": 0.47156695693000006, "repost": 6.83935046985, "them": 0.0941833269093, "bunch": 3.70130197411, "studi": 0.426470272221, "keep": 0.7141523446729999, "quit": 1.05951513684, "increas": 0.277820718929, "explain": 0.955700427358, "singl": 0.475916769059, "mayb": 3.0471714458899997, "correl": 2.57915918803, "support": 0.237880610037, "abl": 0.599303982475, "mani": 0.0433157581221, "beginn": 3.9788316751, "overview": 2.54006626224, "regress": 3.9359915164199997, "consid": 0.214894723824, "build": 0.491137452091, "want": 0.6916366062549999, "second": 0.10713976337999999, "power": 0.292396282715, "linear": 2.63027764196, "perform": 0.42618085058, "but": 0.0161923720719, "represent": 1.7797382876499999, "need": 0.362740163442, "exact": 1.2437647732500001, "classif": 2.08779073629, "imput": 5.18392744417, "scienc": 0.841436178891, "final": 0.292733863948, "possibl": 0.348805474891, "titl": 0.6273339619899999, "num": 0.00031499039539700004, "lead": 0.23620402986699998, "phase": 1.4589111108700001, "exampl": 0.40868267499899996, "right": 0.34035985417, "has": 0.0427239448548, "enough": 0.802884439169, "replac": 0.444874803592, "classier": 6.83935046985, "take": 0.130691962197, "use": 0.0292080197316, "follow": 0.045356911094199995, "now": 0.149092945021, "might": 0.7683410765340001, "resourc": 1.08137694258, "name": 0.09723316638430002, "hard": 1.00522796406, "model": 0.7374500731110001, "good": 0.418589404907, "sure": 2.0086865552, "basic": 1.00436774895, "concept": 0.977224437103, "output": 2.03822657827, "much": 0.17749572930100002, "cours": 0.765899404133, "also": 0.0146571578, "hidden": 2.0557880052, "start": 0.236443369291, "target": 1.1690639496200002, "mean": 0.37092128352, "vector": 3.25419887797, "process": 0.527829199025, "not": 0.0155524130075, "will": 0.202786534915, "becaus": 0.139343158825, "xnum": 3.6761117252800006, "fanci": 3.42065993074, "classificationregress": 6.83935046985, "tour": 1.2492418381000001, "overwhelm": 1.92626315167, "signific": 0.373571744332, "the": 0.0, "may": 0.050709995284400004, "becom": 0.11771217648900001, "whole": 0.8306818244059999, "read": 0.83939268088, "logist": 2.6461370052, "peopl": 0.193265578473, "whi": 1.18068843047, "challeng": 0.9392919688950001, "those": 0.17854939087299998, "said": 0.436653165815, "problem": 0.569140724273, "complic": 1.7312682430000002, "separ": 0.470759772949, "great": 0.235805258079, "themselv": 0.7225497843690001, "chang": 0.166275625058, "let": 1.2488025672799998, "world": 0.107420248621, "meter": 2.26282185982, "actual": 0.628514181648, "network": 0.9530830530519999, "comment": 1.11826753454, "weight": 1.58492352612, "random": 1.9727214065099998, "featur": 0.423387418142, "non": 2.8320172846099996, "practic": 0.533182530867, "respons": 0.40991566230300003, "simpl": 1.2232212893899999, "other": 0.00987474791976, "appli": 0.8316941898119999, "algorithm": 3.33044239518, "one": 0.0062553516455, "valu": 0.823193310148, "softwar": 2.32849096333, "begin": 0.285584668268, "see": 0.240921585492, "moreov": 2.02287119019, "accuraci": 2.5464765406, "shulga": 6.83935046985, "invest": 1.42586787018, "get": 0.579769005782, "easier": 2.05923883436, "like": 0.139053576545, "small": 0.307101805059, "without": 0.258874517941, "lot": 1.4835969502500002, "some": 0.0395735090645, "hiredscor": 6.83935046985, "dot": 2.93797215393, "drop": 0.8999535106219999, "both": 0.050842533389300004, "opinion": 1.33617333331, "tell": 1.21236434401, "part": 0.04239531098280001, "alon": 1.09766791236, "independ": 0.463424162503, "that": 0.00397614837964, "supervis": 2.04648105583, "baselin": 4.05579271624, "understand": 1.0880858756799998, "think": 1.06717661175, "languag": 0.8306818244059999, "ani": 0.125608358366, "question": 0.790310929014, "divid": 0.8402679544589999, "time": 0.0112115188626, "from": 0.000567054168866, "engin": 0.904767558276, "back": 0.23166743089699998, "newbi": 6.0349776541799995, "work": 0.109034567273, "differ": 0.212321121312, "variabl": 2.1687230672, "statist": 1.4451883070700002, "first": 0.0075872898121599995, "idea": 0.73863592212, "between": 0.033953681165299995, "left": 0.364552414753, "add": 1.52875583713, "for": 0.00031499039539700004, "depend": 0.806969815, "predict": 1.6457402376899999, "all": 0.011402632097799998, "top": 0.609100637788, "closer": 1.7167880323700002, "input": 2.50167533539, "today": 0.559395353679, "our": 0.8576392141820001, "with": 0.00119749171339, "fantast": 3.1925192519800003, "reason": 0.544301552962, "someth": 1.18830712273, "there": 0.0400978929255, "are": 0.0294674735827, "includ": 0.0188846813905, "better": 0.6964279406, "degre": 0.910387304568, "case": 0.395406268889, "effort": 0.637887211057, "result": 0.136378908381, "look": 0.6463866936, "plot": 1.68334240509, "more": 0.017024931599999998, "and": 6.29901420636e-05, "dima": 6.238576609419999, "veri": 0.230159793238, "type": 0.707101485387, "task": 1.35748680661, "achiev": 0.6270980851169999, "these": 0.0715336194008, "prepar": 0.8879422790620001, "life": 0.315183699277, "statistician": 4.425539741740001, "yourself": 3.28064670051, "forest": 1.5883097076, "master": 1.14779935699, "befor": 0.0956377718795, "familiar": 1.92626315167, "then": 0.08303386523089999, "select": 0.704804687133, "can": 0.162341096394, "glanc": 3.8348333667400003, "make": 0.07349765782289999, "way": 0.19809150993500002, "search": 1.1798682540899998, "onli": 0.025324268329099998, "each": 0.173741689304, "size": 0.9138372060609999, "bring": 0.7110694905930001, "this": 0.0037864490525, "step": 1.03954505698, "error": 1.7985854343, "two": 0.0136988443582, "move": 0.255615859253, "ask": 0.776797209847, "hous": 0.38009061238799996, "data": 1.2168205848, "matter": 0.8951625270360001, "scientist": 1.54634128444, "machin": 1.39235958062, "into": 0.0149128632287, "sometim": 0.538025155343, "bio": 3.7456377879300002, "bias": 2.61984276467, "origin": 0.128612437587, "realiti": 1.51806363875, "learn": 0.842752064745, "requir": 0.424253510675, "concis": 3.1200559268700006, "import": 0.292818277066, "decis": 0.7701082216959999, "what": 0.225887296827, "nonlinear": 4.59738999867, "pipelin": 3.47002829672, "nice": 2.8768580387299996, "have": 0.0147850023412, "relationship": 0.871847185184, "while": 0.04324998379380001, "help": 0.336207721344, "year": 0.047402238894600005, "when": 0.0205549888584, "measur": 0.880014199726, "often": 0.258140393351, "repetit": 3.0171234635400004, "notion": 1.9956266680799999, "line": 0.349430614452, "conclus": 1.57818536893, "even": 0.152388564834}, "freq": {"after": 2, "real": 2, "deal": 1, "squar": 2, "unsupervis": 1, "onc": 2, "relat": 2, "fit": 2, "here": 2, "pearson": 1, "weigh": 1, "neuron": 1, "truth": 1, "onlin": 2, "layer": 3, "experi": 1, "evalu": 1, "than": 2, "repres": 1, "about": 6, "function": 2, "deploy": 1, "job": 3, "say": 2, "neural": 6, "thing": 4, "precisionrecal": 1, "just": 2, "saw": 1, "product": 3, "tri": 3, "tree": 1, "price": 4, "technic": 1, "toy": 1, "permiss": 1, "xnum\u00b2": 1, "know": 3, "most": 6, "should": 3, "won": 1, "where": 2, "rememb": 1, "complex": 2, "instead": 1, "post": 4, "tool": 2, "few": 1, "noth": 1, "how": 4, "repost": 1, "them": 2, "bunch": 1, "studi": 1, "keep": 2, "quit": 1, "increas": 2, "explain": 1, "singl": 1, "mayb": 1, "correl": 1, "support": 1, "abl": 3, "mani": 1, "beginn": 1, "overview": 1, "regress": 23, "consid": 1, "build": 5, "want": 2, "second": 1, "power": 2, "linear": 16, "perform": 2, "but": 5, "represent": 3, "need": 1, "exact": 2, "classif": 3, "imput": 2, "scienc": 5, "final": 1, "possibl": 1, "titl": 1, "num": 32, "lead": 1, "phase": 1, "exampl": 6, "right": 2, "has": 1, "enough": 2, "replac": 1, "classier": 1, "take": 2, "use": 5, "follow": 1, "now": 1, "might": 3, "resourc": 1, "name": 1, "hard": 2, "model": 11, "good": 7, "sure": 1, "basic": 1, "concept": 3, "output": 1, "much": 1, "cours": 2, "also": 2, "hidden": 1, "start": 7, "target": 1, "mean": 1, "vector": 1, "process": 5, "not": 6, "will": 5, "becaus": 5, "xnum": 3, "fanci": 2, "classificationregress": 2, "tour": 1, "overwhelm": 1, "signific": 1, "the": 60, "may": 3, "becom": 5, "whole": 1, "read": 3, "logist": 15, "peopl": 2, "whi": 2, "challeng": 1, "those": 2, "said": 1, "problem": 3, "complic": 1, "separ": 2, "great": 3, "themselv": 1, "chang": 2, "let": 1, "world": 1, "meter": 1, "actual": 3, "network": 7, "comment": 1, "weight": 2, "random": 1, "featur": 9, "non": 2, "practic": 1, "respons": 1, "simpl": 8, "other": 2, "appli": 1, "algorithm": 10, "one": 3, "valu": 1, "softwar": 2, "begin": 1, "see": 3, "moreov": 2, "accuraci": 2, "shulga": 2, "invest": 1, "get": 4, "easier": 1, "like": 5, "small": 2, "without": 1, "lot": 3, "some": 3, "hiredscor": 2, "dot": 2, "drop": 1, "both": 1, "opinion": 1, "tell": 1, "part": 11, "alon": 1, "independ": 1, "that": 18, "supervis": 2, "baselin": 1, "understand": 6, "think": 4, "languag": 1, "ani": 1, "question": 2, "divid": 1, "time": 3, "from": 2, "engin": 3, "back": 2, "newbi": 1, "work": 2, "differ": 1, "variabl": 1, "statist": 1, "first": 8, "idea": 3, "between": 2, "left": 1, "add": 1, "for": 13, "depend": 2, "predict": 1, "all": 2, "top": 2, "closer": 1, "input": 2, "today": 2, "our": 1, "with": 11, "fantast": 1, "reason": 1, "someth": 1, "there": 4, "are": 5, "includ": 1, "better": 2, "degre": 1, "case": 2, "effort": 1, "result": 3, "look": 5, "plot": 1, "more": 7, "and": 31, "dima": 2, "veri": 7, "type": 1, "task": 2, "achiev": 2, "these": 1, "prepar": 2, "life": 2, "statistician": 1, "yourself": 1, "forest": 1, "master": 2, "befor": 1, "familiar": 3, "then": 3, "select": 2, "can": 16, "glanc": 1, "make": 1, "way": 5, "search": 1, "onli": 6, "each": 2, "size": 1, "bring": 1, "this": 15, "step": 1, "error": 1, "two": 1, "move": 1, "ask": 1, "hous": 4, "data": 26, "matter": 1, "scientist": 5, "machin": 6, "into": 2, "sometim": 2, "bio": 1, "bias": 1, "origin": 2, "realiti": 1, "learn": 21, "requir": 1, "concis": 1, "import": 5, "decis": 1, "what": 3, "nonlinear": 2, "pipelin": 7, "nice": 1, "have": 5, "relationship": 2, "while": 1, "help": 4, "year": 1, "when": 5, "measur": 2, "often": 1, "repetit": 1, "notion": 1, "line": 2, "conclus": 1, "even": 1}, "idf": {"after": 1.02070207021, "real": 2.28103448276, "deal": 2.18346857379, "squar": 3.26666666667, "unsupervis": 345.13043478300006, "onc": 1.4974533106999999, "relat": 1.23750876919, "fit": 3.37070063694, "here": 2.42307692308, "pearson": 23.5898959881, "weigh": 10.9944598338, "neuron": 64.2753036437, "truth": 5.745928338759999, "onlin": 2.6051854282900004, "layer": 8.14153846154, "experi": 1.87062566278, "evalu": 6.9509632224199995, "than": 1.03278688525, "repres": 1.46972782818, "about": 1.06486015159, "function": 2.495441685, "deploy": 7.41869158879, "job": 3.2539454806299997, "say": 1.7544480053, "neural": 59.4606741573, "thing": 2.4065484311099996, "precisionrecal": 933.882352941, "just": 1.33580143037, "saw": 1.94845360825, "product": 1.62264922322, "tri": 1.8544562551099997, "tree": 4.127925117, "price": 3.4838709677399997, "technic": 3.1400316455699997, "toy": 15.383720930199999, "permiss": 6.280063291139999, "xnum\u00b2": 933.882352941, "know": 2.59327017315, "most": 1.02096463023, "should": 1.6643254009900001, "won": 2.31732593782, "where": 1.06715063521, "rememb": 4.88793103448, "complex": 2.34021226415, "instead": 1.59461631177, "post": 2.23826307627, "tool": 4.99716713881, "few": 1.31729173581, "noth": 3.46410648047, "how": 1.60250328051, "repost": 933.882352941, "them": 1.09876115994, "bunch": 40.5, "studi": 1.53184098804, "keep": 2.04245465071, "quit": 2.8849718335500003, "increas": 1.32024948025, "explain": 2.60049140049, "singl": 1.60948905109, "mayb": 21.0557029178, "correl": 13.1860465116, "support": 1.2685577307200002, "abl": 1.8208510150200001, "mani": 1.04426757877, "beginn": 53.4545454545, "overview": 12.6805111821, "regress": 51.2129032258, "consid": 1.2397313759200002, "build": 1.6341739578, "want": 1.99698113208, "second": 1.1130898128, "power": 1.3396337861799998, "linear": 13.8776223776, "perform": 1.5313977042500002, "but": 1.01632417899, "represent": 5.928304705, "need": 1.4372623574099999, "exact": 3.46864758575, "classif": 8.067073170730001, "imput": 178.38202247200002, "scienc": 2.31969608416, "final": 1.34008609775, "possibl": 1.4173734488, "titl": 1.87261146497, "num": 1.00031504001, "lead": 1.2664326739, "phase": 4.3012733676499995, "exampl": 1.50483412322, "right": 1.4054532577899999, "has": 1.0436497502, "enough": 2.2319696330700003, "replac": 1.5602948402899999, "classier": 933.882352941, "take": 1.13961668222, "use": 1.0296387573799999, "follow": 1.04640126549, "now": 1.160780873, "might": 2.1561863370900003, "resourc": 2.9487369985100003, "name": 1.10211732037, "hard": 2.73253012048, "model": 2.0905978404, "good": 1.51981619759, "sure": 7.453521126760001, "basic": 2.7301805675, "concept": 2.65707112971, "output": 7.676982591880001, "much": 1.1942229577299999, "cours": 2.15092805853, "also": 1.01476510067, "hidden": 7.81299212598, "start": 1.26673581744, "target": 3.2189781021900004, "mean": 1.44906900329, "vector": 25.898858075, "process": 1.69524826482, "not": 1.01567398119, "will": 1.22481098596, "becaus": 1.1495184997499999, "xnum": 39.4925373134, "fanci": 30.5895953757, "classificationregress": 933.882352941, "tour": 3.4876977152900004, "overwhelm": 6.86381322957, "signific": 1.4529147982100001, "the": 1.0, "may": 1.05201775893, "becom": 1.12492028626, "whole": 2.29488291414, "read": 2.3149606299200003, "logist": 14.0994671403, "peopl": 1.21320495186, "whi": 3.2566153846200003, "challeng": 2.55816951337, "those": 1.19548192771, "said": 1.54751925139, "problem": 1.76674827509, "complic": 5.6478121664900005, "separ": 1.6012102874399998, "great": 1.26592775696, "themselv": 2.05967825636, "chang": 1.1808985421, "let": 3.48616600791, "world": 1.11340206186, "meter": 9.61016949153, "actual": 1.87482286254, "network": 2.59369384088, "comment": 3.05954904606, "weight": 4.878918254459999, "random": 7.1902173913, "featur": 1.52712581762, "non": 16.979679144400002, "practic": 1.70434782609, "respons": 1.5066907089300001, "simpl": 3.3981164383599998, "other": 1.00992366412, "appli": 2.2972073506, "algorithm": 27.9507042254, "one": 1.00627495722, "valu": 2.2777618364400003, "softwar": 10.2624434389, "begin": 1.3305397251100002, "see": 1.27242125511, "moreov": 7.56, "accuraci": 12.7620578778, "shulga": 933.882352941, "invest": 4.16146788991, "get": 1.78562591385, "easier": 7.84, "like": 1.14918566775, "small": 1.3594793629, "without": 1.29547123623, "lot": 4.40877534018, "some": 1.04036697248, "hiredscor": 933.882352941, "dot": 18.8775267539, "drop": 2.4594887684, "both": 1.05215720061, "opinion": 3.8044572250199997, "tell": 3.36142282448, "part": 1.04330682789, "alon": 2.99716820842, "independ": 1.58950740889, "that": 1.00398406375, "supervis": 7.74061433447, "baselin": 57.7309090909, "understand": 2.96858638743, "think": 2.90715986083, "languag": 2.29488291414, "ani": 1.13383802314, "question": 2.20408163265, "divid": 2.3169877408099997, "time": 1.01127460348, "from": 1.00056721497, "engin": 2.47135740971, "back": 1.26070038911, "newbi": 417.78947368400003, "work": 1.11520089913, "differ": 1.23654490225, "variabl": 8.747107438019999, "statist": 4.24265098878, "first": 1.00761614623, "idea": 2.0930784443, "between": 1.03453668708, "left": 1.4398693996, "add": 4.61243463103, "for": 1.00031504001, "depend": 2.2411067193700003, "predict": 5.18484650555, "all": 1.01146788991, "top": 1.8387769284200002, "closer": 5.5666199158500005, "input": 12.2029208301, "today": 1.74961428257, "our": 2.35758835759, "with": 1.0011982089899998, "fantast": 24.3496932515, "reason": 1.72340425532, "someth": 3.28152128979, "there": 1.04091266719, "are": 1.02990593578, "includ": 1.0190641247799999, "better": 2.0065722952500002, "degre": 2.4852849092, "case": 1.48498737256, "effort": 1.89247824532, "result": 1.14611608432, "look": 1.9086318826599997, "plot": 5.383519837230001, "more": 1.0171706817, "and": 1.00006299213, "dima": 512.129032258, "veri": 1.25880114177, "type": 2.0281042411900003, "task": 3.88641370869, "achiev": 1.87216981132, "these": 1.07415426252, "prepar": 2.43012398592, "life": 1.37051104972, "statistician": 83.55789473680001, "yourself": 26.592964824099997, "forest": 4.89546716004, "master": 3.15125049623, "befor": 1.10036041031, "familiar": 6.86381322957, "then": 1.08657860516, "select": 2.02345144022, "can": 1.17626139142, "glanc": 46.285714285699996, "make": 1.0762660158600001, "way": 1.2190739461, "search": 3.2539454806299997, "onli": 1.0256476516600002, "each": 1.18974820144, "size": 2.49387370405, "bring": 2.03616775683, "this": 1.00379362671, "step": 2.8279301745599996, "error": 6.04109589041, "two": 1.01379310345, "move": 1.29125660838, "ask": 2.1744966443, "hous": 1.4624170965399999, "data": 3.37643555934, "matter": 2.44773358002, "scientist": 4.69426374926, "machin": 4.02433460076, "into": 1.01502461479, "sometim": 1.7126213592200001, "bio": 42.336000000000006, "bias": 13.7335640138, "origin": 1.13724928367, "realiti": 4.563380281690001, "learn": 2.32275054865, "requir": 1.52844902282, "concis": 22.647646219699997, "import": 1.3401992233700002, "decis": 2.16, "what": 1.25343439128, "nonlinear": 99.225, "pipelin": 32.1376518219, "nice": 17.7583892617, "have": 1.0148948411399998, "relationship": 2.39132399458, "while": 1.0441988950299999, "help": 1.39962972759, "year": 1.0485436893200002, "when": 1.02076769755, "measur": 2.41093394077, "often": 1.29452054795, "repetit": 20.4324324324, "notion": 7.356811862839999, "line": 1.4182597820299998, "conclus": 4.84615384615, "even": 1.16461267606}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  5 Reasons Logistic Regression should be the first thing you learn when becoming a Data Scientist</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/05/5-reasons-logistic-regression-first-data-scientist.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb 5 Reasons Logistic Regression should be the first thing you learn when becoming a Data Scientist Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/05/anaconda-best-practices-scaling-data-science-across-organization.html\" rel=\"prev\" title=\"Best Practices for Scaling Data Science Across the Organization\"/>\n<link href=\"https://www.kdnuggets.com/2018/05/udemy-top-data-science-machine-learning-courses.html\" rel=\"next\" title=\"Top Data Science, Machine Learning Courses from Udemy \u2013 May 2018\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2018/05/5-reasons-logistic-regression-first-data-scientist.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=80765\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2018/05/5-reasons-logistic-regression-first-data-scientist.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-80765 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 8-May, 2018  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/05/index.html\">May</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/05/tutorials.html\">Tutorials, Overviews</a> \u00bb 5 Reasons Logistic Regression should be the first thing you learn when becoming a Data Scientist (\u00a0<a href=\"/2018/n19.html\">18:n19</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\"><img align=\"right\" alt=\"Silver Blog\" src=\"/images/tkb-1805-s.png\" width=\"94\"/>5 Reasons Logistic Regression should be the first thing you learn when becoming a Data Scientist</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/05/anaconda-best-practices-scaling-data-science-across-organization.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/05/udemy-top-data-science-machine-learning-courses.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/data-scientist\" rel=\"tag\">Data Scientist</a>, <a href=\"https://www.kdnuggets.com/tag/logistic-regression\" rel=\"tag\">Logistic Regression</a>, <a href=\"https://www.kdnuggets.com/tag/machine-learning\" rel=\"tag\">Machine Learning</a></div>\n<br/>\n<p class=\"excerpt\">\n     Learn Logistic Regression first to become familiar with the pipeline and not being overwhelmed with fancy algorithms.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"https://www.linkedin.com/in/shudima/\" rel=\"noopener noreferrer\" target=\"_blank\">Dima Shulga</a>, Data Scientist at HiredScore</b></p>\n<p><img alt=\"Header image\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/2000/1*J7INDY_O0ilYDYE2NuM46A.jpeg\" width=\"99%\"/></p>\n<p>I started my way in the Data Science world a few years back. I was a Software Engineer back then and I started to learn online first (before starting my Master\u2019s degree). I remember that as I searched for online resources I saw only names of learning algorithms\u200a\u2014\u200aLinear Regression, Support Vector Machine, Decision Tree, Random Forest, Neural Networks and so on. It was very hard to understand where I should start.\u00a0Today I know that the most important thing to learn to become a Data Scientist is the pipeline, i.e, the process of getting and processing data, understanding the data, building the model, evaluating the results (both of the model and the data processing phase) and deployment.\u00a0So as a\u00a0<strong>TL;DR</strong>\u00a0for this post: Learn Logistic Regression first to become familiar with the pipeline and not being overwhelmed with fancy algorithms.</p>\n<p>You can read more about my experience of moving from Software Engineering into Data Science in this\u00a0<a href=\"https://towardsdatascience.com/experiencing-the-science-in-data-science-b37862b3fbfb\" rel=\"noopener noreferrer\" target=\"_blank\">post</a>.</p>\n<p>So here\u2019s my 5 reasons why today I think that we should start with Logistic Regression first to become a Data Scientist. This is only my opinion of course, for other people it might be easier to do things in a different way.</p>\n<p>\u00a0<br>\n<strong>Because the learning algorithm is just a part of the pipeline</strong></br></p>\n<p>As I said in the beginning, the Data Science work is not just model building. It includes these steps:</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/0*yt4EAnw2EfixSEJW.png\" width=\"99%\"/></p>\n<p>You can see that \u201cModeling\u201d is one part of this repetitive process. When building a data product, it is a good practice to build your whole pipeline first, keep it simple as possible, understand what exactly you\u2019re trying to achieve, how can you measure yourself and what is your baseline. After that, you can do fancy Machine Learning and be able to know if you\u2019re getting better.</p>\n<p>By the way, Logistic Regression (or any ML algorithm) may be used not only in the \u201cModeling\u201d part but also in \u201cData Understanding\u201d and \u201cData Preparation\u201d, imputing is one example for this.</p>\n<p>\u00a0<br>\n<strong>Because you\u2019ll better understand Machine Learning</strong></br></p>\n<p>I think that the first question people ask themselves when reading this post title is why \u201cLogistic\u201d and not \u201cLinear\u201d regression. And the truth is that it doesn\u2019t matter. This question alone brings to notion 2 types of supervised learning algorithms\u200a\u2014\u200aClassification (Logistic Regression) and Regression (Linear Regression). When you build your pipeline with Logistic or Linear Regression you\u2019re becoming familiar with most of the Machine Learning concepts while keeping things simple. Concepts like Supervised and Unsupervised Learning, Classification vs Regression, Linear vs Non-Linear problems and many more. Also you\u2019re getting an idea about how to prepare your data, what challenges might be there (like imputing and feature selection), how do you measure your model, should you use \u201cAccuracy\u201d, \u201cPrecision-Recall\u201d, \u201cROC AUC\u201d? or maybe \u201cMean Squared Error\u201d and \u201cPearson Correlation\u201d?. All those concepts are the most important part of the Data Science process. After you\u2019re familiar with them, you\u2019ll be able to replace your simple model with much more complex one onces you\u2019ve mastered them.</p>\n<p>\u00a0<br>\n<strong>Because \u201cLogistic Regression\u201d is (sometimes) enough</strong></br></p>\n<p>Logistic regression is a very powerful algorithm, even for very complex problems it may do a good job. Take MNIST for example, you can achieve 95% accuracy using Logistic Regression only, it\u2019s not a great result, but its more than good enough to make sure you pipeline works. Actually, with the right representation of the features, it can do a fantastic job. When dealing with non-linear problems, we sometimes try to represent the original data in a way that may be explained linearly. Here\u2019s a small example of this idea: We want to perform a simple classification task on the following data:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>X1    x2    |  Y\r\n==================\r\n-2    0        1\r\n 2    0        1\r\n-1    0        0\r\n 1    0        0</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>If we plot this data, we\u2019ll be able to see that there is no single line that can separate it:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>plt.scatter([-2, 2], [0, 0 ], c='b')\r\nplt.scatter([-1, 1], [0, 0 ], c='r')</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*UDOPk5DKXNID-NSSEMhOnw.png\" width=\"99%\"/></p>\n<p>In this case, Logistic Regression without doing something with the data won\u2019t help us, but if we drop our\u00a0<code>x2</code>\u00a0feature and use\u00a0<code>x1\u00b2</code>\u00a0instead, it will look like this:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>X1    x1^2  |  Y\r\n==================\r\n-2    4       1\r\n 2    4       1\r\n-1    1       0\r\n 1    1       0</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*Ql2EiNpvNpxx-NWgqb3usg.png\" width=\"99%\"/></p>\n<p>Now, there is a simple line that can separate the data. Of course, this toy example is nothing like real life, and in real life, it will be very hard to tell how exactly you need to change your data so a linear classier will help you, but, if you invest some time in feature engineering and feature selection your Logistic Regression might do a very good job.</p>\n<p>\u00a0<br/>\n<strong>Because it is an important tool in Statistics</strong></p>\n<p>Linear Regression is good not only for prediction, once you have a fitted Linear Regression model you can learn things about relationships between the depended and the independent variables, or in more \u201cML\u201d language, you can learn the relations between your features and you target value. Consider a simple example where we have data about house pricing, we have a bunch of features and the actual price. We fit a Linear Regression model and get good results. We can look at the actual weights the model learned for each feature, and if those are significant, we can say that some feature is more important than others, moreover, we can say that the house size, for example, responsible for 50% of the change in the house price and increase in 1 square meter will lead to increase in 10K in house price. Linear Regression is a powerful tool to learn relationships from data and statisticians use it quite often.</p>\n<p>\u00a0<br/>\n<strong>Because its a great start to learning Neural Networks</strong></p>\n<p>For me, studying Logistic regression first helped a lot when I started to learn Neural Networks. You can think of each neuron in the network as a Logistic Regression, it has the input, the weights, the bias you do a dot product to all of that, then apply some non linear function. Moreover, the final layer of a neural network is a simple linear model (most of the time). Take a look at this very basic neural network:</p>\n<p><img alt=\"Image result for neural network binary classification\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/0*IlHu39jf2c7QC4kn.\" width=\"99%\"/></p>\n<p>Let\u2019s look closer at the \u201coutput layer\u201d, you can see that this is a simple linear (or logistic) regression, we have the input (hidden layer 2), we have the weighs, we do a dot product and then add a non linear function (depends on the task). A nice way to think about neural networks is dividing the NN into two parts, the representation part, and the classification/regression part:</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*rVrfKo5-MvdeI38PgSRlXQ.png\" width=\"99%\"/></p>\n<p>The first part (on the left) is trying to learn a good representation of the data that will help the second part (on the right) to perform a linear classification/regression. You can read more about that idea in this great\u00a0<a href=\"http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\" rel=\"noopener noreferrer\" target=\"_blank\">post</a>.</p>\n<p>\u00a0<br/>\n<strong>Conclusion</strong></p>\n<p>There\u2019s a lot to know if you want to become a Data Scientist, and at first glance, it looks like the learning algorithms are the most important part. The reality is that the learning algorithms are very complicated in most cases and require a lot of time and effort to understand, but are only a small part of the Data Science pipeline.</p>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"https://www.linkedin.com/in/shudima/\" rel=\"noopener noreferrer\" target=\"_blank\">Dima Shulga</a></b> is a Data Scientist at HiredScore.</p>\n<p><a href=\"https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/02/tour-top-10-algorithms-machine-learning-newbies.html\">A Tour of The Top 10 Algorithms for Machine Learning Newbies</a>\n<li><a href=\"/2017/10/top-10-machine-learning-algorithms-beginners.html\">Top 10 Machine Learning Algorithms for Beginners</a>\n<li><a href=\"/2018/02/logistic-regression-concise-technical-overview.html\">Logistic Regression: A Concise Technical Overview</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/05/anaconda-best-practices-scaling-data-science-across-organization.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/05/udemy-top-data-science-machine-learning-courses.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/05/index.html\">May</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/05/tutorials.html\">Tutorials, Overviews</a> \u00bb 5 Reasons Logistic Regression should be the first thing you learn when becoming a Data Scientist (\u00a0<a href=\"/2018/n19.html\">18:n19</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556351048\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></body></html>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n\n\n<!-- Dynamic page generated in 0.610 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 03:44:08 -->\n<!-- Compression = gzip -->", "content_tokenized": ["comment", "dima", "shulga", "data", "scientist", "hiredscor", "start", "way", "the", "data", "scienc", "world", "few", "year", "back", "softwar", "engin", "back", "then", "and", "start", "learn", "onlin", "first", "befor", "start", "master", "degre", "rememb", "that", "search", "for", "onlin", "resourc", "saw", "onli", "name", "learn", "algorithm", "linear", "regress", "support", "vector", "machin", "decis", "tree", "random", "forest", "neural", "network", "and", "veri", "hard", "understand", "where", "should", "start", "today", "know", "that", "the", "most", "import", "thing", "learn", "becom", "data", "scientist", "the", "pipelin", "the", "process", "get", "and", "process", "data", "understand", "the", "data", "build", "the", "model", "evalu", "the", "result", "both", "the", "model", "and", "the", "data", "process", "phase", "and", "deploy", "for", "this", "post", "learn", "logist", "regress", "first", "becom", "familiar", "with", "the", "pipelin", "and", "not", "overwhelm", "with", "fanci", "algorithm", "can", "read", "more", "about", "experi", "move", "from", "softwar", "engin", "into", "data", "scienc", "this", "post", "here", "num", "reason", "whi", "today", "think", "that", "should", "start", "with", "logist", "regress", "first", "becom", "data", "scientist", "this", "onli", "opinion", "cours", "for", "other", "peopl", "might", "easier", "thing", "differ", "way", "becaus", "the", "learn", "algorithm", "just", "part", "the", "pipelin", "said", "the", "begin", "the", "data", "scienc", "work", "not", "just", "model", "build", "includ", "these", "step", "can", "see", "that", "model", "one", "part", "this", "repetit", "process", "when", "build", "data", "product", "good", "practic", "build", "whole", "pipelin", "first", "keep", "simpl", "possibl", "understand", "what", "exact", "tri", "achiev", "how", "can", "measur", "yourself", "and", "what", "baselin", "after", "that", "can", "fanci", "machin", "learn", "and", "abl", "know", "get", "better", "the", "way", "logist", "regress", "ani", "algorithm", "may", "use", "not", "onli", "the", "model", "part", "but", "also", "data", "understand", "and", "data", "prepar", "imput", "one", "exampl", "for", "this", "becaus", "better", "understand", "machin", "learn", "think", "that", "the", "first", "question", "peopl", "ask", "themselv", "when", "read", "this", "post", "titl", "whi", "logist", "and", "not", "linear", "regress", "and", "the", "truth", "that", "matter", "this", "question", "alon", "bring", "notion", "num", "type", "supervis", "learn", "algorithm", "classif", "logist", "regress", "and", "regress", "linear", "regress", "when", "build", "pipelin", "with", "logist", "linear", "regress", "becom", "familiar", "with", "most", "the", "machin", "learn", "concept", "while", "keep", "thing", "simpl", "concept", "like", "supervis", "and", "unsupervis", "learn", "classif", "regress", "linear", "nonlinear", "problem", "and", "mani", "more", "also", "get", "idea", "about", "how", "prepar", "data", "what", "challeng", "might", "there", "like", "imput", "and", "featur", "select", "how", "measur", "model", "should", "use", "accuraci", "precisionrecal", "mayb", "mean", "squar", "error", "and", "pearson", "correl", "all", "those", "concept", "are", "the", "most", "import", "part", "the", "data", "scienc", "process", "after", "familiar", "with", "them", "abl", "replac", "simpl", "model", "with", "much", "more", "complex", "one", "onc", "master", "them", "becaus", "logist", "regress", "sometim", "enough", "logist", "regress", "veri", "power", "algorithm", "even", "for", "veri", "complex", "problem", "may", "good", "job", "take", "for", "exampl", "can", "achiev", "num", "accuraci", "use", "logist", "regress", "onli", "not", "great", "result", "but", "more", "than", "good", "enough", "make", "sure", "pipelin", "work", "actual", "with", "the", "right", "represent", "the", "featur", "can", "fantast", "job", "when", "deal", "with", "nonlinear", "problem", "sometim", "tri", "repres", "the", "origin", "data", "way", "that", "may", "explain", "linear", "here", "small", "exampl", "this", "idea", "want", "perform", "simpl", "classif", "task", "the", "follow", "data", "xnum", "num", "num", "num", "num", "num", "num", "num", "num", "num", "num", "num", "num", "plot", "this", "data", "abl", "see", "that", "there", "singl", "line", "that", "can", "separ", "this", "case", "logist", "regress", "without", "someth", "with", "the", "data", "won", "help", "but", "drop", "our", "xnum", "featur", "and", "use", "xnum\u00b2", "instead", "will", "look", "like", "this", "xnum", "num", "num", "num", "num", "num", "num", "num", "num", "num", "num", "num", "num", "now", "there", "simpl", "line", "that", "can", "separ", "the", "data", "cours", "this", "toy", "exampl", "noth", "like", "real", "life", "and", "real", "life", "will", "veri", "hard", "tell", "how", "exact", "need", "chang", "data", "linear", "classier", "will", "help", "but", "invest", "some", "time", "featur", "engin", "and", "featur", "select", "logist", "regress", "might", "veri", "good", "job", "becaus", "import", "tool", "statist", "linear", "regress", "good", "not", "onli", "for", "predict", "onc", "have", "fit", "linear", "regress", "model", "can", "learn", "thing", "about", "relationship", "between", "the", "depend", "and", "the", "independ", "variabl", "more", "languag", "can", "learn", "the", "relat", "between", "featur", "and", "target", "valu", "consid", "simpl", "exampl", "where", "have", "data", "about", "hous", "price", "have", "bunch", "featur", "and", "the", "actual", "price", "fit", "linear", "regress", "model", "and", "get", "good", "result", "can", "look", "the", "actual", "weight", "the", "model", "learn", "for", "each", "featur", "and", "those", "are", "signific", "can", "say", "that", "some", "featur", "more", "import", "than", "other", "moreov", "can", "say", "that", "the", "hous", "size", "for", "exampl", "respons", "for", "num", "the", "chang", "the", "hous", "price", "and", "increas", "num", "squar", "meter", "will", "lead", "increas", "hous", "price", "linear", "regress", "power", "tool", "learn", "relationship", "from", "data", "and", "statistician", "use", "quit", "often", "becaus", "great", "start", "learn", "neural", "network", "for", "studi", "logist", "regress", "first", "help", "lot", "when", "start", "learn", "neural", "network", "can", "think", "each", "neuron", "the", "network", "logist", "regress", "has", "the", "input", "the", "weight", "the", "bias", "dot", "product", "all", "that", "then", "appli", "some", "non", "linear", "function", "moreov", "the", "final", "layer", "neural", "network", "simpl", "linear", "model", "most", "the", "time", "take", "look", "this", "veri", "basic", "neural", "network", "let", "look", "closer", "the", "output", "layer", "can", "see", "that", "this", "simpl", "linear", "logist", "regress", "have", "the", "input", "hidden", "layer", "num", "have", "the", "weigh", "dot", "product", "and", "then", "add", "non", "linear", "function", "depend", "the", "task", "nice", "way", "think", "about", "neural", "network", "divid", "the", "into", "two", "part", "the", "represent", "part", "and", "the", "classificationregress", "part", "the", "first", "part", "the", "left", "tri", "learn", "good", "represent", "the", "data", "that", "will", "help", "the", "second", "part", "the", "right", "perform", "linear", "classificationregress", "can", "read", "more", "about", "that", "idea", "this", "great", "post", "conclus", "there", "lot", "know", "want", "becom", "data", "scientist", "and", "first", "glanc", "look", "like", "the", "learn", "algorithm", "are", "the", "most", "import", "part", "the", "realiti", "that", "the", "learn", "algorithm", "are", "veri", "complic", "most", "case", "and", "requir", "lot", "time", "and", "effort", "understand", "but", "are", "onli", "small", "part", "the", "data", "scienc", "pipelin", "bio", "dima", "shulga", "data", "scientist", "hiredscor", "origin", "repost", "with", "permiss", "relat", "tour", "the", "top", "num", "algorithm", "for", "machin", "learn", "newbi", "top", "num", "machin", "learn", "algorithm", "for", "beginn", "logist", "regress", "concis", "technic", "overview"], "timestamp_scraper": 1556362830.392803, "title": "5 Reasons Logistic Regression should be the first thing you learn when becoming a Data Scientist", "read_time": 372.9, "content_html": "<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"https://www.linkedin.com/in/shudima/\" rel=\"noopener noreferrer\" target=\"_blank\">Dima Shulga</a>, Data Scientist at HiredScore</b></p>\n<p><img alt=\"Header image\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/2000/1*J7INDY_O0ilYDYE2NuM46A.jpeg\" width=\"99%\"/></p>\n<p>I started my way in the Data Science world a few years back. I was a Software Engineer back then and I started to learn online first (before starting my Master\u2019s degree). I remember that as I searched for online resources I saw only names of learning algorithms\u200a\u2014\u200aLinear Regression, Support Vector Machine, Decision Tree, Random Forest, Neural Networks and so on. It was very hard to understand where I should start.\u00a0Today I know that the most important thing to learn to become a Data Scientist is the pipeline, i.e, the process of getting and processing data, understanding the data, building the model, evaluating the results (both of the model and the data processing phase) and deployment.\u00a0So as a\u00a0<strong>TL;DR</strong>\u00a0for this post: Learn Logistic Regression first to become familiar with the pipeline and not being overwhelmed with fancy algorithms.</p>\n<p>You can read more about my experience of moving from Software Engineering into Data Science in this\u00a0<a href=\"https://towardsdatascience.com/experiencing-the-science-in-data-science-b37862b3fbfb\" rel=\"noopener noreferrer\" target=\"_blank\">post</a>.</p>\n<p>So here\u2019s my 5 reasons why today I think that we should start with Logistic Regression first to become a Data Scientist. This is only my opinion of course, for other people it might be easier to do things in a different way.</p>\n<p>\u00a0<br>\n<strong>Because the learning algorithm is just a part of the pipeline</strong></br></p>\n<p>As I said in the beginning, the Data Science work is not just model building. It includes these steps:</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/0*yt4EAnw2EfixSEJW.png\" width=\"99%\"/></p>\n<p>You can see that \u201cModeling\u201d is one part of this repetitive process. When building a data product, it is a good practice to build your whole pipeline first, keep it simple as possible, understand what exactly you\u2019re trying to achieve, how can you measure yourself and what is your baseline. After that, you can do fancy Machine Learning and be able to know if you\u2019re getting better.</p>\n<p>By the way, Logistic Regression (or any ML algorithm) may be used not only in the \u201cModeling\u201d part but also in \u201cData Understanding\u201d and \u201cData Preparation\u201d, imputing is one example for this.</p>\n<p>\u00a0<br>\n<strong>Because you\u2019ll better understand Machine Learning</strong></br></p>\n<p>I think that the first question people ask themselves when reading this post title is why \u201cLogistic\u201d and not \u201cLinear\u201d regression. And the truth is that it doesn\u2019t matter. This question alone brings to notion 2 types of supervised learning algorithms\u200a\u2014\u200aClassification (Logistic Regression) and Regression (Linear Regression). When you build your pipeline with Logistic or Linear Regression you\u2019re becoming familiar with most of the Machine Learning concepts while keeping things simple. Concepts like Supervised and Unsupervised Learning, Classification vs Regression, Linear vs Non-Linear problems and many more. Also you\u2019re getting an idea about how to prepare your data, what challenges might be there (like imputing and feature selection), how do you measure your model, should you use \u201cAccuracy\u201d, \u201cPrecision-Recall\u201d, \u201cROC AUC\u201d? or maybe \u201cMean Squared Error\u201d and \u201cPearson Correlation\u201d?. All those concepts are the most important part of the Data Science process. After you\u2019re familiar with them, you\u2019ll be able to replace your simple model with much more complex one onces you\u2019ve mastered them.</p>\n<p>\u00a0<br>\n<strong>Because \u201cLogistic Regression\u201d is (sometimes) enough</strong></br></p>\n<p>Logistic regression is a very powerful algorithm, even for very complex problems it may do a good job. Take MNIST for example, you can achieve 95% accuracy using Logistic Regression only, it\u2019s not a great result, but its more than good enough to make sure you pipeline works. Actually, with the right representation of the features, it can do a fantastic job. When dealing with non-linear problems, we sometimes try to represent the original data in a way that may be explained linearly. Here\u2019s a small example of this idea: We want to perform a simple classification task on the following data:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>X1    x2    |  Y\r\n==================\r\n-2    0        1\r\n 2    0        1\r\n-1    0        0\r\n 1    0        0</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>If we plot this data, we\u2019ll be able to see that there is no single line that can separate it:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>plt.scatter([-2, 2], [0, 0 ], c='b')\r\nplt.scatter([-1, 1], [0, 0 ], c='r')</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*UDOPk5DKXNID-NSSEMhOnw.png\" width=\"99%\"/></p>\n<p>In this case, Logistic Regression without doing something with the data won\u2019t help us, but if we drop our\u00a0<code>x2</code>\u00a0feature and use\u00a0<code>x1\u00b2</code>\u00a0instead, it will look like this:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>X1    x1^2  |  Y\r\n==================\r\n-2    4       1\r\n 2    4       1\r\n-1    1       0\r\n 1    1       0</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*Ql2EiNpvNpxx-NWgqb3usg.png\" width=\"99%\"/></p>\n<p>Now, there is a simple line that can separate the data. Of course, this toy example is nothing like real life, and in real life, it will be very hard to tell how exactly you need to change your data so a linear classier will help you, but, if you invest some time in feature engineering and feature selection your Logistic Regression might do a very good job.</p>\n<p>\u00a0<br/>\n<strong>Because it is an important tool in Statistics</strong></p>\n<p>Linear Regression is good not only for prediction, once you have a fitted Linear Regression model you can learn things about relationships between the depended and the independent variables, or in more \u201cML\u201d language, you can learn the relations between your features and you target value. Consider a simple example where we have data about house pricing, we have a bunch of features and the actual price. We fit a Linear Regression model and get good results. We can look at the actual weights the model learned for each feature, and if those are significant, we can say that some feature is more important than others, moreover, we can say that the house size, for example, responsible for 50% of the change in the house price and increase in 1 square meter will lead to increase in 10K in house price. Linear Regression is a powerful tool to learn relationships from data and statisticians use it quite often.</p>\n<p>\u00a0<br/>\n<strong>Because its a great start to learning Neural Networks</strong></p>\n<p>For me, studying Logistic regression first helped a lot when I started to learn Neural Networks. You can think of each neuron in the network as a Logistic Regression, it has the input, the weights, the bias you do a dot product to all of that, then apply some non linear function. Moreover, the final layer of a neural network is a simple linear model (most of the time). Take a look at this very basic neural network:</p>\n<p><img alt=\"Image result for neural network binary classification\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/0*IlHu39jf2c7QC4kn.\" width=\"99%\"/></p>\n<p>Let\u2019s look closer at the \u201coutput layer\u201d, you can see that this is a simple linear (or logistic) regression, we have the input (hidden layer 2), we have the weighs, we do a dot product and then add a non linear function (depends on the task). A nice way to think about neural networks is dividing the NN into two parts, the representation part, and the classification/regression part:</p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*rVrfKo5-MvdeI38PgSRlXQ.png\" width=\"99%\"/></p>\n<p>The first part (on the left) is trying to learn a good representation of the data that will help the second part (on the right) to perform a linear classification/regression. You can read more about that idea in this great\u00a0<a href=\"http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\" rel=\"noopener noreferrer\" target=\"_blank\">post</a>.</p>\n<p>\u00a0<br/>\n<strong>Conclusion</strong></p>\n<p>There\u2019s a lot to know if you want to become a Data Scientist, and at first glance, it looks like the learning algorithms are the most important part. The reality is that the learning algorithms are very complicated in most cases and require a lot of time and effort to understand, but are only a small part of the Data Science pipeline.</p>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"https://www.linkedin.com/in/shudima/\" rel=\"noopener noreferrer\" target=\"_blank\">Dima Shulga</a></b> is a Data Scientist at HiredScore.</p>\n<p><a href=\"https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/02/tour-top-10-algorithms-machine-learning-newbies.html\">A Tour of The Top 10 Algorithms for Machine Learning Newbies</a>\n<li><a href=\"/2017/10/top-10-machine-learning-algorithms-beginners.html\">Top 10 Machine Learning Algorithms for Beginners</a>\n<li><a href=\"/2018/02/logistic-regression-concise-technical-overview.html\">Logistic Regression: A Concise Technical Overview</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}