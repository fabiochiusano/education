{"content": "By Marco Bonzanini, Independent Data Science Consultant . Last Saturday ( Editor's note: happened a while ago ) was the closing day of the\u00a0 Six Nations Championship , an annual international\u00a0 rugby \u00a0competition. Before turning on the TV to watch Italy being trashed by Wales, I decided to use this event to collect some data from Twitter and perform some exploratory text analysis on something more interesting than the small list of my tweets. This article continues the tutorial on Twitter Data Mining, re-using what we discussed in the previous articles with some more realistic data. It also expands the analysis by introducing the concept of term co-occurrence. The Application Domain \u00a0 As the name suggests, six teams are involved in the competition: England, Ireland, Wales, Scotland, France and Italy. This means that we can expect the event to be tweeted in multiple languages (English, French, Italian, Welsh, Gaelic, possibly other languages as well), with English being the major language. Assuming the team names will be mentioned frequently, we could decide to look also for their nicknames, e.g.\u00a0 Les Bleus \u00a0for France or\u00a0 Azzurri \u00a0for Italy. During the last day of the competition, three matches are played sequentially. Three teams in particular had a shot for the title: England, Ireland and Wales. At the end, Ireland won the competition but everything was open until the very last minute. Setting Up \u00a0 I used the\u00a0 streaming API \u00a0to download all the tweets containing the string #rbs6nations \u00a0during the day. Obviously not all the tweets about the event contained the hashtag, but this is a good baseline. The time frame for the download was from around\u00a012:15PM to 7:15PM GMT, that is from about 15 minutes before the first match, to about 15 minutes after the last match was over. At the end, more than\u00a018,000 tweets\u00a0have been downloaded in JSON format, making for about\u00a075Mb\u00a0of data. This should be small enough to quickly do some processing in memory, and at the same time big enough to observe something possibly interesting. The textual content of the tweets has been pre-processed with tokenisation and lowercasing \u00a0using the\u00a0  \u00a0function introduced in Part 2 of the tutorial. Interesting terms and hashtags \u00a0 Following what we discussed in\u00a0 Part 3 (Term Frequencies) , we want to observe the most common terms and hashtags used during day. If you have followed the discussion about creating different lists of tokens in order to capture terms without hashtags, hashtags only, removing stop-words, etc. you can play around with the different lists. This is the unsurprising list of top 10 most frequent terms ( terms_only \u00a0in Part 3) in the data set. [( 'ireland' , 3163 ), ( 'england' , 2584 ), ( 'wales' , 2271 ), \r ( '' , 2068 ), ( 'day' , 1479 ), ( 'france' , 1380 ), ( 'win' , 1338 ), \r ( 'rugby' , 1253 ), ( 'points' , 1221 ), ( 'title' , 1180 )]\r The first three terms correspond to the teams who had a go for the title. The frequencies also respect the order in the final table. The fourth term is instead a punctuation mark that we missed and didn\u2019t include in the list of stop-words. This is because\u00a0 string.punctuation \u00a0only contains ASCII symbols, while here we\u2019re dealing with a unicode character. If we dig into the data, there will be more examples like this, but for the moment we don\u2019t worry about it. After adding the suspension-points symbol to the list of stop-words, we have a new entry at the end of the list: [( 'ireland' , 3163 ), ( 'england' , 2584 ), ( 'wales' , 2271 ), \r ( 'day' , 1479 ), ( 'france' , 1380 ), ( 'win' , 1338 ), ( 'rugby' , 1253 ), \r ( 'points' , 1221 ), ( 'title' , 1180 ), ( '__SHAMROCK_SYMBOL__' , 1154 )]\r Interestingly, a new token we didn\u2019t account for, an\u00a0 Emoji symbol \u00a0(in this case, the\u00a0 Irish Shamrock ). If we have a look at the most common hashtags, we need to consider that #rbs6nations \u00a0will be by far the most common token (that\u2019s our search term for downloading the tweets), so we can exclude it from the list. This leave us with: [( '#engvfra' , 1701 ), ( '#itavwal' , 927 ), ( '#rugby' , 880 ), \r ( '#scovire' , 692 ), ( '#ireland' , 686 ), ( '#angfra' , 554 ), \r ( '#xvdefrance' , 508 ), ( '#crunch' , 500 ), ( '#wales' , 446 ), \r ( '#england' , 406 )]\r We can observe that the most common hashtags, a part from\u00a0 #rugby , are related to the individual matches. In particular England v France has received the highest number of mentions, probably being the last match of the day with a dramatic finale. Something interesting to notice is that a fair amount of tweets also contained terms in French: the count for #angfra \u00a0should in fact be added to\u00a0 #engvfra . Those unfamiliar with rugby probably wouldn\u2019t recognise that also\u00a0 #crunch \u00a0should be included with #EngvFra \u00a0match, as\u00a0 Le Crunch \u00a0is the traditional name for this event. So by far, the last match has received a lot of attention. Term co-occurrences \u00a0 Sometimes we are interested in the terms that occur together. This is mainly because the\u00a0 context \u00a0gives us a better insight about the meaning of a term, supporting applications such as word disambiguation or semantic similarity. We discussed the option of using\u00a0 bigrams \u00a0 in the previous article , but we want to extend the context of a term to the whole tweet. We can refactor the code from\u00a0 the previous article \u00a0in order to capture the\u00a0co-occurrences. We build a co-occurrence matrix\u00a0 com \u00a0such that com[x][y] \u00a0contains the number of times the term\u00a0 x \u00a0has been seen in the same tweet as the term\u00a0 y : from collections import defaultdict\r # remember to include the other import from the previous post \r \r com = )\r \r # f is the file pointer to the JSON data set \r for line in f: \r tweet = \r terms_only = [term for term in  \r if term not in stop \r and not )]\r \r # Build co-occurrence matrix \r for i in range ( len (terms_only) - 1 ): \r for j in range (i + 1 , len (terms_only)):\r w1, w2 = sorted ([terms_only[i], terms_only[j]]) \r if w1 != w2:\r com[w1][w2] += 1 \r While building the co-occurrence matrix, we don\u2019t want to count the same term pair twice, e.g.\u00a0 com[A][B] == com[B][A] , so the inner for loop starts from\u00a0 i+1 \u00a0in order to build a triangular matrix, while\u00a0 sorted \u00a0will preserve the alphabetical order of the terms. For each term, we then extract the 5 most frequent co-occurrent terms, creating a list of tuples in the form\u00a0 ((term1, term2), count) : com_max = []\r # For each term, look for the most common co-occurrent terms \r for t1 in com:\r t1_max_terms = sorted (com[t1]., key = , reverse = True )[: 5 ]\r for t2, t2_count in t1_max_terms:\r , t2_count))\r # Get the most frequent co-occurrences \r terms_max = sorted (com_max, key = , reverse = True )\r print (terms_max[: 5 ])\r The results: [(( '6' , 'nations' ), 845 ), (( 'champions' , 'ireland' ), 760 ), \r (( 'nations' , 'rbs' ), 742 ), (( 'day' , 'ireland' ), 731 ), \r (( 'ireland' , 'wales' ), 674 )]\r This implementation is pretty straightforward, but depending on the data set and on the use of the matrix, one might want to look into tools like\u00a0 scipy.sparse \u00a0for building a sparse matrix. We could also look for a specific term and extract its most frequent co-occurrences. We simply need to modify the main loop including an extra counter, for example: search_word = sys.argv[ 1 ] # pass a term as a command-line argument \r count_search = \r for line in f:\r tweet = \r terms_only = [term for term in  \r if term not in stop \r and not )]\r if search_word in terms_only:\r \r print ( \"Co-occurrence for %s :\" % search_word)\r print ()\r The outcome for \u201cireland\u201d: [( 'champions' , 756 ), ( 'day' , 727 ), ( 'nations' , 659 ), ( 'wales' , 654 ), ( '2015' , 638 ), \r ( '6' , 613 ), ( 'rbs' , 585 ), ( '/y0nvsvayln' , 559 ), ( '__SHAMROCK_SYMBOL__' , 526 ), ( '10' , 522 ), \r ( 'win' , 377 ), ( 'england' , 377 ), ( 'twickenham' , 361 ), ( '40' , 360 ), ( 'points' , 356 ), \r ( 'sco' , 355 ), ( 'ire' , 355 ), ( 'title' , 346 ), ( 'scotland' , 301 ), ( 'turn' , 295 )]\r The outcome for \u201crugby\u201d: [( 'day' , 476 ), ( 'game' , 160 ), ( 'ireland' , 143 ), ( 'england' , 132 ), ( 'great' , 105 ), \r ( 'today' , 104 ), ( 'best' , 97 ), ( 'well' , 90 ), ( 'ever' , 89 ), ( 'incredible' , 87 ), \r ( 'amazing' , 84 ), ( 'done' , 82 ), ( 'amp' , 71 ), ( 'games' , 66 ), ( 'points' , 64 ), \r ( 'monumental' , 58 ), ( 'strap' , 56 ), ( 'world' , 55 ), ( 'team' , 55 ), ( '/bhmeorr19i' , 53 )]\r Overall, quite interesting. Summary \u00a0 This article has discussed a toy example of Text Mining on Twitter, using some realistic data taken during a sport event. Using what we have learnt in the previous episodes, we have downloaded some data using the streaming API, pre-processed the data in JSON format and extracted some interesting terms and hashtags from the tweets. The article has also introduced the concept of term co-occurrence, shown how to build a co-occurrence matrix and discussed how to use it to find some interesting insight. Bio: Marco Bonzanini is a Data Scientist based in London, UK. Active in the PyData community, he enjoys working in text analytics and data mining applications. He's the author of \" Mastering Social Media Mining with Python \" (Packt Publishing, July 2016). Original . Reposted with permission. Related : Mining Twitter Data with Python Part 1: Collecting Data Mining Twitter Data with Python Part 2: Text Pre-processing Mining Twitter Data with Python Part 3: Term Frequencies", "title_html": "<h1 id=\"title\">Mining Twitter Data with Python Part 4: Rugby and Term Co-occurrences</h1> ", "url": "https://www.kdnuggets.com/2016/06/mining-twitter-data-python-part-4.html", "tfidf": {"tfidf": {"after": 2.04140414042, "mark": 1.5079787234, "watch": 3.92581602374, "semant": 39.1034482759, "realist": 25.898858075, "relat": 2.47501753838, "nummb": 324.0, "termsmax": 2442.46153846, "form": 1.12755681818, "tnumcount": 2442.46153846, "unicod": 115.043478261, "new": 2.0357761108, "extra": 5.33826496301, "champion": 10.07360406092, "recognis": 5.93939393939, "number": 2.20285833218, "follow": 2.09280253098, "taken": 1.6012102874399998, "etc": 4.2066772655, "specif": 1.8719490626099997, "function": 2.495441685, "alphabet": 16.4008264463, "python": 225.1914893616, "well": 2.1311497416, "done": 2.3302509907499998, "searchword": 3663.6923076900002, "toy": 15.383720930199999, "their": 1.01547908405, "won": 2.31732593782, "particular": 2.7629655412400003, "ever": 1.9697270471500001, "creat": 2.4985835694, "multipl": 2.74813917258, "extend": 1.9604840701400004, "how": 3.20500656102, "repost": 933.882352941, "fair": 3.20533010297, "spars": 21.0, "good": 1.51981619759, "crunch": 355.43283582000004, "end": 3.3204127161300003, "frame": 6.280063291139999, "word": 1.7965372864099998, "interest": 14.429812159169998, "tutori": 118.9213483146, "depend": 2.2411067193700003, "will": 4.89924394384, "event": 7.678467788750001, "consid": 1.2397313759200002, "obvious": 6.44841592201, "work": 1.11520089913, "inum": 97.3987730061, "bhmeorrnumi": 1221.23076923, "scotland": 14.531807780320001, "tabl": 3.82093862816, "perform": 1.5313977042500002, "but": 5.0816208949499995, "need": 2.8745247148199997, "our": 2.35758835759, "final": 2.6801721955, "expect": 2.20011086475, "bleus": 933.882352941, "amaz": 15.250720461099998, "applic": 10.28016404058, "sport": 3.29651162791, "termnum": 2442.46153846, "has": 6.261898501199999, "enough": 4.463939266140001, "stop": 4.3567508232800005, "pass": 1.61818367139, "scovir": 1221.23076923, "bonzanini": 2442.46153846, "annual": 2.280706795, "stringpunctu": 1221.23076923, "enjoy": 3.3269069572500003, "format": 5.0625, "quit": 2.8849718335500003, "concept": 5.31414225942, "cooccurr": 15875.999999990001, "azzurri": 1221.23076923, "dig": 18.5034965035, "tradit": 1.60802187785, "count": 10.44473684211, "learnt": 56.9032258065, "print": 9.89158878504, "not": 5.07836990595, "far": 3.42044597652, "involv": 1.4498630137000001, "textual": 41.4516971279, "respect": 1.6443293630200002, "whole": 2.29488291414, "charact": 2.51720310766, "happen": 2.96359902931, "quick": 2.205, "shown": 2.76923076923, "english": 3.4865488086000003, "then": 1.08657860516, "twickenham": 260.262295082, "consult": 5.21721984883, "last": 7.270340406060001, "social": 1.9904714142400002, "world": 1.11340206186, "scienc": 2.31969608416, "outcom": 14.97735849056, "major": 1.14852058164, "tokenis": 1221.23076923, "twice": 3.7638691322900004, "day": 11.837160751599999, "mine": 34.13144963143999, "twitter": 199.28033472780004, "pair": 4.36873968079, "len": 41.505882353000004, "such": 2.12302754748, "those": 1.19548192771, "three": 3.1986568166700002, "lot": 4.40877534018, "some": 8.32293577984, "small": 2.7189587258, "collect": 4.92329956584, "baselin": 57.7309090909, "commandlin": 1221.23076923, "result": 1.14611608432, "com": 798.7924528304, "content": 3.5421686747, "use": 10.296387573799999, "receiv": 2.6109694926400002, "media": 2.59369384088, "from": 10.0056721497, "account": 1.94463498285, "tweet": 1206.9473684216, "num": 98.03087392098, "editor": 4.33060556465, "miss": 3.53664513255, "incred": 18.227324913900002, "exclud": 5.31859296482, "for": 30.009451200300003, "ireland": 57.220183486290004, "discuss": 13.18057285182, "match": 24.973483146080003, "seen": 1.61079545455, "with": 14.016774925859997, "competit": 12.2784222738, "episod": 4.435875943, "shamrock": 189.0, "are": 4.11962374312, "true": 5.11139729556, "better": 2.0065722952500002, "frequent": 10.525059665850002, "preprocess": 3663.6923076900002, "look": 9.543159413299998, "rbsnumnat": 2442.46153846, "option": 4.04896710023, "summari": 7.80147420147, "pointer": 68.4310344828, "triangular": 37.1803278689, "veri": 1.25880114177, "extract": 23.109170305680003, "intern": 1.30355530011, "ago": 6.05954198473, "nicknam": 5.30083472454, "master": 3.15125049623, "wale": 59.294117647040004, "remov": 2.0058117498400003, "lowercas": 162.0, "give": 1.3653250774, "great": 1.26592775696, "marco": 37.4876033058, "wnum": 675.574468086, "juli": 1.43466473884, "big": 2.7400759406299997, "note": 1.42449528937, "commax": 2442.46153846, "probabl": 5.29111814698, "scientist": 4.69426374926, "into": 2.03004922958, "italian": 3.63878065551, "turn": 2.7677824267799997, "set": 4.74831763124, "franc": 11.615452151000001, "tnummaxterm": 2442.46153846, "what": 3.7603031738399997, "highest": 2.50212765957, "saturday": 7.367053364269999, "process": 1.69524826482, "main": 2.50607734806, "find": 1.7294117647099998, "sco": 170.709677419, "instead": 1.59461631177, "line": 2.8365195640599996, "welsh": 14.112, "base": 1.14628158845, "occur": 1.7453825857499998, "deal": 2.18346857379, "fact": 1.73375559681, "who": 1.06279287723, "decid": 3.8515283842800003, "permiss": 6.280063291139999, "preserv": 3.1062414400300002, "post": 2.23826307627, "here": 2.42307692308, "punctuat": 42.1114058355, "ire": 89.19101123600001, "attent": 2.81040892193, "titl": 9.36305732485, "make": 1.0762660158600001, "togeth": 1.58095996813, "contain": 7.9907388766, "about": 7.45402106113, "stream": 13.023789991800001, "communiti": 1.96121062384, "context": 8.51945264288, "search": 3.2539454806299997, "french": 4.23925233644, "disambigu": 97.3987730061, "simpli": 2.5192002538900002, "assum": 2.9575260804799997, "dure": 4.201389348319999, "frequenc": 26.430632630399998, "should": 4.99297620297, "activ": 1.46403541129, "rememb": 4.88793103448, "tool": 4.99716713881, "start": 1.26673581744, "articl": 12.108300495719998, "scipyspars": 1221.23076923, "insight": 23.6074349442, "bigram": 1221.23076923, "gaelic": 54.3698630137, "observ": 6.67339218159, "order": 6.23125834055, "expand": 2.2260235558000003, "tnum": 115.8832116789, "until": 1.14852058164, "support": 1.2685577307200002, "play": 2.92780082988, "had": 2.0951501154799996, "string": 8.37783641161, "over": 1.02525024217, "engvfra": 3663.6923076900002, "build": 9.805043746800001, "analysi": 6.95705521472, "itavw": 1221.23076923, "team": 11.374122367100002, "emoji": 1221.23076923, "same": 3.35573874444, "itali": 11.35893155259, "matrix": 158.3076923078, "possibl": 2.8347468976, "modifi": 4.45329593268, "unsurpris": 128.032258065, "best": 1.5828514456600002, "origin": 1.13724928367, "continu": 1.13928955867, "previous": 7.1423429908000005, "might": 2.1561863370900003, "name": 3.3063519611100003, "code": 3.8807137619199996, "than": 2.0655737705, "publish": 1.36885669943, "entri": 3.9909502262400003, "languag": 6.88464874242, "also": 7.10335570469, "around": 2.42789417342, "mean": 2.89813800658, "symbol": 10.253606028, "revers": 8.59788789602, "notic": 4.36994219653, "irish": 5.56077057793, "england": 20.52157052832, "tupl": 661.5, "the": 111.0, "open": 1.24556723678, "six": 3.1105015674, "want": 7.98792452832, "captur": 5.76052249638, "argument": 5.09335899904, "individu": 1.8004082558400003, "token": 101.12101910819999, "minut": 9.336992746529999, "rugbi": 90.5721271392, "term": 46.041655681559995, "other": 2.01984732824, "termson": 9769.84615384, "les": 9.50658682635, "one": 1.00627495722, "sequenti": 39.5910224439, "becaus": 2.2990369994999997, "loop": 27.0229787234, "get": 1.78562591385, "like": 2.2983713355, "implement": 3.57648118946, "moment": 4.262013422819999, "stopword": 3663.6923076900002, "defaultdict": 1221.23076923, "file": 3.7710213776699995, "part": 7.30314779523, "independ": 1.58950740889, "analyt": 17.256521739100002, "memori": 2.57392996109, "trash": 43.3770491803, "includ": 4.076256499119999, "shot": 3.85339805825, "everyth": 4.81967213115, "straightforward": 27.7552447552, "amp": 72.8256880734, "this": 14.05311077394, "countsearch": 1221.23076923, "championship": 6.837209302330001, "rang": 3.5696458684599994, "time": 3.03382381044, "leav": 1.6615384615399997, "worri": 10.302401038300001, "similar": 1.37514075357, "differ": 2.4730898045, "been": 3.0717832957199995, "win": 8.25871336917, "most": 9.18868167207, "hashtag": 3969.0, "domain": 9.39408284024, "suspensionpoint": 1221.23076923, "monument": 6.74426508071, "all": 2.02293577982, "top": 1.8387769284200002, "today": 1.74961428257, "someth": 9.84456386937, "there": 1.04091266719, "xvdefranc": 1221.23076923, "case": 1.48498737256, "inner": 6.432739059969999, "author": 1.4229631621399998, "sort": 20.752941176479997, "more": 4.0686827268, "and": 16.00100787408, "angfra": 2442.46153846, "list": 12.26893353939, "correspond": 3.32481675393, "packt": 1221.23076923, "london": 1.97782484116, "amount": 2.27027027027, "could": 2.4087391898, "reus": 29.7861163227, "befor": 2.20072082062, "exploratori": 68.4310344828, "point": 5.03960003176, "download": 73.22878228799999, "close": 1.2848818387799998, "can": 5.8813069571, "pydata": 1221.23076923, "counter": 6.77592829706, "strap": 48.6993865031, "rbs": 1094.896551724, "refactor": 1221.23076923, "onli": 2.0512953033200003, "each": 2.37949640288, "ynumnvsvayln": 1221.23076923, "sysargv": 1221.23076923, "key": 4.5601034037, "unfamiliar": 37.7102137767, "pretti": 15.75, "common": 7.012987012999999, "data": 60.77584006812, "sometim": 1.7126213592200001, "bio": 42.336000000000006, "overal": 3.0442953020099996, "exampl": 4.51450236966, "that": 10.0398406375, "text": 12.51310344828, "introduc": 5.177519295570001, "fourth": 2.5647819063, "import": 2.6803984467400004, "first": 2.01523229246, "game": 5.15957101072, "have": 6.089369046839999, "while": 4.176795580119999, "dramat": 3.9849397590400004, "without": 1.29547123623, "suggest": 1.7571665744299998, "mention": 5.07788261634, "nation": 4.90529893404}, "logtfidf": {"after": 0.040981389296199995, "mark": 0.410770160338, "watch": 1.36757423376, "semant": 3.6662106543, "realist": 5.12210339482, "relat": 0.42620060330799997, "nummb": 5.78074351579, "termsmax": 14.215228912879999, "form": 0.120053184191, "tnumcount": 14.215228912879999, "unicod": 4.74531012875, "new": 0.0354598937022, "extra": 1.67490068688, "champion": 3.2335427258599996, "recognis": 1.78160709776, "number": 0.1932171568372, "follow": 0.09071382218839999, "taken": 0.470759772949, "etc": 1.4366730879700003, "specif": 0.626980167541, "function": 0.914465741594, "alphabet": 2.79733172663, "python": 16.12262697184, "well": 0.1270288766312, "done": 0.845975983129, "searchword": 21.322843369319997, "toy": 2.73330986786, "their": 0.015360505122700001, "won": 0.8404139079, "particular": 0.646314787608, "ever": 0.6778949784020001, "creat": 0.445153637028, "multipl": 1.01092401812, "extend": 0.673191417311, "how": 0.9431339138600001, "repost": 6.83935046985, "fair": 1.16481508131, "spars": 3.04452243772, "good": 0.418589404907, "crunch": 14.324172041850002, "end": 0.304430395854, "frame": 1.8373800586400002, "word": 0.585861082385, "interest": 4.248646001838, "tutori": 8.170630311, "depend": 0.806969815, "will": 0.81114613966, "event": 2.144910540735, "consid": 0.214894723824, "obvious": 1.86383450716, "work": 0.109034567273, "inum": 4.5788136131, "bhmeorrnumi": 7.1076144564399995, "scotland": 3.96638541274, "tabl": 1.34049610661, "perform": 0.42618085058, "but": 0.0809618603595, "need": 0.725480326884, "our": 0.8576392141820001, "final": 0.585467727896, "expect": 0.78850775216, "bleus": 6.83935046985, "amaz": 2.7246267452900006, "applic": 3.6948117854699998, "sport": 1.19286482691, "termnum": 14.215228912879999, "has": 0.2563436691288, "enough": 1.605768878338, "stop": 1.557158749926, "pass": 0.48130432974, "scovir": 7.1076144564399995, "bonzanini": 14.215228912879999, "annual": 0.8244853927339999, "stringpunctu": 7.1076144564399995, "enjoy": 1.2020430306899998, "format": 1.8574265037459998, "quit": 1.05951513684, "concept": 1.954448874206, "cooccurr": 92.39898793372, "azzurri": 7.1076144564399995, "dig": 2.91795971441, "tradit": 0.47500477629199994, "count": 3.7424577341700003, "learnt": 4.04135203208, "print": 3.57921746901, "not": 0.0777620650375, "far": 1.073247529006, "involv": 0.371469078658, "textual": 3.7245288247199992, "respect": 0.49733261904, "whole": 0.8306818244059999, "charact": 0.923148407239, "happen": 1.08640441802, "quick": 0.790727508899, "shown": 1.01856958099, "english": 1.11153037267, "then": 0.08303386523089999, "twickenham": 5.561689949730001, "consult": 1.6519646640099999, "last": 1.1522618676660001, "social": 0.688371502261, "world": 0.107420248621, "scienc": 0.841436178891, "outcom": 4.02678489248, "major": 0.138474663439, "tokenis": 7.1076144564399995, "twice": 1.32544745286, "day": 1.6865870631700002, "mine": 11.090163607460001, "twitter": 21.017718488459998, "pair": 1.47447456495, "len": 6.06537596016, "such": 0.119391955612, "those": 0.17854939087299998, "three": 0.1923560646747, "lot": 1.4835969502500002, "some": 0.316588072516, "small": 0.614203610118, "collect": 1.48609998156, "baselin": 4.05579271624, "commandlin": 7.1076144564399995, "result": 0.136378908381, "com": 36.82927689344, "content": 1.26473915954, "use": 0.292080197316, "receiv": 0.533148849844, "media": 0.9530830530519999, "from": 0.00567054168866, "account": 0.665074289973, "tweet": 58.9017033462, "num": 0.030869058748906004, "editor": 1.4657073855, "miss": 1.2631785751200002, "incred": 2.9029218370499996, "exclud": 1.67120878808, "for": 0.009449711861910001, "ireland": 18.1391256365, "discuss": 4.7219071357199995, "match": 8.90333107118, "seen": 0.47672812813, "with": 0.01676488398746, "competit": 4.48619629604, "episod": 1.48972510314, "shamrock": 5.24174701506, "are": 0.1178698943308, "true": 1.876651259268, "better": 0.6964279406, "frequent": 3.721605680425, "preprocess": 21.322843369319997, "look": 3.2319334680000003, "rbsnumnat": 14.215228912879999, "option": 1.39846181161, "summari": 2.0543127160299997, "pointer": 4.225826442240001, "triangular": 3.61577980067, "veri": 0.230159793238, "extract": 6.124851699030001, "intern": 0.265095377816, "ago": 1.80163421715, "nicknam": 1.66786430335, "master": 1.14779935699, "wale": 16.0245485032, "remov": 0.6960488415880001, "lowercas": 5.08759633523, "give": 0.311392552224, "great": 0.235805258079, "marco": 5.8617262385, "wnum": 28.34282354112, "juli": 0.360931190445, "big": 1.00798563557, "note": 0.353817568083, "commax": 14.215228912879999, "probabl": 1.945764825826, "scientist": 1.54634128444, "into": 0.0298257264574, "italian": 1.2916486407799999, "turn": 0.649798502128, "set": 0.685984045156, "franc": 4.214491906139999, "tnummaxterm": 14.215228912879999, "what": 0.677661890481, "highest": 0.917141433754, "saturday": 1.99701781137, "process": 0.527829199025, "main": 0.451143081176, "find": 0.547781330288, "sco": 5.13996432075, "instead": 0.46663315041500003, "line": 0.698861228904, "welsh": 2.64702549926, "base": 0.13652330228700002, "occur": 0.556973778473, "deal": 0.780914701253, "fact": 0.5502899207949999, "who": 0.0609002329859, "decid": 1.310645743786, "permiss": 1.8373800586400002, "preserv": 1.13341345513, "post": 0.8057001527009999, "here": 0.8850381883700001, "punctuat": 3.7403186264499997, "ire": 4.49078026361, "attent": 1.03332999658, "titl": 3.1366698099499994, "make": 0.07349765782289999, "togeth": 0.458032237308, "contain": 2.34422659118, "about": 0.43990434232220005, "stream": 3.7472610077199997, "communiti": 0.673561947791, "context": 2.89840982884, "search": 1.1798682540899998, "french": 1.502479474784, "disambigu": 4.5788136131, "simpli": 0.923941491586, "assum": 1.08435313525, "dure": 0.1964836267576, "frequenc": 6.52773412719, "should": 1.5282596302740001, "activ": 0.381196603284, "rememb": 1.5867691126199999, "tool": 1.60887117963, "start": 0.236443369291, "articl": 4.212790437444, "scipyspars": 7.1076144564399995, "insight": 4.93682904374, "bigram": 7.1076144564399995, "gaelic": 3.9958100116300006, "observ": 2.398548044796, "order": 1.1007019039650001, "expand": 0.80021683492, "tnum": 10.96191179823, "until": 0.138474663439, "support": 0.237880610037, "play": 0.7622087812839999, "had": 0.0929560488222, "string": 2.1255896963900005, "over": 0.0249367214957, "engvfra": 21.322843369319997, "build": 2.9468247125460003, "analysi": 2.4932182058400003, "itavw": 7.1076144564399995, "team": 4.10951447443, "emoji": 7.1076144564399995, "same": 0.336178948812, "itali": 3.9941762001899996, "matrix": 21.83041286916, "possibl": 0.697610949782, "modifi": 1.4936444810499998, "unsurpris": 4.8522822483, "best": 0.459227932947, "origin": 0.128612437587, "continu": 0.13040487398700001, "previous": 1.7830148003150001, "might": 0.7683410765340001, "name": 0.29169949915290005, "code": 1.35601909597, "than": 0.0645217244364, "publish": 0.313975865467, "entri": 1.38402935449, "languag": 2.492045473218, "also": 0.1026001046, "around": 0.38775421156400003, "mean": 0.74184256704, "symbol": 3.68705148798, "revers": 2.9167387981, "notic": 1.47474978168, "irish": 1.7157366918099999, "england": 7.536280097312, "tupl": 6.4945099835599995, "the": 0.0, "open": 0.219591038029, "six": 0.883273616636, "want": 2.7665464250199996, "captur": 2.1157620024200003, "argument": 1.62793753414, "individu": 0.588013447985, "token": 10.553117159670002, "minut": 3.4061158079399996, "rugbi": 17.92165458433, "term": 10.990286457018001, "other": 0.01974949583952, "termson": 56.860915651519996, "les": 2.25198490849, "one": 0.0062553516455, "sequenti": 3.6786023866, "becaus": 0.27868631765, "loop": 5.20708077464, "get": 0.579769005782, "like": 0.27810715309, "implement": 1.27437940907, "moment": 1.4497416830899998, "stopword": 21.322843369319997, "defaultdict": 7.1076144564399995, "file": 1.32734588723, "part": 0.2967671768796, "independ": 0.463424162503, "analyt": 2.8481901438599997, "memori": 0.9454338986599999, "trash": 3.7699304805000002, "includ": 0.075538725562, "shot": 1.3489553715600002, "everyth": 1.57270590317, "straightforward": 3.3234248225200003, "amp": 4.28806875111, "this": 0.053010286734999995, "countsearch": 7.1076144564399995, "championship": 1.92237965165, "rang": 1.158638427606, "time": 0.0336345565878, "leav": 0.507743957229, "worri": 2.3323769785799997, "similar": 0.318556092114, "differ": 0.424642242624, "been": 0.07093794710520002, "win": 3.0379695608699997, "most": 0.18673106666039999, "hashtag": 49.6546232888, "domain": 2.24008000599, "suspensionpoint": 7.1076144564399995, "monument": 1.9086925260799998, "all": 0.022805264195599997, "top": 0.609100637788, "today": 0.559395353679, "someth": 3.56492136819, "there": 0.0400978929255, "xvdefranc": 7.1076144564399995, "case": 0.395406268889, "inner": 1.86140042888, "author": 0.35274143130999996, "sort": 6.58557447584, "more": 0.06809972639999999, "and": 0.0010078422730176, "angfra": 14.215228912879999, "list": 2.7886118535540003, "correspond": 1.20141456099, "packt": 7.1076144564399995, "london": 0.6819976757709999, "amount": 0.819898886199, "could": 0.37191254458000006, "reus": 3.3940423897400005, "befor": 0.191275543759, "exploratori": 4.225826442240001, "point": 0.9241294361319999, "download": 13.4207531595, "close": 0.250666759864, "can": 0.8117054819699999, "pydata": 7.1076144564399995, "counter": 1.9133763754, "strap": 3.88566643254, "rbs": 12.610535967839999, "refactor": 7.1076144564399995, "onli": 0.050648536658199995, "each": 0.347483378608, "ynumnvsvayln": 7.1076144564399995, "sysargv": 7.1076144564399995, "key": 1.64839623792, "unfamiliar": 3.6299309802199997, "pretti": 2.75684036527, "common": 1.6916290263549998, "data": 21.9027705264, "sometim": 0.538025155343, "bio": 3.7456377879300002, "overal": 1.1132694464700001, "exampl": 1.2260480249969998, "that": 0.039761483796399995, "text": 4.56192803996, "introduc": 1.6371412572780002, "fourth": 0.941873448224, "import": 0.585636554132, "first": 0.015174579624319999, "game": 1.8954125160420001, "have": 0.0887100140472, "while": 0.17299993517520004, "dramat": 1.3825221952000002, "without": 0.258874517941, "suggest": 0.563702610877, "mention": 1.863494372672, "nation": 0.816086699192}, "logidf": {"after": 0.020490694648099998, "mark": 0.410770160338, "watch": 1.36757423376, "semant": 3.6662106543, "realist": 2.56105169741, "relat": 0.21310030165399999, "nummb": 5.78074351579, "termsmax": 7.1076144564399995, "form": 0.120053184191, "tnumcount": 7.1076144564399995, "unicod": 4.74531012875, "new": 0.0177299468511, "extra": 1.67490068688, "champion": 1.6167713629299998, "recognis": 1.78160709776, "number": 0.0966085784186, "follow": 0.045356911094199995, "taken": 0.470759772949, "etc": 1.4366730879700003, "specif": 0.626980167541, "function": 0.914465741594, "alphabet": 2.79733172663, "python": 4.03065674296, "well": 0.0635144383156, "done": 0.845975983129, "searchword": 7.1076144564399995, "toy": 2.73330986786, "their": 0.015360505122700001, "won": 0.8404139079, "particular": 0.323157393804, "ever": 0.6778949784020001, "creat": 0.222576818514, "multipl": 1.01092401812, "extend": 0.673191417311, "how": 0.47156695693000006, "repost": 6.83935046985, "fair": 1.16481508131, "spars": 3.04452243772, "good": 0.418589404907, "crunch": 4.77472401395, "end": 0.101476798618, "frame": 1.8373800586400002, "word": 0.585861082385, "interest": 0.47207177798199995, "tutori": 4.0853151555, "depend": 0.806969815, "will": 0.202786534915, "event": 0.428982108147, "consid": 0.214894723824, "obvious": 1.86383450716, "work": 0.109034567273, "inum": 4.5788136131, "bhmeorrnumi": 7.1076144564399995, "scotland": 1.98319270637, "tabl": 1.34049610661, "perform": 0.42618085058, "but": 0.0161923720719, "need": 0.362740163442, "our": 0.8576392141820001, "final": 0.292733863948, "expect": 0.78850775216, "bleus": 6.83935046985, "amaz": 2.7246267452900006, "applic": 1.23160392849, "sport": 1.19286482691, "termnum": 7.1076144564399995, "has": 0.0427239448548, "enough": 0.802884439169, "stop": 0.778579374963, "pass": 0.48130432974, "scovir": 7.1076144564399995, "bonzanini": 7.1076144564399995, "annual": 0.8244853927339999, "stringpunctu": 7.1076144564399995, "enjoy": 1.2020430306899998, "format": 0.9287132518729999, "quit": 1.05951513684, "concept": 0.977224437103, "cooccurr": 7.1076144564399995, "azzurri": 7.1076144564399995, "dig": 2.91795971441, "tradit": 0.47500477629199994, "count": 1.24748591139, "learnt": 4.04135203208, "print": 1.19307248967, "not": 0.0155524130075, "far": 0.536623764503, "involv": 0.371469078658, "textual": 3.7245288247199992, "respect": 0.49733261904, "whole": 0.8306818244059999, "charact": 0.923148407239, "happen": 1.08640441802, "quick": 0.790727508899, "shown": 1.01856958099, "english": 0.555765186335, "then": 0.08303386523089999, "twickenham": 5.561689949730001, "consult": 1.6519646640099999, "last": 0.19204364461100001, "social": 0.688371502261, "world": 0.107420248621, "scienc": 0.841436178891, "outcom": 2.01339244624, "major": 0.138474663439, "tokenis": 7.1076144564399995, "twice": 1.32544745286, "day": 0.16865870631700003, "mine": 1.58430908678, "twitter": 3.50295308141, "pair": 1.47447456495, "len": 3.03268798008, "such": 0.059695977806, "those": 0.17854939087299998, "three": 0.06411868822490001, "lot": 1.4835969502500002, "some": 0.0395735090645, "small": 0.307101805059, "collect": 0.49536666052, "baselin": 4.05579271624, "commandlin": 7.1076144564399995, "result": 0.136378908381, "com": 4.60365961168, "content": 1.26473915954, "use": 0.0292080197316, "receiv": 0.266574424922, "media": 0.9530830530519999, "from": 0.000567054168866, "account": 0.665074289973, "tweet": 4.5309002574, "num": 0.00031499039539700004, "editor": 1.4657073855, "miss": 1.2631785751200002, "incred": 2.9029218370499996, "exclud": 1.67120878808, "for": 0.00031499039539700004, "ireland": 1.6490114215, "discuss": 0.78698452262, "match": 1.27190443874, "seen": 0.47672812813, "with": 0.00119749171339, "competit": 1.12154907401, "episod": 1.48972510314, "shamrock": 5.24174701506, "are": 0.0294674735827, "true": 0.938325629634, "better": 0.6964279406, "frequent": 0.7443211360850001, "preprocess": 7.1076144564399995, "look": 0.6463866936, "rbsnumnat": 7.1076144564399995, "option": 1.39846181161, "summari": 2.0543127160299997, "pointer": 4.225826442240001, "triangular": 3.61577980067, "veri": 0.230159793238, "extract": 2.04161723301, "intern": 0.265095377816, "ago": 1.80163421715, "nicknam": 1.66786430335, "master": 1.14779935699, "wale": 2.0030685629, "remov": 0.6960488415880001, "lowercas": 5.08759633523, "give": 0.311392552224, "great": 0.235805258079, "marco": 2.93086311925, "wnum": 4.72380392352, "juli": 0.360931190445, "big": 1.00798563557, "note": 0.353817568083, "commax": 7.1076144564399995, "probabl": 0.972882412913, "scientist": 1.54634128444, "into": 0.0149128632287, "italian": 1.2916486407799999, "turn": 0.324899251064, "set": 0.171496011289, "franc": 0.8428983812279999, "tnummaxterm": 7.1076144564399995, "what": 0.225887296827, "highest": 0.917141433754, "saturday": 1.99701781137, "process": 0.527829199025, "main": 0.225571540588, "find": 0.547781330288, "sco": 5.13996432075, "instead": 0.46663315041500003, "line": 0.349430614452, "welsh": 2.64702549926, "base": 0.13652330228700002, "occur": 0.556973778473, "deal": 0.780914701253, "fact": 0.5502899207949999, "who": 0.0609002329859, "decid": 0.655322871893, "permiss": 1.8373800586400002, "preserv": 1.13341345513, "post": 0.8057001527009999, "here": 0.8850381883700001, "punctuat": 3.7403186264499997, "ire": 4.49078026361, "attent": 1.03332999658, "titl": 0.6273339619899999, "make": 0.07349765782289999, "togeth": 0.458032237308, "contain": 0.468845318236, "about": 0.0628434774746, "stream": 1.8736305038599999, "communiti": 0.673561947791, "context": 1.44920491442, "search": 1.1798682540899998, "french": 0.751239737392, "disambigu": 4.5788136131, "simpli": 0.923941491586, "assum": 1.08435313525, "dure": 0.0491209066894, "frequenc": 2.1759113757299997, "should": 0.509419876758, "activ": 0.381196603284, "rememb": 1.5867691126199999, "tool": 1.60887117963, "start": 0.236443369291, "articl": 0.702131739574, "scipyspars": 7.1076144564399995, "insight": 2.46841452187, "bigram": 7.1076144564399995, "gaelic": 3.9958100116300006, "observ": 0.7995160149320001, "order": 0.22014038079300002, "expand": 0.80021683492, "tnum": 3.65397059941, "until": 0.138474663439, "support": 0.237880610037, "play": 0.38110439064199997, "had": 0.0464780244111, "string": 2.1255896963900005, "over": 0.0249367214957, "engvfra": 7.1076144564399995, "build": 0.491137452091, "analysi": 1.2466091029200002, "itavw": 7.1076144564399995, "team": 0.821902894886, "emoji": 7.1076144564399995, "same": 0.112059649604, "itali": 1.3313920667299999, "matrix": 3.1186304098799997, "possibl": 0.348805474891, "modifi": 1.4936444810499998, "unsurpris": 4.8522822483, "best": 0.459227932947, "origin": 0.128612437587, "continu": 0.13040487398700001, "previous": 0.356602960063, "might": 0.7683410765340001, "name": 0.09723316638430002, "code": 1.35601909597, "than": 0.0322608622182, "publish": 0.313975865467, "entri": 1.38402935449, "languag": 0.8306818244059999, "also": 0.0146571578, "around": 0.19387710578200001, "mean": 0.37092128352, "symbol": 1.22901716266, "revers": 1.45836939905, "notic": 1.47474978168, "irish": 1.7157366918099999, "england": 0.942035012164, "tupl": 6.4945099835599995, "the": 0.0, "open": 0.219591038029, "six": 0.441636808318, "want": 0.6916366062549999, "captur": 1.0578810012100002, "argument": 1.62793753414, "individu": 0.588013447985, "token": 3.5177057198900004, "minut": 1.1353719359799999, "rugbi": 2.56023636919, "term": 0.33303898354600003, "other": 0.00987474791976, "termson": 7.1076144564399995, "les": 2.25198490849, "one": 0.0062553516455, "sequenti": 3.6786023866, "becaus": 0.139343158825, "loop": 2.60354038732, "get": 0.579769005782, "like": 0.139053576545, "implement": 1.27437940907, "moment": 1.4497416830899998, "stopword": 7.1076144564399995, "defaultdict": 7.1076144564399995, "file": 1.32734588723, "part": 0.04239531098280001, "independ": 0.463424162503, "analyt": 2.8481901438599997, "memori": 0.9454338986599999, "trash": 3.7699304805000002, "includ": 0.0188846813905, "shot": 1.3489553715600002, "everyth": 1.57270590317, "straightforward": 3.3234248225200003, "amp": 4.28806875111, "this": 0.0037864490525, "countsearch": 7.1076144564399995, "championship": 1.92237965165, "rang": 0.579319213803, "time": 0.0112115188626, "leav": 0.507743957229, "worri": 2.3323769785799997, "similar": 0.318556092114, "differ": 0.212321121312, "been": 0.023645982368400004, "win": 1.01265652029, "most": 0.020747896295599998, "hashtag": 6.2068279111, "domain": 2.24008000599, "suspensionpoint": 7.1076144564399995, "monument": 1.9086925260799998, "all": 0.011402632097799998, "top": 0.609100637788, "today": 0.559395353679, "someth": 1.18830712273, "there": 0.0400978929255, "xvdefranc": 7.1076144564399995, "case": 0.395406268889, "inner": 1.86140042888, "author": 0.35274143130999996, "sort": 1.64639361896, "more": 0.017024931599999998, "and": 6.29901420636e-05, "angfra": 7.1076144564399995, "list": 0.309845761506, "correspond": 1.20141456099, "packt": 7.1076144564399995, "london": 0.6819976757709999, "amount": 0.819898886199, "could": 0.18595627229000003, "reus": 3.3940423897400005, "befor": 0.0956377718795, "exploratori": 4.225826442240001, "point": 0.23103235903299998, "download": 2.6841506319, "close": 0.250666759864, "can": 0.162341096394, "pydata": 7.1076144564399995, "counter": 1.9133763754, "strap": 3.88566643254, "rbs": 6.305267983919999, "refactor": 7.1076144564399995, "onli": 0.025324268329099998, "each": 0.173741689304, "ynumnvsvayln": 7.1076144564399995, "sysargv": 7.1076144564399995, "key": 0.82419811896, "unfamiliar": 3.6299309802199997, "pretti": 2.75684036527, "common": 0.338325805271, "data": 1.2168205848, "sometim": 0.538025155343, "bio": 3.7456377879300002, "overal": 1.1132694464700001, "exampl": 0.40868267499899996, "that": 0.00397614837964, "text": 1.14048200999, "introduc": 0.5457137524260001, "fourth": 0.941873448224, "import": 0.292818277066, "first": 0.0075872898121599995, "game": 0.9477062580210001, "have": 0.0147850023412, "while": 0.04324998379380001, "dramat": 1.3825221952000002, "without": 0.258874517941, "suggest": 0.563702610877, "mention": 0.931747186336, "nation": 0.204021674798}, "freq": {"after": 2, "mark": 1, "watch": 1, "semant": 1, "realist": 2, "relat": 2, "nummb": 1, "termsmax": 2, "form": 1, "tnumcount": 2, "unicod": 1, "new": 2, "extra": 1, "champion": 2, "recognis": 1, "number": 2, "follow": 2, "taken": 1, "etc": 1, "specif": 1, "function": 1, "alphabet": 1, "python": 4, "well": 2, "done": 1, "searchword": 3, "toy": 1, "their": 1, "won": 1, "particular": 2, "ever": 1, "creat": 2, "multipl": 1, "extend": 1, "how": 2, "repost": 1, "fair": 1, "spars": 1, "good": 1, "crunch": 3, "end": 3, "frame": 1, "word": 1, "interest": 9, "tutori": 2, "depend": 1, "will": 4, "event": 5, "consid": 1, "obvious": 1, "work": 1, "inum": 1, "bhmeorrnumi": 1, "scotland": 2, "tabl": 1, "perform": 1, "but": 5, "need": 2, "our": 1, "final": 2, "expect": 1, "bleus": 1, "amaz": 1, "applic": 3, "sport": 1, "termnum": 2, "has": 6, "enough": 2, "stop": 2, "pass": 1, "scovir": 1, "bonzanini": 2, "annual": 1, "stringpunctu": 1, "enjoy": 1, "format": 2, "quit": 1, "concept": 2, "cooccurr": 13, "azzurri": 1, "dig": 1, "tradit": 1, "count": 3, "learnt": 1, "print": 3, "not": 5, "far": 2, "involv": 1, "textual": 1, "respect": 1, "whole": 1, "charact": 1, "happen": 1, "quick": 1, "shown": 1, "english": 2, "then": 1, "twickenham": 1, "consult": 1, "last": 6, "social": 1, "world": 1, "scienc": 1, "outcom": 2, "major": 1, "tokenis": 1, "twice": 1, "day": 10, "mine": 7, "twitter": 6, "pair": 1, "len": 2, "such": 2, "those": 1, "three": 3, "lot": 1, "some": 8, "small": 2, "collect": 3, "baselin": 1, "commandlin": 1, "result": 1, "com": 8, "content": 1, "use": 10, "receiv": 2, "media": 1, "from": 10, "account": 1, "tweet": 13, "num": 98, "editor": 1, "miss": 1, "incred": 1, "exclud": 1, "for": 30, "ireland": 11, "discuss": 6, "match": 7, "seen": 1, "with": 14, "competit": 4, "episod": 1, "shamrock": 1, "are": 4, "true": 2, "better": 1, "frequent": 5, "preprocess": 3, "look": 5, "rbsnumnat": 2, "option": 1, "summari": 1, "pointer": 1, "triangular": 1, "veri": 1, "extract": 3, "intern": 1, "ago": 1, "nicknam": 1, "master": 1, "wale": 8, "remov": 1, "lowercas": 1, "give": 1, "great": 1, "marco": 2, "wnum": 6, "juli": 1, "big": 1, "note": 1, "commax": 2, "probabl": 2, "scientist": 1, "into": 2, "italian": 1, "turn": 2, "set": 4, "franc": 5, "tnummaxterm": 2, "what": 3, "highest": 1, "saturday": 1, "process": 1, "main": 2, "find": 1, "sco": 1, "instead": 1, "line": 2, "welsh": 1, "base": 1, "occur": 1, "deal": 1, "fact": 1, "who": 1, "decid": 2, "permiss": 1, "preserv": 1, "post": 1, "here": 1, "punctuat": 1, "ire": 1, "attent": 1, "titl": 5, "make": 1, "togeth": 1, "contain": 5, "about": 7, "stream": 2, "communiti": 1, "context": 2, "search": 1, "french": 2, "disambigu": 1, "simpli": 1, "assum": 1, "dure": 4, "frequenc": 3, "should": 3, "activ": 1, "rememb": 1, "tool": 1, "start": 1, "articl": 6, "scipyspars": 1, "insight": 2, "bigram": 1, "gaelic": 1, "observ": 3, "order": 5, "expand": 1, "tnum": 3, "until": 1, "support": 1, "play": 2, "had": 2, "string": 1, "over": 1, "engvfra": 3, "build": 6, "analysi": 2, "itavw": 1, "team": 5, "emoji": 1, "same": 3, "itali": 3, "matrix": 7, "possibl": 2, "modifi": 1, "unsurpris": 1, "best": 1, "origin": 1, "continu": 1, "previous": 5, "might": 1, "name": 3, "code": 1, "than": 2, "publish": 1, "entri": 1, "languag": 3, "also": 7, "around": 2, "mean": 2, "symbol": 3, "revers": 2, "notic": 1, "irish": 1, "england": 8, "tupl": 1, "the": 111, "open": 1, "six": 2, "want": 4, "captur": 2, "argument": 1, "individu": 1, "token": 3, "minut": 3, "rugbi": 7, "term": 33, "other": 2, "termson": 8, "les": 1, "one": 1, "sequenti": 1, "becaus": 2, "loop": 2, "get": 1, "like": 2, "implement": 1, "moment": 1, "stopword": 3, "defaultdict": 1, "file": 1, "part": 7, "independ": 1, "analyt": 1, "memori": 1, "trash": 1, "includ": 4, "shot": 1, "everyth": 1, "straightforward": 1, "amp": 1, "this": 14, "countsearch": 1, "championship": 1, "rang": 2, "time": 3, "leav": 1, "worri": 1, "similar": 1, "differ": 2, "been": 3, "win": 3, "most": 9, "hashtag": 8, "domain": 1, "suspensionpoint": 1, "monument": 1, "all": 2, "top": 1, "today": 1, "someth": 3, "there": 1, "xvdefranc": 1, "case": 1, "inner": 1, "author": 1, "sort": 4, "more": 4, "and": 16, "angfra": 2, "list": 9, "correspond": 1, "packt": 1, "london": 1, "amount": 1, "could": 2, "reus": 1, "befor": 2, "exploratori": 1, "point": 4, "download": 5, "close": 1, "can": 5, "pydata": 1, "counter": 1, "strap": 1, "rbs": 2, "refactor": 1, "onli": 2, "each": 2, "ynumnvsvayln": 1, "sysargv": 1, "key": 2, "unfamiliar": 1, "pretti": 1, "common": 5, "data": 18, "sometim": 1, "bio": 1, "overal": 1, "exampl": 3, "that": 10, "text": 4, "introduc": 3, "fourth": 1, "import": 2, "first": 2, "game": 2, "have": 6, "while": 4, "dramat": 1, "without": 1, "suggest": 1, "mention": 2, "nation": 4}, "idf": {"after": 1.02070207021, "mark": 1.5079787234, "watch": 3.92581602374, "semant": 39.1034482759, "realist": 12.9494290375, "relat": 1.23750876919, "nummb": 324.0, "termsmax": 1221.23076923, "form": 1.12755681818, "tnumcount": 1221.23076923, "unicod": 115.043478261, "new": 1.0178880554, "extra": 5.33826496301, "champion": 5.03680203046, "recognis": 5.93939393939, "number": 1.10142916609, "follow": 1.04640126549, "taken": 1.6012102874399998, "etc": 4.2066772655, "specif": 1.8719490626099997, "function": 2.495441685, "alphabet": 16.4008264463, "python": 56.2978723404, "well": 1.0655748708, "done": 2.3302509907499998, "searchword": 1221.23076923, "toy": 15.383720930199999, "their": 1.01547908405, "won": 2.31732593782, "particular": 1.3814827706200001, "ever": 1.9697270471500001, "creat": 1.2492917847, "multipl": 2.74813917258, "extend": 1.9604840701400004, "how": 1.60250328051, "repost": 933.882352941, "fair": 3.20533010297, "spars": 21.0, "good": 1.51981619759, "crunch": 118.47761194, "end": 1.10680423871, "frame": 6.280063291139999, "word": 1.7965372864099998, "interest": 1.60331246213, "tutori": 59.4606741573, "depend": 2.2411067193700003, "will": 1.22481098596, "event": 1.5356935577500002, "consid": 1.2397313759200002, "obvious": 6.44841592201, "work": 1.11520089913, "inum": 97.3987730061, "bhmeorrnumi": 1221.23076923, "scotland": 7.265903890160001, "tabl": 3.82093862816, "perform": 1.5313977042500002, "but": 1.01632417899, "need": 1.4372623574099999, "our": 2.35758835759, "final": 1.34008609775, "expect": 2.20011086475, "bleus": 933.882352941, "amaz": 15.250720461099998, "applic": 3.42672134686, "sport": 3.29651162791, "termnum": 1221.23076923, "has": 1.0436497502, "enough": 2.2319696330700003, "stop": 2.1783754116400003, "pass": 1.61818367139, "scovir": 1221.23076923, "bonzanini": 1221.23076923, "annual": 2.280706795, "stringpunctu": 1221.23076923, "enjoy": 3.3269069572500003, "format": 2.53125, "quit": 2.8849718335500003, "concept": 2.65707112971, "cooccurr": 1221.23076923, "azzurri": 1221.23076923, "dig": 18.5034965035, "tradit": 1.60802187785, "count": 3.48157894737, "learnt": 56.9032258065, "print": 3.29719626168, "not": 1.01567398119, "far": 1.71022298826, "involv": 1.4498630137000001, "textual": 41.4516971279, "respect": 1.6443293630200002, "whole": 2.29488291414, "charact": 2.51720310766, "happen": 2.96359902931, "quick": 2.205, "shown": 2.76923076923, "english": 1.7432744043000001, "then": 1.08657860516, "twickenham": 260.262295082, "consult": 5.21721984883, "last": 1.2117234010100002, "social": 1.9904714142400002, "world": 1.11340206186, "scienc": 2.31969608416, "outcom": 7.48867924528, "major": 1.14852058164, "tokenis": 1221.23076923, "twice": 3.7638691322900004, "day": 1.18371607516, "mine": 4.875921375919999, "twitter": 33.213389121300004, "pair": 4.36873968079, "len": 20.752941176500002, "such": 1.06151377374, "those": 1.19548192771, "three": 1.06621893889, "lot": 4.40877534018, "some": 1.04036697248, "small": 1.3594793629, "collect": 1.64109985528, "baselin": 57.7309090909, "commandlin": 1221.23076923, "result": 1.14611608432, "com": 99.8490566038, "content": 3.5421686747, "use": 1.0296387573799999, "receiv": 1.3054847463200001, "media": 2.59369384088, "from": 1.00056721497, "account": 1.94463498285, "tweet": 92.8421052632, "num": 1.00031504001, "editor": 4.33060556465, "miss": 3.53664513255, "incred": 18.227324913900002, "exclud": 5.31859296482, "for": 1.00031504001, "ireland": 5.20183486239, "discuss": 2.19676214197, "match": 3.5676404494400002, "seen": 1.61079545455, "with": 1.0011982089899998, "competit": 3.06960556845, "episod": 4.435875943, "shamrock": 189.0, "are": 1.02990593578, "true": 2.55569864778, "better": 2.0065722952500002, "frequent": 2.10501193317, "preprocess": 1221.23076923, "look": 1.9086318826599997, "rbsnumnat": 1221.23076923, "option": 4.04896710023, "summari": 7.80147420147, "pointer": 68.4310344828, "triangular": 37.1803278689, "veri": 1.25880114177, "extract": 7.703056768560001, "intern": 1.30355530011, "ago": 6.05954198473, "nicknam": 5.30083472454, "master": 3.15125049623, "wale": 7.4117647058800005, "remov": 2.0058117498400003, "lowercas": 162.0, "give": 1.3653250774, "great": 1.26592775696, "marco": 18.7438016529, "wnum": 112.595744681, "juli": 1.43466473884, "big": 2.7400759406299997, "note": 1.42449528937, "commax": 1221.23076923, "probabl": 2.64555907349, "scientist": 4.69426374926, "into": 1.01502461479, "italian": 3.63878065551, "turn": 1.3838912133899999, "set": 1.18707940781, "franc": 2.3230904302, "tnummaxterm": 1221.23076923, "what": 1.25343439128, "highest": 2.50212765957, "saturday": 7.367053364269999, "process": 1.69524826482, "main": 1.25303867403, "find": 1.7294117647099998, "sco": 170.709677419, "instead": 1.59461631177, "line": 1.4182597820299998, "welsh": 14.112, "base": 1.14628158845, "occur": 1.7453825857499998, "deal": 2.18346857379, "fact": 1.73375559681, "who": 1.06279287723, "decid": 1.9257641921400002, "permiss": 6.280063291139999, "preserv": 3.1062414400300002, "post": 2.23826307627, "here": 2.42307692308, "punctuat": 42.1114058355, "ire": 89.19101123600001, "attent": 2.81040892193, "titl": 1.87261146497, "make": 1.0762660158600001, "togeth": 1.58095996813, "contain": 1.59814777532, "about": 1.06486015159, "stream": 6.5118949959000005, "communiti": 1.96121062384, "context": 4.25972632144, "search": 3.2539454806299997, "french": 2.11962616822, "disambigu": 97.3987730061, "simpli": 2.5192002538900002, "assum": 2.9575260804799997, "dure": 1.0503473370799998, "frequenc": 8.8102108768, "should": 1.6643254009900001, "activ": 1.46403541129, "rememb": 4.88793103448, "tool": 4.99716713881, "start": 1.26673581744, "articl": 2.01805008262, "scipyspars": 1221.23076923, "insight": 11.8037174721, "bigram": 1221.23076923, "gaelic": 54.3698630137, "observ": 2.22446406053, "order": 1.24625166811, "expand": 2.2260235558000003, "tnum": 38.6277372263, "until": 1.14852058164, "support": 1.2685577307200002, "play": 1.46390041494, "had": 1.0475750577399998, "string": 8.37783641161, "over": 1.02525024217, "engvfra": 1221.23076923, "build": 1.6341739578, "analysi": 3.47852760736, "itavw": 1221.23076923, "team": 2.2748244734200003, "emoji": 1221.23076923, "same": 1.11857958148, "itali": 3.78631051753, "matrix": 22.6153846154, "possibl": 1.4173734488, "modifi": 4.45329593268, "unsurpris": 128.032258065, "best": 1.5828514456600002, "origin": 1.13724928367, "continu": 1.13928955867, "previous": 1.42846859816, "might": 2.1561863370900003, "name": 1.10211732037, "code": 3.8807137619199996, "than": 1.03278688525, "publish": 1.36885669943, "entri": 3.9909502262400003, "languag": 2.29488291414, "also": 1.01476510067, "around": 1.21394708671, "mean": 1.44906900329, "symbol": 3.4178686760000003, "revers": 4.29894394801, "notic": 4.36994219653, "irish": 5.56077057793, "england": 2.56519631604, "tupl": 661.5, "the": 1.0, "open": 1.24556723678, "six": 1.5552507837, "want": 1.99698113208, "captur": 2.88026124819, "argument": 5.09335899904, "individu": 1.8004082558400003, "token": 33.7070063694, "minut": 3.11233091551, "rugbi": 12.9388753056, "term": 1.39520168732, "other": 1.00992366412, "termson": 1221.23076923, "les": 9.50658682635, "one": 1.00627495722, "sequenti": 39.5910224439, "becaus": 1.1495184997499999, "loop": 13.5114893617, "get": 1.78562591385, "like": 1.14918566775, "implement": 3.57648118946, "moment": 4.262013422819999, "stopword": 1221.23076923, "defaultdict": 1221.23076923, "file": 3.7710213776699995, "part": 1.04330682789, "independ": 1.58950740889, "analyt": 17.256521739100002, "memori": 2.57392996109, "trash": 43.3770491803, "includ": 1.0190641247799999, "shot": 3.85339805825, "everyth": 4.81967213115, "straightforward": 27.7552447552, "amp": 72.8256880734, "this": 1.00379362671, "countsearch": 1221.23076923, "championship": 6.837209302330001, "rang": 1.7848229342299997, "time": 1.01127460348, "leav": 1.6615384615399997, "worri": 10.302401038300001, "similar": 1.37514075357, "differ": 1.23654490225, "been": 1.0239277652399998, "win": 2.75290445639, "most": 1.02096463023, "hashtag": 496.125, "domain": 9.39408284024, "suspensionpoint": 1221.23076923, "monument": 6.74426508071, "all": 1.01146788991, "top": 1.8387769284200002, "today": 1.74961428257, "someth": 3.28152128979, "there": 1.04091266719, "xvdefranc": 1221.23076923, "case": 1.48498737256, "inner": 6.432739059969999, "author": 1.4229631621399998, "sort": 5.188235294119999, "more": 1.0171706817, "and": 1.00006299213, "angfra": 1221.23076923, "list": 1.36321483771, "correspond": 3.32481675393, "packt": 1221.23076923, "london": 1.97782484116, "amount": 2.27027027027, "could": 1.2043695949, "reus": 29.7861163227, "befor": 1.10036041031, "exploratori": 68.4310344828, "point": 1.25990000794, "download": 14.6457564576, "close": 1.2848818387799998, "can": 1.17626139142, "pydata": 1221.23076923, "counter": 6.77592829706, "strap": 48.6993865031, "rbs": 547.448275862, "refactor": 1221.23076923, "onli": 1.0256476516600002, "each": 1.18974820144, "ynumnvsvayln": 1221.23076923, "sysargv": 1221.23076923, "key": 2.28005170185, "unfamiliar": 37.7102137767, "pretti": 15.75, "common": 1.4025974025999999, "data": 3.37643555934, "sometim": 1.7126213592200001, "bio": 42.336000000000006, "overal": 3.0442953020099996, "exampl": 1.50483412322, "that": 1.00398406375, "text": 3.12827586207, "introduc": 1.7258397651900002, "fourth": 2.5647819063, "import": 1.3401992233700002, "first": 1.00761614623, "game": 2.57978550536, "have": 1.0148948411399998, "while": 1.0441988950299999, "dramat": 3.9849397590400004, "without": 1.29547123623, "suggest": 1.7571665744299998, "mention": 2.53894130817, "nation": 1.22632473351}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Mining Twitter Data with Python Part 4: Rugby and Term Co-occurrences</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2016/06/mining-twitter-data-python-part-4.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Mining Twitter Data with Python Part 4: Rugby and Term Co-occurrences Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2016/06/algorithmia-improving-nudity-detection-nsfw-image-recognition.html\" rel=\"prev\" title=\"Improving Nudity Detection and NSFW Image Recognition\"/>\n<link href=\"https://www.kdnuggets.com/2016/06/bigdeub-primitive-interactive-big-data-spark.html\" rel=\"next\" title=\"BigDebug: Debugging Primitives for Interactive Big Data Processing in Spark\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2016/06/mining-twitter-data-python-part-4.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=51384\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2016/06/mining-twitter-data-python-part-4.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-51384 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 27-Jun, 2016  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2016/index.html\">2016</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/06/index.html\">Jun</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/06/tutorials.html\">Tutorials, Overviews</a> \u00bb Mining Twitter Data with Python Part 4: Rugby and Term Co-occurrences (\u00a0<a href=\"/2016/n23.html\">16:n23</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Mining Twitter Data with Python Part 4: Rugby and Term Co-occurrences</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2016/06/algorithmia-improving-nudity-detection-nsfw-image-recognition.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2016/06/bigdeub-primitive-interactive-big-data-spark.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/python\" rel=\"tag\">Python</a>, <a href=\"https://www.kdnuggets.com/tag/social-media\" rel=\"tag\">Social Media</a>, <a href=\"https://www.kdnuggets.com/tag/social-media-analytics\" rel=\"tag\">Social Media Analytics</a>, <a href=\"https://www.kdnuggets.com/tag/text-mining\" rel=\"tag\">Text Mining</a>, <a href=\"https://www.kdnuggets.com/tag/twitter\" rel=\"tag\">Twitter</a></div>\n<br/>\n<p class=\"excerpt\">\n     Part 4 of this series employs some of the lessons learned thus far to analyze tweets related to rugby matches and term co-occurrences.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><b>By Marco Bonzanini, Independent Data Science Consultant</b>.</p>\n<p>Last Saturday (<em>Editor's note: happened a while ago</em>) was the closing day of the\u00a0<a href=\"http://en.wikipedia.org/wiki/Six_Nations_Championship\">Six Nations Championship</a>, an annual international\u00a0<a href=\"http://en.wikipedia.org/wiki/Rugby_union\">rugby</a>\u00a0competition. Before turning on the TV to watch Italy being trashed by Wales, I decided to use this event to collect some data from Twitter and perform some exploratory text analysis on something more interesting than the small list of my tweets.</p>\n<p>This article continues the tutorial on Twitter Data Mining, re-using what we discussed in the previous articles with some more realistic data. It also expands the analysis by introducing the concept of term co-occurrence.</p>\n<p><img alt=\"Twitter\" src=\"/wp-content/uploads/twitter-banner.jpg\" width=\"99%\"/></p>\n<h3>The Application Domain</h3>\n<p>\u00a0<br>\nAs the name suggests, six teams are involved in the competition: England, Ireland, Wales, Scotland, France and Italy. This means that we can expect the event to be tweeted in multiple languages (English, French, Italian, Welsh, Gaelic, possibly other languages as well), with English being the major language. Assuming the team names will be mentioned frequently, we could decide to look also for their nicknames, e.g.\u00a0<em>Les Bleus</em>\u00a0for France or\u00a0<em>Azzurri</em>\u00a0for Italy. During the last day of the competition, three matches are played sequentially. Three teams in particular had a shot for the title: England, Ireland and Wales. At the end, Ireland won the competition but everything was open until the very last minute.</br></p>\n<h3>Setting Up</h3>\n<p>\u00a0<br>\nI used the\u00a0<a href=\"/2016/06/mining-twitter-data-python-part-1.html\">streaming API</a>\u00a0to download all the tweets containing the string <tt>#rbs6nations</tt>\u00a0during the day. Obviously not all the tweets about the event contained the hashtag, but this is a good baseline. The time frame for the download was from around\u00a012:15PM to 7:15PM GMT, that is from about 15 minutes before the first match, to about 15 minutes after the last match was over. At the end, more than\u00a018,000 tweets\u00a0have been downloaded in JSON format, making for about\u00a075Mb\u00a0of data. This should be small enough to quickly do some processing in memory, and at the same time big enough to observe something possibly interesting.</br></p>\n<p>The textual content of the tweets has been pre-processed with <a href=\"/2016/06/mining-twitter-data-python-part-2.html\">tokenisation and lowercasing</a>\u00a0using the\u00a0<tt>preprocess()</tt>\u00a0function introduced in Part 2 of the tutorial.</p>\n<h3>Interesting terms and hashtags</h3>\n<p>\u00a0<br>\nFollowing what we discussed in\u00a0<a href=\"/2016/06/mining-twitter-data-python-part-3.html\">Part 3 (Term Frequencies)</a>, we want to observe the most common terms and hashtags used during day. If you have followed the discussion about creating different lists of tokens in order to capture terms without hashtags, hashtags only, removing stop-words, etc. you can play around with the different lists.</br></p>\n<p>This is the unsurprising list of top 10 most frequent terms (<tt>terms_only</tt>\u00a0in Part 3) in the data set.</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'ireland'</span>, <span style=\"color:#3387cc\">3163</span>), (<span style=\"color:#65b042\">'england'</span>, <span style=\"color:#3387cc\">2584</span>), (<span style=\"color:#65b042\">'wales'</span>, <span style=\"color:#3387cc\">2271</span>), \r\n  (<span style=\"color:#65b042\">''</span>, <span style=\"color:#3387cc\">2068</span>), (<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#3387cc\">1479</span>), (<span style=\"color:#65b042\">'france'</span>, <span style=\"color:#3387cc\">1380</span>), (<span style=\"color:#65b042\">'win'</span>, <span style=\"color:#3387cc\">1338</span>), \r\n  (<span style=\"color:#65b042\">'rugby'</span>, <span style=\"color:#3387cc\">1253</span>), (<span style=\"color:#65b042\">'points'</span>, <span style=\"color:#3387cc\">1221</span>), (<span style=\"color:#65b042\">'title'</span>, <span style=\"color:#3387cc\">1180</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The first three terms correspond to the teams who had a go for the title. The frequencies also respect the order in the final table. The fourth term is instead a punctuation mark that we missed and didn\u2019t include in the list of stop-words. This is because\u00a0<tt>string.punctuation</tt>\u00a0only contains ASCII symbols, while here we\u2019re dealing with a unicode character. If we dig into the data, there will be more examples like this, but for the moment we don\u2019t worry about it.</p>\n<p>After adding the suspension-points symbol to the list of stop-words, we have a new entry at the end of the list:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'ireland'</span>, <span style=\"color:#3387cc\">3163</span>), (<span style=\"color:#65b042\">'england'</span>, <span style=\"color:#3387cc\">2584</span>), (<span style=\"color:#65b042\">'wales'</span>, <span style=\"color:#3387cc\">2271</span>), \r\n  (<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#3387cc\">1479</span>), (<span style=\"color:#65b042\">'france'</span>, <span style=\"color:#3387cc\">1380</span>), (<span style=\"color:#65b042\">'win'</span>, <span style=\"color:#3387cc\">1338</span>), (<span style=\"color:#65b042\">'rugby'</span>, <span style=\"color:#3387cc\">1253</span>), \r\n  (<span style=\"color:#65b042\">'points'</span>, <span style=\"color:#3387cc\">1221</span>), (<span style=\"color:#65b042\">'title'</span>, <span style=\"color:#3387cc\">1180</span>), (<span style=\"color:#65b042\">'__SHAMROCK_SYMBOL__'</span>, <span style=\"color:#3387cc\">1154</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Interestingly, a new token we didn\u2019t account for, an\u00a0<a href=\"http://en.wikipedia.org/wiki/Emoji\">Emoji symbol</a>\u00a0(in this case, the\u00a0<a href=\"http://en.wikipedia.org/wiki/Shamrock\">Irish Shamrock</a>).</p>\n<p>If we have a look at the most common hashtags, we need to consider that<tt>#rbs6nations</tt>\u00a0will be by far the most common token (that\u2019s our search term for downloading the tweets), so we can exclude it from the list. This leave us with:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'#engvfra'</span>, <span style=\"color:#3387cc\">1701</span>), (<span style=\"color:#65b042\">'#itavwal'</span>, <span style=\"color:#3387cc\">927</span>), (<span style=\"color:#65b042\">'#rugby'</span>, <span style=\"color:#3387cc\">880</span>), \r\n  (<span style=\"color:#65b042\">'#scovire'</span>, <span style=\"color:#3387cc\">692</span>), (<span style=\"color:#65b042\">'#ireland'</span>, <span style=\"color:#3387cc\">686</span>), (<span style=\"color:#65b042\">'#angfra'</span>, <span style=\"color:#3387cc\">554</span>), \r\n  (<span style=\"color:#65b042\">'#xvdefrance'</span>, <span style=\"color:#3387cc\">508</span>), (<span style=\"color:#65b042\">'#crunch'</span>, <span style=\"color:#3387cc\">500</span>), (<span style=\"color:#65b042\">'#wales'</span>, <span style=\"color:#3387cc\">446</span>), \r\n  (<span style=\"color:#65b042\">'#england'</span>, <span style=\"color:#3387cc\">406</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>We can observe that the most common hashtags, a part from\u00a0<tt>#rugby</tt>, are related to the individual matches. In particular England v France has received the highest number of mentions, probably being the last match of the day with a dramatic finale. Something interesting to notice is that a fair amount of tweets also contained terms in French: the count for<tt>#angfra</tt>\u00a0should in fact be added to\u00a0<tt>#engvfra</tt>. Those unfamiliar with rugby probably wouldn\u2019t recognise that also\u00a0<tt>#crunch</tt>\u00a0should be included with<tt>#EngvFra</tt>\u00a0match, as\u00a0<em>Le Crunch</em>\u00a0is the traditional name for this event. So by far, the last match has received a lot of attention.</p>\n<h3>Term co-occurrences</h3>\n<p>\u00a0<br/>\nSometimes we are interested in the terms that occur together. This is mainly because the\u00a0<em>context</em>\u00a0gives us a better insight about the meaning of a term, supporting applications such as word disambiguation or semantic similarity. We discussed the option of using\u00a0<em>bigrams</em>\u00a0<a href=\"/2016/06/mining-twitter-data-python-part-3.html\">in the previous article</a>, but we want to extend the context of a term to the whole tweet.</p>\n<p>We can refactor the code from\u00a0<a href=\"/2016/06/mining-twitter-data-python-part-3.html\">the previous article</a>\u00a0in order to capture the\u00a0co-occurrences. We build a co-occurrence matrix\u00a0<tt>com</tt>\u00a0such that<tt>com[x][y]</tt>\u00a0contains the number of times the term\u00a0<tt>x</tt>\u00a0has been seen in the same tweet as the term\u00a0<tt>y</tt>:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\"><span style=\"color:#e28964\">from</span> collections <span style=\"color:#e28964\">import</span> defaultdict\r\n<span style=\"color:#aeaeae;font-style:italic\"># remember to include the other import from the previous post</span>\r\n \r\ncom <span style=\"color:#e28964\">=</span> defaultdict(<span style=\"color:#99cf50\">lambda</span> : defaultdict(<span style=\"color:#9b859d\">int</span>))\r\n \r\n<span style=\"color:#aeaeae;font-style:italic\"># f is the file pointer to the JSON data set</span>\r\n<span style=\"color:#e28964\">for</span> line <span style=\"color:#e28964\">in</span> f: \r\n    tweet <span style=\"color:#e28964\">=</span> json.loads(line)\r\n    terms_only <span style=\"color:#e28964\">=</span> [term <span style=\"color:#e28964\">for</span> term <span style=\"color:#e28964\">in</span> preprocess(tweet[<span style=\"color:#65b042\">'text'</span>]) \r\n                  <span style=\"color:#e28964\">if</span> term <span style=\"color:#e28964\">not</span> <span style=\"color:#e28964\">in</span> stop \r\n                  <span style=\"color:#e28964\">and</span> <span style=\"color:#e28964\">not</span> term.startswith((<span style=\"color:#65b042\">'#'</span>, <span style=\"color:#65b042\">'@'</span>))]\r\n \r\n    <span style=\"color:#aeaeae;font-style:italic\"># Build co-occurrence matrix</span>\r\n    <span style=\"color:#e28964\">for</span> i <span style=\"color:#e28964\">in</span> <span style=\"color:#dad085\">range</span>(<span style=\"color:#dad085\">len</span>(terms_only)<span style=\"color:#e28964\">-</span><span style=\"color:#3387cc\">1</span>):            \r\n        <span style=\"color:#e28964\">for</span> j <span style=\"color:#e28964\">in</span> <span style=\"color:#dad085\">range</span>(i<span style=\"color:#e28964\">+</span><span style=\"color:#3387cc\">1</span>, <span style=\"color:#dad085\">len</span>(terms_only)):\r\n            w1, w2 <span style=\"color:#e28964\">=</span> <span style=\"color:#dad085\">sorted</span>([terms_only[i], terms_only[j]])                \r\n            <span style=\"color:#e28964\">if</span> w1 <span style=\"color:#e28964\">!=</span> w2:\r\n                com[w1][w2] <span style=\"color:#e28964\">+=</span> <span style=\"color:#3387cc\">1</span>\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>While building the co-occurrence matrix, we don\u2019t want to count the same term pair twice, e.g.\u00a0<tt>com[A][B] == com[B][A]</tt>, so the inner for loop starts from\u00a0<tt>i+1</tt>\u00a0in order to build a triangular matrix, while\u00a0<tt>sorted</tt>\u00a0will preserve the alphabetical order of the terms.</p>\n<p>For each term, we then extract the 5 most frequent co-occurrent terms, creating a list of tuples in the form\u00a0<tt>((term1, term2), count)</tt>:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">com_max <span style=\"color:#e28964\">=</span> []\r\n<span style=\"color:#aeaeae;font-style:italic\"># For each term, look for the most common co-occurrent terms</span>\r\n<span style=\"color:#e28964\">for</span> t1 <span style=\"color:#e28964\">in</span> com:\r\n    t1_max_terms <span style=\"color:#e28964\">=</span> <span style=\"color:#dad085\">sorted</span>(com[t1].items(), <span style=\"color:#3e87e3\">key</span><span style=\"color:#e28964\">=</span>operator.itemgetter(<span style=\"color:#3387cc\">1</span>), <span style=\"color:#3e87e3\">reverse</span><span style=\"color:#e28964\">=</span><span style=\"color:#3387cc\">True</span>)[:<span style=\"color:#3387cc\">5</span>]\r\n    <span style=\"color:#e28964\">for</span> t2, t2_count <span style=\"color:#e28964\">in</span> t1_max_terms:\r\n        com_max.append(((t1, t2), t2_count))\r\n<span style=\"color:#aeaeae;font-style:italic\"># Get the most frequent co-occurrences</span>\r\nterms_max <span style=\"color:#e28964\">=</span> <span style=\"color:#dad085\">sorted</span>(com_max, <span style=\"color:#3e87e3\">key</span><span style=\"color:#e28964\">=</span>operator.itemgetter(<span style=\"color:#3387cc\">1</span>), <span style=\"color:#3e87e3\">reverse</span><span style=\"color:#e28964\">=</span><span style=\"color:#3387cc\">True</span>)\r\n<span style=\"color:#e28964\">print</span>(terms_max[:<span style=\"color:#3387cc\">5</span>])\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The results:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[((<span style=\"color:#65b042\">'6'</span>, <span style=\"color:#65b042\">'nations'</span>), <span style=\"color:#3387cc\">845</span>), ((<span style=\"color:#65b042\">'champions'</span>, <span style=\"color:#65b042\">'ireland'</span>), <span style=\"color:#3387cc\">760</span>), \r\n  ((<span style=\"color:#65b042\">'nations'</span>, <span style=\"color:#65b042\">'rbs'</span>), <span style=\"color:#3387cc\">742</span>), ((<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#65b042\">'ireland'</span>), <span style=\"color:#3387cc\">731</span>), \r\n  ((<span style=\"color:#65b042\">'ireland'</span>, <span style=\"color:#65b042\">'wales'</span>), <span style=\"color:#3387cc\">674</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>This implementation is pretty straightforward, but depending on the data set and on the use of the matrix, one might want to look into tools like\u00a0<tt>scipy.sparse</tt>\u00a0for building a sparse matrix.</p>\n<p>We could also look for a specific term and extract its most frequent co-occurrences. We simply need to modify the main loop including an extra counter, for example:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">search_word <span style=\"color:#e28964\">=</span> sys.argv[<span style=\"color:#3387cc\">1</span>] <span style=\"color:#aeaeae;font-style:italic\"># pass a term as a command-line argument</span>\r\ncount_search <span style=\"color:#e28964\">=</span> Counter()\r\n<span style=\"color:#e28964\">for</span> line <span style=\"color:#e28964\">in</span> f:\r\n    tweet <span style=\"color:#e28964\">=</span> json.loads(line)\r\n    terms_only <span style=\"color:#e28964\">=</span> [term <span style=\"color:#e28964\">for</span> term <span style=\"color:#e28964\">in</span> preprocess(tweet[<span style=\"color:#65b042\">'text'</span>]) \r\n                  <span style=\"color:#e28964\">if</span> term <span style=\"color:#e28964\">not</span> <span style=\"color:#e28964\">in</span> stop \r\n                  <span style=\"color:#e28964\">and</span> <span style=\"color:#e28964\">not</span> term.startswith((<span style=\"color:#65b042\">'#'</span>, <span style=\"color:#65b042\">'@'</span>))]\r\n    <span style=\"color:#e28964\">if</span> search_word <span style=\"color:#e28964\">in</span> terms_only:\r\n        count_search.update(terms_only)\r\n<span style=\"color:#e28964\">print</span>(<span style=\"color:#65b042\">\"Co-occurrence for <span style=\"color:#ddf2a4\">%s</span>:\"</span> <span style=\"color:#e28964\">%</span> search_word)\r\n<span style=\"color:#e28964\">print</span>(count_search.most_common(<span style=\"color:#3387cc\">20</span>))\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The outcome for \u201cireland\u201d:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'champions'</span>, <span style=\"color:#3387cc\">756</span>), (<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#3387cc\">727</span>), (<span style=\"color:#65b042\">'nations'</span>, <span style=\"color:#3387cc\">659</span>), (<span style=\"color:#65b042\">'wales'</span>, <span style=\"color:#3387cc\">654</span>), (<span style=\"color:#65b042\">'2015'</span>, <span style=\"color:#3387cc\">638</span>), \r\n  (<span style=\"color:#65b042\">'6'</span>, <span style=\"color:#3387cc\">613</span>), (<span style=\"color:#65b042\">'rbs'</span>, <span style=\"color:#3387cc\">585</span>), (<span style=\"color:#65b042\">'http://t.co/y0nvsvayln'</span>, <span style=\"color:#3387cc\">559</span>), (<span style=\"color:#65b042\">'__SHAMROCK_SYMBOL__'</span>, <span style=\"color:#3387cc\">526</span>), (<span style=\"color:#65b042\">'10'</span>, <span style=\"color:#3387cc\">522</span>), \r\n  (<span style=\"color:#65b042\">'win'</span>, <span style=\"color:#3387cc\">377</span>), (<span style=\"color:#65b042\">'england'</span>, <span style=\"color:#3387cc\">377</span>), (<span style=\"color:#65b042\">'twickenham'</span>, <span style=\"color:#3387cc\">361</span>), (<span style=\"color:#65b042\">'40'</span>, <span style=\"color:#3387cc\">360</span>), (<span style=\"color:#65b042\">'points'</span>, <span style=\"color:#3387cc\">356</span>), \r\n  (<span style=\"color:#65b042\">'sco'</span>, <span style=\"color:#3387cc\">355</span>), (<span style=\"color:#65b042\">'ire'</span>, <span style=\"color:#3387cc\">355</span>), (<span style=\"color:#65b042\">'title'</span>, <span style=\"color:#3387cc\">346</span>), (<span style=\"color:#65b042\">'scotland'</span>, <span style=\"color:#3387cc\">301</span>), (<span style=\"color:#65b042\">'turn'</span>, <span style=\"color:#3387cc\">295</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The outcome for \u201crugby\u201d:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#3387cc\">476</span>), (<span style=\"color:#65b042\">'game'</span>, <span style=\"color:#3387cc\">160</span>), (<span style=\"color:#65b042\">'ireland'</span>, <span style=\"color:#3387cc\">143</span>), (<span style=\"color:#65b042\">'england'</span>, <span style=\"color:#3387cc\">132</span>), (<span style=\"color:#65b042\">'great'</span>, <span style=\"color:#3387cc\">105</span>), \r\n  (<span style=\"color:#65b042\">'today'</span>, <span style=\"color:#3387cc\">104</span>), (<span style=\"color:#65b042\">'best'</span>, <span style=\"color:#3387cc\">97</span>), (<span style=\"color:#65b042\">'well'</span>, <span style=\"color:#3387cc\">90</span>), (<span style=\"color:#65b042\">'ever'</span>, <span style=\"color:#3387cc\">89</span>), (<span style=\"color:#65b042\">'incredible'</span>, <span style=\"color:#3387cc\">87</span>), \r\n  (<span style=\"color:#65b042\">'amazing'</span>, <span style=\"color:#3387cc\">84</span>), (<span style=\"color:#65b042\">'done'</span>, <span style=\"color:#3387cc\">82</span>), (<span style=\"color:#65b042\">'amp'</span>, <span style=\"color:#3387cc\">71</span>), (<span style=\"color:#65b042\">'games'</span>, <span style=\"color:#3387cc\">66</span>), (<span style=\"color:#65b042\">'points'</span>, <span style=\"color:#3387cc\">64</span>), \r\n  (<span style=\"color:#65b042\">'monumental'</span>, <span style=\"color:#3387cc\">58</span>), (<span style=\"color:#65b042\">'strap'</span>, <span style=\"color:#3387cc\">56</span>), (<span style=\"color:#65b042\">'world'</span>, <span style=\"color:#3387cc\">55</span>), (<span style=\"color:#65b042\">'team'</span>, <span style=\"color:#3387cc\">55</span>), (<span style=\"color:#65b042\">'http://t.co/bhmeorr19i'</span>, <span style=\"color:#3387cc\">53</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Overall, quite interesting.</p>\n<h3>Summary</h3>\n<p>\u00a0<br/>\nThis article has discussed a toy example of Text Mining on Twitter, using some realistic data taken during a sport event. Using what we have learnt in the previous episodes, we have downloaded some data using the streaming API, pre-processed the data in JSON format and extracted some interesting terms and hashtags from the tweets. The article has also introduced the concept of term co-occurrence, shown how to build a co-occurrence matrix and discussed how to use it to find some interesting insight.</p>\n<p><b>Bio: <a href=\"https://twitter.com/marcobonzanini\">Marco Bonzanini</a></b> is a Data Scientist based in London, UK. Active in the PyData community, he enjoys working in text analytics and data mining applications. He's the author of \"<a href=\"https://www.amazon.com/Mastering-Social-Media-Mining-Python-ebook/dp/B01BFD2Z2Q\">Mastering Social Media Mining with Python</a>\" (Packt Publishing, July 2016).</p>\n<p><a href=\"https://marcobonzanini.com/2015/03/23/mining-twitter-data-with-python-part-4-rugby-and-term-co-occurrences/\">Original</a>. Reposted with permission.</p>\n<p><b>Related</b>:</p>\n<ul class=\"three_ul\">\n<li><a href=\"/2016/06/mining-twitter-data-python-part-1.html\">Mining Twitter Data with Python Part 1: Collecting Data</a></li>\n<li><a href=\"/2016/06/mining-twitter-data-python-part-2.html\">Mining Twitter Data with Python Part 2: Text Pre-processing</a></li>\n<li><a href=\"/2016/06/mining-twitter-data-python-part-3.html\">Mining Twitter Data with Python Part 3: Term Frequencies</a></li>\n</ul>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2016/06/algorithmia-improving-nudity-detection-nsfw-image-recognition.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2016/06/bigdeub-primitive-interactive-big-data-spark.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2016/index.html\">2016</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/06/index.html\">Jun</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/06/tutorials.html\">Tutorials, Overviews</a> \u00bb Mining Twitter Data with Python Part 4: Rugby and Term Co-occurrences (\u00a0<a href=\"/2016/n23.html\">16:n23</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556363358\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.765 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 07:09:18 -->\n<!-- Compression = gzip -->", "content_tokenized": ["marco", "bonzanini", "independ", "data", "scienc", "consult", "last", "saturday", "editor", "note", "happen", "while", "ago", "the", "close", "day", "the", "six", "nation", "championship", "annual", "intern", "rugbi", "competit", "befor", "turn", "the", "watch", "itali", "trash", "wale", "decid", "use", "this", "event", "collect", "some", "data", "from", "twitter", "and", "perform", "some", "exploratori", "text", "analysi", "someth", "more", "interest", "than", "the", "small", "list", "tweet", "this", "articl", "continu", "the", "tutori", "twitter", "data", "mine", "reus", "what", "discuss", "the", "previous", "articl", "with", "some", "more", "realist", "data", "also", "expand", "the", "analysi", "introduc", "the", "concept", "term", "cooccurr", "the", "applic", "domain", "the", "name", "suggest", "six", "team", "are", "involv", "the", "competit", "england", "ireland", "wale", "scotland", "franc", "and", "itali", "this", "mean", "that", "can", "expect", "the", "event", "tweet", "multipl", "languag", "english", "french", "italian", "welsh", "gaelic", "possibl", "other", "languag", "well", "with", "english", "the", "major", "languag", "assum", "the", "team", "name", "will", "mention", "frequent", "could", "decid", "look", "also", "for", "their", "nicknam", "les", "bleus", "for", "franc", "azzurri", "for", "itali", "dure", "the", "last", "day", "the", "competit", "three", "match", "are", "play", "sequenti", "three", "team", "particular", "had", "shot", "for", "the", "titl", "england", "ireland", "and", "wale", "the", "end", "ireland", "won", "the", "competit", "but", "everyth", "open", "until", "the", "veri", "last", "minut", "set", "use", "the", "stream", "download", "all", "the", "tweet", "contain", "the", "string", "rbsnumnat", "dure", "the", "day", "obvious", "not", "all", "the", "tweet", "about", "the", "event", "contain", "the", "hashtag", "but", "this", "good", "baselin", "the", "time", "frame", "for", "the", "download", "from", "around", "that", "from", "about", "num", "minut", "befor", "the", "first", "match", "about", "num", "minut", "after", "the", "last", "match", "over", "the", "end", "more", "than", "num", "tweet", "have", "been", "download", "format", "make", "for", "about", "nummb", "data", "this", "should", "small", "enough", "quick", "some", "process", "memori", "and", "the", "same", "time", "big", "enough", "observ", "someth", "possibl", "interest", "the", "textual", "content", "the", "tweet", "has", "been", "preprocess", "with", "tokenis", "and", "lowercas", "use", "the", "function", "introduc", "part", "num", "the", "tutori", "interest", "term", "and", "hashtag", "follow", "what", "discuss", "part", "num", "term", "frequenc", "want", "observ", "the", "most", "common", "term", "and", "hashtag", "use", "dure", "day", "have", "follow", "the", "discuss", "about", "creat", "differ", "list", "token", "order", "captur", "term", "without", "hashtag", "hashtag", "onli", "remov", "stopword", "etc", "can", "play", "around", "with", "the", "differ", "list", "this", "the", "unsurpris", "list", "top", "num", "most", "frequent", "term", "termson", "part", "num", "the", "data", "set", "ireland", "num", "england", "num", "wale", "num", "num", "day", "num", "franc", "num", "win", "num", "rugbi", "num", "point", "num", "titl", "num", "the", "first", "three", "term", "correspond", "the", "team", "who", "had", "for", "the", "titl", "the", "frequenc", "also", "respect", "the", "order", "the", "final", "tabl", "the", "fourth", "term", "instead", "punctuat", "mark", "that", "miss", "and", "includ", "the", "list", "stopword", "this", "becaus", "stringpunctu", "onli", "contain", "symbol", "while", "here", "deal", "with", "unicod", "charact", "dig", "into", "the", "data", "there", "will", "more", "exampl", "like", "this", "but", "for", "the", "moment", "worri", "about", "after", "the", "suspensionpoint", "symbol", "the", "list", "stopword", "have", "new", "entri", "the", "end", "the", "list", "ireland", "num", "england", "num", "wale", "num", "day", "num", "franc", "num", "win", "num", "rugbi", "num", "point", "num", "titl", "num", "num", "interest", "new", "token", "account", "for", "emoji", "symbol", "this", "case", "the", "irish", "shamrock", "have", "look", "the", "most", "common", "hashtag", "need", "consid", "that", "rbsnumnat", "will", "far", "the", "most", "common", "token", "that", "our", "search", "term", "for", "download", "the", "tweet", "can", "exclud", "from", "the", "list", "this", "leav", "with", "engvfra", "num", "itavw", "num", "rugbi", "num", "scovir", "num", "ireland", "num", "angfra", "num", "xvdefranc", "num", "crunch", "num", "wale", "num", "england", "num", "can", "observ", "that", "the", "most", "common", "hashtag", "part", "from", "rugbi", "are", "relat", "the", "individu", "match", "particular", "england", "franc", "has", "receiv", "the", "highest", "number", "mention", "probabl", "the", "last", "match", "the", "day", "with", "dramat", "final", "someth", "interest", "notic", "that", "fair", "amount", "tweet", "also", "contain", "term", "french", "the", "count", "for", "angfra", "should", "fact", "engvfra", "those", "unfamiliar", "with", "rugbi", "probabl", "recognis", "that", "also", "crunch", "should", "includ", "with", "engvfra", "match", "crunch", "the", "tradit", "name", "for", "this", "event", "far", "the", "last", "match", "has", "receiv", "lot", "attent", "term", "cooccurr", "sometim", "are", "interest", "the", "term", "that", "occur", "togeth", "this", "main", "becaus", "the", "context", "give", "better", "insight", "about", "the", "mean", "term", "support", "applic", "such", "word", "disambigu", "semant", "similar", "discuss", "the", "option", "use", "bigram", "the", "previous", "articl", "but", "want", "extend", "the", "context", "term", "the", "whole", "tweet", "can", "refactor", "the", "code", "from", "the", "previous", "articl", "order", "captur", "the", "cooccurr", "build", "cooccurr", "matrix", "com", "such", "that", "com", "contain", "the", "number", "time", "the", "term", "has", "been", "seen", "the", "same", "tweet", "the", "term", "from", "collect", "import", "defaultdict", "rememb", "includ", "the", "other", "import", "from", "the", "previous", "post", "com", "the", "file", "pointer", "the", "data", "set", "for", "line", "tweet", "termson", "term", "for", "term", "term", "not", "stop", "and", "not", "build", "cooccurr", "matrix", "for", "rang", "len", "termson", "num", "for", "rang", "num", "len", "termson", "wnum", "wnum", "sort", "termson", "termson", "wnum", "wnum", "com", "wnum", "wnum", "num", "while", "build", "the", "cooccurr", "matrix", "want", "count", "the", "same", "term", "pair", "twice", "com", "com", "the", "inner", "for", "loop", "start", "from", "inum", "order", "build", "triangular", "matrix", "while", "sort", "will", "preserv", "the", "alphabet", "order", "the", "term", "for", "each", "term", "then", "extract", "the", "num", "most", "frequent", "cooccurr", "term", "creat", "list", "tupl", "the", "form", "termnum", "termnum", "count", "commax", "for", "each", "term", "look", "for", "the", "most", "common", "cooccurr", "term", "for", "tnum", "com", "tnummaxterm", "sort", "com", "tnum", "key", "revers", "true", "num", "for", "tnum", "tnumcount", "tnummaxterm", "tnumcount", "get", "the", "most", "frequent", "cooccurr", "termsmax", "sort", "commax", "key", "revers", "true", "print", "termsmax", "num", "the", "result", "num", "nation", "num", "champion", "ireland", "num", "nation", "rbs", "num", "day", "ireland", "num", "ireland", "wale", "num", "this", "implement", "pretti", "straightforward", "but", "depend", "the", "data", "set", "and", "the", "use", "the", "matrix", "one", "might", "want", "look", "into", "tool", "like", "scipyspars", "for", "build", "spars", "matrix", "could", "also", "look", "for", "specif", "term", "and", "extract", "most", "frequent", "cooccurr", "simpli", "need", "modifi", "the", "main", "loop", "includ", "extra", "counter", "for", "exampl", "searchword", "sysargv", "num", "pass", "term", "commandlin", "argument", "countsearch", "for", "line", "tweet", "termson", "term", "for", "term", "term", "not", "stop", "and", "not", "searchword", "termson", "print", "cooccurr", "for", "searchword", "print", "the", "outcom", "for", "ireland", "champion", "num", "day", "num", "nation", "num", "wale", "num", "num", "num", "num", "num", "rbs", "num", "ynumnvsvayln", "num", "num", "num", "num", "win", "num", "england", "num", "twickenham", "num", "num", "num", "point", "num", "sco", "num", "ire", "num", "titl", "num", "scotland", "num", "turn", "num", "the", "outcom", "for", "rugbi", "day", "num", "game", "num", "ireland", "num", "england", "num", "great", "num", "today", "num", "best", "num", "well", "num", "ever", "num", "incred", "num", "amaz", "num", "done", "num", "amp", "num", "game", "num", "point", "num", "monument", "num", "strap", "num", "world", "num", "team", "num", "bhmeorrnumi", "num", "overal", "quit", "interest", "summari", "this", "articl", "has", "discuss", "toy", "exampl", "text", "mine", "twitter", "use", "some", "realist", "data", "taken", "dure", "sport", "event", "use", "what", "have", "learnt", "the", "previous", "episod", "have", "download", "some", "data", "use", "the", "stream", "preprocess", "the", "data", "format", "and", "extract", "some", "interest", "term", "and", "hashtag", "from", "the", "tweet", "the", "articl", "has", "also", "introduc", "the", "concept", "term", "cooccurr", "shown", "how", "build", "cooccurr", "matrix", "and", "discuss", "how", "use", "find", "some", "interest", "insight", "bio", "marco", "bonzanini", "data", "scientist", "base", "london", "activ", "the", "pydata", "communiti", "enjoy", "work", "text", "analyt", "and", "data", "mine", "applic", "the", "author", "master", "social", "media", "mine", "with", "python", "packt", "publish", "juli", "num", "origin", "repost", "with", "permiss", "relat", "mine", "twitter", "data", "with", "python", "part", "num", "collect", "data", "mine", "twitter", "data", "with", "python", "part", "num", "text", "preprocess", "mine", "twitter", "data", "with", "python", "part", "num", "term", "frequenc"], "timestamp_scraper": 1556363359.716152, "title": "Mining Twitter Data with Python Part 4: Rugby and Term Co-occurrences", "read_time": 522.9, "content_html": "<div class=\"post\" id=\"post-\">\n<p><b>By Marco Bonzanini, Independent Data Science Consultant</b>.</p>\n<p>Last Saturday (<em>Editor's note: happened a while ago</em>) was the closing day of the\u00a0<a href=\"http://en.wikipedia.org/wiki/Six_Nations_Championship\">Six Nations Championship</a>, an annual international\u00a0<a href=\"http://en.wikipedia.org/wiki/Rugby_union\">rugby</a>\u00a0competition. Before turning on the TV to watch Italy being trashed by Wales, I decided to use this event to collect some data from Twitter and perform some exploratory text analysis on something more interesting than the small list of my tweets.</p>\n<p>This article continues the tutorial on Twitter Data Mining, re-using what we discussed in the previous articles with some more realistic data. It also expands the analysis by introducing the concept of term co-occurrence.</p>\n<p><img alt=\"Twitter\" src=\"/wp-content/uploads/twitter-banner.jpg\" width=\"99%\"/></p>\n<h3>The Application Domain</h3>\n<p>\u00a0<br>\nAs the name suggests, six teams are involved in the competition: England, Ireland, Wales, Scotland, France and Italy. This means that we can expect the event to be tweeted in multiple languages (English, French, Italian, Welsh, Gaelic, possibly other languages as well), with English being the major language. Assuming the team names will be mentioned frequently, we could decide to look also for their nicknames, e.g.\u00a0<em>Les Bleus</em>\u00a0for France or\u00a0<em>Azzurri</em>\u00a0for Italy. During the last day of the competition, three matches are played sequentially. Three teams in particular had a shot for the title: England, Ireland and Wales. At the end, Ireland won the competition but everything was open until the very last minute.</br></p>\n<h3>Setting Up</h3>\n<p>\u00a0<br>\nI used the\u00a0<a href=\"/2016/06/mining-twitter-data-python-part-1.html\">streaming API</a>\u00a0to download all the tweets containing the string <tt>#rbs6nations</tt>\u00a0during the day. Obviously not all the tweets about the event contained the hashtag, but this is a good baseline. The time frame for the download was from around\u00a012:15PM to 7:15PM GMT, that is from about 15 minutes before the first match, to about 15 minutes after the last match was over. At the end, more than\u00a018,000 tweets\u00a0have been downloaded in JSON format, making for about\u00a075Mb\u00a0of data. This should be small enough to quickly do some processing in memory, and at the same time big enough to observe something possibly interesting.</br></p>\n<p>The textual content of the tweets has been pre-processed with <a href=\"/2016/06/mining-twitter-data-python-part-2.html\">tokenisation and lowercasing</a>\u00a0using the\u00a0<tt>preprocess()</tt>\u00a0function introduced in Part 2 of the tutorial.</p>\n<h3>Interesting terms and hashtags</h3>\n<p>\u00a0<br>\nFollowing what we discussed in\u00a0<a href=\"/2016/06/mining-twitter-data-python-part-3.html\">Part 3 (Term Frequencies)</a>, we want to observe the most common terms and hashtags used during day. If you have followed the discussion about creating different lists of tokens in order to capture terms without hashtags, hashtags only, removing stop-words, etc. you can play around with the different lists.</br></p>\n<p>This is the unsurprising list of top 10 most frequent terms (<tt>terms_only</tt>\u00a0in Part 3) in the data set.</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'ireland'</span>, <span style=\"color:#3387cc\">3163</span>), (<span style=\"color:#65b042\">'england'</span>, <span style=\"color:#3387cc\">2584</span>), (<span style=\"color:#65b042\">'wales'</span>, <span style=\"color:#3387cc\">2271</span>), \r\n  (<span style=\"color:#65b042\">''</span>, <span style=\"color:#3387cc\">2068</span>), (<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#3387cc\">1479</span>), (<span style=\"color:#65b042\">'france'</span>, <span style=\"color:#3387cc\">1380</span>), (<span style=\"color:#65b042\">'win'</span>, <span style=\"color:#3387cc\">1338</span>), \r\n  (<span style=\"color:#65b042\">'rugby'</span>, <span style=\"color:#3387cc\">1253</span>), (<span style=\"color:#65b042\">'points'</span>, <span style=\"color:#3387cc\">1221</span>), (<span style=\"color:#65b042\">'title'</span>, <span style=\"color:#3387cc\">1180</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The first three terms correspond to the teams who had a go for the title. The frequencies also respect the order in the final table. The fourth term is instead a punctuation mark that we missed and didn\u2019t include in the list of stop-words. This is because\u00a0<tt>string.punctuation</tt>\u00a0only contains ASCII symbols, while here we\u2019re dealing with a unicode character. If we dig into the data, there will be more examples like this, but for the moment we don\u2019t worry about it.</p>\n<p>After adding the suspension-points symbol to the list of stop-words, we have a new entry at the end of the list:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'ireland'</span>, <span style=\"color:#3387cc\">3163</span>), (<span style=\"color:#65b042\">'england'</span>, <span style=\"color:#3387cc\">2584</span>), (<span style=\"color:#65b042\">'wales'</span>, <span style=\"color:#3387cc\">2271</span>), \r\n  (<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#3387cc\">1479</span>), (<span style=\"color:#65b042\">'france'</span>, <span style=\"color:#3387cc\">1380</span>), (<span style=\"color:#65b042\">'win'</span>, <span style=\"color:#3387cc\">1338</span>), (<span style=\"color:#65b042\">'rugby'</span>, <span style=\"color:#3387cc\">1253</span>), \r\n  (<span style=\"color:#65b042\">'points'</span>, <span style=\"color:#3387cc\">1221</span>), (<span style=\"color:#65b042\">'title'</span>, <span style=\"color:#3387cc\">1180</span>), (<span style=\"color:#65b042\">'__SHAMROCK_SYMBOL__'</span>, <span style=\"color:#3387cc\">1154</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Interestingly, a new token we didn\u2019t account for, an\u00a0<a href=\"http://en.wikipedia.org/wiki/Emoji\">Emoji symbol</a>\u00a0(in this case, the\u00a0<a href=\"http://en.wikipedia.org/wiki/Shamrock\">Irish Shamrock</a>).</p>\n<p>If we have a look at the most common hashtags, we need to consider that<tt>#rbs6nations</tt>\u00a0will be by far the most common token (that\u2019s our search term for downloading the tweets), so we can exclude it from the list. This leave us with:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'#engvfra'</span>, <span style=\"color:#3387cc\">1701</span>), (<span style=\"color:#65b042\">'#itavwal'</span>, <span style=\"color:#3387cc\">927</span>), (<span style=\"color:#65b042\">'#rugby'</span>, <span style=\"color:#3387cc\">880</span>), \r\n  (<span style=\"color:#65b042\">'#scovire'</span>, <span style=\"color:#3387cc\">692</span>), (<span style=\"color:#65b042\">'#ireland'</span>, <span style=\"color:#3387cc\">686</span>), (<span style=\"color:#65b042\">'#angfra'</span>, <span style=\"color:#3387cc\">554</span>), \r\n  (<span style=\"color:#65b042\">'#xvdefrance'</span>, <span style=\"color:#3387cc\">508</span>), (<span style=\"color:#65b042\">'#crunch'</span>, <span style=\"color:#3387cc\">500</span>), (<span style=\"color:#65b042\">'#wales'</span>, <span style=\"color:#3387cc\">446</span>), \r\n  (<span style=\"color:#65b042\">'#england'</span>, <span style=\"color:#3387cc\">406</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>We can observe that the most common hashtags, a part from\u00a0<tt>#rugby</tt>, are related to the individual matches. In particular England v France has received the highest number of mentions, probably being the last match of the day with a dramatic finale. Something interesting to notice is that a fair amount of tweets also contained terms in French: the count for<tt>#angfra</tt>\u00a0should in fact be added to\u00a0<tt>#engvfra</tt>. Those unfamiliar with rugby probably wouldn\u2019t recognise that also\u00a0<tt>#crunch</tt>\u00a0should be included with<tt>#EngvFra</tt>\u00a0match, as\u00a0<em>Le Crunch</em>\u00a0is the traditional name for this event. So by far, the last match has received a lot of attention.</p>\n<h3>Term co-occurrences</h3>\n<p>\u00a0<br/>\nSometimes we are interested in the terms that occur together. This is mainly because the\u00a0<em>context</em>\u00a0gives us a better insight about the meaning of a term, supporting applications such as word disambiguation or semantic similarity. We discussed the option of using\u00a0<em>bigrams</em>\u00a0<a href=\"/2016/06/mining-twitter-data-python-part-3.html\">in the previous article</a>, but we want to extend the context of a term to the whole tweet.</p>\n<p>We can refactor the code from\u00a0<a href=\"/2016/06/mining-twitter-data-python-part-3.html\">the previous article</a>\u00a0in order to capture the\u00a0co-occurrences. We build a co-occurrence matrix\u00a0<tt>com</tt>\u00a0such that<tt>com[x][y]</tt>\u00a0contains the number of times the term\u00a0<tt>x</tt>\u00a0has been seen in the same tweet as the term\u00a0<tt>y</tt>:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\"><span style=\"color:#e28964\">from</span> collections <span style=\"color:#e28964\">import</span> defaultdict\r\n<span style=\"color:#aeaeae;font-style:italic\"># remember to include the other import from the previous post</span>\r\n \r\ncom <span style=\"color:#e28964\">=</span> defaultdict(<span style=\"color:#99cf50\">lambda</span> : defaultdict(<span style=\"color:#9b859d\">int</span>))\r\n \r\n<span style=\"color:#aeaeae;font-style:italic\"># f is the file pointer to the JSON data set</span>\r\n<span style=\"color:#e28964\">for</span> line <span style=\"color:#e28964\">in</span> f: \r\n    tweet <span style=\"color:#e28964\">=</span> json.loads(line)\r\n    terms_only <span style=\"color:#e28964\">=</span> [term <span style=\"color:#e28964\">for</span> term <span style=\"color:#e28964\">in</span> preprocess(tweet[<span style=\"color:#65b042\">'text'</span>]) \r\n                  <span style=\"color:#e28964\">if</span> term <span style=\"color:#e28964\">not</span> <span style=\"color:#e28964\">in</span> stop \r\n                  <span style=\"color:#e28964\">and</span> <span style=\"color:#e28964\">not</span> term.startswith((<span style=\"color:#65b042\">'#'</span>, <span style=\"color:#65b042\">'@'</span>))]\r\n \r\n    <span style=\"color:#aeaeae;font-style:italic\"># Build co-occurrence matrix</span>\r\n    <span style=\"color:#e28964\">for</span> i <span style=\"color:#e28964\">in</span> <span style=\"color:#dad085\">range</span>(<span style=\"color:#dad085\">len</span>(terms_only)<span style=\"color:#e28964\">-</span><span style=\"color:#3387cc\">1</span>):            \r\n        <span style=\"color:#e28964\">for</span> j <span style=\"color:#e28964\">in</span> <span style=\"color:#dad085\">range</span>(i<span style=\"color:#e28964\">+</span><span style=\"color:#3387cc\">1</span>, <span style=\"color:#dad085\">len</span>(terms_only)):\r\n            w1, w2 <span style=\"color:#e28964\">=</span> <span style=\"color:#dad085\">sorted</span>([terms_only[i], terms_only[j]])                \r\n            <span style=\"color:#e28964\">if</span> w1 <span style=\"color:#e28964\">!=</span> w2:\r\n                com[w1][w2] <span style=\"color:#e28964\">+=</span> <span style=\"color:#3387cc\">1</span>\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>While building the co-occurrence matrix, we don\u2019t want to count the same term pair twice, e.g.\u00a0<tt>com[A][B] == com[B][A]</tt>, so the inner for loop starts from\u00a0<tt>i+1</tt>\u00a0in order to build a triangular matrix, while\u00a0<tt>sorted</tt>\u00a0will preserve the alphabetical order of the terms.</p>\n<p>For each term, we then extract the 5 most frequent co-occurrent terms, creating a list of tuples in the form\u00a0<tt>((term1, term2), count)</tt>:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">com_max <span style=\"color:#e28964\">=</span> []\r\n<span style=\"color:#aeaeae;font-style:italic\"># For each term, look for the most common co-occurrent terms</span>\r\n<span style=\"color:#e28964\">for</span> t1 <span style=\"color:#e28964\">in</span> com:\r\n    t1_max_terms <span style=\"color:#e28964\">=</span> <span style=\"color:#dad085\">sorted</span>(com[t1].items(), <span style=\"color:#3e87e3\">key</span><span style=\"color:#e28964\">=</span>operator.itemgetter(<span style=\"color:#3387cc\">1</span>), <span style=\"color:#3e87e3\">reverse</span><span style=\"color:#e28964\">=</span><span style=\"color:#3387cc\">True</span>)[:<span style=\"color:#3387cc\">5</span>]\r\n    <span style=\"color:#e28964\">for</span> t2, t2_count <span style=\"color:#e28964\">in</span> t1_max_terms:\r\n        com_max.append(((t1, t2), t2_count))\r\n<span style=\"color:#aeaeae;font-style:italic\"># Get the most frequent co-occurrences</span>\r\nterms_max <span style=\"color:#e28964\">=</span> <span style=\"color:#dad085\">sorted</span>(com_max, <span style=\"color:#3e87e3\">key</span><span style=\"color:#e28964\">=</span>operator.itemgetter(<span style=\"color:#3387cc\">1</span>), <span style=\"color:#3e87e3\">reverse</span><span style=\"color:#e28964\">=</span><span style=\"color:#3387cc\">True</span>)\r\n<span style=\"color:#e28964\">print</span>(terms_max[:<span style=\"color:#3387cc\">5</span>])\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The results:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[((<span style=\"color:#65b042\">'6'</span>, <span style=\"color:#65b042\">'nations'</span>), <span style=\"color:#3387cc\">845</span>), ((<span style=\"color:#65b042\">'champions'</span>, <span style=\"color:#65b042\">'ireland'</span>), <span style=\"color:#3387cc\">760</span>), \r\n  ((<span style=\"color:#65b042\">'nations'</span>, <span style=\"color:#65b042\">'rbs'</span>), <span style=\"color:#3387cc\">742</span>), ((<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#65b042\">'ireland'</span>), <span style=\"color:#3387cc\">731</span>), \r\n  ((<span style=\"color:#65b042\">'ireland'</span>, <span style=\"color:#65b042\">'wales'</span>), <span style=\"color:#3387cc\">674</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>This implementation is pretty straightforward, but depending on the data set and on the use of the matrix, one might want to look into tools like\u00a0<tt>scipy.sparse</tt>\u00a0for building a sparse matrix.</p>\n<p>We could also look for a specific term and extract its most frequent co-occurrences. We simply need to modify the main loop including an extra counter, for example:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">search_word <span style=\"color:#e28964\">=</span> sys.argv[<span style=\"color:#3387cc\">1</span>] <span style=\"color:#aeaeae;font-style:italic\"># pass a term as a command-line argument</span>\r\ncount_search <span style=\"color:#e28964\">=</span> Counter()\r\n<span style=\"color:#e28964\">for</span> line <span style=\"color:#e28964\">in</span> f:\r\n    tweet <span style=\"color:#e28964\">=</span> json.loads(line)\r\n    terms_only <span style=\"color:#e28964\">=</span> [term <span style=\"color:#e28964\">for</span> term <span style=\"color:#e28964\">in</span> preprocess(tweet[<span style=\"color:#65b042\">'text'</span>]) \r\n                  <span style=\"color:#e28964\">if</span> term <span style=\"color:#e28964\">not</span> <span style=\"color:#e28964\">in</span> stop \r\n                  <span style=\"color:#e28964\">and</span> <span style=\"color:#e28964\">not</span> term.startswith((<span style=\"color:#65b042\">'#'</span>, <span style=\"color:#65b042\">'@'</span>))]\r\n    <span style=\"color:#e28964\">if</span> search_word <span style=\"color:#e28964\">in</span> terms_only:\r\n        count_search.update(terms_only)\r\n<span style=\"color:#e28964\">print</span>(<span style=\"color:#65b042\">\"Co-occurrence for <span style=\"color:#ddf2a4\">%s</span>:\"</span> <span style=\"color:#e28964\">%</span> search_word)\r\n<span style=\"color:#e28964\">print</span>(count_search.most_common(<span style=\"color:#3387cc\">20</span>))\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The outcome for \u201cireland\u201d:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'champions'</span>, <span style=\"color:#3387cc\">756</span>), (<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#3387cc\">727</span>), (<span style=\"color:#65b042\">'nations'</span>, <span style=\"color:#3387cc\">659</span>), (<span style=\"color:#65b042\">'wales'</span>, <span style=\"color:#3387cc\">654</span>), (<span style=\"color:#65b042\">'2015'</span>, <span style=\"color:#3387cc\">638</span>), \r\n  (<span style=\"color:#65b042\">'6'</span>, <span style=\"color:#3387cc\">613</span>), (<span style=\"color:#65b042\">'rbs'</span>, <span style=\"color:#3387cc\">585</span>), (<span style=\"color:#65b042\">'http://t.co/y0nvsvayln'</span>, <span style=\"color:#3387cc\">559</span>), (<span style=\"color:#65b042\">'__SHAMROCK_SYMBOL__'</span>, <span style=\"color:#3387cc\">526</span>), (<span style=\"color:#65b042\">'10'</span>, <span style=\"color:#3387cc\">522</span>), \r\n  (<span style=\"color:#65b042\">'win'</span>, <span style=\"color:#3387cc\">377</span>), (<span style=\"color:#65b042\">'england'</span>, <span style=\"color:#3387cc\">377</span>), (<span style=\"color:#65b042\">'twickenham'</span>, <span style=\"color:#3387cc\">361</span>), (<span style=\"color:#65b042\">'40'</span>, <span style=\"color:#3387cc\">360</span>), (<span style=\"color:#65b042\">'points'</span>, <span style=\"color:#3387cc\">356</span>), \r\n  (<span style=\"color:#65b042\">'sco'</span>, <span style=\"color:#3387cc\">355</span>), (<span style=\"color:#65b042\">'ire'</span>, <span style=\"color:#3387cc\">355</span>), (<span style=\"color:#65b042\">'title'</span>, <span style=\"color:#3387cc\">346</span>), (<span style=\"color:#65b042\">'scotland'</span>, <span style=\"color:#3387cc\">301</span>), (<span style=\"color:#65b042\">'turn'</span>, <span style=\"color:#3387cc\">295</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The outcome for \u201crugby\u201d:</p>\n<div style=\"background:#000;width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre style=\"background:#000;color:#f8f8f8\">[(<span style=\"color:#65b042\">'day'</span>, <span style=\"color:#3387cc\">476</span>), (<span style=\"color:#65b042\">'game'</span>, <span style=\"color:#3387cc\">160</span>), (<span style=\"color:#65b042\">'ireland'</span>, <span style=\"color:#3387cc\">143</span>), (<span style=\"color:#65b042\">'england'</span>, <span style=\"color:#3387cc\">132</span>), (<span style=\"color:#65b042\">'great'</span>, <span style=\"color:#3387cc\">105</span>), \r\n  (<span style=\"color:#65b042\">'today'</span>, <span style=\"color:#3387cc\">104</span>), (<span style=\"color:#65b042\">'best'</span>, <span style=\"color:#3387cc\">97</span>), (<span style=\"color:#65b042\">'well'</span>, <span style=\"color:#3387cc\">90</span>), (<span style=\"color:#65b042\">'ever'</span>, <span style=\"color:#3387cc\">89</span>), (<span style=\"color:#65b042\">'incredible'</span>, <span style=\"color:#3387cc\">87</span>), \r\n  (<span style=\"color:#65b042\">'amazing'</span>, <span style=\"color:#3387cc\">84</span>), (<span style=\"color:#65b042\">'done'</span>, <span style=\"color:#3387cc\">82</span>), (<span style=\"color:#65b042\">'amp'</span>, <span style=\"color:#3387cc\">71</span>), (<span style=\"color:#65b042\">'games'</span>, <span style=\"color:#3387cc\">66</span>), (<span style=\"color:#65b042\">'points'</span>, <span style=\"color:#3387cc\">64</span>), \r\n  (<span style=\"color:#65b042\">'monumental'</span>, <span style=\"color:#3387cc\">58</span>), (<span style=\"color:#65b042\">'strap'</span>, <span style=\"color:#3387cc\">56</span>), (<span style=\"color:#65b042\">'world'</span>, <span style=\"color:#3387cc\">55</span>), (<span style=\"color:#65b042\">'team'</span>, <span style=\"color:#3387cc\">55</span>), (<span style=\"color:#65b042\">'http://t.co/bhmeorr19i'</span>, <span style=\"color:#3387cc\">53</span>)]\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Overall, quite interesting.</p>\n<h3>Summary</h3>\n<p>\u00a0<br/>\nThis article has discussed a toy example of Text Mining on Twitter, using some realistic data taken during a sport event. Using what we have learnt in the previous episodes, we have downloaded some data using the streaming API, pre-processed the data in JSON format and extracted some interesting terms and hashtags from the tweets. The article has also introduced the concept of term co-occurrence, shown how to build a co-occurrence matrix and discussed how to use it to find some interesting insight.</p>\n<p><b>Bio: <a href=\"https://twitter.com/marcobonzanini\">Marco Bonzanini</a></b> is a Data Scientist based in London, UK. Active in the PyData community, he enjoys working in text analytics and data mining applications. He's the author of \"<a href=\"https://www.amazon.com/Mastering-Social-Media-Mining-Python-ebook/dp/B01BFD2Z2Q\">Mastering Social Media Mining with Python</a>\" (Packt Publishing, July 2016).</p>\n<p><a href=\"https://marcobonzanini.com/2015/03/23/mining-twitter-data-with-python-part-4-rugby-and-term-co-occurrences/\">Original</a>. Reposted with permission.</p>\n<p><b>Related</b>:</p>\n<ul class=\"three_ul\">\n<li><a href=\"/2016/06/mining-twitter-data-python-part-1.html\">Mining Twitter Data with Python Part 1: Collecting Data</a></li>\n<li><a href=\"/2016/06/mining-twitter-data-python-part-2.html\">Mining Twitter Data with Python Part 2: Text Pre-processing</a></li>\n<li><a href=\"/2016/06/mining-twitter-data-python-part-3.html\">Mining Twitter Data with Python Part 3: Term Frequencies</a></li>\n</ul>\n</div> ", "website": "kdnuggets"}