{"content": "By Pedro Lopez , KDnuggets. comments Deep Learning, one of the subfields of Machine Learning and Statistical Learning has been advancing in impressive levels in the past years. Cloud computing, robust open source tools and vast amounts of available data have been some of the levers for these impressive breakthroughs. The criteria used to select the 20 top papers is by using citation counts from academic.microsoft.com . It is important to mention that\u00a0these metrics are changing rapidly so the citations valued must be considered as the numbers when\u00a0this article was published. In this list of papers more than 75% refer to deep learning and neural networks, specifically Convolutional Neural Networks (CNN). Almost 50% of them refer to pattern recognition applications in the field of computer vision. I believe tools like TensorFlow, Theano and advancements in the use of GPUs have paved the way for data scientists and machine learning engineers to extend the field. 1.\u00a0 Deep Learning , by Yann L., Yoshua B. & Geoffrey H. (2015)\u00a0(Cited: 5,716) Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. 2.\u00a0 TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems , by Mart\u00edn A., Ashish A. B., Eugene B. C., et al. (2015) (Cited: 2,423) The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. 3.\u00a0 TensorFlow: a system for large-scale machine learning , by Mart\u00edn A., Paul B., Jianmin C., Zhifeng C., Andy D. et al. (2016) (Cited: 2,227) TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. 4.\u00a0 Deep learning\u00a0in neural networks , by Juergen Schmidhuber (2015)\u00a0(Cited: 2,196) This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks. 5.\u00a0 Human-level control through deep reinforcement learning , by Volodymyr M., Koray K., David S., Andrei A. R., Joel V et al (2015) (Cited: 2,086) Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. 6.\u00a0 Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks , by Shaoqing R., Kaiming H., Ross B. G. & Jian S. (2015) (Cited: 1,421) In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. 7.\u00a0 Long-term recurrent convolutional networks for visual recognition and description , by Jeff D., Lisa Anne H., Sergio G., Marcus R., Subhashini V. et al. (2015) (Cited: 1,285) In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \u201cdoubly deep\u201d in that they can be compositional in spatial and temporal \u201clayers\u201d. 8.\u00a0 MatConvNet: Convolutional Neural Networks for MATLAB , by \u00a0 Andrea Vedaldi & Karel Lenc (2015) (Cited: 1,148) It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox. 9.\u00a0 Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks , by Alec R., Luke M. & Soumith C. (2015)\u00a0(Cited: 1,054) In this work, we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. 10.\u00a0 U-Net: Convolutional Networks for Biomedical Image Segmentation , by Olaf R., Philipp F. &Thomas B. (2015) (Cited: 975) There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. 11.\u00a0 Conditional Random Fields as Recurrent Neural Networks , by Shuai Z., Sadeep J., Bernardino R., Vibhav V. et al (2015)\u00a0(Cited: 760) We introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks. 12.\u00a0 Image Super-Resolution Using Deep Convolutional Networks , by Chao D., Chen C., Kaiming H. & Xiaoou T. (2014) (Cited: 591) Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one 13.\u00a0 Beyond short snippets: Deep networks for video classification , by Joe Y. Ng, Matthew J. H., Sudheendra V., Oriol V., Rajat M. & George T. (2015) (Cited: 533) In this work, we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. 14.\u00a0 Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning , by Christian S., Sergey I., Vincent V. & Alexander A A. (2017)\u00a0(Cited: 520) Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. With an ensemble of three residual and one Inception-v4, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge. 15.\u00a0 Salient Object Detection: A Discriminative Regional Feature Integration Approach , by Huaizu J., Jingdong W., Zejian Y., Yang W., Nanning Z. & Shipeng Li. (2013) (Cited: 518) In this paper, we formulate saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, utilizes the supervised learning approach to map the regional feature vector to a saliency score. 16.\u00a0 Visual Madlibs: Fill in the Blank Description Generation and Question Answering , by Licheng Y., Eunbyung P., Alexander C. B. & Tamara L. B. (2015) (Cited: 510) In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. 17.\u00a0 Asynchronous methods for deep reinforcement learning , by Volodymyr M., Adri\u00e0 P. B., Mehdi M., Alex G., Tim H. et al. (2016) (Cited: 472) The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input. 18.\u00a0 Theano: A Python framework for fast computation of mathematical expressions . , by by Rami A., Guillaume A., Amjad A., Christof A. et al (2016) (Cited: 451) Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers especially in the machine learning community and has shown steady performance improvements. 19.\u00a0 Deep Learning\u00a0Face Attributes in the Wild , by Ziwei L., Ping L., Xiaogang W. & Xiaoou T. (2015) (Cited: 401) This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with imagelevel attribute tags, their response maps over entire images have strong indication of face locations. 20. Character-level convolutional networks for text classification , by Xiang Z., Junbo Jake Z. & Yann L. (2015) (Cited: 401) This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. \u00a0 Related: 7 Steps to Understanding Deep Learning Deep Learning \u2013 Past, Present, and Future The 10 Deep Learning Methods AI Practitioners Need to Apply", "title_html": "<h1 id=\"title\"><img align=\"right\" alt=\"Gold Blog\" src=\"/images/tkb-1804-g.png\" width=\"94\"/>Top 20 Deep Learning Papers, 2018 Edition</h1> ", "url": "https://www.kdnuggets.com/2018/03/top-20-deep-learning-papers-2018.html", "tfidf": {"tfidf": {"jianmin": 1587.6, "matthew": 6.908616187989999, "googl": 11.388809182200001, "broader": 12.5700712589, "unsupervis": 1380.5217391320002, "eugen": 11.1567111736, "lopez": 46.017391304300006, "relat": 1.23750876919, "region": 8.823921743, "detail": 2.26186066391, "form": 1.12755681818, "data": 13.50574223736, "ross": 9.563855421689999, "wide": 4.6795048143, "new": 3.0536641662, "assign": 3.83663605607, "number": 1.10142916609, "dataset": 774.439024392, "topnum": 933.882352941, "function": 2.495441685, "class": 2.11651779763, "thoma": 2.39673913043, "learner": 75.2417061611, "believ": 1.6450108797, "python": 112.5957446808, "consent": 11.4960173787, "amjad": 721.636363636, "sergio": 47.9637462236, "well": 2.1311497416, "product": 3.24529844644, "drug": 11.87434554974, "approach": 4.15113086678, "path": 4.6421052631599995, "toward": 1.6303142329, "lisa": 18.8327402135, "their": 3.0464372521500005, "automat": 6.787516032490001, "while": 1.0441988950299999, "bound": 5.40735694823, "pairwis": 387.219512195, "multipl": 5.49627834516, "alex": 10.655033557000001, "extend": 1.9604840701400004, "how": 3.20500656102, "test": 5.31414225942, "andi": 11.4545454545, "philipp": 19.6485148515, "segment": 15.11280342694, "present": 2.51103202846, "costfre": 1587.6, "academicmicrosoftcom": 1587.6, "than": 3.0983606557499996, "especi": 1.66712170534, "distribut": 2.7396031061299997, "end": 1.10680423871, "junbo": 1587.6, "heterogen": 52.0524590164, "deep": 97.95521023757999, "matconvnet": 3175.2, "overview": 12.6805111821, "consid": 1.2397313759200002, "snippet": 135.692307692, "thousand": 2.4767550702000003, "koray": 1587.6, "salienc": 1221.23076923, "constraint": 15.0483412322, "breakthrough": 16.434782608699997, "sensori": 40.2944162437, "near": 1.28769567686, "but": 1.01632417899, "demonstr": 2.64997496244, "certain": 1.8077886586200003, "need": 1.4372623574099999, "our": 4.71517671518, "classif": 32.268292682920006, "annot": 45.7521613832, "object": 16.44207723039, "summaris": 53.0969899666, "applic": 6.85344269372, "lnet": 3175.2, "empir": 2.8395635843299996, "ann": 5.08357348703, "robot": 20.0201765448, "has": 5.218248751, "generat": 6.15826221876, "dcgan": 1587.6, "impress": 7.80914904082, "tensorflow": 7938.0, "formul": 19.72173913044, "use": 16.474220118079998, "optim": 11.5377906977, "christian": 2.6785895056499998, "model": 10.452989202000001, "much": 1.1942229577299999, "introduc": 6.903359060760001, "pool": 7.052865393160001, "reli": 4.16146788991, "count": 3.48157894737, "zhifeng": 1587.6, "field": 12.4531600179, "program": 2.02139037433, "short": 2.82591669634, "process": 5.08574479446, "not": 1.01567398119, "involv": 1.4498630137000001, "doubli": 180.409090909, "specif": 1.8719490626099997, "xiaogang": 1587.6, "open": 1.24556723678, "citat": 33.6355932204, "metric": 22.235294117600002, "sourc": 1.69760479042, "graphic": 9.035856573710001, "design": 1.45825296225, "recurr": 142.3856502244, "effect": 1.3963060686000002, "allow": 2.5432118542200004, "scienc": 2.31969608416, "kdnugget": 1587.6, "causal": 36.081818181799996, "lever": 41.889182058, "construct": 1.9320920043799998, "rami": 273.724137931, "review": 2.2099109131400003, "highresolut": 1587.6, "video": 6.59439252336, "steadi": 10.7780040733, "ashish": 1587.6, "respons": 1.5066907089300001, "simpl": 3.3981164383599998, "appear": 1.3214582986499999, "that": 16.06374502, "algorithm": 55.9014084508, "array": 10.1444089457, "offer": 1.53896859248, "spatial": 24.4246153846, "shipeng": 1587.6, "longterm": 512.129032258, "develop": 1.1955719557200002, "oriol": 111.802816901, "compil": 5.696447793330001, "pattern": 3.79173632673, "succeed": 3.3550295857999997, "sever": 3.2172385841699995, "explor": 3.39593582888, "with": 10.011982089899998, "network": 77.8108152264, "three": 1.06621893889, "fast": 4.8729281768, "some": 1.04036697248, "beyond": 2.54586273252, "term": 1.39520168732, "control": 2.93918356012, "locat": 1.59766529134, "result": 1.14611608432, "document": 2.5409731114, "expos": 5.03680203046, "supervis": 23.22184300341, "classic": 2.4087391898, "filter": 33.7787234042, "sadeep": 1587.6, "jian": 189.0, "languag": 4.58976582828, "set": 1.18707940781, "futur": 1.8577112099200002, "question": 2.20408163265, "xiaoou": 3175.2, "outperform": 82.2590673575, "from": 3.00170164491, "answer": 4.64890190337, "work": 4.46080359652, "learnabl": 1587.6, "num": 71.02236784071, "jeff": 9.7339055794, "veri": 1.25880114177, "lowresolut": 1587.6, "ziwei": 1587.6, "multilevel": 441.0, "for": 23.00724592023, "predict": 10.3696930111, "output": 7.676982591880001, "releas": 1.8377126982299998, "fulli": 2.79015817223, "salient": 70.875, "practition": 18.8775267539, "competit": 3.06960556845, "theano": 4762.799999999999, "assum": 2.9575260804799997, "are": 8.23924748624, "marcus": 17.2004333694, "compos": 2.5060773480700003, "current": 3.0651607298, "vibhav": 1587.6, "technic": 3.1400316455699997, "histor": 1.6755672823199999, "shown": 2.76923076923, "gaussian": 144.327272727, "xiang": 182.482758621, "furthermor": 5.50294627383, "cloud": 10.6193979933, "refer": 2.6004914005, "extract": 7.703056768560001, "task": 3.88641370869, "asynchron": 496.125, "integr": 2.8254137746900003, "combin": 3.39520958084, "posit": 1.37252528746, "pave": 12.25, "eunbyung": 1587.6, "focus": 4.02025829324, "shaoq": 1587.6, "general": 1.1218202374200001, "inceptionvnum": 3175.2, "map": 20.36428937915, "and": 41.00258267733, "give": 1.3653250774, "lowhighresolut": 1587.6, "train": 13.5559892657, "they": 3.09051975861, "templat": 43.495890411000005, "shallow": 16.0363636364, "step": 2.8279301745599996, "error": 6.04109589041, "framework": 16.400826446279996, "rajat": 1587.6, "continu": 1.13928955867, "comment": 3.05954904606, "inceptionresnet": 1587.6, "reinforc": 25.814634146360003, "period": 1.3430335843, "scientist": 4.69426374926, "machin": 28.170342205319997, "into": 1.01502461479, "discoveri": 9.49805563864, "relev": 6.938811188810001, "rapid": 2.62586834271, "servic": 1.51300867245, "requir": 1.52844902282, "descript": 16.02018163472, "face": 7.2130849614, "local": 1.51720183486, "tag": 19.7462686567, "largescal": 4762.799999999999, "fullimag": 1587.6, "flexibl": 9.68639414277, "help": 1.39962972759, "larg": 3.55724848755, "instead": 1.59461631177, "potenti": 2.52080025405, "propos": 7.960887551719999, "tempor": 43.795862068999995, "approxim": 2.2132998745299997, "base": 2.2925631769, "natur": 3.0785340314200003, "can": 4.70504556568, "fact": 1.73375559681, "recapitul": 264.6, "atari": 184.6046511628, "here": 2.42307692308, "project": 1.7534791252500002, "express": 5.736239913270001, "yang": 35.9185520362, "averag": 2.60390355913, "through": 1.07074930869, "infer": 84.5592543276, "valu": 2.2777618364400003, "sudheendra": 1587.6, "yoshua": 1587.6, "layer": 16.28307692308, "subfield": 226.8, "evalu": 13.901926444839999, "humanlevel": 1587.6, "shuai": 1587.6, "kaim": 3175.2, "about": 2.12972030318, "problem": 3.53349655018, "deploy": 7.41869158879, "communiti": 1.96121062384, "scene": 3.45055422734, "neural": 772.9887640449, "context": 4.25972632144, "entir": 1.59365589239, "bridg": 3.7067476068199996, "understand": 2.96858638743, "featur": 6.10850327048, "each": 2.37949640288, "blank": 23.3470588235, "them": 1.09876115994, "defin": 2.72830383227, "finetun": 1587.6, "multidimension": 160.363636364, "alec": 40.917525773200005, "contrast": 2.88339992735, "across": 3.4637285916800002, "ensembl": 16.746835443, "cite": 54.6506024096, "tool": 9.99433427762, "articl": 4.03610016524, "enabl": 3.5421686747, "criteria": 11.7426035503, "ping": 110.25, "tim": 8.217391304349999, "vector": 25.898858075, "recognit": 26.401330376940003, "discrimin": 10.6981132075, "singl": 1.60948905109, "luke": 18.4390243902, "georg": 1.8779276082299998, "central": 1.6121039805000001, "imagelevel": 1587.6, "licheng": 1587.6, "soumith": 1587.6, "mani": 3.13280273631, "appli": 2.2972073506, "over": 2.05050048434, "build": 1.6341739578, "pedro": 19.7955112219, "provid": 2.43105428374, "pretrain": 1587.6, "adversari": 52.656716418, "biomed": 50.8846153846, "jingdong": 1587.6, "varieti": 6.8916220518, "subhashini": 1587.6, "unet": 1587.6, "endtoend": 3175.2, "connect": 1.8843916913900003, "inform": 4.72593768606, "meanfield": 1587.6, "crfs": 1587.6, "conduct": 2.2637958077900002, "retriev": 2.16826003824, "guillaum": 57.9416058394, "best": 1.5828514456600002, "repres": 1.46972782818, "hope": 2.50884955752, "take": 1.13961668222, "visual": 26.137635824849998, "previous": 2.85693719632, "karel": 84.44680851060001, "strategi": 8.88416340236, "robust": 19.9447236181, "toolbox": 453.6, "support": 1.2685577307200002, "interact": 4.4185917061, "represent": 17.784914115, "geoffrey": 15.236084453, "possibl": 1.4173734488, "regress": 51.2129032258, "publish": 1.36885669943, "strength": 4.02739726027, "indic": 2.0826446281, "fix": 4.4346368715099995, "chao": 13.0024570025, "also": 2.02953020134, "qnetwork": 1587.6, "genom": 38.255421686700004, "target": 3.2189781021900004, "such": 1.06151377374, "reveal": 5.26392572944, "motor": 6.718578078709999, "one": 4.02509982888, "vedaldi": 1587.6, "spatiotempor": 933.882352941, "composit": 4.629921259840001, "mehdi": 233.470588235, "recept": 8.934158694430002, "research": 3.8840366972400004, "the": 53.0, "area": 1.3881262568900001, "polici": 2.52963671128, "becom": 1.12492028626, "candid": 4.51279135873, "recent": 3.0881151527, "past": 4.03404904078, "peopl": 1.21320495186, "dozen": 5.95275590551, "challeng": 5.11633902674, "andrea": 13.627467811199999, "util": 4.65981802172, "sinc": 1.08368600683, "system": 5.54959363804, "evolutionari": 21.168000000000003, "share": 1.8566249561500001, "although": 1.14968498805, "attribut": 10.2469879518, "paper": 13.314324052350003, "joe": 5.86262924668, "histori": 1.20629131525, "which": 3.015575535, "wild": 5.04640813732, "random": 28.7608695652, "imagenet": 1587.6, "activ": 1.46403541129, "thus": 1.6463756092500001, "abstract": 9.966101694919999, "other": 2.01984732824, "introduct": 2.7808723068799996, "consist": 1.4901445466499998, "show": 3.80111731845, "bank": 2.87400434468, "impact": 2.97526236882, "easytous": 1587.6, "christof": 588.0, "routin": 7.997984886649999, "margin": 6.16783216783, "mathemat": 22.17318435753, "imag": 24.31240428789, "chen": 34.0686695279, "like": 1.14918566775, "implement": 3.57648118946, "mart\u00edn": 3175.2, "lenc": 1587.6, "linear": 13.8776223776, "collect": 1.64109985528, "madlib": 3175.2, "agent": 8.51716738198, "joel": 16.764519535399998, "maze": 64.5365853659, "attempt": 1.4721810088999998, "backpropag": 1587.6, "includ": 2.0381282495599997, "david": 1.84970290108, "millennium": 10.979253112, "gather": 3.78631051753, "opensourc": 1587.6, "superresolut": 1587.6, "paul": 2.1979786792200002, "this": 15.056904400650001, "detect": 21.65155131264, "sampl": 14.46560364464, "time": 2.02254920696, "engin": 2.47135740971, "chang": 1.1808985421, "perform": 6.125590817000001, "andrei": 54.0, "survey": 3.7791002142300005, "actorcrit": 3175.2, "differ": 1.23654490225, "been": 5.119638826199999, "schmidhub": 1221.23076923, "statist": 4.24265098878, "valuabl": 7.46754468485, "most": 1.02096463023, "between": 3.1036100612399995, "huaizu": 1587.6, "game": 2.57978550536, "domain": 28.182248520719998, "credit": 3.04312823462, "vision": 9.76083615124, "largest": 2.0511627907, "top": 1.8387769284200002, "input": 36.6087624903, "action": 1.81855670103, "augment": 16.5202913632, "anet": 1587.6, "strong": 4.931966449199999, "there": 1.04091266719, "condit": 5.774490785639999, "vincent": 11.1802816901, "almost": 1.53584212054, "fillintheblank": 1587.6, "longer": 2.02319357716, "produc": 1.36932896326, "alexand": 8.38890356672, "nan": 1587.6, "architectur": 10.25581395348, "indirect": 8.618892508139998, "zejian": 1587.6, "call": 1.0676529926, "navig": 8.290339425589998, "more": 4.0686827268, "improv": 6.13130792997, "list": 1.36321483771, "novel": 4.06555697823, "achiev": 3.74433962264, "these": 3.22246278756, "compact": 12.451764705899999, "volodymyr": 1221.23076923, "amount": 2.27027027027, "artifici": 8.31639601886, "faster": 7.61438848921, "realtim": 429.081081081, "librari": 2.68266306185, "highdimension": 1587.6, "probabilist": 127.008, "juergen": 793.8, "year": 2.0970873786400004, "simultan": 5.32930513595, "select": 2.02345144022, "fill": 3.33809924306, "avail": 3.4576935642, "surpass": 10.1185468451, "comput": 43.20534388918001, "encod": 29.0237659963, "way": 1.2190739461, "search": 3.2539454806299997, "onli": 2.0512953033200003, "gap": 7.302667893280001, "stateoftheart": 6350.4, "distinguish": 3.36926994907, "bernardino": 79.7788944724, "speech": 7.645557428360001, "depth": 8.24299065421, "adri\u00e0": 1587.6, "link": 2.15151104486, "advanc": 7.9989923164, "chain": 5.17639387023, "jake": 34.588235294099995, "block": 6.40548718984, "yann": 756.0, "cnns": 7938.0, "method": 15.428571428580002, "olaf": 122.123076923, "level": 3.3088786994599997, "convolut": 1820.178343944, "must": 1.9220338983099996, "convnet": 1587.6, "sergey": 85.3548387097, "text": 6.25655172414, "gpus": 1058.4, "variant": 6.8490077653100006, "direct": 2.44452998692, "import": 1.3401992233700002, "success": 3.9600897979500003, "geograph": 4.7306317044099995, "residu": 48.7741935484, "could": 1.2043695949, "have": 7.104263887979998, "characterlevel": 4762.799999999999, "score": 8.576985413300001, "tamara": 129.073170732, "dramat": 3.9849397590400004, "multicor": 1587.6, "half": 1.75813953488, "learn": 76.65076810545001, "sequenti": 39.5910224439, "when": 1.02076769755, "mention": 2.53894130817, "effici": 10.18671799808, "vast": 4.05620848237}, "logtfidf": {"jianmin": 7.369978720910001, "matthew": 1.9327693554900003, "googl": 2.43263122258, "broader": 2.53131869155, "unsupervis": 23.375689669639996, "eugen": 2.4120412158099995, "lopez": 3.8290193968699997, "relat": 0.21310030165399999, "region": 2.84014250412, "detail": 0.816187777173, "form": 0.120053184191, "data": 4.8672823392, "ross": 2.25799093255, "wide": 1.3337400202619998, "new": 0.0531898405533, "assign": 1.3445959556, "number": 0.0966085784186, "dataset": 21.06337826656, "topnum": 6.83935046985, "function": 0.914465741594, "class": 0.7497721899330001, "thoma": 0.874109117838, "learner": 4.320705680430001, "believ": 0.497746997996, "python": 8.06131348592, "consent": 2.44200066049, "amjad": 6.58152136054, "sergio": 3.8704454385300004, "well": 0.1270288766312, "product": 0.968120273072, "drug": 3.56246611248, "approach": 1.4604672291620002, "path": 1.5351679838499999, "toward": 0.48877277716000006, "lisa": 2.9355968558999996, "their": 0.046081515368100005, "automat": 1.9150850473199998, "while": 0.04324998379380001, "bound": 1.68776042417, "pairwis": 5.958991747200001, "multipl": 2.02184803624, "alex": 2.36603241496, "extend": 0.673191417311, "how": 0.9431339138600001, "test": 1.954448874206, "andi": 2.43838663415, "philipp": 2.97800175538, "segment": 4.04479022612, "present": 0.455093309598, "costfre": 7.369978720910001, "academicmicrosoftcom": 7.369978720910001, "than": 0.0967825866546, "especi": 0.511098609709, "distribut": 1.00781305813, "end": 0.101476798618, "junbo": 7.369978720910001, "heterogen": 3.9522520373, "deep": 34.7941836846, "matconvnet": 14.739957441820001, "overview": 2.54006626224, "consid": 0.214894723824, "snippet": 4.91038987911, "thousand": 0.906949263988, "koray": 7.369978720910001, "salienc": 12.828934551760002, "constraint": 2.7112677679900004, "breakthrough": 2.7993999796900004, "sensori": 3.69621290461, "near": 0.252854324034, "but": 0.0161923720719, "demonstr": 0.9745501918189999, "certain": 0.592104362781, "need": 0.362740163442, "our": 1.7152784283640001, "classif": 8.35116294516, "annot": 6.2601837067999995, "object": 5.977535093620999, "summaris": 3.97212024051, "applic": 2.46320785698, "lnet": 14.739957441820001, "empir": 1.04365037288, "ann": 1.62601445662, "robot": 2.99674059227, "has": 0.213619724274, "generat": 2.1575470252080002, "dcgan": 7.369978720910001, "impress": 2.7242976394200005, "tensorflow": 36.849893604550005, "formul": 4.5771487118400005, "use": 0.4673283157056, "optim": 2.4456277954099996, "christian": 0.985290352115, "model": 3.687250365555, "much": 0.17749572930100002, "introduc": 2.1828550097040003, "pool": 1.953433973, "reli": 1.42586787018, "count": 1.24748591139, "zhifeng": 7.369978720910001, "field": 4.032449808457001, "program": 0.7037855787649999, "short": 0.691371251358, "process": 1.583487597075, "not": 0.0155524130075, "involv": 0.371469078658, "doubli": 5.19522699942, "specif": 0.626980167541, "xiaogang": 7.369978720910001, "open": 0.219591038029, "citat": 5.6448752955199994, "metric": 3.1016808515599994, "sourc": 0.529218310751, "graphic": 2.20120072572, "design": 0.377239118022, "recurr": 14.288979447520001, "effect": 0.333830227158, "allow": 0.48056122237800003, "scienc": 0.841436178891, "kdnugget": 7.369978720910001, "causal": 3.58578908699, "lever": 3.73502760882, "construct": 0.658603355972, "rami": 5.61212080336, "review": 0.7929522039210001, "highresolut": 7.369978720910001, "video": 2.38614497934, "steadi": 2.37750739744, "ashish": 7.369978720910001, "respons": 0.40991566230300003, "simpl": 1.2232212893899999, "appear": 0.278735898493, "that": 0.06361837407424, "algorithm": 6.66088479036, "array": 2.31692271093, "offer": 0.431112446902, "spatial": 3.1955914510100003, "shipeng": 7.369978720910001, "longterm": 6.238576609419999, "develop": 0.178624694913, "oriol": 4.7167367562999996, "compil": 1.7398427864200001, "pattern": 1.33282404788, "succeed": 1.2104605888, "sever": 0.20973336119069996, "explor": 1.22257937218, "with": 0.0119749171339, "network": 28.592491591559998, "three": 0.06411868822490001, "fast": 1.5836950247400001, "some": 0.0395735090645, "beyond": 0.934469583725, "term": 0.33303898354600003, "control": 0.7699693231720001, "locat": 0.46854337067199997, "result": 0.136378908381, "document": 0.932547122383, "expos": 1.6167713629299998, "supervis": 6.1394431674900005, "classic": 0.8791034528499999, "filter": 5.65336787728, "sadeep": 7.369978720910001, "jian": 5.24174701506, "languag": 1.6613636488119998, "set": 0.171496011289, "futur": 0.619345197699, "question": 0.790310929014, "xiaoou": 14.739957441820001, "outperform": 4.409873625, "from": 0.0017011625065979999, "answer": 1.5366310419, "work": 0.436138269092, "learnabl": 7.369978720910001, "num": 0.022364318073187003, "jeff": 2.27561521128, "veri": 0.230159793238, "lowresolut": 7.369978720910001, "ziwei": 7.369978720910001, "multilevel": 6.08904487545, "for": 0.007244779094131001, "predict": 3.2914804753799998, "output": 2.03822657827, "releas": 0.608521699544, "fulli": 1.02609828678, "salient": 4.26091776205, "practition": 2.93797215393, "competit": 1.12154907401, "theano": 22.10993616273, "assum": 1.08435313525, "are": 0.2357397886616, "marcus": 2.8449345794, "compos": 0.918718721148, "current": 0.8539056556900001, "vibhav": 7.369978720910001, "technic": 1.14423287808, "histor": 0.516151783952, "shown": 1.01856958099, "gaussian": 4.97208344811, "xiang": 5.206655695249999, "furthermor": 1.70528363496, "cloud": 2.36268232808, "refer": 0.525106493596, "extract": 2.04161723301, "task": 1.35748680661, "asynchron": 15.324646867320002, "integr": 1.03865482279, "combin": 1.058436621502, "posit": 0.316652318608, "pave": 2.50552593699, "eunbyung": 7.369978720910001, "focus": 1.3963979441119998, "shaoq": 7.369978720910001, "general": 0.114952578063, "inceptionvnum": 14.739957441820001, "map": 7.0217246692, "and": 0.0025825958246076, "give": 0.311392552224, "lowhighresolut": 7.369978720910001, "train": 4.626428189873001, "they": 0.0891809843028, "templat": 3.7726664603199995, "shallow": 2.77485887077, "step": 1.03954505698, "error": 1.7985854343, "framework": 4.20836909214, "rajat": 7.369978720910001, "continu": 0.13040487398700001, "comment": 1.11826753454, "inceptionresnet": 7.369978720910001, "reinforc": 7.45858873992, "period": 0.294930924153, "scientist": 1.54634128444, "machin": 9.74651706434, "into": 0.0149128632287, "discoveri": 3.1158798549599998, "relev": 1.9371304613999998, "rapid": 0.965411638564, "servic": 0.41410016674500005, "requir": 0.424253510675, "descript": 5.5502196738, "face": 2.358409485028, "local": 0.416867740206, "tag": 2.98296454472, "largescal": 22.10993616273, "fullimag": 7.369978720910001, "flexibl": 2.2707222351599996, "help": 0.336207721344, "larg": 0.511125181818, "instead": 0.46663315041500003, "potenti": 0.9245764122419999, "propos": 2.7529845359680003, "tempor": 6.1727843181, "approxim": 0.7944845577770001, "base": 0.27304660457400004, "natur": 0.862612678584, "can": 0.649364385576, "fact": 0.5502899207949999, "recapitul": 5.57821925168, "atari": 9.05013867418, "here": 0.8850381883700001, "project": 0.561601885907, "express": 1.9445749189230002, "yang": 3.58125393183, "averag": 0.957011687995, "through": 0.0683586918849, "infer": 12.204632648559999, "valu": 0.823193310148, "sudheendra": 7.369978720910001, "yoshua": 7.369978720910001, "layer": 4.1939583247000005, "subfield": 5.4240685718499995, "evalu": 3.8777604862599997, "humanlevel": 7.369978720910001, "shuai": 7.369978720910001, "kaim": 14.739957441820001, "about": 0.1256869549492, "problem": 1.138281448546, "deploy": 2.00400270589, "communiti": 0.673561947791, "scene": 1.23853486375, "neural": 53.1090970215, "context": 1.44920491442, "entir": 0.46603068026999994, "bridg": 1.31015483629, "understand": 1.0880858756799998, "featur": 1.693549672568, "each": 0.347483378608, "blank": 3.15047101573, "them": 0.0941833269093, "defin": 1.00368010925, "finetun": 7.369978720910001, "multidimension": 5.0774439637699995, "alec": 3.7115584742800003, "contrast": 1.0589701282, "across": 1.098396911882, "ensembl": 2.81820931165, "cite": 20.104559281199997, "tool": 3.21774235926, "articl": 1.404263479148, "enabl": 1.26473915954, "criteria": 2.4632235573, "ping": 4.70275051433, "tim": 2.10625279913, "vector": 3.25419887797, "recognit": 8.889929596320002, "discrimin": 2.37006739018, "singl": 0.475916769059, "luke": 2.9144693094800003, "georg": 0.630168832776, "central": 0.477540146039, "imagelevel": 7.369978720910001, "licheng": 7.369978720910001, "soumith": 7.369978720910001, "mani": 0.1299472743663, "appli": 0.8316941898119999, "over": 0.0498734429914, "build": 0.491137452091, "pedro": 2.98545520604, "provid": 0.39035568865000003, "pretrain": 7.369978720910001, "adversari": 6.54129323436, "biomed": 3.9295606260900002, "jingdong": 7.369978720910001, "varieti": 2.4950825694359997, "subhashini": 7.369978720910001, "unet": 7.369978720910001, "endtoend": 14.739957441820001, "connect": 0.633605058682, "inform": 1.363361113986, "meanfield": 7.369978720910001, "crfs": 7.369978720910001, "conduct": 0.817042965366, "retriev": 0.773925020223, "guillaum": 4.05943570751, "best": 0.459227932947, "repres": 0.38507723275, "hope": 0.919824304455, "take": 0.130691962197, "visual": 8.269691744300001, "previous": 0.713205920126, "karel": 4.43612185107, "strategi": 2.98224623636, "robust": 2.9929646280599997, "toolbox": 6.117215752409999, "support": 0.237880610037, "interact": 1.4858210267899998, "represent": 5.33921486295, "geoffrey": 2.7236665915900002, "possibl": 0.348805474891, "regress": 3.9359915164199997, "publish": 0.313975865467, "strength": 1.3931203261899998, "indic": 0.7336385419149999, "fix": 1.48944573451, "chao": 2.56513833979, "also": 0.0293143156, "qnetwork": 7.369978720910001, "genom": 3.64428529367, "target": 1.1690639496200002, "such": 0.059695977806, "reveal": 1.9354598084299999, "motor": 1.9048765367200002, "one": 0.025021406582, "vedaldi": 7.369978720910001, "spatiotempor": 6.83935046985, "composit": 1.5325398614399999, "mehdi": 5.45305610873, "recept": 2.18988198575, "research": 1.327455636276, "the": 0.0, "area": 0.327954821122, "polici": 0.92807570005, "becom": 0.11771217648900001, "candid": 1.50691588861, "recent": 0.868827482576, "past": 1.4032468315220001, "peopl": 0.193265578473, "dozen": 1.78385428972, "challeng": 1.8785839377900002, "andrea": 2.6120874479, "util": 1.5389763962399998, "sinc": 0.0803681994577, "system": 1.30972138234, "evolutionari": 3.0524906073699998, "share": 0.618760299747, "although": 0.139487981418, "attribut": 3.6851145461100003, "paper": 4.897012698325, "joe": 1.7685981798700001, "histori": 0.187550624069, "which": 0.01553524153629, "wild": 1.61867673028, "random": 7.890885626039999, "imagenet": 7.369978720910001, "activ": 0.381196603284, "thus": 0.49857627139300004, "abstract": 2.29918950399, "other": 0.01974949583952, "introduct": 1.02276465794, "consist": 0.398873126426, "show": 0.710048298039, "bank": 1.0557062993700002, "impact": 1.09033222631, "easytous": 7.369978720910001, "christof": 6.3767269479, "routin": 2.07918962078, "margin": 1.81934742575, "mathemat": 6.00081407481, "imag": 8.94385896561, "chen": 3.5283781797800002, "like": 0.139053576545, "implement": 1.27437940907, "mart\u00edn": 14.739957441820001, "lenc": 7.369978720910001, "linear": 2.63027764196, "collect": 0.49536666052, "madlib": 14.739957441820001, "agent": 2.8978732762, "joel": 2.81926472072, "maze": 4.16723227797, "attempt": 0.38674498075099994, "backpropag": 7.369978720910001, "includ": 0.037769362781, "david": 0.615025032185, "millennium": 2.39600741118, "gather": 1.3313920667299999, "opensourc": 7.369978720910001, "superresolut": 7.369978720910001, "paul": 0.7875381558519999, "this": 0.0567967357875, "detect": 6.75513097972, "sampl": 3.9572529767800004, "time": 0.0224230377252, "engin": 0.904767558276, "chang": 0.166275625058, "perform": 1.70472340232, "andrei": 3.9889840465599997, "survey": 1.3294859427299999, "actorcrit": 14.739957441820001, "differ": 0.212321121312, "been": 0.11822991184200002, "schmidhub": 7.1076144564399995, "statist": 1.4451883070700002, "valuabl": 2.010566255, "most": 0.020747896295599998, "between": 0.10186104349589999, "huaizu": 7.369978720910001, "game": 0.9477062580210001, "domain": 6.720240017969999, "credit": 1.11288601088, "vision": 3.17046177486, "largest": 0.7184068473190001, "top": 0.609100637788, "input": 7.50502600617, "action": 0.598043165069, "augment": 2.8045894049299998, "anet": 7.369978720910001, "strong": 1.491376481808, "there": 0.0400978929255, "condit": 1.964513364618, "vincent": 2.4141516633099998, "almost": 0.42907884333400004, "fillintheblank": 7.369978720910001, "longer": 0.7046772417749999, "produc": 0.314320812003, "alexand": 2.86752529606, "nan": 7.369978720910001, "architectur": 3.26939515838, "indirect": 2.15395659709, "zejian": 7.369978720910001, "call": 0.0654627744488, "navig": 2.11509091229, "more": 0.06809972639999999, "improv": 2.144387411796, "list": 0.309845761506, "novel": 1.40255075163, "achiev": 1.2541961702339999, "these": 0.2146008582024, "compact": 2.5218623563099998, "volodymyr": 12.828934551760002, "amount": 0.819898886199, "artifici": 2.11822899018, "faster": 2.03003967967, "realtim": 6.0616459012599995, "librari": 0.986809980943, "highdimension": 7.369978720910001, "probabilist": 4.8442500766, "juergen": 6.676831540349999, "year": 0.09480447778920001, "simultan": 1.67322086119, "select": 0.704804687133, "fill": 1.20540155609, "avail": 1.094909172578, "surpass": 2.31437006117, "comput": 15.048758075339999, "encod": 3.36811501148, "way": 0.19809150993500002, "search": 1.1798682540899998, "onli": 0.050648536658199995, "gap": 1.98823974622, "stateoftheart": 29.479914883640003, "distinguish": 1.21469608857, "bernardino": 4.37925898918, "speech": 2.6819551405400004, "depth": 2.10936322154, "adri\u00e0": 7.369978720910001, "link": 0.7661704068449999, "advanc": 2.7720848487120002, "chain": 1.64410864979, "jake": 3.54351360384, "block": 2.32801563176, "yann": 11.869788391239998, "cnns": 36.849893604550005, "method": 5.666769653046, "olaf": 4.80502936345, "level": 1.006924379886, "convolut": 83.09372415390001, "must": 0.653383947388, "convnet": 7.369978720910001, "sergey": 4.44681714019, "text": 2.28096401998, "gpus": 6.964513612799999, "variant": 1.92410379, "direct": 0.401411378992, "import": 0.292818277066, "success": 0.8329632377759999, "geograph": 1.55405874632, "residu": 6.388108343380001, "could": 0.18595627229000003, "have": 0.1034950163884, "characterlevel": 22.10993616273, "score": 2.9118706415400006, "tamara": 4.860379458530001, "dramat": 1.3825221952000002, "multicor": 7.369978720910001, "half": 0.564256167492, "learn": 27.810818136585, "sequenti": 3.6786023866, "when": 0.0205549888584, "mention": 0.931747186336, "effici": 3.25587506828, "vast": 1.40024866595}, "logidf": {"jianmin": 7.369978720910001, "matthew": 1.9327693554900003, "googl": 2.43263122258, "broader": 2.53131869155, "unsupervis": 5.843922417409999, "eugen": 2.4120412158099995, "lopez": 3.8290193968699997, "relat": 0.21310030165399999, "region": 0.568028500824, "detail": 0.816187777173, "form": 0.120053184191, "data": 1.2168205848, "ross": 2.25799093255, "wide": 0.44458000675399995, "new": 0.0177299468511, "assign": 1.3445959556, "number": 0.0966085784186, "dataset": 5.26584456664, "topnum": 6.83935046985, "function": 0.914465741594, "class": 0.7497721899330001, "thoma": 0.874109117838, "learner": 4.320705680430001, "believ": 0.497746997996, "python": 4.03065674296, "consent": 2.44200066049, "amjad": 6.58152136054, "sergio": 3.8704454385300004, "well": 0.0635144383156, "product": 0.484060136536, "drug": 1.78123305624, "approach": 0.7302336145810001, "path": 1.5351679838499999, "toward": 0.48877277716000006, "lisa": 2.9355968558999996, "their": 0.015360505122700001, "automat": 1.9150850473199998, "while": 0.04324998379380001, "bound": 1.68776042417, "pairwis": 5.958991747200001, "multipl": 1.01092401812, "alex": 2.36603241496, "extend": 0.673191417311, "how": 0.47156695693000006, "test": 0.977224437103, "andi": 2.43838663415, "philipp": 2.97800175538, "segment": 2.02239511306, "present": 0.227546654799, "costfre": 7.369978720910001, "academicmicrosoftcom": 7.369978720910001, "than": 0.0322608622182, "especi": 0.511098609709, "distribut": 1.00781305813, "end": 0.101476798618, "junbo": 7.369978720910001, "heterogen": 3.9522520373, "deep": 1.2886734698, "matconvnet": 7.369978720910001, "overview": 2.54006626224, "consid": 0.214894723824, "snippet": 4.91038987911, "thousand": 0.906949263988, "koray": 7.369978720910001, "salienc": 6.414467275880001, "constraint": 2.7112677679900004, "breakthrough": 2.7993999796900004, "sensori": 3.69621290461, "near": 0.252854324034, "but": 0.0161923720719, "demonstr": 0.9745501918189999, "certain": 0.592104362781, "need": 0.362740163442, "our": 0.8576392141820001, "classif": 2.08779073629, "annot": 3.1300918533999997, "object": 0.853933584803, "summaris": 3.97212024051, "applic": 1.23160392849, "lnet": 7.369978720910001, "empir": 1.04365037288, "ann": 1.62601445662, "robot": 2.99674059227, "has": 0.0427239448548, "generat": 0.719182341736, "dcgan": 7.369978720910001, "impress": 1.3621488197100002, "tensorflow": 7.369978720910001, "formul": 2.2885743559200002, "use": 0.0292080197316, "optim": 2.4456277954099996, "christian": 0.985290352115, "model": 0.7374500731110001, "much": 0.17749572930100002, "introduc": 0.5457137524260001, "pool": 1.953433973, "reli": 1.42586787018, "count": 1.24748591139, "zhifeng": 7.369978720910001, "field": 0.5760642583510001, "program": 0.7037855787649999, "short": 0.345685625679, "process": 0.527829199025, "not": 0.0155524130075, "involv": 0.371469078658, "doubli": 5.19522699942, "specif": 0.626980167541, "xiaogang": 7.369978720910001, "open": 0.219591038029, "citat": 2.8224376477599997, "metric": 3.1016808515599994, "sourc": 0.529218310751, "graphic": 2.20120072572, "design": 0.377239118022, "recurr": 3.5722448618800002, "effect": 0.333830227158, "allow": 0.24028061118900002, "scienc": 0.841436178891, "kdnugget": 7.369978720910001, "causal": 3.58578908699, "lever": 3.73502760882, "construct": 0.658603355972, "rami": 5.61212080336, "review": 0.7929522039210001, "highresolut": 7.369978720910001, "video": 1.19307248967, "steadi": 2.37750739744, "ashish": 7.369978720910001, "respons": 0.40991566230300003, "simpl": 1.2232212893899999, "appear": 0.278735898493, "that": 0.00397614837964, "algorithm": 3.33044239518, "array": 2.31692271093, "offer": 0.431112446902, "spatial": 3.1955914510100003, "shipeng": 7.369978720910001, "longterm": 6.238576609419999, "develop": 0.178624694913, "oriol": 4.7167367562999996, "compil": 1.7398427864200001, "pattern": 1.33282404788, "succeed": 1.2104605888, "sever": 0.06991112039689999, "explor": 1.22257937218, "with": 0.00119749171339, "network": 0.9530830530519999, "three": 0.06411868822490001, "fast": 1.5836950247400001, "some": 0.0395735090645, "beyond": 0.934469583725, "term": 0.33303898354600003, "control": 0.38498466158600003, "locat": 0.46854337067199997, "result": 0.136378908381, "document": 0.932547122383, "expos": 1.6167713629299998, "supervis": 2.04648105583, "classic": 0.8791034528499999, "filter": 2.82668393864, "sadeep": 7.369978720910001, "jian": 5.24174701506, "languag": 0.8306818244059999, "set": 0.171496011289, "futur": 0.619345197699, "question": 0.790310929014, "xiaoou": 7.369978720910001, "outperform": 4.409873625, "from": 0.000567054168866, "answer": 1.5366310419, "work": 0.109034567273, "learnabl": 7.369978720910001, "num": 0.00031499039539700004, "jeff": 2.27561521128, "veri": 0.230159793238, "lowresolut": 7.369978720910001, "ziwei": 7.369978720910001, "multilevel": 6.08904487545, "for": 0.00031499039539700004, "predict": 1.6457402376899999, "output": 2.03822657827, "releas": 0.608521699544, "fulli": 1.02609828678, "salient": 4.26091776205, "practition": 2.93797215393, "competit": 1.12154907401, "theano": 7.369978720910001, "assum": 1.08435313525, "are": 0.0294674735827, "marcus": 2.8449345794, "compos": 0.918718721148, "current": 0.42695282784500005, "vibhav": 7.369978720910001, "technic": 1.14423287808, "histor": 0.516151783952, "shown": 1.01856958099, "gaussian": 4.97208344811, "xiang": 5.206655695249999, "furthermor": 1.70528363496, "cloud": 2.36268232808, "refer": 0.262553246798, "extract": 2.04161723301, "task": 1.35748680661, "asynchron": 5.10821562244, "integr": 1.03865482279, "combin": 0.529218310751, "posit": 0.316652318608, "pave": 2.50552593699, "eunbyung": 7.369978720910001, "focus": 0.6981989720559999, "shaoq": 7.369978720910001, "general": 0.114952578063, "inceptionvnum": 7.369978720910001, "map": 1.40434493384, "and": 6.29901420636e-05, "give": 0.311392552224, "lowhighresolut": 7.369978720910001, "train": 0.660918312839, "they": 0.0297269947676, "templat": 3.7726664603199995, "shallow": 2.77485887077, "step": 1.03954505698, "error": 1.7985854343, "framework": 2.10418454607, "rajat": 7.369978720910001, "continu": 0.13040487398700001, "comment": 1.11826753454, "inceptionresnet": 7.369978720910001, "reinforc": 1.86464718498, "period": 0.294930924153, "scientist": 1.54634128444, "machin": 1.39235958062, "into": 0.0149128632287, "discoveri": 1.5579399274799999, "relev": 1.9371304613999998, "rapid": 0.965411638564, "servic": 0.41410016674500005, "requir": 0.424253510675, "descript": 1.38755491845, "face": 0.589602371257, "local": 0.416867740206, "tag": 2.98296454472, "largescal": 7.369978720910001, "fullimag": 7.369978720910001, "flexibl": 2.2707222351599996, "help": 0.336207721344, "larg": 0.17037506060600002, "instead": 0.46663315041500003, "potenti": 0.9245764122419999, "propos": 0.6882461339920001, "tempor": 3.08639215905, "approxim": 0.7944845577770001, "base": 0.13652330228700002, "natur": 0.431306339292, "can": 0.162341096394, "fact": 0.5502899207949999, "recapitul": 5.57821925168, "atari": 4.52506933709, "here": 0.8850381883700001, "project": 0.561601885907, "express": 0.648191639641, "yang": 3.58125393183, "averag": 0.957011687995, "through": 0.0683586918849, "infer": 3.0511581621399997, "valu": 0.823193310148, "sudheendra": 7.369978720910001, "yoshua": 7.369978720910001, "layer": 2.0969791623500003, "subfield": 5.4240685718499995, "evalu": 1.9388802431299998, "humanlevel": 7.369978720910001, "shuai": 7.369978720910001, "kaim": 7.369978720910001, "about": 0.0628434774746, "problem": 0.569140724273, "deploy": 2.00400270589, "communiti": 0.673561947791, "scene": 1.23853486375, "neural": 4.0853151555, "context": 1.44920491442, "entir": 0.46603068026999994, "bridg": 1.31015483629, "understand": 1.0880858756799998, "featur": 0.423387418142, "each": 0.173741689304, "blank": 3.15047101573, "them": 0.0941833269093, "defin": 1.00368010925, "finetun": 7.369978720910001, "multidimension": 5.0774439637699995, "alec": 3.7115584742800003, "contrast": 1.0589701282, "across": 0.549198455941, "ensembl": 2.81820931165, "cite": 1.00522796406, "tool": 1.60887117963, "articl": 0.702131739574, "enabl": 1.26473915954, "criteria": 2.4632235573, "ping": 4.70275051433, "tim": 2.10625279913, "vector": 3.25419887797, "recognit": 1.4816549327200002, "discrimin": 2.37006739018, "singl": 0.475916769059, "luke": 2.9144693094800003, "georg": 0.630168832776, "central": 0.477540146039, "imagelevel": 7.369978720910001, "licheng": 7.369978720910001, "soumith": 7.369978720910001, "mani": 0.0433157581221, "appli": 0.8316941898119999, "over": 0.0249367214957, "build": 0.491137452091, "pedro": 2.98545520604, "provid": 0.19517784432500002, "pretrain": 7.369978720910001, "adversari": 3.27064661718, "biomed": 3.9295606260900002, "jingdong": 7.369978720910001, "varieti": 0.8316941898119999, "subhashini": 7.369978720910001, "unet": 7.369978720910001, "endtoend": 7.369978720910001, "connect": 0.633605058682, "inform": 0.454453704662, "meanfield": 7.369978720910001, "crfs": 7.369978720910001, "conduct": 0.817042965366, "retriev": 0.773925020223, "guillaum": 4.05943570751, "best": 0.459227932947, "repres": 0.38507723275, "hope": 0.919824304455, "take": 0.130691962197, "visual": 1.6539383488600001, "previous": 0.356602960063, "karel": 4.43612185107, "strategi": 1.49112311818, "robust": 2.9929646280599997, "toolbox": 6.117215752409999, "support": 0.237880610037, "interact": 1.4858210267899998, "represent": 1.7797382876499999, "geoffrey": 2.7236665915900002, "possibl": 0.348805474891, "regress": 3.9359915164199997, "publish": 0.313975865467, "strength": 1.3931203261899998, "indic": 0.7336385419149999, "fix": 1.48944573451, "chao": 2.56513833979, "also": 0.0146571578, "qnetwork": 7.369978720910001, "genom": 3.64428529367, "target": 1.1690639496200002, "such": 0.059695977806, "reveal": 0.9677299042149999, "motor": 1.9048765367200002, "one": 0.0062553516455, "vedaldi": 7.369978720910001, "spatiotempor": 6.83935046985, "composit": 1.5325398614399999, "mehdi": 5.45305610873, "recept": 2.18988198575, "research": 0.663727818138, "the": 0.0, "area": 0.327954821122, "polici": 0.92807570005, "becom": 0.11771217648900001, "candid": 1.50691588861, "recent": 0.434413741288, "past": 0.7016234157610001, "peopl": 0.193265578473, "dozen": 1.78385428972, "challeng": 0.9392919688950001, "andrea": 2.6120874479, "util": 1.5389763962399998, "sinc": 0.0803681994577, "system": 0.327430345585, "evolutionari": 3.0524906073699998, "share": 0.618760299747, "although": 0.139487981418, "attribut": 1.2283715153700001, "paper": 0.979402539665, "joe": 1.7685981798700001, "histori": 0.187550624069, "which": 0.00517841384543, "wild": 1.61867673028, "random": 1.9727214065099998, "imagenet": 7.369978720910001, "activ": 0.381196603284, "thus": 0.49857627139300004, "abstract": 2.29918950399, "other": 0.00987474791976, "introduct": 1.02276465794, "consist": 0.398873126426, "show": 0.236682766013, "bank": 1.0557062993700002, "impact": 1.09033222631, "easytous": 7.369978720910001, "christof": 6.3767269479, "routin": 2.07918962078, "margin": 1.81934742575, "mathemat": 2.00027135827, "imag": 0.99376210729, "chen": 3.5283781797800002, "like": 0.139053576545, "implement": 1.27437940907, "mart\u00edn": 7.369978720910001, "lenc": 7.369978720910001, "linear": 2.63027764196, "collect": 0.49536666052, "madlib": 7.369978720910001, "agent": 1.4489366381, "joel": 2.81926472072, "maze": 4.16723227797, "attempt": 0.38674498075099994, "backpropag": 7.369978720910001, "includ": 0.0188846813905, "david": 0.615025032185, "millennium": 2.39600741118, "gather": 1.3313920667299999, "opensourc": 7.369978720910001, "superresolut": 7.369978720910001, "paul": 0.7875381558519999, "this": 0.0037864490525, "detect": 1.68878274493, "sampl": 1.9786264883900002, "time": 0.0112115188626, "engin": 0.904767558276, "chang": 0.166275625058, "perform": 0.42618085058, "andrei": 3.9889840465599997, "survey": 1.3294859427299999, "actorcrit": 7.369978720910001, "differ": 0.212321121312, "been": 0.023645982368400004, "schmidhub": 7.1076144564399995, "statist": 1.4451883070700002, "valuabl": 2.010566255, "most": 0.020747896295599998, "between": 0.033953681165299995, "huaizu": 7.369978720910001, "game": 0.9477062580210001, "domain": 2.24008000599, "credit": 1.11288601088, "vision": 1.58523088743, "largest": 0.7184068473190001, "top": 0.609100637788, "input": 2.50167533539, "action": 0.598043165069, "augment": 2.8045894049299998, "anet": 7.369978720910001, "strong": 0.49712549393600003, "there": 0.0400978929255, "condit": 0.654837788206, "vincent": 2.4141516633099998, "almost": 0.42907884333400004, "fillintheblank": 7.369978720910001, "longer": 0.7046772417749999, "produc": 0.314320812003, "alexand": 1.43376264803, "nan": 7.369978720910001, "architectur": 1.63469757919, "indirect": 2.15395659709, "zejian": 7.369978720910001, "call": 0.0654627744488, "navig": 2.11509091229, "more": 0.017024931599999998, "improv": 0.7147958039319999, "list": 0.309845761506, "novel": 1.40255075163, "achiev": 0.6270980851169999, "these": 0.0715336194008, "compact": 2.5218623563099998, "volodymyr": 6.414467275880001, "amount": 0.819898886199, "artifici": 2.11822899018, "faster": 2.03003967967, "realtim": 6.0616459012599995, "librari": 0.986809980943, "highdimension": 7.369978720910001, "probabilist": 4.8442500766, "juergen": 6.676831540349999, "year": 0.047402238894600005, "simultan": 1.67322086119, "select": 0.704804687133, "fill": 1.20540155609, "avail": 0.547454586289, "surpass": 2.31437006117, "comput": 1.36806891594, "encod": 3.36811501148, "way": 0.19809150993500002, "search": 1.1798682540899998, "onli": 0.025324268329099998, "gap": 1.98823974622, "stateoftheart": 7.369978720910001, "distinguish": 1.21469608857, "bernardino": 4.37925898918, "speech": 1.3409775702700002, "depth": 2.10936322154, "adri\u00e0": 7.369978720910001, "link": 0.7661704068449999, "advanc": 0.6930212121780001, "chain": 1.64410864979, "jake": 3.54351360384, "block": 1.16400781588, "yann": 5.934894195619999, "cnns": 7.369978720910001, "method": 0.944461608841, "olaf": 4.80502936345, "level": 0.503462189943, "convolut": 4.61631800855, "must": 0.653383947388, "convnet": 7.369978720910001, "sergey": 4.44681714019, "text": 1.14048200999, "gpus": 6.964513612799999, "variant": 1.92410379, "direct": 0.200705689496, "import": 0.292818277066, "success": 0.27765441259199997, "geograph": 1.55405874632, "residu": 3.1940541716900004, "could": 0.18595627229000003, "have": 0.0147850023412, "characterlevel": 7.369978720910001, "score": 1.4559353207700003, "tamara": 4.860379458530001, "dramat": 1.3825221952000002, "multicor": 7.369978720910001, "half": 0.564256167492, "learn": 0.842752064745, "sequenti": 3.6786023866, "when": 0.0205549888584, "mention": 0.931747186336, "effici": 1.62793753414, "vast": 1.40024866595}, "freq": {"jianmin": 1, "matthew": 1, "googl": 1, "broader": 1, "unsupervis": 4, "eugen": 1, "lopez": 1, "relat": 1, "region": 5, "detail": 1, "form": 1, "data": 4, "ross": 1, "wide": 3, "new": 3, "assign": 1, "number": 1, "dataset": 4, "topnum": 1, "function": 1, "class": 1, "thoma": 1, "learner": 1, "believ": 1, "python": 2, "consent": 1, "amjad": 1, "sergio": 1, "well": 2, "product": 2, "drug": 2, "approach": 2, "path": 1, "toward": 1, "lisa": 1, "their": 3, "automat": 1, "while": 1, "bound": 1, "pairwis": 1, "multipl": 2, "alex": 1, "extend": 1, "how": 2, "test": 2, "andi": 1, "philipp": 1, "segment": 2, "present": 2, "costfre": 1, "academicmicrosoftcom": 1, "than": 3, "especi": 1, "distribut": 1, "end": 1, "junbo": 1, "heterogen": 1, "deep": 27, "matconvnet": 2, "overview": 1, "consid": 1, "snippet": 1, "thousand": 1, "koray": 1, "salienc": 2, "constraint": 1, "breakthrough": 1, "sensori": 1, "near": 1, "but": 1, "demonstr": 1, "certain": 1, "need": 1, "our": 2, "classif": 4, "annot": 2, "object": 7, "summaris": 1, "applic": 2, "lnet": 2, "empir": 1, "ann": 1, "robot": 1, "has": 5, "generat": 3, "dcgan": 1, "impress": 2, "tensorflow": 5, "formul": 2, "use": 16, "optim": 1, "christian": 1, "model": 5, "much": 1, "introduc": 4, "pool": 1, "reli": 1, "count": 1, "zhifeng": 1, "field": 7, "program": 1, "short": 2, "process": 3, "not": 1, "involv": 1, "doubli": 1, "specif": 1, "xiaogang": 1, "open": 1, "citat": 2, "metric": 1, "sourc": 1, "graphic": 1, "design": 1, "recurr": 4, "effect": 1, "allow": 2, "scienc": 1, "kdnugget": 1, "causal": 1, "lever": 1, "construct": 1, "rami": 1, "review": 1, "highresolut": 1, "video": 2, "steadi": 1, "ashish": 1, "respons": 1, "simpl": 1, "appear": 1, "that": 16, "algorithm": 2, "array": 1, "offer": 1, "spatial": 1, "shipeng": 1, "longterm": 1, "develop": 1, "oriol": 1, "compil": 1, "pattern": 1, "succeed": 1, "sever": 3, "explor": 1, "with": 10, "network": 30, "three": 1, "fast": 1, "some": 1, "beyond": 1, "term": 1, "control": 2, "locat": 1, "result": 1, "document": 1, "expos": 1, "supervis": 3, "classic": 1, "filter": 2, "sadeep": 1, "jian": 1, "languag": 2, "set": 1, "futur": 1, "question": 1, "xiaoou": 2, "outperform": 1, "from": 3, "answer": 1, "work": 4, "learnabl": 1, "num": 71, "jeff": 1, "veri": 1, "lowresolut": 1, "ziwei": 1, "multilevel": 1, "for": 23, "predict": 2, "output": 1, "releas": 1, "fulli": 1, "salient": 1, "practition": 1, "competit": 1, "theano": 3, "assum": 1, "are": 8, "marcus": 1, "compos": 1, "current": 2, "vibhav": 1, "technic": 1, "histor": 1, "shown": 1, "gaussian": 1, "xiang": 1, "furthermor": 1, "cloud": 1, "refer": 2, "extract": 1, "task": 1, "asynchron": 3, "integr": 1, "combin": 2, "posit": 1, "pave": 1, "eunbyung": 1, "focus": 2, "shaoq": 1, "general": 1, "inceptionvnum": 2, "map": 5, "and": 41, "give": 1, "lowhighresolut": 1, "train": 7, "they": 3, "templat": 1, "shallow": 1, "step": 1, "error": 1, "framework": 2, "rajat": 1, "continu": 1, "comment": 1, "inceptionresnet": 1, "reinforc": 4, "period": 1, "scientist": 1, "machin": 7, "into": 1, "discoveri": 2, "relev": 1, "rapid": 1, "servic": 1, "requir": 1, "descript": 4, "face": 4, "local": 1, "tag": 1, "largescal": 3, "fullimag": 1, "flexibl": 1, "help": 1, "larg": 3, "instead": 1, "potenti": 1, "propos": 4, "tempor": 2, "approxim": 1, "base": 2, "natur": 2, "can": 4, "fact": 1, "recapitul": 1, "atari": 2, "here": 1, "project": 1, "express": 3, "yang": 1, "averag": 1, "through": 1, "infer": 4, "valu": 1, "sudheendra": 1, "yoshua": 1, "layer": 2, "subfield": 1, "evalu": 2, "humanlevel": 1, "shuai": 1, "kaim": 2, "about": 2, "problem": 2, "deploy": 1, "communiti": 1, "scene": 1, "neural": 13, "context": 1, "entir": 1, "bridg": 1, "understand": 1, "featur": 4, "each": 2, "blank": 1, "them": 1, "defin": 1, "finetun": 1, "multidimension": 1, "alec": 1, "contrast": 1, "across": 2, "ensembl": 1, "cite": 20, "tool": 2, "articl": 2, "enabl": 1, "criteria": 1, "ping": 1, "tim": 1, "vector": 1, "recognit": 6, "discrimin": 1, "singl": 1, "luke": 1, "georg": 1, "central": 1, "imagelevel": 1, "licheng": 1, "soumith": 1, "mani": 3, "appli": 1, "over": 2, "build": 1, "pedro": 1, "provid": 2, "pretrain": 1, "adversari": 2, "biomed": 1, "jingdong": 1, "varieti": 3, "subhashini": 1, "unet": 1, "endtoend": 2, "connect": 1, "inform": 3, "meanfield": 1, "crfs": 1, "conduct": 1, "retriev": 1, "guillaum": 1, "best": 1, "repres": 1, "hope": 1, "take": 1, "visual": 5, "previous": 2, "karel": 1, "strategi": 2, "robust": 1, "toolbox": 1, "support": 1, "interact": 1, "represent": 3, "geoffrey": 1, "possibl": 1, "regress": 1, "publish": 1, "strength": 1, "indic": 1, "fix": 1, "chao": 1, "also": 2, "qnetwork": 1, "genom": 1, "target": 1, "such": 1, "reveal": 2, "motor": 1, "one": 4, "vedaldi": 1, "spatiotempor": 1, "composit": 1, "mehdi": 1, "recept": 1, "research": 2, "the": 53, "area": 1, "polici": 1, "becom": 1, "candid": 1, "recent": 2, "past": 2, "peopl": 1, "dozen": 1, "challeng": 2, "andrea": 1, "util": 1, "sinc": 1, "system": 4, "evolutionari": 1, "share": 1, "although": 1, "attribut": 3, "paper": 5, "joe": 1, "histori": 1, "which": 3, "wild": 1, "random": 4, "imagenet": 1, "activ": 1, "thus": 1, "abstract": 1, "other": 2, "introduct": 1, "consist": 1, "show": 3, "bank": 1, "impact": 1, "easytous": 1, "christof": 1, "routin": 1, "margin": 1, "mathemat": 3, "imag": 9, "chen": 1, "like": 1, "implement": 1, "mart\u00edn": 2, "lenc": 1, "linear": 1, "collect": 1, "madlib": 2, "agent": 2, "joel": 1, "maze": 1, "attempt": 1, "backpropag": 1, "includ": 2, "david": 1, "millennium": 1, "gather": 1, "opensourc": 1, "superresolut": 1, "paul": 1, "this": 15, "detect": 4, "sampl": 2, "time": 2, "engin": 1, "chang": 1, "perform": 4, "andrei": 1, "survey": 1, "actorcrit": 2, "differ": 1, "been": 5, "schmidhub": 1, "statist": 1, "valuabl": 1, "most": 1, "between": 3, "huaizu": 1, "game": 1, "domain": 3, "credit": 1, "vision": 2, "largest": 1, "top": 1, "input": 3, "action": 1, "augment": 1, "anet": 1, "strong": 3, "there": 1, "condit": 3, "vincent": 1, "almost": 1, "fillintheblank": 1, "longer": 1, "produc": 1, "alexand": 2, "nan": 1, "architectur": 2, "indirect": 1, "zejian": 1, "call": 1, "navig": 1, "more": 4, "improv": 3, "list": 1, "novel": 1, "achiev": 2, "these": 3, "compact": 1, "volodymyr": 2, "amount": 1, "artifici": 1, "faster": 1, "realtim": 1, "librari": 1, "highdimension": 1, "probabilist": 1, "juergen": 1, "year": 2, "simultan": 1, "select": 1, "fill": 1, "avail": 2, "surpass": 1, "comput": 11, "encod": 1, "way": 1, "search": 1, "onli": 2, "gap": 1, "stateoftheart": 4, "distinguish": 1, "bernardino": 1, "speech": 2, "depth": 1, "adri\u00e0": 1, "link": 1, "advanc": 4, "chain": 1, "jake": 1, "block": 2, "yann": 2, "cnns": 5, "method": 6, "olaf": 1, "level": 2, "convolut": 18, "must": 1, "convnet": 1, "sergey": 1, "text": 2, "gpus": 1, "variant": 1, "direct": 2, "import": 1, "success": 3, "geograph": 1, "residu": 2, "could": 1, "have": 7, "characterlevel": 3, "score": 2, "tamara": 1, "dramat": 1, "multicor": 1, "half": 1, "learn": 33, "sequenti": 1, "when": 1, "mention": 1, "effici": 2, "vast": 1}, "idf": {"jianmin": 1587.6, "matthew": 6.908616187989999, "googl": 11.388809182200001, "broader": 12.5700712589, "unsupervis": 345.13043478300006, "eugen": 11.1567111736, "lopez": 46.017391304300006, "relat": 1.23750876919, "region": 1.7647843486, "detail": 2.26186066391, "form": 1.12755681818, "data": 3.37643555934, "ross": 9.563855421689999, "wide": 1.5598349381, "new": 1.0178880554, "assign": 3.83663605607, "number": 1.10142916609, "dataset": 193.609756098, "topnum": 933.882352941, "function": 2.495441685, "class": 2.11651779763, "thoma": 2.39673913043, "learner": 75.2417061611, "believ": 1.6450108797, "python": 56.2978723404, "consent": 11.4960173787, "amjad": 721.636363636, "sergio": 47.9637462236, "well": 1.0655748708, "product": 1.62264922322, "drug": 5.93717277487, "approach": 2.07556543339, "path": 4.6421052631599995, "toward": 1.6303142329, "lisa": 18.8327402135, "their": 1.01547908405, "automat": 6.787516032490001, "while": 1.0441988950299999, "bound": 5.40735694823, "pairwis": 387.219512195, "multipl": 2.74813917258, "alex": 10.655033557000001, "extend": 1.9604840701400004, "how": 1.60250328051, "test": 2.65707112971, "andi": 11.4545454545, "philipp": 19.6485148515, "segment": 7.55640171347, "present": 1.25551601423, "costfre": 1587.6, "academicmicrosoftcom": 1587.6, "than": 1.03278688525, "especi": 1.66712170534, "distribut": 2.7396031061299997, "end": 1.10680423871, "junbo": 1587.6, "heterogen": 52.0524590164, "deep": 3.6279707495399998, "matconvnet": 1587.6, "overview": 12.6805111821, "consid": 1.2397313759200002, "snippet": 135.692307692, "thousand": 2.4767550702000003, "koray": 1587.6, "salienc": 610.615384615, "constraint": 15.0483412322, "breakthrough": 16.434782608699997, "sensori": 40.2944162437, "near": 1.28769567686, "but": 1.01632417899, "demonstr": 2.64997496244, "certain": 1.8077886586200003, "need": 1.4372623574099999, "our": 2.35758835759, "classif": 8.067073170730001, "annot": 22.8760806916, "object": 2.3488681757700003, "summaris": 53.0969899666, "applic": 3.42672134686, "lnet": 1587.6, "empir": 2.8395635843299996, "ann": 5.08357348703, "robot": 20.0201765448, "has": 1.0436497502, "generat": 2.05275407292, "dcgan": 1587.6, "impress": 3.90457452041, "tensorflow": 1587.6, "formul": 9.86086956522, "use": 1.0296387573799999, "optim": 11.5377906977, "christian": 2.6785895056499998, "model": 2.0905978404, "much": 1.1942229577299999, "introduc": 1.7258397651900002, "pool": 7.052865393160001, "reli": 4.16146788991, "count": 3.48157894737, "zhifeng": 1587.6, "field": 1.7790228597, "program": 2.02139037433, "short": 1.41295834817, "process": 1.69524826482, "not": 1.01567398119, "involv": 1.4498630137000001, "doubli": 180.409090909, "specif": 1.8719490626099997, "xiaogang": 1587.6, "open": 1.24556723678, "citat": 16.8177966102, "metric": 22.235294117600002, "sourc": 1.69760479042, "graphic": 9.035856573710001, "design": 1.45825296225, "recurr": 35.5964125561, "effect": 1.3963060686000002, "allow": 1.2716059271100002, "scienc": 2.31969608416, "kdnugget": 1587.6, "causal": 36.081818181799996, "lever": 41.889182058, "construct": 1.9320920043799998, "rami": 273.724137931, "review": 2.2099109131400003, "highresolut": 1587.6, "video": 3.29719626168, "steadi": 10.7780040733, "ashish": 1587.6, "respons": 1.5066907089300001, "simpl": 3.3981164383599998, "appear": 1.3214582986499999, "that": 1.00398406375, "algorithm": 27.9507042254, "array": 10.1444089457, "offer": 1.53896859248, "spatial": 24.4246153846, "shipeng": 1587.6, "longterm": 512.129032258, "develop": 1.1955719557200002, "oriol": 111.802816901, "compil": 5.696447793330001, "pattern": 3.79173632673, "succeed": 3.3550295857999997, "sever": 1.07241286139, "explor": 3.39593582888, "with": 1.0011982089899998, "network": 2.59369384088, "three": 1.06621893889, "fast": 4.8729281768, "some": 1.04036697248, "beyond": 2.54586273252, "term": 1.39520168732, "control": 1.46959178006, "locat": 1.59766529134, "result": 1.14611608432, "document": 2.5409731114, "expos": 5.03680203046, "supervis": 7.74061433447, "classic": 2.4087391898, "filter": 16.8893617021, "sadeep": 1587.6, "jian": 189.0, "languag": 2.29488291414, "set": 1.18707940781, "futur": 1.8577112099200002, "question": 2.20408163265, "xiaoou": 1587.6, "outperform": 82.2590673575, "from": 1.00056721497, "answer": 4.64890190337, "work": 1.11520089913, "learnabl": 1587.6, "num": 1.00031504001, "jeff": 9.7339055794, "veri": 1.25880114177, "lowresolut": 1587.6, "ziwei": 1587.6, "multilevel": 441.0, "for": 1.00031504001, "predict": 5.18484650555, "output": 7.676982591880001, "releas": 1.8377126982299998, "fulli": 2.79015817223, "salient": 70.875, "practition": 18.8775267539, "competit": 3.06960556845, "theano": 1587.6, "assum": 2.9575260804799997, "are": 1.02990593578, "marcus": 17.2004333694, "compos": 2.5060773480700003, "current": 1.5325803649, "vibhav": 1587.6, "technic": 3.1400316455699997, "histor": 1.6755672823199999, "shown": 2.76923076923, "gaussian": 144.327272727, "xiang": 182.482758621, "furthermor": 5.50294627383, "cloud": 10.6193979933, "refer": 1.30024570025, "extract": 7.703056768560001, "task": 3.88641370869, "asynchron": 165.375, "integr": 2.8254137746900003, "combin": 1.69760479042, "posit": 1.37252528746, "pave": 12.25, "eunbyung": 1587.6, "focus": 2.01012914662, "shaoq": 1587.6, "general": 1.1218202374200001, "inceptionvnum": 1587.6, "map": 4.0728578758300005, "and": 1.00006299213, "give": 1.3653250774, "lowhighresolut": 1587.6, "train": 1.9365698950999999, "they": 1.03017325287, "templat": 43.495890411000005, "shallow": 16.0363636364, "step": 2.8279301745599996, "error": 6.04109589041, "framework": 8.200413223139998, "rajat": 1587.6, "continu": 1.13928955867, "comment": 3.05954904606, "inceptionresnet": 1587.6, "reinforc": 6.453658536590001, "period": 1.3430335843, "scientist": 4.69426374926, "machin": 4.02433460076, "into": 1.01502461479, "discoveri": 4.74902781932, "relev": 6.938811188810001, "rapid": 2.62586834271, "servic": 1.51300867245, "requir": 1.52844902282, "descript": 4.00504540868, "face": 1.80327124035, "local": 1.51720183486, "tag": 19.7462686567, "largescal": 1587.6, "fullimag": 1587.6, "flexibl": 9.68639414277, "help": 1.39962972759, "larg": 1.18574949585, "instead": 1.59461631177, "potenti": 2.52080025405, "propos": 1.9902218879299998, "tempor": 21.897931034499997, "approxim": 2.2132998745299997, "base": 1.14628158845, "natur": 1.5392670157100001, "can": 1.17626139142, "fact": 1.73375559681, "recapitul": 264.6, "atari": 92.3023255814, "here": 2.42307692308, "project": 1.7534791252500002, "express": 1.9120799710900003, "yang": 35.9185520362, "averag": 2.60390355913, "through": 1.07074930869, "infer": 21.1398135819, "valu": 2.2777618364400003, "sudheendra": 1587.6, "yoshua": 1587.6, "layer": 8.14153846154, "subfield": 226.8, "evalu": 6.9509632224199995, "humanlevel": 1587.6, "shuai": 1587.6, "kaim": 1587.6, "about": 1.06486015159, "problem": 1.76674827509, "deploy": 7.41869158879, "communiti": 1.96121062384, "scene": 3.45055422734, "neural": 59.4606741573, "context": 4.25972632144, "entir": 1.59365589239, "bridg": 3.7067476068199996, "understand": 2.96858638743, "featur": 1.52712581762, "each": 1.18974820144, "blank": 23.3470588235, "them": 1.09876115994, "defin": 2.72830383227, "finetun": 1587.6, "multidimension": 160.363636364, "alec": 40.917525773200005, "contrast": 2.88339992735, "across": 1.7318642958400001, "ensembl": 16.746835443, "cite": 2.73253012048, "tool": 4.99716713881, "articl": 2.01805008262, "enabl": 3.5421686747, "criteria": 11.7426035503, "ping": 110.25, "tim": 8.217391304349999, "vector": 25.898858075, "recognit": 4.40022172949, "discrimin": 10.6981132075, "singl": 1.60948905109, "luke": 18.4390243902, "georg": 1.8779276082299998, "central": 1.6121039805000001, "imagelevel": 1587.6, "licheng": 1587.6, "soumith": 1587.6, "mani": 1.04426757877, "appli": 2.2972073506, "over": 1.02525024217, "build": 1.6341739578, "pedro": 19.7955112219, "provid": 1.21552714187, "pretrain": 1587.6, "adversari": 26.328358209, "biomed": 50.8846153846, "jingdong": 1587.6, "varieti": 2.2972073506, "subhashini": 1587.6, "unet": 1587.6, "endtoend": 1587.6, "connect": 1.8843916913900003, "inform": 1.5753125620200001, "meanfield": 1587.6, "crfs": 1587.6, "conduct": 2.2637958077900002, "retriev": 2.16826003824, "guillaum": 57.9416058394, "best": 1.5828514456600002, "repres": 1.46972782818, "hope": 2.50884955752, "take": 1.13961668222, "visual": 5.22752716497, "previous": 1.42846859816, "karel": 84.44680851060001, "strategi": 4.44208170118, "robust": 19.9447236181, "toolbox": 453.6, "support": 1.2685577307200002, "interact": 4.4185917061, "represent": 5.928304705, "geoffrey": 15.236084453, "possibl": 1.4173734488, "regress": 51.2129032258, "publish": 1.36885669943, "strength": 4.02739726027, "indic": 2.0826446281, "fix": 4.4346368715099995, "chao": 13.0024570025, "also": 1.01476510067, "qnetwork": 1587.6, "genom": 38.255421686700004, "target": 3.2189781021900004, "such": 1.06151377374, "reveal": 2.63196286472, "motor": 6.718578078709999, "one": 1.00627495722, "vedaldi": 1587.6, "spatiotempor": 933.882352941, "composit": 4.629921259840001, "mehdi": 233.470588235, "recept": 8.934158694430002, "research": 1.9420183486200002, "the": 1.0, "area": 1.3881262568900001, "polici": 2.52963671128, "becom": 1.12492028626, "candid": 4.51279135873, "recent": 1.54405757635, "past": 2.01702452039, "peopl": 1.21320495186, "dozen": 5.95275590551, "challeng": 2.55816951337, "andrea": 13.627467811199999, "util": 4.65981802172, "sinc": 1.08368600683, "system": 1.38739840951, "evolutionari": 21.168000000000003, "share": 1.8566249561500001, "although": 1.14968498805, "attribut": 3.4156626506, "paper": 2.6628648104700003, "joe": 5.86262924668, "histori": 1.20629131525, "which": 1.005191845, "wild": 5.04640813732, "random": 7.1902173913, "imagenet": 1587.6, "activ": 1.46403541129, "thus": 1.6463756092500001, "abstract": 9.966101694919999, "other": 1.00992366412, "introduct": 2.7808723068799996, "consist": 1.4901445466499998, "show": 1.26703910615, "bank": 2.87400434468, "impact": 2.97526236882, "easytous": 1587.6, "christof": 588.0, "routin": 7.997984886649999, "margin": 6.16783216783, "mathemat": 7.391061452510001, "imag": 2.70137825421, "chen": 34.0686695279, "like": 1.14918566775, "implement": 3.57648118946, "mart\u00edn": 1587.6, "lenc": 1587.6, "linear": 13.8776223776, "collect": 1.64109985528, "madlib": 1587.6, "agent": 4.25858369099, "joel": 16.764519535399998, "maze": 64.5365853659, "attempt": 1.4721810088999998, "backpropag": 1587.6, "includ": 1.0190641247799999, "david": 1.84970290108, "millennium": 10.979253112, "gather": 3.78631051753, "opensourc": 1587.6, "superresolut": 1587.6, "paul": 2.1979786792200002, "this": 1.00379362671, "detect": 5.41288782816, "sampl": 7.23280182232, "time": 1.01127460348, "engin": 2.47135740971, "chang": 1.1808985421, "perform": 1.5313977042500002, "andrei": 54.0, "survey": 3.7791002142300005, "actorcrit": 1587.6, "differ": 1.23654490225, "been": 1.0239277652399998, "schmidhub": 1221.23076923, "statist": 4.24265098878, "valuabl": 7.46754468485, "most": 1.02096463023, "between": 1.03453668708, "huaizu": 1587.6, "game": 2.57978550536, "domain": 9.39408284024, "credit": 3.04312823462, "vision": 4.88041807562, "largest": 2.0511627907, "top": 1.8387769284200002, "input": 12.2029208301, "action": 1.81855670103, "augment": 16.5202913632, "anet": 1587.6, "strong": 1.6439888163999998, "there": 1.04091266719, "condit": 1.92483026188, "vincent": 11.1802816901, "almost": 1.53584212054, "fillintheblank": 1587.6, "longer": 2.02319357716, "produc": 1.36932896326, "alexand": 4.19445178336, "nan": 1587.6, "architectur": 5.12790697674, "indirect": 8.618892508139998, "zejian": 1587.6, "call": 1.0676529926, "navig": 8.290339425589998, "more": 1.0171706817, "improv": 2.04376930999, "list": 1.36321483771, "novel": 4.06555697823, "achiev": 1.87216981132, "these": 1.07415426252, "compact": 12.451764705899999, "volodymyr": 610.615384615, "amount": 2.27027027027, "artifici": 8.31639601886, "faster": 7.61438848921, "realtim": 429.081081081, "librari": 2.68266306185, "highdimension": 1587.6, "probabilist": 127.008, "juergen": 793.8, "year": 1.0485436893200002, "simultan": 5.32930513595, "select": 2.02345144022, "fill": 3.33809924306, "avail": 1.7288467821, "surpass": 10.1185468451, "comput": 3.9277585353800006, "encod": 29.0237659963, "way": 1.2190739461, "search": 3.2539454806299997, "onli": 1.0256476516600002, "gap": 7.302667893280001, "stateoftheart": 1587.6, "distinguish": 3.36926994907, "bernardino": 79.7788944724, "speech": 3.8227787141800005, "depth": 8.24299065421, "adri\u00e0": 1587.6, "link": 2.15151104486, "advanc": 1.9997480791, "chain": 5.17639387023, "jake": 34.588235294099995, "block": 3.20274359492, "yann": 378.0, "cnns": 1587.6, "method": 2.5714285714300003, "olaf": 122.123076923, "level": 1.6544393497299998, "convolut": 101.121019108, "must": 1.9220338983099996, "convnet": 1587.6, "sergey": 85.3548387097, "text": 3.12827586207, "gpus": 1058.4, "variant": 6.8490077653100006, "direct": 1.22226499346, "import": 1.3401992233700002, "success": 1.32002993265, "geograph": 4.7306317044099995, "residu": 24.3870967742, "could": 1.2043695949, "have": 1.0148948411399998, "characterlevel": 1587.6, "score": 4.2884927066500005, "tamara": 129.073170732, "dramat": 3.9849397590400004, "multicor": 1587.6, "half": 1.75813953488, "learn": 2.32275054865, "sequenti": 39.5910224439, "when": 1.02076769755, "mention": 2.53894130817, "effici": 5.09335899904, "vast": 4.05620848237}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Top 20 Deep Learning Papers, 2018 Edition</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/top-20-deep-learning-papers-2018.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Top 20 Deep Learning Papers, 2018 Edition Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/04/right-chart-your-data.html\" rel=\"prev\" title=\"How To Choose The Right Chart Type For Your Data\"/>\n<link href=\"https://www.kdnuggets.com/2018/04/upcoming-meetings-ai-analytics-big-data-science-machine-learning.html\" rel=\"next\" title=\"Upcoming Meetings in AI, Analytics, Big Data, Data Science, Deep Learning, Machine Learning: April and Beyond\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2018/03/top-20-deep-learning-papers-2018.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=79277\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/top-20-deep-learning-papers-2018.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-79277 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 3-Apr, 2018  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/index.html\">Mar</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/tutorials.html\">Tutorials, Overviews</a> \u00bb Top 20 Deep Learning Papers, 2018 Edition (\u00a0<a href=\"/2018/n14.html\">18:n14</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\"><img align=\"right\" alt=\"Gold Blog\" src=\"/images/tkb-1804-g.png\" width=\"94\"/>Top 20 Deep Learning Papers, 2018 Edition</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/04/right-chart-your-data.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/04/upcoming-meetings-ai-analytics-big-data-science-machine-learning.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/algorithms\" rel=\"tag\">Algorithms</a>, <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/machine-learning\" rel=\"tag\">Machine Learning</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/tensorflow\" rel=\"tag\">TensorFlow</a>, <a href=\"https://www.kdnuggets.com/tag/text-analytics\" rel=\"tag\">Text Analytics</a>, <a href=\"https://www.kdnuggets.com/tag/trends\" rel=\"tag\">Trends</a></div>\n<br/>\n<p class=\"excerpt\">\n     Deep Learning is constantly evolving at a fast pace. New techniques, tools and implementations are changing the field of Machine Learning and bringing excellent results.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/pedro-lopez\" rel=\"author\" title=\"Posts by Pedro Lopez\">Pedro Lopez</a>, KDnuggets.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><img alt=\"\" class=\"wp-image-79446 aligncenter\" sizes=\"(max-width: 600px) 100vw, 600px\" src=\"https://www.kdnuggets.com/wp-content/uploads/wing-features.jpg\" srcset=\"https://www.kdnuggets.com/wp-content/uploads/wing-features.jpg 600w, https://www.kdnuggets.com/wp-content/uploads/wing-features-300x169.jpg 300w\" width=\"80%\"/></p>\n<p>Deep Learning, one of the subfields of Machine Learning and Statistical Learning has been advancing in impressive levels in the past years. Cloud computing, robust open source tools and vast amounts of available data have been some of the levers for these impressive breakthroughs. The criteria used to select the 20 top papers is by using citation counts from <a href=\"http://academic.microsoft.com\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>academic.microsoft.com</strong></a>. It is important to mention that\u00a0these metrics are changing rapidly so the citations valued must be considered as the numbers when\u00a0this article was published.</p>\n<p>In this list of papers more than 75% refer to deep learning and neural networks, specifically Convolutional Neural Networks (CNN). Almost 50% of them refer to pattern recognition applications in the field of computer vision. I believe tools like TensorFlow, Theano and advancements in the use of GPUs have paved the way for data scientists and machine learning engineers to extend the field.</p>\n<p><img alt=\"\" class=\"wp-image-79283 aligncenter\" sizes=\"(max-width: 600px) 100vw, 600px\" src=\"https://www.kdnuggets.com/wp-content/uploads/animal-image-recognition.jpg\" srcset=\"https://www.kdnuggets.com/wp-content/uploads/animal-image-recognition.jpg 600w, https://www.kdnuggets.com/wp-content/uploads/animal-image-recognition-300x229.jpg 300w\" width=\"80%\"/><br>\n<strong>1.\u00a0<a href=\"https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf\">Deep Learning</a>,</strong> by Yann L., Yoshua B. &amp; Geoffrey H. (2015)\u00a0(Cited: 5,716)</br></p>\n<blockquote><p>Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.</p></blockquote>\n<p><strong>2.\u00a0</strong><a href=\"http://download.tensorflow.org/paper/whitepaper2015.pdf\"><strong>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</strong></a><strong>, </strong>by Mart\u00edn A., Ashish A. B., Eugene B. C., et al. (2015) (Cited: 2,423)</p>\n<blockquote><p>The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery.</p></blockquote>\n<p><strong>3.\u00a0<a href=\"https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf rel=\">TensorFlow: a system for large-scale machine learning</a></strong><strong>, </strong>by Mart\u00edn A., Paul B., Jianmin C., Zhifeng C., Andy D. et al. (2016) (Cited: 2,227)</p>\n<blockquote><p>TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research.</p></blockquote>\n<p><strong>4.\u00a0<a href=\"https://arxiv.org/pdf/1404.7828.pdf\">Deep learning\u00a0in neural networks</a></strong>, by Juergen Schmidhuber (2015)\u00a0(Cited: 2,196)</p>\n<blockquote><p>This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning &amp; evolutionary computation, and indirect search for short programs encoding deep and large networks.</p></blockquote>\n<p><strong>5.\u00a0<a href=\"https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\">Human-level control through deep reinforcement learning</a></strong>, by Volodymyr M., Koray K., David S., Andrei A. R., Joel V et al (2015) (Cited: 2,086)</p>\n<blockquote><p>Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games.</p></blockquote>\n<p><strong>6.\u00a0<a href=\"https://arxiv.org/pdf/1506.01497.pdf\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></strong><strong>, </strong>by Shaoqing R., Kaiming H., Ross B. G. &amp; Jian S. (2015) (Cited: 1,421)</p>\n<blockquote><p>In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.</p></blockquote>\n<p><strong>7.\u00a0<a href=\"https://arxiv.org/pdf/1411.4389.pdf\">Long-term recurrent convolutional networks for visual recognition and description</a></strong><strong>, </strong>by Jeff D., Lisa Anne H., Sergio G., Marcus R., Subhashini V. et al. (2015) (Cited: 1,285)</p>\n<blockquote><p>In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \u201cdoubly deep\u201d in that they can be compositional in spatial and temporal \u201clayers\u201d.</p></blockquote>\n<p><strong>8.\u00a0<a href=\"https://arxiv.org/pdf/1412.4564.pdf\">MatConvNet: Convolutional Neural Networks for MATLAB</a>, </strong>by<strong>\u00a0</strong>Andrea Vedaldi &amp; Karel Lenc (2015) (Cited: 1,148)</p>\n<blockquote><p>It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox.</p></blockquote>\n<p><strong>9.\u00a0<a href=\"https://arxiv.org/pdf/1511.06434.pdf\">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></strong><strong>, </strong>by Alec R., Luke M. &amp; Soumith C. (2015)\u00a0(Cited: 1,054)</p>\n<blockquote><p>In this work, we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.</p></blockquote>\n<p><strong>10.\u00a0<a href=\"https://arxiv.org/pdf/1505.04597.pdf\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></strong><strong>,</strong> by Olaf R., Philipp F. &amp;Thomas B. (2015) (Cited: 975)</p>\n<blockquote><p>There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently.</p></blockquote>\n<p><strong>11.\u00a0<a href=\"http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf\">Conditional Random Fields as Recurrent Neural Networks</a></strong><strong>, </strong>by Shuai Z., Sadeep J., Bernardino R., Vibhav V. et al (2015)\u00a0(Cited: 760)</p>\n<blockquote><p>We introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks.</p></blockquote>\n<p><strong>12.\u00a0<a href=\"https://arxiv.org/pdf/1501.00092.pdf\">Image Super-Resolution Using Deep Convolutional Networks</a></strong><strong>, </strong>by Chao D., Chen C., Kaiming H. &amp; Xiaoou T. (2014) (Cited: 591)</p>\n<blockquote><p>Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one</p></blockquote>\n<p><strong>13.\u00a0<a href=\"https://arxiv.org/pdf/1503.08909.pdf\">Beyond short snippets: Deep networks for video classification</a></strong><strong>, </strong>by Joe Y. Ng, Matthew J. H., Sudheendra V., Oriol V., Rajat M. &amp; George T. (2015) (Cited: 533)</p>\n<blockquote><p>In this work, we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted.</p></blockquote>\n<p><strong>14.\u00a0<a href=\"https://arxiv.org/pdf/1602.07261.pdf\">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></strong><strong>, </strong>by Christian S., Sergey I., Vincent V. &amp; Alexander A A. (2017)\u00a0(Cited: 520)</p>\n<blockquote><p>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. With an ensemble of three residual and one Inception-v4, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.</p></blockquote>\n<p><strong>15.\u00a0<a href=\"https://arxiv.org/pdf/1410.5926.pdf\">Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></strong><strong>, </strong>by Huaizu J., Jingdong W., Zejian Y., Yang W., Nanning Z. &amp; Shipeng Li. (2013) (Cited: 518)</p>\n<blockquote><p>In this paper, we formulate saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, utilizes the supervised learning approach to map the regional feature vector to a saliency score.</p></blockquote>\n<p><strong>16.\u00a0<a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yu_Visual_Madlibs_Fill_ICCV_2015_paper.pdf\">Visual Madlibs: Fill in the Blank Description Generation and Question Answering</a></strong>, by Licheng Y., Eunbyung P., Alexander C. B. &amp; Tamara L. B. (2015) (Cited: 510)</p>\n<blockquote><p>In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context.</p></blockquote>\n<p><strong>17.\u00a0<a href=\"http://proceedings.mlr.press/v48/mniha16.pdf\">Asynchronous methods for deep reinforcement learning</a></strong><strong>,</strong> by Volodymyr M., Adri\u00e0 P. B., Mehdi M., Alex G., Tim H. et al. (2016) (Cited: 472)</p>\n<blockquote><p>The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.</p></blockquote>\n<p><strong>18.\u00a0<a href=\"https://arxiv.org/pdf/1605.02688.pdf\">Theano: A Python framework for fast computation of mathematical expressions</a>.</strong>, by by Rami A., Guillaume A., Amjad A., Christof A. et al (2016) (Cited: 451)</p>\n<blockquote><p>Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers especially in the machine learning community and has shown steady performance improvements.</p></blockquote>\n<p><strong>19.\u00a0<a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liu_Deep_Learning_Face_ICCV_2015_paper.pdf\">Deep Learning\u00a0Face Attributes in the Wild</a></strong><strong>, </strong>by Ziwei L., Ping L., Xiaogang W. &amp; Xiaoou T. (2015) (Cited: 401)</p>\n<blockquote><p>This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with imagelevel attribute tags, their response maps over entire images have strong indication of face locations.</p></blockquote>\n<p><strong>20. </strong><a href=\"http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf\"><strong>Character-level convolutional networks for text classification</strong></a><strong>, </strong>by Xiang Z., Junbo Jake Z. &amp; Yann L. (2015) (Cited: 401)</p>\n<blockquote><p>This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results.</p></blockquote>\n<p>\u00a0<br>\n<b>Related:</b></br></p>\n<ul class=\"three_ul\">\n<li><a href=\"https://www.kdnuggets.com/2016/01/seven-steps-deep-learning.html\">7 Steps to Understanding Deep Learning</a></li>\n<li><a href=\"https://www.kdnuggets.com/2017/05/deep-learning-big-deal.html\">Deep Learning \u2013 Past, Present, and Future</a></li>\n<li><a href=\"https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html\">The 10 Deep Learning Methods AI Practitioners Need to Apply</a></li>\n</ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/04/right-chart-your-data.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/04/upcoming-meetings-ai-analytics-big-data-science-machine-learning.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/kdnuggets-editor.html\">Looking for a KDnuggets Editor</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning Experts</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/index.html\">Mar</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/tutorials.html\">Tutorials, Overviews</a> \u00bb Top 20 Deep Learning Papers, 2018 Edition (\u00a0<a href=\"/2018/n14.html\">18:n14</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<div>\n<br/><span style=\"font-size:9px\">By subscribing, you agree to KDnuggets <a href=\"https://www.kdnuggets.com/news/privacy-policy.html\">privacy policy</a></span>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556410606\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.755 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 20:16:46 -->\n<!-- Compression = gzip -->", "content_tokenized": ["pedro", "lopez", "kdnugget", "comment", "deep", "learn", "one", "the", "subfield", "machin", "learn", "and", "statist", "learn", "has", "been", "advanc", "impress", "level", "the", "past", "year", "cloud", "comput", "robust", "open", "sourc", "tool", "and", "vast", "amount", "avail", "data", "have", "been", "some", "the", "lever", "for", "these", "impress", "breakthrough", "the", "criteria", "use", "select", "the", "num", "top", "paper", "use", "citat", "count", "from", "academicmicrosoftcom", "import", "mention", "that", "these", "metric", "are", "chang", "rapid", "the", "citat", "valu", "must", "consid", "the", "number", "when", "this", "articl", "publish", "this", "list", "paper", "more", "than", "num", "refer", "deep", "learn", "and", "neural", "network", "specif", "convolut", "neural", "network", "almost", "num", "them", "refer", "pattern", "recognit", "applic", "the", "field", "comput", "vision", "believ", "tool", "like", "tensorflow", "theano", "and", "advanc", "the", "use", "gpus", "have", "pave", "the", "way", "for", "data", "scientist", "and", "machin", "learn", "engin", "extend", "the", "field", "num", "deep", "learn", "yann", "yoshua", "geoffrey", "num", "cite", "num", "deep", "learn", "allow", "comput", "model", "that", "are", "compos", "multipl", "process", "layer", "learn", "represent", "data", "with", "multipl", "level", "abstract", "these", "method", "have", "dramat", "improv", "the", "stateoftheart", "speech", "recognit", "visual", "object", "recognit", "object", "detect", "and", "mani", "other", "domain", "such", "drug", "discoveri", "and", "genom", "num", "tensorflow", "largescal", "machin", "learn", "heterogen", "distribut", "system", "mart\u00edn", "ashish", "eugen", "num", "cite", "num", "the", "system", "flexibl", "and", "can", "use", "express", "wide", "varieti", "algorithm", "includ", "train", "and", "infer", "algorithm", "for", "deep", "neural", "network", "model", "and", "has", "been", "use", "for", "conduct", "research", "and", "for", "deploy", "machin", "learn", "system", "into", "product", "across", "more", "than", "dozen", "area", "comput", "scienc", "and", "other", "field", "includ", "speech", "recognit", "comput", "vision", "robot", "inform", "retriev", "natur", "languag", "process", "geograph", "inform", "extract", "and", "comput", "drug", "discoveri", "num", "tensorflow", "system", "for", "largescal", "machin", "learn", "mart\u00edn", "paul", "jianmin", "zhifeng", "andi", "num", "cite", "num", "tensorflow", "support", "varieti", "applic", "with", "focus", "train", "and", "infer", "deep", "neural", "network", "sever", "googl", "servic", "use", "tensorflow", "product", "have", "releas", "opensourc", "project", "and", "has", "becom", "wide", "use", "for", "machin", "learn", "research", "num", "deep", "learn", "neural", "network", "juergen", "schmidhub", "num", "cite", "num", "this", "histor", "survey", "compact", "summaris", "relev", "work", "much", "from", "the", "previous", "millennium", "shallow", "and", "deep", "learner", "are", "distinguish", "the", "depth", "their", "credit", "assign", "path", "which", "are", "chain", "possibl", "learnabl", "causal", "link", "between", "action", "and", "effect", "review", "deep", "supervis", "learn", "also", "recapitul", "the", "histori", "backpropag", "unsupervis", "learn", "reinforc", "learn", "evolutionari", "comput", "and", "indirect", "search", "for", "short", "program", "encod", "deep", "and", "larg", "network", "num", "humanlevel", "control", "through", "deep", "reinforc", "learn", "volodymyr", "koray", "david", "andrei", "joel", "num", "cite", "num", "here", "use", "recent", "advanc", "train", "deep", "neural", "network", "develop", "novel", "artifici", "agent", "term", "deep", "qnetwork", "that", "can", "learn", "success", "polici", "direct", "from", "highdimension", "sensori", "input", "use", "endtoend", "reinforc", "learn", "test", "this", "agent", "the", "challeng", "domain", "classic", "atari", "num", "game", "num", "faster", "toward", "realtim", "object", "detect", "with", "region", "propos", "network", "shaoq", "kaim", "ross", "jian", "num", "cite", "num", "this", "work", "introduc", "region", "propos", "network", "that", "share", "fullimag", "convolut", "featur", "with", "the", "detect", "network", "thus", "enabl", "near", "costfre", "region", "propos", "fulli", "convolut", "network", "that", "simultan", "predict", "object", "bound", "and", "object", "score", "each", "posit", "num", "longterm", "recurr", "convolut", "network", "for", "visual", "recognit", "and", "descript", "jeff", "lisa", "ann", "sergio", "marcus", "subhashini", "num", "cite", "num", "contrast", "current", "model", "which", "assum", "fix", "spatiotempor", "recept", "field", "simpl", "tempor", "averag", "for", "sequenti", "process", "recurr", "convolut", "model", "are", "doubli", "deep", "that", "they", "can", "composit", "spatial", "and", "tempor", "layer", "num", "matconvnet", "convolut", "neural", "network", "for", "andrea", "vedaldi", "karel", "lenc", "num", "cite", "num", "expos", "the", "build", "block", "cnns", "easytous", "function", "provid", "routin", "for", "comput", "linear", "convolut", "with", "filter", "bank", "featur", "pool", "and", "mani", "more", "this", "document", "provid", "overview", "cnns", "and", "how", "they", "are", "implement", "matconvnet", "and", "give", "the", "technic", "detail", "each", "comput", "block", "the", "toolbox", "num", "unsupervis", "represent", "learn", "with", "deep", "convolut", "generat", "adversari", "network", "alec", "luke", "soumith", "num", "cite", "num", "this", "work", "hope", "help", "bridg", "the", "gap", "between", "the", "success", "cnns", "for", "supervis", "learn", "and", "unsupervis", "learn", "introduc", "class", "cnns", "call", "deep", "convolut", "generat", "adversari", "network", "dcgan", "that", "have", "certain", "architectur", "constraint", "and", "demonstr", "that", "they", "are", "strong", "candid", "for", "unsupervis", "learn", "num", "unet", "convolut", "network", "for", "biomed", "imag", "segment", "olaf", "philipp", "thoma", "num", "cite", "num", "there", "larg", "consent", "that", "success", "train", "deep", "network", "requir", "mani", "thousand", "annot", "train", "sampl", "this", "paper", "present", "network", "and", "train", "strategi", "that", "reli", "the", "strong", "use", "data", "augment", "use", "the", "avail", "annot", "sampl", "more", "effici", "num", "condit", "random", "field", "recurr", "neural", "network", "shuai", "sadeep", "bernardino", "vibhav", "num", "cite", "num", "introduc", "new", "form", "convolut", "neural", "network", "that", "combin", "the", "strength", "convolut", "neural", "network", "cnns", "and", "condit", "random", "field", "crfs", "base", "probabilist", "graphic", "model", "this", "end", "formul", "meanfield", "approxim", "infer", "for", "the", "condit", "random", "field", "with", "gaussian", "pairwis", "potenti", "recurr", "neural", "network", "num", "imag", "superresolut", "use", "deep", "convolut", "network", "chao", "chen", "kaim", "xiaoou", "num", "cite", "num", "our", "method", "direct", "learn", "endtoend", "map", "between", "the", "lowhighresolut", "imag", "the", "map", "repres", "deep", "convolut", "neural", "network", "that", "take", "the", "lowresolut", "imag", "the", "input", "and", "output", "the", "highresolut", "one", "num", "beyond", "short", "snippet", "deep", "network", "for", "video", "classif", "joe", "matthew", "sudheendra", "oriol", "rajat", "georg", "num", "cite", "num", "this", "work", "propos", "and", "evalu", "sever", "deep", "neural", "network", "architectur", "combin", "imag", "inform", "across", "video", "over", "longer", "time", "period", "than", "previous", "attempt", "num", "inceptionvnum", "inceptionresnet", "and", "the", "impact", "residu", "connect", "learn", "christian", "sergey", "vincent", "alexand", "num", "cite", "num", "veri", "deep", "convolut", "network", "have", "been", "central", "the", "largest", "advanc", "imag", "recognit", "perform", "recent", "year", "with", "ensembl", "three", "residu", "and", "one", "inceptionvnum", "achiev", "num", "topnum", "error", "the", "test", "set", "the", "imagenet", "classif", "challeng", "num", "salient", "object", "detect", "discrimin", "region", "featur", "integr", "approach", "huaizu", "jingdong", "zejian", "yang", "nan", "shipeng", "num", "cite", "num", "this", "paper", "formul", "salienc", "map", "comput", "regress", "problem", "our", "method", "which", "base", "multilevel", "imag", "segment", "util", "the", "supervis", "learn", "approach", "map", "the", "region", "featur", "vector", "salienc", "score", "num", "visual", "madlib", "fill", "the", "blank", "descript", "generat", "and", "question", "answer", "licheng", "eunbyung", "alexand", "tamara", "num", "cite", "num", "this", "paper", "introduc", "new", "dataset", "consist", "num", "focus", "natur", "languag", "descript", "for", "num", "imag", "this", "dataset", "the", "visual", "madlib", "dataset", "collect", "use", "automat", "produc", "fillintheblank", "templat", "design", "gather", "target", "descript", "about", "peopl", "and", "object", "their", "appear", "activ", "and", "interact", "well", "infer", "about", "the", "general", "scene", "broader", "context", "num", "asynchron", "method", "for", "deep", "reinforc", "learn", "volodymyr", "adri\u00e0", "mehdi", "alex", "tim", "num", "cite", "num", "the", "best", "perform", "method", "asynchron", "variant", "actorcrit", "surpass", "the", "current", "stateoftheart", "the", "atari", "domain", "while", "train", "for", "half", "the", "time", "singl", "multicor", "instead", "furthermor", "show", "that", "asynchron", "actorcrit", "succeed", "wide", "varieti", "continu", "motor", "control", "problem", "well", "new", "task", "navig", "random", "maze", "use", "visual", "input", "num", "theano", "python", "framework", "for", "fast", "comput", "mathemat", "express", "rami", "guillaum", "amjad", "christof", "num", "cite", "num", "theano", "python", "librari", "that", "allow", "defin", "optim", "and", "evalu", "mathemat", "express", "involv", "multidimension", "array", "effici", "sinc", "introduct", "has", "been", "one", "the", "most", "use", "and", "mathemat", "compil", "especi", "the", "machin", "learn", "communiti", "and", "has", "shown", "steadi", "perform", "improv", "num", "deep", "learn", "face", "attribut", "the", "wild", "ziwei", "ping", "xiaogang", "xiaoou", "num", "cite", "num", "this", "framework", "not", "onli", "outperform", "the", "stateoftheart", "with", "larg", "margin", "but", "also", "reveal", "valuabl", "fact", "learn", "face", "represent", "num", "show", "how", "the", "perform", "face", "local", "lnet", "and", "attribut", "predict", "anet", "can", "improv", "differ", "pretrain", "strategi", "num", "reveal", "that", "although", "the", "filter", "lnet", "are", "finetun", "onli", "with", "imagelevel", "attribut", "tag", "their", "respons", "map", "over", "entir", "imag", "have", "strong", "indic", "face", "locat", "num", "characterlevel", "convolut", "network", "for", "text", "classif", "xiang", "junbo", "jake", "yann", "num", "cite", "num", "this", "articl", "offer", "empir", "explor", "the", "use", "characterlevel", "convolut", "network", "convnet", "for", "text", "classif", "construct", "sever", "largescal", "dataset", "show", "that", "characterlevel", "convolut", "network", "could", "achiev", "stateoftheart", "competit", "result", "relat", "num", "step", "understand", "deep", "learn", "deep", "learn", "past", "present", "and", "futur", "the", "num", "deep", "learn", "method", "practition", "need", "appli"], "timestamp_scraper": 1556482462.656406, "title": "Top 20 Deep Learning Papers, 2018 Edition", "read_time": 477.59999999999997, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/pedro-lopez\" rel=\"author\" title=\"Posts by Pedro Lopez\">Pedro Lopez</a>, KDnuggets.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><img alt=\"\" class=\"wp-image-79446 aligncenter\" sizes=\"(max-width: 600px) 100vw, 600px\" src=\"https://www.kdnuggets.com/wp-content/uploads/wing-features.jpg\" srcset=\"https://www.kdnuggets.com/wp-content/uploads/wing-features.jpg 600w, https://www.kdnuggets.com/wp-content/uploads/wing-features-300x169.jpg 300w\" width=\"80%\"/></p>\n<p>Deep Learning, one of the subfields of Machine Learning and Statistical Learning has been advancing in impressive levels in the past years. Cloud computing, robust open source tools and vast amounts of available data have been some of the levers for these impressive breakthroughs. The criteria used to select the 20 top papers is by using citation counts from <a href=\"http://academic.microsoft.com\" rel=\"noopener noreferrer\" target=\"_blank\"><strong>academic.microsoft.com</strong></a>. It is important to mention that\u00a0these metrics are changing rapidly so the citations valued must be considered as the numbers when\u00a0this article was published.</p>\n<p>In this list of papers more than 75% refer to deep learning and neural networks, specifically Convolutional Neural Networks (CNN). Almost 50% of them refer to pattern recognition applications in the field of computer vision. I believe tools like TensorFlow, Theano and advancements in the use of GPUs have paved the way for data scientists and machine learning engineers to extend the field.</p>\n<p><img alt=\"\" class=\"wp-image-79283 aligncenter\" sizes=\"(max-width: 600px) 100vw, 600px\" src=\"https://www.kdnuggets.com/wp-content/uploads/animal-image-recognition.jpg\" srcset=\"https://www.kdnuggets.com/wp-content/uploads/animal-image-recognition.jpg 600w, https://www.kdnuggets.com/wp-content/uploads/animal-image-recognition-300x229.jpg 300w\" width=\"80%\"/><br>\n<strong>1.\u00a0<a href=\"https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf\">Deep Learning</a>,</strong> by Yann L., Yoshua B. &amp; Geoffrey H. (2015)\u00a0(Cited: 5,716)</br></p>\n<blockquote><p>Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics.</p></blockquote>\n<p><strong>2.\u00a0</strong><a href=\"http://download.tensorflow.org/paper/whitepaper2015.pdf\"><strong>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</strong></a><strong>, </strong>by Mart\u00edn A., Ashish A. B., Eugene B. C., et al. (2015) (Cited: 2,423)</p>\n<blockquote><p>The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery.</p></blockquote>\n<p><strong>3.\u00a0<a href=\"https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf rel=\">TensorFlow: a system for large-scale machine learning</a></strong><strong>, </strong>by Mart\u00edn A., Paul B., Jianmin C., Zhifeng C., Andy D. et al. (2016) (Cited: 2,227)</p>\n<blockquote><p>TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research.</p></blockquote>\n<p><strong>4.\u00a0<a href=\"https://arxiv.org/pdf/1404.7828.pdf\">Deep learning\u00a0in neural networks</a></strong>, by Juergen Schmidhuber (2015)\u00a0(Cited: 2,196)</p>\n<blockquote><p>This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning &amp; evolutionary computation, and indirect search for short programs encoding deep and large networks.</p></blockquote>\n<p><strong>5.\u00a0<a href=\"https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\">Human-level control through deep reinforcement learning</a></strong>, by Volodymyr M., Koray K., David S., Andrei A. R., Joel V et al (2015) (Cited: 2,086)</p>\n<blockquote><p>Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games.</p></blockquote>\n<p><strong>6.\u00a0<a href=\"https://arxiv.org/pdf/1506.01497.pdf\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></strong><strong>, </strong>by Shaoqing R., Kaiming H., Ross B. G. &amp; Jian S. (2015) (Cited: 1,421)</p>\n<blockquote><p>In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.</p></blockquote>\n<p><strong>7.\u00a0<a href=\"https://arxiv.org/pdf/1411.4389.pdf\">Long-term recurrent convolutional networks for visual recognition and description</a></strong><strong>, </strong>by Jeff D., Lisa Anne H., Sergio G., Marcus R., Subhashini V. et al. (2015) (Cited: 1,285)</p>\n<blockquote><p>In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are \u201cdoubly deep\u201d in that they can be compositional in spatial and temporal \u201clayers\u201d.</p></blockquote>\n<p><strong>8.\u00a0<a href=\"https://arxiv.org/pdf/1412.4564.pdf\">MatConvNet: Convolutional Neural Networks for MATLAB</a>, </strong>by<strong>\u00a0</strong>Andrea Vedaldi &amp; Karel Lenc (2015) (Cited: 1,148)</p>\n<blockquote><p>It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing linear convolutions with filter banks, feature pooling, and many more. This document provides an overview of CNNs and how they are implemented in MatConvNet and gives the technical details of each computational block in the toolbox.</p></blockquote>\n<p><strong>9.\u00a0<a href=\"https://arxiv.org/pdf/1511.06434.pdf\">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></strong><strong>, </strong>by Alec R., Luke M. &amp; Soumith C. (2015)\u00a0(Cited: 1,054)</p>\n<blockquote><p>In this work, we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.</p></blockquote>\n<p><strong>10.\u00a0<a href=\"https://arxiv.org/pdf/1505.04597.pdf\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></strong><strong>,</strong> by Olaf R., Philipp F. &amp;Thomas B. (2015) (Cited: 975)</p>\n<blockquote><p>There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently.</p></blockquote>\n<p><strong>11.\u00a0<a href=\"http://www.robots.ox.ac.uk/~szheng/papers/CRFasRNN.pdf\">Conditional Random Fields as Recurrent Neural Networks</a></strong><strong>, </strong>by Shuai Z., Sadeep J., Bernardino R., Vibhav V. et al (2015)\u00a0(Cited: 760)</p>\n<blockquote><p>We introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks.</p></blockquote>\n<p><strong>12.\u00a0<a href=\"https://arxiv.org/pdf/1501.00092.pdf\">Image Super-Resolution Using Deep Convolutional Networks</a></strong><strong>, </strong>by Chao D., Chen C., Kaiming H. &amp; Xiaoou T. (2014) (Cited: 591)</p>\n<blockquote><p>Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one</p></blockquote>\n<p><strong>13.\u00a0<a href=\"https://arxiv.org/pdf/1503.08909.pdf\">Beyond short snippets: Deep networks for video classification</a></strong><strong>, </strong>by Joe Y. Ng, Matthew J. H., Sudheendra V., Oriol V., Rajat M. &amp; George T. (2015) (Cited: 533)</p>\n<blockquote><p>In this work, we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted.</p></blockquote>\n<p><strong>14.\u00a0<a href=\"https://arxiv.org/pdf/1602.07261.pdf\">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></strong><strong>, </strong>by Christian S., Sergey I., Vincent V. &amp; Alexander A A. (2017)\u00a0(Cited: 520)</p>\n<blockquote><p>Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. With an ensemble of three residual and one Inception-v4, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.</p></blockquote>\n<p><strong>15.\u00a0<a href=\"https://arxiv.org/pdf/1410.5926.pdf\">Salient Object Detection: A Discriminative Regional Feature Integration Approach</a></strong><strong>, </strong>by Huaizu J., Jingdong W., Zejian Y., Yang W., Nanning Z. &amp; Shipeng Li. (2013) (Cited: 518)</p>\n<blockquote><p>In this paper, we formulate saliency map computation as a regression problem. Our method, which is based on multi-level image segmentation, utilizes the supervised learning approach to map the regional feature vector to a saliency score.</p></blockquote>\n<p><strong>16.\u00a0<a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yu_Visual_Madlibs_Fill_ICCV_2015_paper.pdf\">Visual Madlibs: Fill in the Blank Description Generation and Question Answering</a></strong>, by Licheng Y., Eunbyung P., Alexander C. B. &amp; Tamara L. B. (2015) (Cited: 510)</p>\n<blockquote><p>In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context.</p></blockquote>\n<p><strong>17.\u00a0<a href=\"http://proceedings.mlr.press/v48/mniha16.pdf\">Asynchronous methods for deep reinforcement learning</a></strong><strong>,</strong> by Volodymyr M., Adri\u00e0 P. B., Mehdi M., Alex G., Tim H. et al. (2016) (Cited: 472)</p>\n<blockquote><p>The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.</p></blockquote>\n<p><strong>18.\u00a0<a href=\"https://arxiv.org/pdf/1605.02688.pdf\">Theano: A Python framework for fast computation of mathematical expressions</a>.</strong>, by by Rami A., Guillaume A., Amjad A., Christof A. et al (2016) (Cited: 451)</p>\n<blockquote><p>Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers especially in the machine learning community and has shown steady performance improvements.</p></blockquote>\n<p><strong>19.\u00a0<a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Liu_Deep_Learning_Face_ICCV_2015_paper.pdf\">Deep Learning\u00a0Face Attributes in the Wild</a></strong><strong>, </strong>by Ziwei L., Ping L., Xiaogang W. &amp; Xiaoou T. (2015) (Cited: 401)</p>\n<blockquote><p>This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with imagelevel attribute tags, their response maps over entire images have strong indication of face locations.</p></blockquote>\n<p><strong>20. </strong><a href=\"http://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf\"><strong>Character-level convolutional networks for text classification</strong></a><strong>, </strong>by Xiang Z., Junbo Jake Z. &amp; Yann L. (2015) (Cited: 401)</p>\n<blockquote><p>This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several largescale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results.</p></blockquote>\n<p>\u00a0<br>\n<b>Related:</b></br></p>\n<ul class=\"three_ul\">\n<li><a href=\"https://www.kdnuggets.com/2016/01/seven-steps-deep-learning.html\">7 Steps to Understanding Deep Learning</a></li>\n<li><a href=\"https://www.kdnuggets.com/2017/05/deep-learning-big-deal.html\">Deep Learning \u2013 Past, Present, and Future</a></li>\n<li><a href=\"https://www.kdnuggets.com/2017/12/10-deep-learning-methods-ai-practitioners-need-apply.html\">The 10 Deep Learning Methods AI Practitioners Need to Apply</a></li>\n</ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}