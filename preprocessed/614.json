{"content": "By Adrian Colyer , Venture Partner, Accel. Achieving Human Parity in Conversational Speech Recognition ,\u00a0Xiong et al.\u00a0 Microsoft Technical Report, 2016 The headline story here is that for the first time a system has been developed that exceeds human performance in one of the most difficult of all human speech recognition tasks: natural conversations held over the telephone. This is known as conversational telephone speech, or\u00a0 CTS . [CTS] is especially difficult due to the spontaneous (neither read nor planned) nature of the speech, its informality, and the self-corrections, hesitations, and other disfluencies that are pervasive. The reference datasets for this task are the Switchboard and Fisher data collections from the 1990s and early 2000s. The apocryphal story here is that human performance on the task is about 4% error rate. But no-one can quite pin down where that 4% number comes from. So the Microsoft team took advantage of an existing professional transcription service used by Microsoft: To measure human performance, we leveraged an existing pipeline in which Microsoft data is transcribed on a weekly basis. This pipeline uses a large commercial vendor to perform a two-pass transcription. In the first pass, a transcriber works from scratch to transcribe the data. In the second pass, a second listener monitors the data to do error correction. Dozens of hours of test data are processed in each batch. One week, we added the NIST 2000 CTS evaluation data to the work-list, without further comment\u2026 For the switchboard portion of the dataset, the professional human transcribers achieved a 5.9% error rate, and for the \u2018call-home\u2019 portion of the test set 11.3%. \u201cThe same informality, multiple speakers per channel, and recording conditions that make CallHome hard for computers make it difficult for people as well.\u201d Notably, the performance of our artificial system aligns almost exactly with the performance of people on both sets. And so one can\u2019t help but wonder how much longer those professional transcribers will continue to be needed! Did they know they were gathering the evidence that one day might help lead to the elimination of their jobs??? Here\u2019s a table full of cryptic acronyms that show the performance of the system that Microsoft put together, using various acoustic models. The bottom-line (strictly, bottom two lines) is what matters here: parity on the switchboard dataset, and a slight advantage for the ASR (Automatic Speech Recognition) system of the CallHome dataset. How did the Microsoft team manage to pull this off? Our system\u2019s performance can be attributed to the systematic use of LSTMs for both acoustic and language modeling, as well as CNNs in the acoustic model, and extensive combination of complementary models. Training was made feasible (reducing times from month to 1-3 weeks) by using Microsoft\u2019s\u00a0 CNTK \u00a0Cognitive Toolkit to parallelize SGD training, coupled with the use of the\u00a0 1-bit SGD parallelization techique \u00a0from prior work: In [65], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next. Each time a sub-gradient is quantized, the quantization error is computed and remembered, and then added to the next minibatch\u2019s sub-gradient. This reduces the required bandwidth 32-fold with minimal loss in accuracy. How it Works Under the Covers \u00a0 The model details are concisely explained, targeting an audience of speech recognition experts (i.e.\u00a0not me!). It is still possible for a lay-reader to gain some appreciation of what\u2019s involved though. It\u2019s also another reminder that we\u2019re rapidly assembling a powerful collection of building blocks that through good systems engineering can be combined into very effective systems. Expect to see an explosion of\u00a0 applied \u00a0AI/ML/whatever-your-preferred-phrase-is applications as this trend continues. Our progress is a result of the careful engineering and optimization of convolutional and recurrent neural networks. While the basic structures have been well known for a long period, it is only recently that they have emerged as the best models for speech recognition. Surprisingly, this is the case for both acoustic modeling and language modeling. The CNN and RNN based acoustic models can model a large amount of acoustic context with temporal invariance, and in the case of CNNs, with frequency invariance as well. In language modeling, RNNs improve on classical N-gram models through the use of an unbounded word history and the generalization ability of\u00a0 continuous word representations . The paper describes a whole family of systems that were explored in order to find the best performing combination. The best acoustic model was formed by combining independently trained ResNet and VGG models using a score fusion weight. \u2018VGG\u2019 stands for the University of Oxford Visual Geometry Group and the architecture they developed in \u201c Very deep convolutional networks for large-scale visual recognition \u201d. Their networks use 16-19 layers with small 3\u00d73 filters in all convolutional layers. The\u00a0 ResNet \u00a0architecture is also borrowed from the field of image recognition. Speaker adaptive modeling \u00a0is then applied by conditioning the network on an\u00a0 i-vector \u00a0characterization of each speaker using 100-dimensional i-vectors. The i-vectors are added to the activation of each CNN layer via a learnable weight matrix. After initial training, model parameters are optimized using a maximum mutual information (MMI) objective function: where\u00a0 w \u00a0is a word sequence, and\u00a0 a \u00a0is an acoustic realization of a word sequence. The performance improvements obtained from this lattice-free MMI (LFMMI) training phase, as well as i-vectors, can be seen in the following table: An initial decoding of acoustic model outputs is done with a WFST ( Weighted Finite State Transducer ) decoder. We use an N-gram language model trained and pruned with the SRILM toolkit. The first-pass LM has approximately 15.9 million bigrams, trigrams, and 4-grams, and a vocabulary of 30500 words, and give a perplexity of 54 on RT-03 speech transcripts. The N-best performing hypotheses from the WFST decoding are then rescored using a combination of a large N-gram language model and neural net language models. The best performing language model used LSTMs with three hidden layers, and 1000 hidden units in each layer. \u201cFor the final system, we interpolated two LSTM-LMs with an N-gram LM for the forward direction LM, and similarly for the backward direction LM.\u201d Bio: Adrian Colyer was CTO of SpringSource, then CTO for Apps at VMware and subsequently Pivotal. He is now a Venture Partner at Accel Partners in London, working with early stage and startup companies across Europe. If you\u2019re working on an interesting technology-related business he would love to hear from you: you can reach him at acolyer at accel dot com . Original . Reposted with permission. Related: Data Science of Sales Calls: The Surprising Words That Signal Trouble or Success A Survey of Available Corpora for Building Data-driven Dialogue Systems Microsoft is Becoming crosoft", "title_html": "<h1 id=\"title\">Achieving Human Parity in Conversational Speech Recognition</h1> ", "url": "https://www.kdnuggets.com/2016/12/achieving-human-parity-conversational-speech-recognition.html", "tfidf": {"tfidf": {"after": 1.02070207021, "case": 2.96997474512, "relat": 1.23750876919, "stage": 2.0831911822599998, "basi": 2.42122922068, "detail": 2.26186066391, "troubl": 4.99088337001, "form": 1.12755681818, "data": 23.63504891538, "signal": 5.12459651388, "would": 1.0828729281799998, "number": 1.10142916609, "oxford": 3.5139442231099998, "been": 2.0478555304799997, "dataset": 774.439024392, "function": 2.495441685, "progress": 2.44697903822, "hear": 4.17899447223, "resnet": 3175.2, "well": 5.3278743539999995, "done": 2.3302509907499998, "assembl": 3.0011342155, "technic": 3.1400316455699997, "geometri": 25.4423076923, "transcript": 39.9563758389, "know": 2.59327017315, "their": 2.0309581681, "vocabulari": 23.2785923754, "acoly": 1587.6, "automat": 6.787516032490001, "hesit": 18.6776470588, "measur": 2.41093394077, "ventur": 15.47368421052, "multipl": 2.74813917258, "how": 4.80750984153, "test": 5.31414225942, "repost": 933.882352941, "million": 1.7279059643, "transcrib": 152.36084453, "technologyrel": 1587.6, "good": 1.51981619759, "especi": 1.66712170534, "expert": 5.36713995943, "noon": 26.328358209, "due": 1.23789473684, "word": 10.779223718459999, "interest": 1.60331246213, "invari": 44.8474576272, "deep": 3.6279707495399998, "will": 1.22481098596, "datadriven": 1587.6, "pin": 16.233128834400002, "second": 2.2261796256, "vendor": 27.2783505155, "portion": 6.603993344419999, "next": 2.9901120632800002, "numbit": 1587.6, "perform": 18.376772451, "but": 2.03264835798, "were": 2.04917715392, "obtain": 2.68629441624, "our": 7.07276507277, "carri": 1.66869875972, "longer": 2.02319357716, "final": 1.34008609775, "where": 2.13430127042, "expect": 2.20011086475, "record": 1.42334588488, "trigram": 1587.6, "applic": 3.42672134686, "hidden": 15.62598425196, "has": 2.0872995004, "have": 2.0297896822799997, "paramet": 17.256521739100002, "twopass": 1587.6, "listen": 6.97846153846, "pass": 3.23636734278, "subsequ": 1.7534791252500002, "surpris": 8.73267326732, "model": 43.9025546484, "quit": 2.8849718335500003, "forward": 3.66566612792, "toolkit": 378.0, "much": 1.1942229577299999, "under": 1.0781663837, "optim": 23.0755813954, "field": 1.7790228597, "loss": 2.42529789184, "lstmlms": 1587.6, "involv": 1.4498630137000001, "firstpass": 1587.6, "appreci": 8.11241696474, "whole": 2.29488291414, "minim": 6.10850327049, "down": 1.35889754344, "not": 1.01567398119, "stori": 4.04793472718, "wonder": 7.265903890160001, "though": 1.36076112111, "then": 4.34631442064, "they": 4.12069301148, "anoth": 1.13643521832, "sale": 3.4274611399, "reach": 1.49801849406, "effect": 1.3963060686000002, "scienc": 2.31969608416, "task": 11.65924112607, "worklist": 1587.6, "abil": 2.70875277256, "that": 15.059760956249999, "togeth": 1.58095996813, "valu": 2.2777618364400003, "startup": 68.4310344828, "develop": 2.3911439114400004, "accuraci": 12.7620578778, "those": 1.19548192771, "explor": 3.39593582888, "network": 10.37477536352, "three": 1.06621893889, "some": 1.04036697248, "small": 1.3594793629, "collect": 3.28219971056, "put": 1.65806788512, "result": 1.14611608432, "com": 99.8490566038, "off": 1.5121440137200002, "classic": 2.4087391898, "filter": 16.8893617021, "quantize": 547.448275862, "languag": 16.06418039898, "set": 2.37415881562, "two": 2.0275862069, "and": 30.0018897639, "from": 10.0056721497, "complementari": 32.466257668699996, "disfluenc": 1587.6, "work": 5.57600449565, "learnabl": 1587.6, "num": 15.004725600150001, "for": 19.00598576019, "difficult": 7.4687156970299995, "num\u00d7num": 1587.6, "output": 7.676982591880001, "bottom": 6.27261951798, "seen": 1.61079545455, "with": 12.014378507879997, "acoust": 180.4090909095, "commerci": 2.4036336109, "are": 7.20934155046, "compani": 1.5523613963, "techiqu": 1587.6, "month": 1.5079787234, "veri": 2.51760228354, "object": 2.3488681757700003, "combin": 8.4880239521, "nbest": 1587.6, "align": 8.10413476263, "net": 6.96315789474, "subgradi": 3175.2, "general": 1.1218202374200001, "decod": 155.1400651467, "spontan": 16.1834862385, "group": 1.20996875238, "vmware": 1443.27272727, "convers": 10.045981860360001, "realize": 30.24, "give": 1.3653250774, "train": 11.6194193706, "scratch": 25.8146341463, "error": 30.20547945205, "continu": 3.4178686760100003, "fusion": 17.7583892617, "matter": 2.44773358002, "quantiz": 496.125, "period": 1.3430335843, "into": 1.01502461479, "paper": 2.6628648104700003, "prior": 2.17807655371, "ngram": 6350.4, "rapid": 2.62586834271, "servic": 1.51300867245, "requir": 1.52844902282, "concis": 22.647646219699997, "what": 2.50686878256, "largescal": 1587.6, "comment\u2026": 1587.6, "minibatch": 3175.2, "apocryph": 70.2477876106, "help": 2.79925945518, "crosoft": 1587.6, "larg": 3.55724848755, "cognit": 21.454054054100002, "borrow": 8.02223345124, "cover": 1.69380134429, "tempor": 21.897931034499997, "success": 1.32002993265, "approxim": 2.2132998745299997, "base": 1.14628158845, "natur": 3.0785340314200003, "can": 9.41009113136, "hour": 2.25960717336, "univers": 1.24889867841, "permiss": 6.280063291139999, "remind": 12.403125, "here": 9.69230769232, "dialogu": 9.19281991893, "week": 5.41596543099, "report": 1.3634489866, "microsoft": 198.76056338, "maximum": 4.80072573329, "describ": 1.47027227264, "long": 1.2657259028899999, "human": 11.37928562898, "layer": 40.7076923077, "evalu": 6.9509632224199995, "parallel": 9.15835015864, "care": 2.49426551453, "about": 1.06486015159, "evid": 2.24872521246, "notabl": 1.82840032247, "job": 3.2539454806299997, "neural": 118.9213483146, "context": 4.25972632144, "just": 1.33580143037, "telephon": 17.87837837838, "adapt": 3.32272917539, "accel": 4762.799999999999, "correct": 3.6631287494199998, "him": 1.63434218653, "frequenc": 8.8102108768, "selfcorrect": 1587.6, "recognit": 30.80155210643, "switchboard": 898.641509433, "pivot": 17.798206278, "across": 1.7318642958400001, "rememb": 4.88793103448, "nor": 3.3479544496, "took": 1.4009883515700001, "love": 2.97303370787, "bigram": 1587.6, "explain": 2.60049140049, "singl": 1.60948905109, "perplex": 108.739726027, "famili": 1.48804948917, "target": 3.2189781021900004, "pull": 5.18654034629, "appli": 4.5944147012, "over": 2.05050048434, "build": 3.2683479156, "trend": 5.43140608963, "systemat": 8.338235294119999, "monitor": 6.05723006486, "initi": 2.7, "team": 4.5496489468400005, "earli": 2.24936242562, "order": 1.24625166811, "rnns": 1587.6, "character": 4.563380281690001, "exact": 3.46864758575, "explos": 6.72142252329, "colyer": 3175.2, "inform": 4.72593768606, "matrix": 22.6153846154, "latticefre": 1587.6, "possibl": 1.4173734488, "best": 6.331405782640001, "origin": 1.13724928367, "slight": 3.25327868852, "pariti": 92.3023255814, "visual": 10.45505432994, "follow": 1.04640126549, "now": 1.160780873, "further": 1.3618116315, "hard": 2.73253012048, "strict": 4.7235941684, "per": 1.9597580545599997, "full": 1.66729678639, "represent": 5.928304705, "basic": 2.7301805675, "unit": 1.15394679459, "power": 1.3396337861799998, "leverag": 35.7567567568, "reduc": 3.97396745932, "also": 2.02953020134, "coupl": 3.2572835453400004, "interpol": 50.4, "batch": 35.6764044944, "plan": 1.5356935577500002, "adrian": 39.1034482758, "numdimension": 1587.6, "the": 76.0, "becom": 1.12492028626, "peopl": 2.42640990372, "stand": 2.0845588235299997, "dozen": 5.95275590551, "via": 2.2978723404299997, "amount": 2.27027027027, "advantag": 6.64824120604, "callhom": 4762.799999999999, "system": 13.8739840951, "extens": 1.99171998495, "partner": 12.520504731870002, "various": 1.3323262839899999, "attribut": 3.4156626506, "acronym": 35.0463576159, "backward": 14.605335786600001, "histori": 1.20629131525, "which": 1.005191845, "rescor": 1587.6, "activ": 1.46403541129, "other": 1.00992366412, "refer": 1.30024570025, "without": 1.29547123623, "one": 6.03764974332, "show": 2.5340782123, "known": 2.1718194254400003, "see": 1.27242125511, "elimin": 3.67670217693, "imag": 2.70137825421, "still": 1.1866357724799999, "artifici": 8.31639601886, "numfold": 1587.6, "audienc": 4.4784203103, "through": 2.14149861738, "manag": 1.6448404475799998, "dot": 18.8775267539, "unbound": 96.80487804879999, "process": 1.69524826482, "both": 3.15647160183, "independ": 1.58950740889, "lstms": 3175.2, "neither": 3.6622837370199997, "recent": 1.54405757635, "structur": 2.0580762250499998, "tabl": 7.64187725632, "gather": 3.78631051753, "this": 8.03034901368, "time": 3.03382381044, "finit": 28.1989342806, "engin": 4.94271481942, "direct": 2.44452998692, "bit": 8.33385826772, "similar": 1.37514075357, "survey": 3.7791002142300005, "same": 1.11857958148, "profession": 7.916888297879999, "bandwidth": 72.16363636359999, "most": 1.02096463023, "held": 1.44353518822, "need": 1.4372623574099999, "all": 2.02293577982, "ivector": 6350.4, "headlin": 12.9705882353, "mutual": 6.615, "gradient": 41.889182058, "condit": 3.84966052376, "almost": 1.53584212054, "exist": 2.9294215333599998, "feasibl": 17.8181818182, "day": 1.18371607516, "weight": 14.636754763379997, "architectur": 10.25581395348, "call": 1.0676529926, "rate": 4.2809761359, "improv": 4.08753861998, "xiong": 417.78947368400003, "aimlwhateveryourpreferredphrasei": 1587.6, "prune": 131.20661157, "london": 1.97782484116, "achiev": 3.74433962264, "exceed": 5.0689655172400006, "gain": 1.84819557625, "cryptic": 96.2181818182, "made": 1.07038834951, "emerg": 2.1131372288, "lead": 1.2664326739, "recurr": 35.5964125561, "state": 1.0477133240899998, "transduc": 206.181818182, "numgram": 1587.6, "channel": 3.6784059314199995, "avail": 1.7288467821, "comput": 7.855517070760001, "make": 2.1525320317200003, "onli": 1.0256476516600002, "each": 5.948741007200001, "layread": 1587.6, "fisher": 16.3670103093, "speech": 30.582229713440004, "find": 1.7294117647099998, "read": 2.3149606299200003, "hypothes": 20.644993497999998, "europ": 2.0172808132099997, "while": 1.0441988950299999, "block": 3.20274359492, "cnns": 3175.2, "sequenc": 12.14225621414, "phase": 4.3012733676499995, "convolut": 303.363057324, "bio": 42.336000000000006, "come": 1.32831325301, "might": 2.1561863370900003, "line": 1.4182597820299998, "speaker": 19.05882352941, "springsourc": 1587.6, "first": 2.01523229246, "app": 35.837471783299996, "busi": 2.05541170378, "use": 14.414942603319998, "score": 4.2884927066500005, "bottomlin": 1587.6, "pervas": 40.3969465649, "corpora": 481.09090909099996, "pipelin": 64.2753036438}, "logtfidf": {"after": 0.020490694648099998, "case": 0.790812537778, "relat": 0.21310030165399999, "stage": 0.733900940237, "basi": 0.884275353639, "detail": 0.816187777173, "troubl": 1.60761292215, "form": 0.120053184191, "data": 8.5177440936, "signal": 1.6340517929299998, "would": 0.0796176279647, "number": 0.0966085784186, "oxford": 1.25673911688, "been": 0.04729196473680001, "dataset": 21.06337826656, "function": 0.914465741594, "progress": 0.894854218108, "hear": 1.43007066072, "resnet": 14.739957441820001, "well": 0.317572191578, "done": 0.845975983129, "assembl": 1.09899028905, "technic": 1.14423287808, "geometri": 3.2364134455299998, "transcript": 7.76752789884, "know": 0.952919694398, "their": 0.030721010245400002, "vocabulari": 3.14753415606, "acoly": 7.369978720910001, "automat": 1.9150850473199998, "hesit": 2.9273274644200002, "measur": 0.880014199726, "ventur": 4.09198721522, "multipl": 1.01092401812, "how": 1.4147008707900002, "test": 1.954448874206, "repost": 6.83935046985, "million": 0.5469102500940001, "transcrib": 17.084068860749998, "technologyrel": 7.369978720910001, "good": 0.418589404907, "especi": 0.511098609709, "expert": 1.68029517063, "noon": 3.27064661718, "due": 0.21341214386399998, "word": 3.51516649431, "interest": 0.47207177798199995, "invari": 6.220239440419999, "deep": 1.2886734698, "will": 0.202786534915, "datadriven": 7.369978720910001, "pin": 2.7870541438700003, "second": 0.21427952675999998, "vendor": 3.30609336617, "portion": 2.3890546753799997, "next": 0.804327370998, "numbit": 7.369978720910001, "perform": 5.11417020696, "but": 0.0323847441438, "were": 0.048582287362199994, "obtain": 0.988162703503, "our": 2.5729176425460003, "carri": 0.512044136911, "longer": 0.7046772417749999, "final": 0.292733863948, "where": 0.1299842774914, "expect": 0.78850775216, "record": 0.353010356953, "trigram": 7.369978720910001, "applic": 1.23160392849, "hidden": 4.1115760104, "has": 0.0854478897096, "have": 0.0295700046824, "paramet": 2.8481901438599997, "twopass": 7.369978720910001, "listen": 1.94282848252, "pass": 0.96260865948, "subsequ": 0.561601885907, "surpris": 2.94784871722, "model": 15.486451535331001, "quit": 1.05951513684, "forward": 1.29901007269, "toolkit": 10.48349403012, "much": 0.17749572930100002, "under": 0.07526180538319999, "optim": 4.891255590819999, "field": 0.5760642583510001, "loss": 0.885954358842, "lstmlms": 7.369978720910001, "involv": 0.371469078658, "firstpass": 7.369978720910001, "appreci": 2.09339584651, "whole": 0.8306818244059999, "minim": 1.80968177926, "down": 0.306673741186, "not": 0.0155524130075, "stori": 1.410119253174, "wonder": 1.98319270637, "though": 0.308044191079, "then": 0.33213546092359997, "they": 0.1189079790704, "anoth": 0.127896361652, "sale": 1.23181979465, "reach": 0.40414323085000003, "effect": 0.333830227158, "scienc": 0.841436178891, "task": 4.0724604198300005, "worklist": 7.369978720910001, "abil": 0.996488297427, "that": 0.059642225694599996, "togeth": 0.458032237308, "valu": 0.823193310148, "startup": 4.225826442240001, "develop": 0.357249389826, "accuraci": 2.5464765406, "those": 0.17854939087299998, "explor": 1.22257937218, "network": 3.8123322122079997, "three": 0.06411868822490001, "some": 0.0395735090645, "small": 0.307101805059, "collect": 0.99073332104, "put": 0.505652999854, "result": 0.136378908381, "com": 4.60365961168, "off": 0.41352852038800003, "classic": 0.8791034528499999, "filter": 2.82668393864, "quantize": 11.22424160672, "languag": 5.814772770842, "set": 0.342992022578, "two": 0.0273976887164, "and": 0.001889704261908, "from": 0.00567054168866, "complementari": 3.4802013244300003, "disfluenc": 7.369978720910001, "work": 0.545172836365, "learnabl": 7.369978720910001, "num": 0.004724855930955001, "for": 0.005984817512543001, "difficult": 2.736332302764, "num\u00d7num": 7.369978720910001, "output": 2.03822657827, "bottom": 1.8361940533599999, "seen": 0.47672812813, "with": 0.01436990056068, "acoust": 26.98202179881, "commerci": 0.8769815969470001, "are": 0.2062723150789, "compani": 0.439777253097, "techiqu": 7.369978720910001, "month": 0.410770160338, "veri": 0.460319586476, "object": 0.853933584803, "combin": 2.6460915537550003, "nbest": 7.369978720910001, "align": 2.09237439596, "net": 1.9406330919499999, "subgradi": 14.739957441820001, "general": 0.114952578063, "decod": 11.83714819896, "spontan": 2.7839913543400003, "group": 0.190594534797, "vmware": 7.2746685411000005, "convers": 3.6256813529999996, "realize": 3.4091655513099997, "give": 0.311392552224, "train": 3.965509877034, "scratch": 3.2509415461, "error": 8.9929271715, "continu": 0.39121462196100004, "fusion": 2.8768580387299996, "matter": 0.8951625270360001, "quantiz": 11.02736146108, "period": 0.294930924153, "into": 0.0149128632287, "paper": 0.979402539665, "prior": 0.778442172521, "ngram": 29.479914883640003, "rapid": 0.965411638564, "servic": 0.41410016674500005, "requir": 0.424253510675, "concis": 3.1200559268700006, "what": 0.451774593654, "largescal": 7.369978720910001, "comment\u2026": 7.369978720910001, "minibatch": 14.739957441820001, "apocryph": 4.252028814630001, "help": 0.672415442688, "crosoft": 7.369978720910001, "larg": 0.511125181818, "cognit": 3.0659136276999996, "borrow": 2.0822168683, "cover": 0.526975319156, "tempor": 3.08639215905, "success": 0.27765441259199997, "approxim": 0.7944845577770001, "base": 0.13652330228700002, "natur": 0.862612678584, "can": 1.298728771152, "hour": 0.815190981077, "univers": 0.222262105686, "permiss": 1.8373800586400002, "remind": 2.51794845699, "here": 3.5401527534800006, "dialogu": 2.21842273576, "week": 1.7722165924859998, "report": 0.31001750903700004, "microsoft": 25.70127487624, "maximum": 1.5687671009200002, "describ": 0.385447603125, "long": 0.235645793878, "human": 3.840211099458, "layer": 10.48489581175, "evalu": 1.9388802431299998, "parallel": 3.04303773644, "care": 0.9139943029109999, "about": 0.0628434774746, "evid": 0.8103634834160001, "notabl": 0.603441443842, "job": 1.1798682540899998, "neural": 8.170630311, "context": 1.44920491442, "just": 0.289531434109, "telephon": 4.3808897807, "adapt": 1.2007864860200002, "accel": 22.10993616273, "correct": 1.29831763181, "him": 0.49124039099699995, "frequenc": 2.1759113757299997, "selfcorrect": 7.369978720910001, "recognit": 10.371584529040001, "switchboard": 17.10681570105, "pivot": 2.87909768132, "across": 0.549198455941, "rememb": 1.5867691126199999, "nor": 1.2083495472799999, "took": 0.337177952953, "love": 1.08958288195, "bigram": 7.369978720910001, "explain": 0.955700427358, "singl": 0.475916769059, "perplex": 4.68895719219, "famili": 0.39746619471100003, "target": 1.1690639496200002, "pull": 1.6460668749599998, "appli": 1.6633883796239999, "over": 0.0498734429914, "build": 0.982274904182, "trend": 1.6921980487900001, "systemat": 2.12085159855, "monitor": 1.80125261058, "initi": 0.6002091849, "team": 1.643805789772, "earli": 0.234999258216, "order": 0.22014038079300002, "rnns": 7.369978720910001, "character": 1.51806363875, "exact": 1.2437647732500001, "explos": 1.90529981715, "colyer": 14.739957441820001, "inform": 1.363361113986, "matrix": 3.1186304098799997, "latticefre": 7.369978720910001, "possibl": 0.348805474891, "best": 1.836911731788, "origin": 0.128612437587, "slight": 1.17966331506, "pariti": 7.663844313060001, "visual": 3.3078766977200003, "follow": 0.045356911094199995, "now": 0.149092945021, "further": 0.308815895297, "hard": 1.00522796406, "strict": 1.55256998618, "per": 0.672821024072, "full": 0.511203624148, "represent": 1.7797382876499999, "basic": 1.00436774895, "unit": 0.143188061817, "power": 0.292396282715, "leverag": 3.5767392514699994, "reduc": 1.373235550286, "also": 0.0293143156, "coupl": 1.18089357972, "interpol": 3.9199911750800003, "batch": 3.5744895317400003, "plan": 0.428982108147, "adrian": 5.94612694748, "numdimension": 7.369978720910001, "the": 0.0, "becom": 0.11771217648900001, "peopl": 0.386531156946, "stand": 0.7345572374320001, "dozen": 1.78385428972, "via": 0.831983625414, "amount": 0.819898886199, "advantag": 2.40241031766, "callhom": 22.10993616273, "system": 3.27430345585, "extens": 0.6889985794750001, "partner": 4.286266170719999, "various": 0.28692650007, "attribut": 1.2283715153700001, "acronym": 3.5566716884199994, "backward": 2.68138692678, "histori": 0.187550624069, "which": 0.00517841384543, "rescor": 7.369978720910001, "activ": 0.381196603284, "other": 0.00987474791976, "refer": 0.262553246798, "without": 0.258874517941, "one": 0.037532109873, "show": 0.473365532026, "known": 0.1648361611984, "see": 0.240921585492, "elimin": 1.30201620283, "imag": 0.99376210729, "still": 0.17112222142900002, "artifici": 2.11822899018, "numfold": 7.369978720910001, "audienc": 1.4992703749399998, "through": 0.1367173837698, "manag": 0.497643387158, "dot": 2.93797215393, "unbound": 4.572697386080001, "process": 0.527829199025, "both": 0.1525276001679, "independ": 0.463424162503, "lstms": 14.739957441820001, "neither": 1.29808692469, "recent": 0.434413741288, "structur": 0.7217716751350001, "tabl": 2.68099221322, "gather": 1.3313920667299999, "this": 0.03029159242, "time": 0.0336345565878, "finit": 3.33928418576, "engin": 1.809535116552, "direct": 0.401411378992, "bit": 2.12032652634, "similar": 0.318556092114, "survey": 1.3294859427299999, "same": 0.112059649604, "profession": 2.911157844819, "bandwidth": 4.27893626755, "most": 0.020747896295599998, "held": 0.36709509683499997, "need": 0.362740163442, "all": 0.022805264195599997, "ivector": 29.479914883640003, "headlin": 2.56268435083, "mutual": 1.88933979757, "gradient": 3.73502760882, "condit": 1.309675576412, "almost": 0.42907884333400004, "exist": 0.7633155881739999, "feasibl": 2.88021938643, "day": 0.16865870631700003, "weight": 4.7547705783600005, "architectur": 3.26939515838, "call": 0.0654627744488, "rate": 1.522067744332, "improv": 1.4295916078639999, "xiong": 6.0349776541799995, "aimlwhateveryourpreferredphrasei": 7.369978720910001, "prune": 4.87677326831, "london": 0.6819976757709999, "achiev": 1.2541961702339999, "exceed": 1.62313675679, "gain": 0.6142097989249999, "cryptic": 4.56661834, "made": 0.0680215260973, "emerg": 0.748173681534, "lead": 0.23620402986699998, "recurr": 3.5722448618800002, "state": 0.0466100027668, "transduc": 5.32875839205, "numgram": 7.369978720910001, "channel": 1.30247948752, "avail": 0.547454586289, "comput": 2.73613783188, "make": 0.14699531564579998, "onli": 0.025324268329099998, "each": 0.86870844652, "layread": 7.369978720910001, "fisher": 2.79526774241, "speech": 10.727820562160002, "find": 0.547781330288, "read": 0.83939268088, "hypothes": 3.0274728444, "europ": 0.7017504724920001, "while": 0.04324998379380001, "block": 1.16400781588, "cnns": 14.739957441820001, "sequenc": 3.6070888748, "phase": 1.4589111108700001, "convolut": 13.848954025650002, "bio": 3.7456377879300002, "come": 0.28390990653000003, "might": 0.7683410765340001, "line": 0.349430614452, "speaker": 5.54675364921, "springsourc": 7.369978720910001, "first": 0.015174579624319999, "app": 3.57899404386, "busi": 0.720476170355, "use": 0.4089122762424, "score": 1.4559353207700003, "bottomlin": 7.369978720910001, "pervas": 3.69875420203, "corpora": 6.17605625244, "pipelin": 6.94005659344}, "logidf": {"after": 0.020490694648099998, "case": 0.395406268889, "relat": 0.21310030165399999, "stage": 0.733900940237, "basi": 0.884275353639, "detail": 0.816187777173, "troubl": 1.60761292215, "form": 0.120053184191, "data": 1.2168205848, "signal": 1.6340517929299998, "would": 0.0796176279647, "number": 0.0966085784186, "oxford": 1.25673911688, "been": 0.023645982368400004, "dataset": 5.26584456664, "function": 0.914465741594, "progress": 0.894854218108, "hear": 1.43007066072, "resnet": 7.369978720910001, "well": 0.0635144383156, "done": 0.845975983129, "assembl": 1.09899028905, "technic": 1.14423287808, "geometri": 3.2364134455299998, "transcript": 2.58917596628, "know": 0.952919694398, "their": 0.015360505122700001, "vocabulari": 3.14753415606, "acoly": 7.369978720910001, "automat": 1.9150850473199998, "hesit": 2.9273274644200002, "measur": 0.880014199726, "ventur": 2.04599360761, "multipl": 1.01092401812, "how": 0.47156695693000006, "test": 0.977224437103, "repost": 6.83935046985, "million": 0.5469102500940001, "transcrib": 3.41681377215, "technologyrel": 7.369978720910001, "good": 0.418589404907, "especi": 0.511098609709, "expert": 1.68029517063, "noon": 3.27064661718, "due": 0.21341214386399998, "word": 0.585861082385, "interest": 0.47207177798199995, "invari": 3.1101197202099997, "deep": 1.2886734698, "will": 0.202786534915, "datadriven": 7.369978720910001, "pin": 2.7870541438700003, "second": 0.10713976337999999, "vendor": 3.30609336617, "portion": 1.1945273376899999, "next": 0.402163685499, "numbit": 7.369978720910001, "perform": 0.42618085058, "but": 0.0161923720719, "were": 0.024291143681099997, "obtain": 0.988162703503, "our": 0.8576392141820001, "carri": 0.512044136911, "longer": 0.7046772417749999, "final": 0.292733863948, "where": 0.0649921387457, "expect": 0.78850775216, "record": 0.353010356953, "trigram": 7.369978720910001, "applic": 1.23160392849, "hidden": 2.0557880052, "has": 0.0427239448548, "have": 0.0147850023412, "paramet": 2.8481901438599997, "twopass": 7.369978720910001, "listen": 1.94282848252, "pass": 0.48130432974, "subsequ": 0.561601885907, "surpris": 1.47392435861, "model": 0.7374500731110001, "quit": 1.05951513684, "forward": 1.29901007269, "toolkit": 5.24174701506, "much": 0.17749572930100002, "under": 0.07526180538319999, "optim": 2.4456277954099996, "field": 0.5760642583510001, "loss": 0.885954358842, "lstmlms": 7.369978720910001, "involv": 0.371469078658, "firstpass": 7.369978720910001, "appreci": 2.09339584651, "whole": 0.8306818244059999, "minim": 1.80968177926, "down": 0.306673741186, "not": 0.0155524130075, "stori": 0.705059626587, "wonder": 1.98319270637, "though": 0.308044191079, "then": 0.08303386523089999, "they": 0.0297269947676, "anoth": 0.127896361652, "sale": 1.23181979465, "reach": 0.40414323085000003, "effect": 0.333830227158, "scienc": 0.841436178891, "task": 1.35748680661, "worklist": 7.369978720910001, "abil": 0.996488297427, "that": 0.00397614837964, "togeth": 0.458032237308, "valu": 0.823193310148, "startup": 4.225826442240001, "develop": 0.178624694913, "accuraci": 2.5464765406, "those": 0.17854939087299998, "explor": 1.22257937218, "network": 0.9530830530519999, "three": 0.06411868822490001, "some": 0.0395735090645, "small": 0.307101805059, "collect": 0.49536666052, "put": 0.505652999854, "result": 0.136378908381, "com": 4.60365961168, "off": 0.41352852038800003, "classic": 0.8791034528499999, "filter": 2.82668393864, "quantize": 5.61212080336, "languag": 0.8306818244059999, "set": 0.171496011289, "two": 0.0136988443582, "and": 6.29901420636e-05, "from": 0.000567054168866, "complementari": 3.4802013244300003, "disfluenc": 7.369978720910001, "work": 0.109034567273, "learnabl": 7.369978720910001, "num": 0.00031499039539700004, "for": 0.00031499039539700004, "difficult": 0.912110767588, "num\u00d7num": 7.369978720910001, "output": 2.03822657827, "bottom": 1.8361940533599999, "seen": 0.47672812813, "with": 0.00119749171339, "acoust": 2.99800242209, "commerci": 0.8769815969470001, "are": 0.0294674735827, "compani": 0.439777253097, "techiqu": 7.369978720910001, "month": 0.410770160338, "veri": 0.230159793238, "object": 0.853933584803, "combin": 0.529218310751, "nbest": 7.369978720910001, "align": 2.09237439596, "net": 1.9406330919499999, "subgradi": 7.369978720910001, "general": 0.114952578063, "decod": 3.9457160663199997, "spontan": 2.7839913543400003, "group": 0.190594534797, "vmware": 7.2746685411000005, "convers": 1.2085604509999999, "realize": 3.4091655513099997, "give": 0.311392552224, "train": 0.660918312839, "scratch": 3.2509415461, "error": 1.7985854343, "continu": 0.13040487398700001, "fusion": 2.8768580387299996, "matter": 0.8951625270360001, "quantiz": 5.51368073054, "period": 0.294930924153, "into": 0.0149128632287, "paper": 0.979402539665, "prior": 0.778442172521, "ngram": 7.369978720910001, "rapid": 0.965411638564, "servic": 0.41410016674500005, "requir": 0.424253510675, "concis": 3.1200559268700006, "what": 0.225887296827, "largescal": 7.369978720910001, "comment\u2026": 7.369978720910001, "minibatch": 7.369978720910001, "apocryph": 4.252028814630001, "help": 0.336207721344, "crosoft": 7.369978720910001, "larg": 0.17037506060600002, "cognit": 3.0659136276999996, "borrow": 2.0822168683, "cover": 0.526975319156, "tempor": 3.08639215905, "success": 0.27765441259199997, "approxim": 0.7944845577770001, "base": 0.13652330228700002, "natur": 0.431306339292, "can": 0.162341096394, "hour": 0.815190981077, "univers": 0.222262105686, "permiss": 1.8373800586400002, "remind": 2.51794845699, "here": 0.8850381883700001, "dialogu": 2.21842273576, "week": 0.5907388641619999, "report": 0.31001750903700004, "microsoft": 3.21265935953, "maximum": 1.5687671009200002, "describ": 0.385447603125, "long": 0.235645793878, "human": 0.640035183243, "layer": 2.0969791623500003, "evalu": 1.9388802431299998, "parallel": 1.52151886822, "care": 0.9139943029109999, "about": 0.0628434774746, "evid": 0.8103634834160001, "notabl": 0.603441443842, "job": 1.1798682540899998, "neural": 4.0853151555, "context": 1.44920491442, "just": 0.289531434109, "telephon": 2.19044489035, "adapt": 1.2007864860200002, "accel": 7.369978720910001, "correct": 1.29831763181, "him": 0.49124039099699995, "frequenc": 2.1759113757299997, "selfcorrect": 7.369978720910001, "recognit": 1.4816549327200002, "switchboard": 5.7022719003499995, "pivot": 2.87909768132, "across": 0.549198455941, "rememb": 1.5867691126199999, "nor": 1.2083495472799999, "took": 0.337177952953, "love": 1.08958288195, "bigram": 7.369978720910001, "explain": 0.955700427358, "singl": 0.475916769059, "perplex": 4.68895719219, "famili": 0.39746619471100003, "target": 1.1690639496200002, "pull": 1.6460668749599998, "appli": 0.8316941898119999, "over": 0.0249367214957, "build": 0.491137452091, "trend": 1.6921980487900001, "systemat": 2.12085159855, "monitor": 1.80125261058, "initi": 0.30010459245, "team": 0.821902894886, "earli": 0.117499629108, "order": 0.22014038079300002, "rnns": 7.369978720910001, "character": 1.51806363875, "exact": 1.2437647732500001, "explos": 1.90529981715, "colyer": 7.369978720910001, "inform": 0.454453704662, "matrix": 3.1186304098799997, "latticefre": 7.369978720910001, "possibl": 0.348805474891, "best": 0.459227932947, "origin": 0.128612437587, "slight": 1.17966331506, "pariti": 3.8319221565300006, "visual": 1.6539383488600001, "follow": 0.045356911094199995, "now": 0.149092945021, "further": 0.308815895297, "hard": 1.00522796406, "strict": 1.55256998618, "per": 0.672821024072, "full": 0.511203624148, "represent": 1.7797382876499999, "basic": 1.00436774895, "unit": 0.143188061817, "power": 0.292396282715, "leverag": 3.5767392514699994, "reduc": 0.686617775143, "also": 0.0146571578, "coupl": 1.18089357972, "interpol": 3.9199911750800003, "batch": 3.5744895317400003, "plan": 0.428982108147, "adrian": 2.97306347374, "numdimension": 7.369978720910001, "the": 0.0, "becom": 0.11771217648900001, "peopl": 0.193265578473, "stand": 0.7345572374320001, "dozen": 1.78385428972, "via": 0.831983625414, "amount": 0.819898886199, "advantag": 1.20120515883, "callhom": 7.369978720910001, "system": 0.327430345585, "extens": 0.6889985794750001, "partner": 1.4287553902399999, "various": 0.28692650007, "attribut": 1.2283715153700001, "acronym": 3.5566716884199994, "backward": 2.68138692678, "histori": 0.187550624069, "which": 0.00517841384543, "rescor": 7.369978720910001, "activ": 0.381196603284, "other": 0.00987474791976, "refer": 0.262553246798, "without": 0.258874517941, "one": 0.0062553516455, "show": 0.236682766013, "known": 0.0824180805992, "see": 0.240921585492, "elimin": 1.30201620283, "imag": 0.99376210729, "still": 0.17112222142900002, "artifici": 2.11822899018, "numfold": 7.369978720910001, "audienc": 1.4992703749399998, "through": 0.0683586918849, "manag": 0.497643387158, "dot": 2.93797215393, "unbound": 4.572697386080001, "process": 0.527829199025, "both": 0.050842533389300004, "independ": 0.463424162503, "lstms": 7.369978720910001, "neither": 1.29808692469, "recent": 0.434413741288, "structur": 0.7217716751350001, "tabl": 1.34049610661, "gather": 1.3313920667299999, "this": 0.0037864490525, "time": 0.0112115188626, "finit": 3.33928418576, "engin": 0.904767558276, "direct": 0.200705689496, "bit": 2.12032652634, "similar": 0.318556092114, "survey": 1.3294859427299999, "same": 0.112059649604, "profession": 0.970385948273, "bandwidth": 4.27893626755, "most": 0.020747896295599998, "held": 0.36709509683499997, "need": 0.362740163442, "all": 0.011402632097799998, "ivector": 7.369978720910001, "headlin": 2.56268435083, "mutual": 1.88933979757, "gradient": 3.73502760882, "condit": 0.654837788206, "almost": 0.42907884333400004, "exist": 0.38165779408699996, "feasibl": 2.88021938643, "day": 0.16865870631700003, "weight": 1.58492352612, "architectur": 1.63469757919, "call": 0.0654627744488, "rate": 0.761033872166, "improv": 0.7147958039319999, "xiong": 6.0349776541799995, "aimlwhateveryourpreferredphrasei": 7.369978720910001, "prune": 4.87677326831, "london": 0.6819976757709999, "achiev": 0.6270980851169999, "exceed": 1.62313675679, "gain": 0.6142097989249999, "cryptic": 4.56661834, "made": 0.0680215260973, "emerg": 0.748173681534, "lead": 0.23620402986699998, "recurr": 3.5722448618800002, "state": 0.0466100027668, "transduc": 5.32875839205, "numgram": 7.369978720910001, "channel": 1.30247948752, "avail": 0.547454586289, "comput": 1.36806891594, "make": 0.07349765782289999, "onli": 0.025324268329099998, "each": 0.173741689304, "layread": 7.369978720910001, "fisher": 2.79526774241, "speech": 1.3409775702700002, "find": 0.547781330288, "read": 0.83939268088, "hypothes": 3.0274728444, "europ": 0.7017504724920001, "while": 0.04324998379380001, "block": 1.16400781588, "cnns": 7.369978720910001, "sequenc": 1.8035444374, "phase": 1.4589111108700001, "convolut": 4.61631800855, "bio": 3.7456377879300002, "come": 0.28390990653000003, "might": 0.7683410765340001, "line": 0.349430614452, "speaker": 1.8489178830700002, "springsourc": 7.369978720910001, "first": 0.0075872898121599995, "app": 3.57899404386, "busi": 0.720476170355, "use": 0.0292080197316, "score": 1.4559353207700003, "bottomlin": 7.369978720910001, "pervas": 3.69875420203, "corpora": 6.17605625244, "pipelin": 3.47002829672}, "freq": {"after": 1, "case": 2, "relat": 1, "stage": 1, "basi": 1, "detail": 1, "troubl": 1, "form": 1, "data": 7, "signal": 1, "would": 1, "number": 1, "oxford": 1, "been": 2, "dataset": 4, "function": 1, "progress": 1, "hear": 1, "resnet": 2, "well": 5, "done": 1, "assembl": 1, "technic": 1, "geometri": 1, "transcript": 3, "know": 1, "their": 2, "vocabulari": 1, "acoly": 1, "automat": 1, "hesit": 1, "measur": 1, "ventur": 2, "multipl": 1, "how": 3, "test": 2, "repost": 1, "million": 1, "transcrib": 5, "technologyrel": 1, "good": 1, "especi": 1, "expert": 1, "noon": 1, "due": 1, "word": 6, "interest": 1, "invari": 2, "deep": 1, "will": 1, "datadriven": 1, "pin": 1, "second": 2, "vendor": 1, "portion": 2, "next": 2, "numbit": 1, "perform": 12, "but": 2, "were": 2, "obtain": 1, "our": 3, "carri": 1, "longer": 1, "final": 1, "where": 2, "expect": 1, "record": 1, "trigram": 1, "applic": 1, "hidden": 2, "has": 2, "have": 2, "paramet": 1, "twopass": 1, "listen": 1, "pass": 2, "subsequ": 1, "surpris": 2, "model": 21, "quit": 1, "forward": 1, "toolkit": 2, "much": 1, "under": 1, "optim": 2, "field": 1, "loss": 1, "lstmlms": 1, "involv": 1, "firstpass": 1, "appreci": 1, "whole": 1, "minim": 1, "down": 1, "not": 1, "stori": 2, "wonder": 1, "though": 1, "then": 4, "they": 4, "anoth": 1, "sale": 1, "reach": 1, "effect": 1, "scienc": 1, "task": 3, "worklist": 1, "abil": 1, "that": 15, "togeth": 1, "valu": 1, "startup": 1, "develop": 2, "accuraci": 1, "those": 1, "explor": 1, "network": 4, "three": 1, "some": 1, "small": 1, "collect": 2, "put": 1, "result": 1, "com": 1, "off": 1, "classic": 1, "filter": 1, "quantize": 2, "languag": 7, "set": 2, "two": 2, "and": 30, "from": 10, "complementari": 1, "disfluenc": 1, "work": 5, "learnabl": 1, "num": 15, "for": 19, "difficult": 3, "num\u00d7num": 1, "output": 1, "bottom": 1, "seen": 1, "with": 12, "acoust": 9, "commerci": 1, "are": 7, "compani": 1, "techiqu": 1, "month": 1, "veri": 2, "object": 1, "combin": 5, "nbest": 1, "align": 1, "net": 1, "subgradi": 2, "general": 1, "decod": 3, "spontan": 1, "group": 1, "vmware": 1, "convers": 3, "realize": 1, "give": 1, "train": 6, "scratch": 1, "error": 5, "continu": 3, "fusion": 1, "matter": 1, "quantiz": 2, "period": 1, "into": 1, "paper": 1, "prior": 1, "ngram": 4, "rapid": 1, "servic": 1, "requir": 1, "concis": 1, "what": 2, "largescal": 1, "comment\u2026": 1, "minibatch": 2, "apocryph": 1, "help": 2, "crosoft": 1, "larg": 3, "cognit": 1, "borrow": 1, "cover": 1, "tempor": 1, "success": 1, "approxim": 1, "base": 1, "natur": 2, "can": 8, "hour": 1, "univers": 1, "permiss": 1, "remind": 1, "here": 4, "dialogu": 1, "week": 3, "report": 1, "microsoft": 8, "maximum": 1, "describ": 1, "long": 1, "human": 6, "layer": 5, "evalu": 1, "parallel": 2, "care": 1, "about": 1, "evid": 1, "notabl": 1, "job": 1, "neural": 2, "context": 1, "just": 1, "telephon": 2, "adapt": 1, "accel": 3, "correct": 1, "him": 1, "frequenc": 1, "selfcorrect": 1, "recognit": 7, "switchboard": 3, "pivot": 1, "across": 1, "rememb": 1, "nor": 1, "took": 1, "love": 1, "bigram": 1, "explain": 1, "singl": 1, "perplex": 1, "famili": 1, "target": 1, "pull": 1, "appli": 2, "over": 2, "build": 2, "trend": 1, "systemat": 1, "monitor": 1, "initi": 2, "team": 2, "earli": 2, "order": 1, "rnns": 1, "character": 1, "exact": 1, "explos": 1, "colyer": 2, "inform": 3, "matrix": 1, "latticefre": 1, "possibl": 1, "best": 4, "origin": 1, "slight": 1, "pariti": 2, "visual": 2, "follow": 1, "now": 1, "further": 1, "hard": 1, "strict": 1, "per": 1, "full": 1, "represent": 1, "basic": 1, "unit": 1, "power": 1, "leverag": 1, "reduc": 2, "also": 2, "coupl": 1, "interpol": 1, "batch": 1, "plan": 1, "adrian": 2, "numdimension": 1, "the": 76, "becom": 1, "peopl": 2, "stand": 1, "dozen": 1, "via": 1, "amount": 1, "advantag": 2, "callhom": 3, "system": 10, "extens": 1, "partner": 3, "various": 1, "attribut": 1, "acronym": 1, "backward": 1, "histori": 1, "which": 1, "rescor": 1, "activ": 1, "other": 1, "refer": 1, "without": 1, "one": 6, "show": 2, "known": 2, "see": 1, "elimin": 1, "imag": 1, "still": 1, "artifici": 1, "numfold": 1, "audienc": 1, "through": 2, "manag": 1, "dot": 1, "unbound": 1, "process": 1, "both": 3, "independ": 1, "lstms": 2, "neither": 1, "recent": 1, "structur": 1, "tabl": 2, "gather": 1, "this": 8, "time": 3, "finit": 1, "engin": 2, "direct": 2, "bit": 1, "similar": 1, "survey": 1, "same": 1, "profession": 3, "bandwidth": 1, "most": 1, "held": 1, "need": 1, "all": 2, "ivector": 4, "headlin": 1, "mutual": 1, "gradient": 1, "condit": 2, "almost": 1, "exist": 2, "feasibl": 1, "day": 1, "weight": 3, "architectur": 2, "call": 1, "rate": 2, "improv": 2, "xiong": 1, "aimlwhateveryourpreferredphrasei": 1, "prune": 1, "london": 1, "achiev": 2, "exceed": 1, "gain": 1, "cryptic": 1, "made": 1, "emerg": 1, "lead": 1, "recurr": 1, "state": 1, "transduc": 1, "numgram": 1, "channel": 1, "avail": 1, "comput": 2, "make": 2, "onli": 1, "each": 5, "layread": 1, "fisher": 1, "speech": 8, "find": 1, "read": 1, "hypothes": 1, "europ": 1, "while": 1, "block": 1, "cnns": 2, "sequenc": 2, "phase": 1, "convolut": 3, "bio": 1, "come": 1, "might": 1, "line": 1, "speaker": 3, "springsourc": 1, "first": 2, "app": 1, "busi": 1, "use": 14, "score": 1, "bottomlin": 1, "pervas": 1, "corpora": 1, "pipelin": 2}, "idf": {"after": 1.02070207021, "case": 1.48498737256, "relat": 1.23750876919, "stage": 2.0831911822599998, "basi": 2.42122922068, "detail": 2.26186066391, "troubl": 4.99088337001, "form": 1.12755681818, "data": 3.37643555934, "signal": 5.12459651388, "would": 1.0828729281799998, "number": 1.10142916609, "oxford": 3.5139442231099998, "been": 1.0239277652399998, "dataset": 193.609756098, "function": 2.495441685, "progress": 2.44697903822, "hear": 4.17899447223, "resnet": 1587.6, "well": 1.0655748708, "done": 2.3302509907499998, "assembl": 3.0011342155, "technic": 3.1400316455699997, "geometri": 25.4423076923, "transcript": 13.318791946300001, "know": 2.59327017315, "their": 1.01547908405, "vocabulari": 23.2785923754, "acoly": 1587.6, "automat": 6.787516032490001, "hesit": 18.6776470588, "measur": 2.41093394077, "ventur": 7.73684210526, "multipl": 2.74813917258, "how": 1.60250328051, "test": 2.65707112971, "repost": 933.882352941, "million": 1.7279059643, "transcrib": 30.472168906, "technologyrel": 1587.6, "good": 1.51981619759, "especi": 1.66712170534, "expert": 5.36713995943, "noon": 26.328358209, "due": 1.23789473684, "word": 1.7965372864099998, "interest": 1.60331246213, "invari": 22.4237288136, "deep": 3.6279707495399998, "will": 1.22481098596, "datadriven": 1587.6, "pin": 16.233128834400002, "second": 1.1130898128, "vendor": 27.2783505155, "portion": 3.3019966722099996, "next": 1.4950560316400001, "numbit": 1587.6, "perform": 1.5313977042500002, "but": 1.01632417899, "were": 1.02458857696, "obtain": 2.68629441624, "our": 2.35758835759, "carri": 1.66869875972, "longer": 2.02319357716, "final": 1.34008609775, "where": 1.06715063521, "expect": 2.20011086475, "record": 1.42334588488, "trigram": 1587.6, "applic": 3.42672134686, "hidden": 7.81299212598, "has": 1.0436497502, "have": 1.0148948411399998, "paramet": 17.256521739100002, "twopass": 1587.6, "listen": 6.97846153846, "pass": 1.61818367139, "subsequ": 1.7534791252500002, "surpris": 4.36633663366, "model": 2.0905978404, "quit": 2.8849718335500003, "forward": 3.66566612792, "toolkit": 189.0, "much": 1.1942229577299999, "under": 1.0781663837, "optim": 11.5377906977, "field": 1.7790228597, "loss": 2.42529789184, "lstmlms": 1587.6, "involv": 1.4498630137000001, "firstpass": 1587.6, "appreci": 8.11241696474, "whole": 2.29488291414, "minim": 6.10850327049, "down": 1.35889754344, "not": 1.01567398119, "stori": 2.02396736359, "wonder": 7.265903890160001, "though": 1.36076112111, "then": 1.08657860516, "they": 1.03017325287, "anoth": 1.13643521832, "sale": 3.4274611399, "reach": 1.49801849406, "effect": 1.3963060686000002, "scienc": 2.31969608416, "task": 3.88641370869, "worklist": 1587.6, "abil": 2.70875277256, "that": 1.00398406375, "togeth": 1.58095996813, "valu": 2.2777618364400003, "startup": 68.4310344828, "develop": 1.1955719557200002, "accuraci": 12.7620578778, "those": 1.19548192771, "explor": 3.39593582888, "network": 2.59369384088, "three": 1.06621893889, "some": 1.04036697248, "small": 1.3594793629, "collect": 1.64109985528, "put": 1.65806788512, "result": 1.14611608432, "com": 99.8490566038, "off": 1.5121440137200002, "classic": 2.4087391898, "filter": 16.8893617021, "quantize": 273.724137931, "languag": 2.29488291414, "set": 1.18707940781, "two": 1.01379310345, "and": 1.00006299213, "from": 1.00056721497, "complementari": 32.466257668699996, "disfluenc": 1587.6, "work": 1.11520089913, "learnabl": 1587.6, "num": 1.00031504001, "for": 1.00031504001, "difficult": 2.48957189901, "num\u00d7num": 1587.6, "output": 7.676982591880001, "bottom": 6.27261951798, "seen": 1.61079545455, "with": 1.0011982089899998, "acoust": 20.0454545455, "commerci": 2.4036336109, "are": 1.02990593578, "compani": 1.5523613963, "techiqu": 1587.6, "month": 1.5079787234, "veri": 1.25880114177, "object": 2.3488681757700003, "combin": 1.69760479042, "nbest": 1587.6, "align": 8.10413476263, "net": 6.96315789474, "subgradi": 1587.6, "general": 1.1218202374200001, "decod": 51.713355048900006, "spontan": 16.1834862385, "group": 1.20996875238, "vmware": 1443.27272727, "convers": 3.3486606201200004, "realize": 30.24, "give": 1.3653250774, "train": 1.9365698950999999, "scratch": 25.8146341463, "error": 6.04109589041, "continu": 1.13928955867, "fusion": 17.7583892617, "matter": 2.44773358002, "quantiz": 248.0625, "period": 1.3430335843, "into": 1.01502461479, "paper": 2.6628648104700003, "prior": 2.17807655371, "ngram": 1587.6, "rapid": 2.62586834271, "servic": 1.51300867245, "requir": 1.52844902282, "concis": 22.647646219699997, "what": 1.25343439128, "largescal": 1587.6, "comment\u2026": 1587.6, "minibatch": 1587.6, "apocryph": 70.2477876106, "help": 1.39962972759, "crosoft": 1587.6, "larg": 1.18574949585, "cognit": 21.454054054100002, "borrow": 8.02223345124, "cover": 1.69380134429, "tempor": 21.897931034499997, "success": 1.32002993265, "approxim": 2.2132998745299997, "base": 1.14628158845, "natur": 1.5392670157100001, "can": 1.17626139142, "hour": 2.25960717336, "univers": 1.24889867841, "permiss": 6.280063291139999, "remind": 12.403125, "here": 2.42307692308, "dialogu": 9.19281991893, "week": 1.80532181033, "report": 1.3634489866, "microsoft": 24.8450704225, "maximum": 4.80072573329, "describ": 1.47027227264, "long": 1.2657259028899999, "human": 1.8965476048299998, "layer": 8.14153846154, "evalu": 6.9509632224199995, "parallel": 4.57917507932, "care": 2.49426551453, "about": 1.06486015159, "evid": 2.24872521246, "notabl": 1.82840032247, "job": 3.2539454806299997, "neural": 59.4606741573, "context": 4.25972632144, "just": 1.33580143037, "telephon": 8.93918918919, "adapt": 3.32272917539, "accel": 1587.6, "correct": 3.6631287494199998, "him": 1.63434218653, "frequenc": 8.8102108768, "selfcorrect": 1587.6, "recognit": 4.40022172949, "switchboard": 299.547169811, "pivot": 17.798206278, "across": 1.7318642958400001, "rememb": 4.88793103448, "nor": 3.3479544496, "took": 1.4009883515700001, "love": 2.97303370787, "bigram": 1587.6, "explain": 2.60049140049, "singl": 1.60948905109, "perplex": 108.739726027, "famili": 1.48804948917, "target": 3.2189781021900004, "pull": 5.18654034629, "appli": 2.2972073506, "over": 1.02525024217, "build": 1.6341739578, "trend": 5.43140608963, "systemat": 8.338235294119999, "monitor": 6.05723006486, "initi": 1.35, "team": 2.2748244734200003, "earli": 1.12468121281, "order": 1.24625166811, "rnns": 1587.6, "character": 4.563380281690001, "exact": 3.46864758575, "explos": 6.72142252329, "colyer": 1587.6, "inform": 1.5753125620200001, "matrix": 22.6153846154, "latticefre": 1587.6, "possibl": 1.4173734488, "best": 1.5828514456600002, "origin": 1.13724928367, "slight": 3.25327868852, "pariti": 46.1511627907, "visual": 5.22752716497, "follow": 1.04640126549, "now": 1.160780873, "further": 1.3618116315, "hard": 2.73253012048, "strict": 4.7235941684, "per": 1.9597580545599997, "full": 1.66729678639, "represent": 5.928304705, "basic": 2.7301805675, "unit": 1.15394679459, "power": 1.3396337861799998, "leverag": 35.7567567568, "reduc": 1.98698372966, "also": 1.01476510067, "coupl": 3.2572835453400004, "interpol": 50.4, "batch": 35.6764044944, "plan": 1.5356935577500002, "adrian": 19.5517241379, "numdimension": 1587.6, "the": 1.0, "becom": 1.12492028626, "peopl": 1.21320495186, "stand": 2.0845588235299997, "dozen": 5.95275590551, "via": 2.2978723404299997, "amount": 2.27027027027, "advantag": 3.32412060302, "callhom": 1587.6, "system": 1.38739840951, "extens": 1.99171998495, "partner": 4.173501577290001, "various": 1.3323262839899999, "attribut": 3.4156626506, "acronym": 35.0463576159, "backward": 14.605335786600001, "histori": 1.20629131525, "which": 1.005191845, "rescor": 1587.6, "activ": 1.46403541129, "other": 1.00992366412, "refer": 1.30024570025, "without": 1.29547123623, "one": 1.00627495722, "show": 1.26703910615, "known": 1.0859097127200001, "see": 1.27242125511, "elimin": 3.67670217693, "imag": 2.70137825421, "still": 1.1866357724799999, "artifici": 8.31639601886, "numfold": 1587.6, "audienc": 4.4784203103, "through": 1.07074930869, "manag": 1.6448404475799998, "dot": 18.8775267539, "unbound": 96.80487804879999, "process": 1.69524826482, "both": 1.05215720061, "independ": 1.58950740889, "lstms": 1587.6, "neither": 3.6622837370199997, "recent": 1.54405757635, "structur": 2.0580762250499998, "tabl": 3.82093862816, "gather": 3.78631051753, "this": 1.00379362671, "time": 1.01127460348, "finit": 28.1989342806, "engin": 2.47135740971, "direct": 1.22226499346, "bit": 8.33385826772, "similar": 1.37514075357, "survey": 3.7791002142300005, "same": 1.11857958148, "profession": 2.6389627659599997, "bandwidth": 72.16363636359999, "most": 1.02096463023, "held": 1.44353518822, "need": 1.4372623574099999, "all": 1.01146788991, "ivector": 1587.6, "headlin": 12.9705882353, "mutual": 6.615, "gradient": 41.889182058, "condit": 1.92483026188, "almost": 1.53584212054, "exist": 1.4647107666799999, "feasibl": 17.8181818182, "day": 1.18371607516, "weight": 4.878918254459999, "architectur": 5.12790697674, "call": 1.0676529926, "rate": 2.14048806795, "improv": 2.04376930999, "xiong": 417.78947368400003, "aimlwhateveryourpreferredphrasei": 1587.6, "prune": 131.20661157, "london": 1.97782484116, "achiev": 1.87216981132, "exceed": 5.0689655172400006, "gain": 1.84819557625, "cryptic": 96.2181818182, "made": 1.07038834951, "emerg": 2.1131372288, "lead": 1.2664326739, "recurr": 35.5964125561, "state": 1.0477133240899998, "transduc": 206.181818182, "numgram": 1587.6, "channel": 3.6784059314199995, "avail": 1.7288467821, "comput": 3.9277585353800006, "make": 1.0762660158600001, "onli": 1.0256476516600002, "each": 1.18974820144, "layread": 1587.6, "fisher": 16.3670103093, "speech": 3.8227787141800005, "find": 1.7294117647099998, "read": 2.3149606299200003, "hypothes": 20.644993497999998, "europ": 2.0172808132099997, "while": 1.0441988950299999, "block": 3.20274359492, "cnns": 1587.6, "sequenc": 6.07112810707, "phase": 4.3012733676499995, "convolut": 101.121019108, "bio": 42.336000000000006, "come": 1.32831325301, "might": 2.1561863370900003, "line": 1.4182597820299998, "speaker": 6.35294117647, "springsourc": 1587.6, "first": 1.00761614623, "app": 35.837471783299996, "busi": 2.05541170378, "use": 1.0296387573799999, "score": 4.2884927066500005, "bottomlin": 1587.6, "pervas": 40.3969465649, "corpora": 481.09090909099996, "pipelin": 32.1376518219}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Achieving Human Parity in Conversational Speech Recognition</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2016/12/achieving-human-parity-conversational-speech-recognition.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Achieving Human Parity in Conversational Speech Recognition Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2016/12/global-ai-conference-santa-clara-january.html\" rel=\"prev\" title=\"Global AI Conference, Santa Clara, Jan 19-21 2017\"/>\n<link href=\"https://www.kdnuggets.com/2016/12/data-analytics-models-quantitative-finance-risk-management.html\" rel=\"next\" title=\"Data Analytics Models in Quantitative Finance and Risk Management\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2016/12/achieving-human-parity-conversational-speech-recognition.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=59116\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2016/12/achieving-human-parity-conversational-speech-recognition.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-59116 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 13-Dec, 2016  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2016/index.html\">2016</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/12/index.html\">Dec</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/12/news-features.html\">News, Features</a> \u00bb Achieving Human Parity in Conversational Speech Recognition (\u00a0<a href=\"/2016/n44.html\">16:n44</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Achieving Human Parity in Conversational Speech Recognition</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2016/12/global-ai-conference-santa-clara-january.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2016/12/data-analytics-models-quantitative-finance-risk-management.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <span class=\"http-likes\" style=\"float: left; font-size:14px\">http likes 30</span> <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/acoustics\" rel=\"tag\">Acoustics</a>, <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/microsoft\" rel=\"tag\">Microsoft</a>, <a href=\"https://www.kdnuggets.com/tag/speech-recognition\" rel=\"tag\">Speech Recognition</a></div>\n<br/>\n<p class=\"excerpt\">\n     This is an overview of the paper which outlines, for the first time, a system has been developed that exceeds human performance in one of the most difficult of all human speech recognition tasks: natural conversations held over the telephone.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/adrian-colyer\" rel=\"author\" title=\"Posts by Adrian Colyer\">Adrian Colyer</a>, Venture Partner, Accel.</b></div>\n<p><a href=\"https://arxiv.org/abs/1610.05256\" target=\"_blank\">Achieving Human Parity in Conversational Speech Recognition</a>,\u00a0Xiong et al.\u00a0<em>Microsoft Technical Report, 2016</em></p>\n<p>The headline story here is that for the first time a system has been developed that exceeds human performance in one of the most difficult of all human speech recognition tasks: natural conversations held over the telephone. This is known as conversational telephone speech, or\u00a0<em>CTS</em>.</p>\n<p><img alt=\"Microsoft\" src=\"/wp-content/uploads/microsoft-wall-logo.jpg\" width=\"99%\"/></p>\n<blockquote><p>[CTS] is especially difficult due to the spontaneous (neither read nor planned) nature of the speech, its informality, and the self-corrections, hesitations, and other disfluencies that are pervasive.</p></blockquote>\n<p>The reference datasets for this task are the Switchboard and Fisher data collections from the 1990s and early 2000s. The apocryphal story here is that human performance on the task is about 4% error rate. But no-one can quite pin down where that 4% number comes from. So the Microsoft team took advantage of an existing professional transcription service used by Microsoft:</p>\n<blockquote><p>To measure human performance, we leveraged an existing pipeline in which Microsoft data is transcribed on a weekly basis. This pipeline uses a large commercial vendor to perform a two-pass transcription. In the first pass, a transcriber works from scratch to transcribe the data. In the second pass, a second listener monitors the data to do error correction. Dozens of hours of test data are processed in each batch. One week, we added the NIST 2000 CTS evaluation data to the work-list, without further comment\u2026</p></blockquote>\n<p>For the switchboard portion of the dataset, the professional human transcribers achieved a 5.9% error rate, and for the \u2018call-home\u2019 portion of the test set 11.3%. \u201cThe same informality, multiple speakers per channel, and recording conditions that make CallHome hard for computers make it difficult for people as well.\u201d</p>\n<blockquote><p>Notably, the performance of our artificial system aligns almost exactly with the performance of people on both sets.</p></blockquote>\n<p>And so one can\u2019t help but wonder how much longer those professional transcribers will continue to be needed! Did they know they were gathering the evidence that one day might help lead to the elimination of their jobs???</p>\n<p>Here\u2019s a table full of cryptic acronyms that show the performance of the system that Microsoft put together, using various acoustic models. The bottom-line (strictly, bottom two lines) is what matters here: parity on the switchboard dataset, and a slight advantage for the ASR (Automatic Speech Recognition) system of the CallHome dataset.</p>\n<p><img alt=\"Table\" src=\"https://adriancolyer.files.wordpress.com/2016/11/human-level-asr-fig-7.png?w=600\" width=\"99%\"/></p>\n<p>How did the Microsoft team manage to pull this off?</p>\n<blockquote><p>Our system\u2019s performance can be attributed to the systematic use of LSTMs for both acoustic and language modeling, as well as CNNs in the acoustic model, and extensive combination of complementary models.</p></blockquote>\n<p>Training was made feasible (reducing times from month to 1-3 weeks) by using Microsoft\u2019s\u00a0<a href=\"https://www.microsoft.com/en-us/research/product/cognitive-toolkit/\" target=\"_blank\">CNTK</a>\u00a0Cognitive Toolkit to parallelize SGD training, coupled with the use of the\u00a0<em>1-bit SGD parallelization techique</em>\u00a0from prior work:</p>\n<blockquote><p>In [65], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next. Each time a sub-gradient is quantized, the quantization error is computed and remembered, and then added to the next minibatch\u2019s sub-gradient. This reduces the required bandwidth 32-fold with minimal loss in accuracy.</p></blockquote>\n<h3>How it Works Under the Covers</h3>\n<p>\u00a0<br>\nThe model details are concisely explained, targeting an audience of speech recognition experts (i.e.\u00a0not me!). It is still possible for a lay-reader to gain some appreciation of what\u2019s involved though. It\u2019s also another reminder that we\u2019re rapidly assembling a powerful collection of building blocks that through good systems engineering can be combined into very effective systems. Expect to see an explosion of\u00a0<em>applied</em>\u00a0AI/ML/whatever-your-preferred-phrase-is applications as this trend continues.</br></p>\n<blockquote><p>Our progress is a result of the careful engineering and optimization of convolutional and recurrent neural networks. While the basic structures have been well known for a long period, it is only recently that they have emerged as the best models for speech recognition. Surprisingly, this is the case for both acoustic modeling and language modeling.</p></blockquote>\n<p>The CNN and RNN based acoustic models can model a large amount of acoustic context with temporal invariance, and in the case of CNNs, with frequency invariance as well. In language modeling, RNNs improve on classical N-gram models through the use of an unbounded word history and the generalization ability of\u00a0<a href=\"https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\" target=\"_blank\">continuous word representations</a>.</p>\n<p>The paper describes a whole family of systems that were explored in order to find the best performing combination. The best acoustic model was formed by combining independently trained ResNet and VGG models using a score fusion weight.</p>\n<p>\u2018VGG\u2019 stands for the University of Oxford Visual Geometry Group and the architecture they developed in \u201c<a href=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/\" target=\"_blank\">Very deep convolutional networks for large-scale visual recognition</a>\u201d. Their networks use 16-19 layers with small 3\u00d73 filters in all convolutional layers. The\u00a0<a href=\"https://github.com/KaimingHe/deep-residual-networks\" target=\"_blank\">ResNet</a>\u00a0architecture is also borrowed from the field of image recognition.</p>\n<p><em>Speaker adaptive modeling</em>\u00a0is then applied by conditioning the network on an\u00a0<a href=\"http://habla.dc.uba.ar/gravano/ith-2014/presentaciones/Dehak_et_al_2010.pdf\" target=\"_blank\"><em>i-vector</em></a>\u00a0characterization of each speaker using 100-dimensional i-vectors. The i-vectors are added to the activation of each CNN layer via a learnable weight matrix.</p>\n<p>After initial training, model parameters are optimized using a maximum mutual information (MMI) objective function:</p>\n<p><center><img alt=\"Equation\" data-pin-nopin=\"true\" src=\"https://adriancolyer.files.wordpress.com/2016/11/human-level-asr-mmi.png?w=600\" width=\"70%\"/></center></p>\n<p>where\u00a0<em>w</em>\u00a0is a word sequence, and\u00a0<em>a</em>\u00a0is an acoustic realization of a word sequence. The performance improvements obtained from this lattice-free MMI (LFMMI) training phase, as well as i-vectors, can be seen in the following table:</p>\n<p><img alt=\"Table\" src=\"https://adriancolyer.files.wordpress.com/2016/11/human-level-asr-table-3.png?w=600\" width=\"99%\"/></p>\n<p>An initial decoding of acoustic model outputs is done with a WFST (<a href=\"https://www-i6.informatik.rwth-aachen.de/publications/download/740/HoffmeisterB.HeigoldG.RybachD.Schl%7Bu%7DterR.NeyH.--WFSTEnabledSolutionstoASRProblemsBeyondHMMDecoding--2012.pdf\" target=\"_blank\">Weighted Finite State Transducer</a>) decoder.</p>\n<blockquote><p>We use an N-gram language model trained and pruned with the SRILM toolkit. The first-pass LM has approximately 15.9 million bigrams, trigrams, and 4-grams, and a vocabulary of 30500 words, and give a perplexity of 54 on RT-03 speech transcripts.</p></blockquote>\n<p>The N-best performing hypotheses from the WFST decoding are then rescored using a combination of a large N-gram language model and neural net language models. The best performing language model used LSTMs with three hidden layers, and 1000 hidden units in each layer. \u201cFor the final system, we interpolated two LSTM-LMs with an N-gram LM for the forward direction LM, and similarly for the backward direction LM.\u201d</p>\n<p><b>Bio: <a href=\"https://twitter.com/adriancolyer\" target=\"_blank\">Adrian Colyer</a></b> was CTO of SpringSource, then CTO for Apps at VMware and subsequently Pivotal. He is now a Venture Partner at Accel Partners in London, working with early stage and startup companies across Europe. <em>If you\u2019re working on an interesting technology-related business he would love to hear from you: you can reach him at acolyer at accel dot com</em>.</p>\n<p><a href=\"https://blog.acolyer.org/2016/11/22/achieving-human-parity-in-conversational-speech-recognition/\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2016/09/data-science-sales-calls-words-trouble-success.html\">Data Science of Sales Calls: The Surprising Words That Signal Trouble or Success</a>\n<li><a href=\"/2016/07/survey-available-corpora-building-data-driven-dialog-systems.html\">A Survey of Available Corpora for Building Data-driven Dialogue Systems</a>\n<li><a href=\"/2016/04/microsoft-becoming-m-ai-crosoft.html\">Microsoft is Becoming M(ai)crosoft</a>\n</li></li></li></ul>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2016/12/global-ai-conference-santa-clara-january.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2016/12/data-analytics-models-quantitative-finance-risk-management.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2016/index.html\">2016</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/12/index.html\">Dec</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/12/news-features.html\">News, Features</a> \u00bb Achieving Human Parity in Conversational Speech Recognition (\u00a0<a href=\"/2016/n44.html\">16:n44</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556374258\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></body></html>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n\n\n<!-- Dynamic page generated in 0.659 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 10:10:58 -->\n<!-- Compression = gzip -->", "content_tokenized": ["adrian", "colyer", "ventur", "partner", "accel", "achiev", "human", "pariti", "convers", "speech", "recognit", "xiong", "microsoft", "technic", "report", "num", "the", "headlin", "stori", "here", "that", "for", "the", "first", "time", "system", "has", "been", "develop", "that", "exceed", "human", "perform", "one", "the", "most", "difficult", "all", "human", "speech", "recognit", "task", "natur", "convers", "held", "over", "the", "telephon", "this", "known", "convers", "telephon", "speech", "especi", "difficult", "due", "the", "spontan", "neither", "read", "nor", "plan", "natur", "the", "speech", "inform", "and", "the", "selfcorrect", "hesit", "and", "other", "disfluenc", "that", "are", "pervas", "the", "refer", "dataset", "for", "this", "task", "are", "the", "switchboard", "and", "fisher", "data", "collect", "from", "the", "num", "and", "earli", "num", "the", "apocryph", "stori", "here", "that", "human", "perform", "the", "task", "about", "num", "error", "rate", "but", "noon", "can", "quit", "pin", "down", "where", "that", "num", "number", "come", "from", "the", "microsoft", "team", "took", "advantag", "exist", "profession", "transcript", "servic", "use", "microsoft", "measur", "human", "perform", "leverag", "exist", "pipelin", "which", "microsoft", "data", "transcrib", "week", "basi", "this", "pipelin", "use", "larg", "commerci", "vendor", "perform", "twopass", "transcript", "the", "first", "pass", "transcrib", "work", "from", "scratch", "transcrib", "the", "data", "the", "second", "pass", "second", "listen", "monitor", "the", "data", "error", "correct", "dozen", "hour", "test", "data", "are", "process", "each", "batch", "one", "week", "the", "num", "evalu", "data", "the", "worklist", "without", "further", "comment\u2026", "for", "the", "switchboard", "portion", "the", "dataset", "the", "profession", "human", "transcrib", "achiev", "num", "error", "rate", "and", "for", "the", "callhom", "portion", "the", "test", "set", "num", "the", "same", "inform", "multipl", "speaker", "per", "channel", "and", "record", "condit", "that", "make", "callhom", "hard", "for", "comput", "make", "difficult", "for", "peopl", "well", "notabl", "the", "perform", "our", "artifici", "system", "align", "almost", "exact", "with", "the", "perform", "peopl", "both", "set", "and", "one", "can", "help", "but", "wonder", "how", "much", "longer", "those", "profession", "transcrib", "will", "continu", "need", "they", "know", "they", "were", "gather", "the", "evid", "that", "one", "day", "might", "help", "lead", "the", "elimin", "their", "job", "here", "tabl", "full", "cryptic", "acronym", "that", "show", "the", "perform", "the", "system", "that", "microsoft", "put", "togeth", "use", "various", "acoust", "model", "the", "bottomlin", "strict", "bottom", "two", "line", "what", "matter", "here", "pariti", "the", "switchboard", "dataset", "and", "slight", "advantag", "for", "the", "automat", "speech", "recognit", "system", "the", "callhom", "dataset", "how", "the", "microsoft", "team", "manag", "pull", "this", "off", "our", "system", "perform", "can", "attribut", "the", "systemat", "use", "lstms", "for", "both", "acoust", "and", "languag", "model", "well", "cnns", "the", "acoust", "model", "and", "extens", "combin", "complementari", "model", "train", "made", "feasibl", "reduc", "time", "from", "month", "num", "week", "use", "microsoft", "cognit", "toolkit", "parallel", "train", "coupl", "with", "the", "use", "the", "numbit", "parallel", "techiqu", "from", "prior", "work", "num", "show", "that", "gradient", "valu", "can", "quantiz", "just", "singl", "bit", "one", "carri", "over", "the", "quantize", "error", "from", "one", "minibatch", "the", "next", "each", "time", "subgradi", "quantiz", "the", "quantize", "error", "comput", "and", "rememb", "and", "then", "the", "next", "minibatch", "subgradi", "this", "reduc", "the", "requir", "bandwidth", "numfold", "with", "minim", "loss", "accuraci", "how", "work", "under", "the", "cover", "the", "model", "detail", "are", "concis", "explain", "target", "audienc", "speech", "recognit", "expert", "not", "still", "possibl", "for", "layread", "gain", "some", "appreci", "what", "involv", "though", "also", "anoth", "remind", "that", "rapid", "assembl", "power", "collect", "build", "block", "that", "through", "good", "system", "engin", "can", "combin", "into", "veri", "effect", "system", "expect", "see", "explos", "appli", "aimlwhateveryourpreferredphrasei", "applic", "this", "trend", "continu", "our", "progress", "result", "the", "care", "engin", "and", "optim", "convolut", "and", "recurr", "neural", "network", "while", "the", "basic", "structur", "have", "been", "well", "known", "for", "long", "period", "onli", "recent", "that", "they", "have", "emerg", "the", "best", "model", "for", "speech", "recognit", "surpris", "this", "the", "case", "for", "both", "acoust", "model", "and", "languag", "model", "the", "and", "base", "acoust", "model", "can", "model", "larg", "amount", "acoust", "context", "with", "tempor", "invari", "and", "the", "case", "cnns", "with", "frequenc", "invari", "well", "languag", "model", "rnns", "improv", "classic", "ngram", "model", "through", "the", "use", "unbound", "word", "histori", "and", "the", "general", "abil", "continu", "word", "represent", "the", "paper", "describ", "whole", "famili", "system", "that", "were", "explor", "order", "find", "the", "best", "perform", "combin", "the", "best", "acoust", "model", "form", "combin", "independ", "train", "resnet", "and", "model", "use", "score", "fusion", "weight", "stand", "for", "the", "univers", "oxford", "visual", "geometri", "group", "and", "the", "architectur", "they", "develop", "veri", "deep", "convolut", "network", "for", "largescal", "visual", "recognit", "their", "network", "use", "num", "layer", "with", "small", "num\u00d7num", "filter", "all", "convolut", "layer", "the", "resnet", "architectur", "also", "borrow", "from", "the", "field", "imag", "recognit", "speaker", "adapt", "model", "then", "appli", "condit", "the", "network", "ivector", "character", "each", "speaker", "use", "numdimension", "ivector", "the", "ivector", "are", "the", "activ", "each", "layer", "via", "learnabl", "weight", "matrix", "after", "initi", "train", "model", "paramet", "are", "optim", "use", "maximum", "mutual", "inform", "object", "function", "where", "word", "sequenc", "and", "acoust", "realize", "word", "sequenc", "the", "perform", "improv", "obtain", "from", "this", "latticefre", "train", "phase", "well", "ivector", "can", "seen", "the", "follow", "tabl", "initi", "decod", "acoust", "model", "output", "done", "with", "weight", "finit", "state", "transduc", "decod", "use", "ngram", "languag", "model", "train", "and", "prune", "with", "the", "toolkit", "the", "firstpass", "has", "approxim", "num", "million", "bigram", "trigram", "and", "numgram", "and", "vocabulari", "num", "word", "and", "give", "perplex", "num", "speech", "transcript", "the", "nbest", "perform", "hypothes", "from", "the", "decod", "are", "then", "rescor", "use", "combin", "larg", "ngram", "languag", "model", "and", "neural", "net", "languag", "model", "the", "best", "perform", "languag", "model", "use", "lstms", "with", "three", "hidden", "layer", "and", "num", "hidden", "unit", "each", "layer", "for", "the", "final", "system", "interpol", "two", "lstmlms", "with", "ngram", "for", "the", "forward", "direct", "and", "similar", "for", "the", "backward", "direct", "bio", "adrian", "colyer", "springsourc", "then", "for", "app", "vmware", "and", "subsequ", "pivot", "now", "ventur", "partner", "accel", "partner", "london", "work", "with", "earli", "stage", "and", "startup", "compani", "across", "europ", "work", "interest", "technologyrel", "busi", "would", "love", "hear", "from", "can", "reach", "him", "acoly", "accel", "dot", "com", "origin", "repost", "with", "permiss", "relat", "data", "scienc", "sale", "call", "the", "surpris", "word", "that", "signal", "troubl", "success", "survey", "avail", "corpora", "for", "build", "datadriven", "dialogu", "system", "microsoft", "becom", "crosoft"], "timestamp_scraper": 1556374258.876922, "title": "Achieving Human Parity in Conversational Speech Recognition", "read_time": 336.59999999999997, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/adrian-colyer\" rel=\"author\" title=\"Posts by Adrian Colyer\">Adrian Colyer</a>, Venture Partner, Accel.</b></div>\n<p><a href=\"https://arxiv.org/abs/1610.05256\" target=\"_blank\">Achieving Human Parity in Conversational Speech Recognition</a>,\u00a0Xiong et al.\u00a0<em>Microsoft Technical Report, 2016</em></p>\n<p>The headline story here is that for the first time a system has been developed that exceeds human performance in one of the most difficult of all human speech recognition tasks: natural conversations held over the telephone. This is known as conversational telephone speech, or\u00a0<em>CTS</em>.</p>\n<p><img alt=\"Microsoft\" src=\"/wp-content/uploads/microsoft-wall-logo.jpg\" width=\"99%\"/></p>\n<blockquote><p>[CTS] is especially difficult due to the spontaneous (neither read nor planned) nature of the speech, its informality, and the self-corrections, hesitations, and other disfluencies that are pervasive.</p></blockquote>\n<p>The reference datasets for this task are the Switchboard and Fisher data collections from the 1990s and early 2000s. The apocryphal story here is that human performance on the task is about 4% error rate. But no-one can quite pin down where that 4% number comes from. So the Microsoft team took advantage of an existing professional transcription service used by Microsoft:</p>\n<blockquote><p>To measure human performance, we leveraged an existing pipeline in which Microsoft data is transcribed on a weekly basis. This pipeline uses a large commercial vendor to perform a two-pass transcription. In the first pass, a transcriber works from scratch to transcribe the data. In the second pass, a second listener monitors the data to do error correction. Dozens of hours of test data are processed in each batch. One week, we added the NIST 2000 CTS evaluation data to the work-list, without further comment\u2026</p></blockquote>\n<p>For the switchboard portion of the dataset, the professional human transcribers achieved a 5.9% error rate, and for the \u2018call-home\u2019 portion of the test set 11.3%. \u201cThe same informality, multiple speakers per channel, and recording conditions that make CallHome hard for computers make it difficult for people as well.\u201d</p>\n<blockquote><p>Notably, the performance of our artificial system aligns almost exactly with the performance of people on both sets.</p></blockquote>\n<p>And so one can\u2019t help but wonder how much longer those professional transcribers will continue to be needed! Did they know they were gathering the evidence that one day might help lead to the elimination of their jobs???</p>\n<p>Here\u2019s a table full of cryptic acronyms that show the performance of the system that Microsoft put together, using various acoustic models. The bottom-line (strictly, bottom two lines) is what matters here: parity on the switchboard dataset, and a slight advantage for the ASR (Automatic Speech Recognition) system of the CallHome dataset.</p>\n<p><img alt=\"Table\" src=\"https://adriancolyer.files.wordpress.com/2016/11/human-level-asr-fig-7.png?w=600\" width=\"99%\"/></p>\n<p>How did the Microsoft team manage to pull this off?</p>\n<blockquote><p>Our system\u2019s performance can be attributed to the systematic use of LSTMs for both acoustic and language modeling, as well as CNNs in the acoustic model, and extensive combination of complementary models.</p></blockquote>\n<p>Training was made feasible (reducing times from month to 1-3 weeks) by using Microsoft\u2019s\u00a0<a href=\"https://www.microsoft.com/en-us/research/product/cognitive-toolkit/\" target=\"_blank\">CNTK</a>\u00a0Cognitive Toolkit to parallelize SGD training, coupled with the use of the\u00a0<em>1-bit SGD parallelization techique</em>\u00a0from prior work:</p>\n<blockquote><p>In [65], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next. Each time a sub-gradient is quantized, the quantization error is computed and remembered, and then added to the next minibatch\u2019s sub-gradient. This reduces the required bandwidth 32-fold with minimal loss in accuracy.</p></blockquote>\n<h3>How it Works Under the Covers</h3>\n<p>\u00a0<br>\nThe model details are concisely explained, targeting an audience of speech recognition experts (i.e.\u00a0not me!). It is still possible for a lay-reader to gain some appreciation of what\u2019s involved though. It\u2019s also another reminder that we\u2019re rapidly assembling a powerful collection of building blocks that through good systems engineering can be combined into very effective systems. Expect to see an explosion of\u00a0<em>applied</em>\u00a0AI/ML/whatever-your-preferred-phrase-is applications as this trend continues.</br></p>\n<blockquote><p>Our progress is a result of the careful engineering and optimization of convolutional and recurrent neural networks. While the basic structures have been well known for a long period, it is only recently that they have emerged as the best models for speech recognition. Surprisingly, this is the case for both acoustic modeling and language modeling.</p></blockquote>\n<p>The CNN and RNN based acoustic models can model a large amount of acoustic context with temporal invariance, and in the case of CNNs, with frequency invariance as well. In language modeling, RNNs improve on classical N-gram models through the use of an unbounded word history and the generalization ability of\u00a0<a href=\"https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/\" target=\"_blank\">continuous word representations</a>.</p>\n<p>The paper describes a whole family of systems that were explored in order to find the best performing combination. The best acoustic model was formed by combining independently trained ResNet and VGG models using a score fusion weight.</p>\n<p>\u2018VGG\u2019 stands for the University of Oxford Visual Geometry Group and the architecture they developed in \u201c<a href=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/\" target=\"_blank\">Very deep convolutional networks for large-scale visual recognition</a>\u201d. Their networks use 16-19 layers with small 3\u00d73 filters in all convolutional layers. The\u00a0<a href=\"https://github.com/KaimingHe/deep-residual-networks\" target=\"_blank\">ResNet</a>\u00a0architecture is also borrowed from the field of image recognition.</p>\n<p><em>Speaker adaptive modeling</em>\u00a0is then applied by conditioning the network on an\u00a0<a href=\"http://habla.dc.uba.ar/gravano/ith-2014/presentaciones/Dehak_et_al_2010.pdf\" target=\"_blank\"><em>i-vector</em></a>\u00a0characterization of each speaker using 100-dimensional i-vectors. The i-vectors are added to the activation of each CNN layer via a learnable weight matrix.</p>\n<p>After initial training, model parameters are optimized using a maximum mutual information (MMI) objective function:</p>\n<p><center><img alt=\"Equation\" data-pin-nopin=\"true\" src=\"https://adriancolyer.files.wordpress.com/2016/11/human-level-asr-mmi.png?w=600\" width=\"70%\"/></center></p>\n<p>where\u00a0<em>w</em>\u00a0is a word sequence, and\u00a0<em>a</em>\u00a0is an acoustic realization of a word sequence. The performance improvements obtained from this lattice-free MMI (LFMMI) training phase, as well as i-vectors, can be seen in the following table:</p>\n<p><img alt=\"Table\" src=\"https://adriancolyer.files.wordpress.com/2016/11/human-level-asr-table-3.png?w=600\" width=\"99%\"/></p>\n<p>An initial decoding of acoustic model outputs is done with a WFST (<a href=\"https://www-i6.informatik.rwth-aachen.de/publications/download/740/HoffmeisterB.HeigoldG.RybachD.Schl%7Bu%7DterR.NeyH.--WFSTEnabledSolutionstoASRProblemsBeyondHMMDecoding--2012.pdf\" target=\"_blank\">Weighted Finite State Transducer</a>) decoder.</p>\n<blockquote><p>We use an N-gram language model trained and pruned with the SRILM toolkit. The first-pass LM has approximately 15.9 million bigrams, trigrams, and 4-grams, and a vocabulary of 30500 words, and give a perplexity of 54 on RT-03 speech transcripts.</p></blockquote>\n<p>The N-best performing hypotheses from the WFST decoding are then rescored using a combination of a large N-gram language model and neural net language models. The best performing language model used LSTMs with three hidden layers, and 1000 hidden units in each layer. \u201cFor the final system, we interpolated two LSTM-LMs with an N-gram LM for the forward direction LM, and similarly for the backward direction LM.\u201d</p>\n<p><b>Bio: <a href=\"https://twitter.com/adriancolyer\" target=\"_blank\">Adrian Colyer</a></b> was CTO of SpringSource, then CTO for Apps at VMware and subsequently Pivotal. He is now a Venture Partner at Accel Partners in London, working with early stage and startup companies across Europe. <em>If you\u2019re working on an interesting technology-related business he would love to hear from you: you can reach him at acolyer at accel dot com</em>.</p>\n<p><a href=\"https://blog.acolyer.org/2016/11/22/achieving-human-parity-in-conversational-speech-recognition/\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2016/09/data-science-sales-calls-words-trouble-success.html\">Data Science of Sales Calls: The Surprising Words That Signal Trouble or Success</a>\n<li><a href=\"/2016/07/survey-available-corpora-building-data-driven-dialog-systems.html\">A Survey of Available Corpora for Building Data-driven Dialogue Systems</a>\n<li><a href=\"/2016/04/microsoft-becoming-m-ai-crosoft.html\">Microsoft is Becoming M(ai)crosoft</a>\n</li></li></li></ul>\n</div> ", "website": "kdnuggets"}