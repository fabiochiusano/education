{"content": "comments By Kavita Ganesan , Data Scientist . Based on some recent conversations, I realized that text preprocessing is a severely overlooked topic. A few people I spoke to mentioned inconsistent results from their NLP applications only to realize that they were not preprocessing their text or were using the wrong kind of text preprocessing for their project. With that in mind, I thought of shedding some light around what text preprocessing really is, the different methods of text preprocessing, and a way to estimate how much preprocessing you may need. For those interested, I\u2019ve also made some\u00a0 text preprocessing code snippets \u00a0for you to try. Now, let\u2019s get started! What is text preprocessing? To preprocess your text simply means to bring your text into a form that is\u00a0 predictable \u00a0 and\u00a0 analyzable \u00a0 for your task. A task here is a combination of approach and domain. For example, extracting top keywords with TF-IDF (approach) from Tweets (domain) is an example of a\u00a0 Task . Task = approach +\u00a0domain One task\u2019s ideal preprocessing can become another task\u2019s worst nightmare. So take note: text preprocessing is not directly transferable from task to task. Let\u2019s take a very simple example, let\u2019s say you are trying to discover commonly used words in a news dataset. If your pre-processing step involves removing\u00a0 stop words \u00a0because some other task used it, then you are probably going to miss out on some of the common words as you have ALREADY eliminated it. So really, it\u2019s not a one-size-fits-all approach. Types of text preprocessing techniques There are different ways to preprocess your text. Here are some of the approaches that you should know about and I will try to highlight the importance of each. Lowercasing Lowercasing ALL your text data, although commonly overlooked, is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output. Quite recently, one of my blog readers trained a\u00a0 word embedding model for similarity lookups . He found that different variation in input capitalization (e.g. \u2018Canada\u2019 vs. \u2018canada\u2019) gave him different types of output or no output at all. This was probably happening because the dataset had mixed-case occurrences of the word \u2018Canada\u2019 and there was insufficient evidence for the neural-network to effectively learn the weights for the less common version. This type of issue is bound to happen when your dataset is fairly small, and lowercasing is a great way to deal with sparsity issues. Here is an example of how lowercasing solves the sparsity issue, where the same words with different cases map to the same lowercase form: Word with different cases all map to the same lowercase form Another example where lowercasing is very useful is for search. Imagine, you are looking for documents containing \u201cusa\u201d. However, no results were showing up because \u201cusa\u201d was indexed as\u00a0 \u201cUSA\u201d. \u00a0Now, who should we blame? The U.I. designer who set-up the interface or the engineer who set-up the search index? While lowercasing should be standard practice, I\u2019ve also had situations where preserving the capitalization was important. For example, in predicting the programming language of a source code file. The word\u00a0System\u00a0in Java is quite different from\u00a0system\u00a0in python. Lowercasing the two makes them identical, causing the classifier to lose important predictive features. While lowercasing\u00a0 is generally\u00a0 helpful, it may not be applicable for all tasks. Stemming Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The \u201croot\u201d in this case may not be a real root word, but just a canonical form of the original word. Stemming uses a crude heuristic process that chops off the ends of words in the hope of correctly transforming words into its root form. So the words \u201ctrouble\u201d, \u201ctroubled\u201d and \u201ctroubles\u201d might actually be converted to\u00a0troublinstead of\u00a0trouble\u00a0because the ends were just chopped off (ughh, how crude!). There are different algorithms for stemming. The most common algorithm, which is also known to be empirically effective for English, is\u00a0 Porters Algorithm . Here is an example of stemming in action with Porter Stemmer: Effects of stemming inflected words Stemming is useful for dealing with sparsity issues as well as standardizing vocabulary. I\u2019ve had success with stemming in search applications in particular. The idea is that, if say you search for \u201cdeep learning classes\u201d, you also want to surface documents that mention \u201cdeep learning\u00a0 class \u201d as well as \u201cdeep\u00a0 learn \u00a0classes\u201d, although the latter doesn\u2019t sound right. But you get where we are going with this. You want to match all variations of a word to bring up the most relevant documents. In most of my previous text classification work however, stemming only marginally helped improved classification accuracy as opposed to using better engineered features and text enrichment approaches such as using word embeddings. Lemmatization Lemmatization on the surface is very similar to stemming, where the goal is to remove inflections and map a word to its root form. The only difference is that, lemmatization tries to do it the proper way. It doesn\u2019t just chop things off, it actually transforms words to the actual root. For example, the word \u201cbetter\u201d would map to \u201cgood\u201d. It may use a dictionary such as\u00a0 WordNet for mappings \u00a0or some special\u00a0 rule-based approaches . Here is an example of lemmatization in action using a WordNet-based approach: Effects of Lemmatization with\u00a0WordNet In my experience, lemmatization provides no significant benefit over stemming for search and text classification purposes. In fact, depending on the algorithm you choose, it could be much slower compared to using a very basic stemmer and you may have to know the part-of-speech of the word in question in order to get a correct lemma.\u00a0 This paper \u00a0finds that lemmatization has no significant impact on accuracy for text classification with neural architectures. I would personally use lemmatization sparingly. The additional overhead may or may not be worth it. But you could always try it to see the impact it has on your performance metric. Stopword Removal Stop words are a set of commonly used words in a language. Examples of stop words in English are \u201ca\u201d, \u201cthe\u201d, \u201cis\u201d, \u201care\u201d and etc. The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead. For example, in the context of a search system, if your search query is \u00a0\u201cwhat is text preprocessing?\u201d , you want the search system to focus on surfacing documents that talk about\u00a0text preprocessing\u00a0over documents that talk about\u00a0what is. This can be done by preventing all words from your stop word list from being analyzed. Stop words are commonly applied in search systems, text classification applications, topic modeling, topic extraction and others. In my experience, stop word removal, while effective in search and topic extraction systems, showed to be non-critical in classification systems. However, it does help reduce the number of features in consideration which helps keep your models decently sized. Here is an example of stop word removal in action. All stop words are replaced with a dummy character,\u00a0 W : Sentence before and after stop word\u00a0removal Stop word lists \u00a0can come from pre-established sets or you can create a\u00a0 custom one for your domain . Some libraries (e.g. sklearn) allow you to remove words that appeared in X% of your documents, which can also give you a stop word removal effect. Normalization A highly overlooked preprocessing step is text normalization. Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word \u201cgooood\u201d and \u201cgud\u201d can be transformed to \u201cgood\u201d, its canonical form. Another example is mapping of near identical words such as \u201cstopwords\u201d, \u201cstop-words\u201d and \u201cstop words\u201d to just \u201cstopwords\u201d. Text normalization is important for noisy texts such as social media comments, text messages and comments to blog posts where abbreviations, misspellings and use of out-of-vocabulary words (oov) are prevalent.\u00a0 This paper \u00a0showed that by using a text normalization strategy for Tweets, they were able to improve sentiment classification accuracy by ~4%. Here\u2019s an example of words before and after normalization: Effects of Text Normalization Notice how the variations, map to the same canonical form. In my experience, text normalization has even been effective for analyzing\u00a0 highly unstructured clinical texts \u00a0where physicians take notes in non-standard ways. I\u2019ve also found it useful for\u00a0 topic extraction \u00a0where near synonyms and spelling differences are common (e.g. topic modelling, topic modeling, topic-modeling, topic-modelling). Unfortunately, unlike stemming and lemmatization, there isn\u2019t a standard way to normalize texts. It typically depends on the task. For example, the way you would normalize clinical texts would arguably be different from how you normalize sms text messages. Some common approaches to text normalization include dictionary mappings (easiest), statistical machine translation (SMT) and spelling-correction based approaches.\u00a0 This interesting article \u00a0compares the use of a dictionary based approach and a SMT approach for normalizing text messages. Noise Removal Noise removal is about removing\u00a0characters\u00a0digits\u00a0and\u00a0pieces of text that can interfere with your text analysis. Noise removal is one of the most essential text preprocessing steps. It is also highly domain dependent. For example, in Tweets, noise could be all special characters except hashtags as it signifies concepts that can characterize a Tweet. The problem with noise is that it can produce results that are inconsistent in your downstream tasks. Let\u2019s take the example below: Stemming\u00a0without\u00a0Noise\u00a0Removal Notice that all the raw words above have some surrounding noise in them. If you stem these words, you can see that the stemmed result does not look very pretty. None of them have a correct stem. However, with some cleaning as applied in\u00a0 this notebook , the results now look much better: Stemming\u00a0 with \u00a0Noise\u00a0Removal Noise removal is one of the first things you should be looking into when it comes to Text Mining and NLP. There are various ways to remove noise. This includes\u00a0 punctuation removal ,\u00a0 special character removal ,\u00a0 numbers removal, html formatting removal, domain specific keyword removal \u00a0 (e.g. \u2018RT\u2019 for retweet), source code removal, header removal \u00a0and more. It all depends on which domain you are working in and what entails noise for your task. The\u00a0 code snippet in my notebook \u00a0shows how to do some basic noise removal. Text Enrichment / Augmentation Text enrichment involves augmenting your original text data with information that you did not previously have. Text enrichment provides more semantics to your original text, thereby improving its predictive power and the depth of analysis you can perform on your data. In an information retrieval example, expanding a user\u2019s query to improve the matching of keywords is a form of augmentation. A query like\u00a0text mining could become\u00a0text document mining analysis. While this doesn\u2019t make sense to a human, it can help fetch documents that are more relevant. You can get really creative with how you enrich your text. You can use\u00a0 part-of-speech tagging \u00a0to get more granular information about the words in your text. For example, in a document classification problem, the appearance of the word\u00a0 book \u00a0as a\u00a0 noun \u00a0could result in a different classification than\u00a0 book \u00a0as a\u00a0 verb \u00a0as one is used in the context of reading and the other is used in the context of reserving something.\u00a0 This article \u00a0talks about how Chinese text classification is improved with a combination of nouns and verbs as input features. With the availability of large amounts texts however, people have started using\u00a0 embeddings \u00a0to enrich the meaning of words, phrases and sentences for classification, search, summarization and text generation in general. This is especially true in deep learning based NLP approaches where a\u00a0 word level embedding layer \u00a0is quite common. You can either start with\u00a0 pre-established embeddings \u00a0or create your own and use it in downstream tasks. Other ways to enrich your text data include\u00a0 phrase extraction , where you recognize compound words as one (aka chunking),\u00a0 expansion with synonyms and\u00a0 dependency parsing . Do you need it\u00a0all? Not really, but you do have to do some of it for sure if you want good, consistent results. To give you an idea of what the bare minimum should be, I\u2019ve broken it down to\u00a0 Must Do ,\u00a0 Should Do \u00a0and\u00a0 Task Dependent . Everything that falls under task dependent can be quantitatively or qualitatively tested before deciding you actually need it. Remember, less is more and you want to keep your approach as elegant as possible. The more overhead you add, the more layers you will have to peel back when you run into issues. Must Do: Noise removal Lowercasing (can be task dependent in some cases) Should Do: Simple normalization\u200a\u2014\u200a(e.g. standardize near identical words) Task Dependent: Advanced normalization (e.g. addressing out-of-vocabulary words) Stop-word removal Stemming / lemmatization Text enrichment / augmentation So, for any task, the minimum you should do is try to lowercase your text and remove noise. What entails noise depends on your domain (see section on Noise Removal). You can also do some basic normalization steps for more consistency and then systematically add other layers as you see fit. General Rule of\u00a0Thumb Not all tasks need the same level of preprocessing. For some tasks, you can get away with the minimum. However, for others, the dataset is so noisy that, if you don\u2019t preprocess enough, it\u2019s going to be garbage-in-garbage-out. Here\u2019s a general rule of thumb. This will not always hold true, but works for most cases. If you have a lot of well written texts to work with in a fairly general domain, then preprocessing is not extremely critical; you can get away with the bare minimum (e.g. training a word embedding model using all of Wikipedia texts or Reuters news articles). However, if you are working in a very narrow domain (e.g. Tweets about health foods) and data is sparse and noisy, you could benefit from more preprocessing layers, although each layer you add (e.g. stop word removal, stemming, normalization) needs to be quantitatively or qualitatively verified as a meaningful layer. Here\u2019s a table that summarizes how much preprocessing you should be performing on your text data I hope the ideas here steer you towards the right preprocessing steps for your projects. Remember,\u00a0 less is more . A friend of mine once mentioned to me how he made a large e-commerce search system more efficient and less buggy just by throwing out layers of\u00a0 unneeded \u00a0preprocessing. Resources Python code for basic text preprocessing using NLTK and regex Constructing custom stop word lists Source code for phrase extraction References For an updated list of papers, please see\u00a0 my original article Bio: Kavita Ganesan is a Data Scientist with expertise in Natural Language Processing, Text Mining, Search and Machine Learning. Over the last decade, she worked for various technology organizations including GitHub (Microsoft), 3M Health Information Systems and eBay. Original . Reposted with permission. Resources: On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning education Software for Analytics, Data Science, Data Mining, and Machine Learning Related: Getting started with NLP using the PyTorch framework Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision Beyond news contents: the role of social context for fake news detection", "title_html": "<h1 id=\"title\">All you need to know about text preprocessing for NLP and Machine Learning</h1> ", "url": "https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html", "tfidf": {"tfidf": {"after": 2.04140414042, "real": 2.28103448276, "food": 2.9613878007800003, "arguabl": 12.928338762200001, "fall": 1.6945244956799999, "onc": 1.4974533106999999, "relat": 1.23750876919, "fit": 3.37070063694, "troubl": 34.93618359007, "form": 13.53068181816, "kind": 2.5806241872599998, "insuffici": 10.911340206199998, "with": 30.035946269699995, "addit": 1.24634950542, "would": 4.331491712719999, "nois": 187.051546392, "recogn": 2.54954231572, "etc": 4.2066772655, "dataset": 968.0487804899999, "wikipedia": 40.5, "class": 6.34955339289, "ecommerc": 1443.27272727, "outofvocabulari": 3175.2, "python": 112.5957446808, "thought": 1.9854927463699998, "tag": 19.7462686567, "well": 3.1967246123999997, "unlik": 2.42529789184, "done": 2.3302509907499998, "approach": 29.05791606746, "user": 7.71053909665, "permiss": 6.280063291139999, "reader": 6.437956204380001, "know": 5.1865403463, "their": 4.0619163362, "abbrevi": 12.326086956500001, "highlight": 5.83033419023, "similar": 2.75028150714, "noun": 60.136363636400006, "origin": 5.6862464183500006, "bound": 5.40735694823, "creat": 2.4985835694, "wordnetbas": 1587.6, "summar": 30.2112274024, "how": 16.0250328051, "test": 2.65707112971, "news": 8.32730133752, "fair": 6.41066020594, "format": 2.53125, "light": 1.9102394417, "inconsist": 28.7869446962, "spars": 21.0, "had": 3.1427251732199997, "especi": 1.66712170534, "troublinstead": 1587.6, "end": 2.21360847742, "found": 2.2277415281, "gud": 1587.6, "word": 97.01301346613998, "interest": 3.20662492426, "deep": 14.511882998159999, "sentiment": 9.9225, "will": 3.67443295788, "specif": 1.8719490626099997, "spoke": 5.5125, "realli": 18.990430622, "lookup": 223.605633803, "chop": 130.1311475409, "power": 1.3396337861799998, "updat": 5.56466876972, "tabl": 3.82093862816, "below": 2.25607503197, "near": 3.86308703058, "but": 5.0816208949499995, "success": 1.32002993265, "surround": 2.49858356941, "need": 7.186311787049999, "classif": 88.73780487803002, "output": 23.03094777564, "sure": 7.453521126760001, "off": 4.53643204116, "entail": 42.5060240964, "capit": 4.9159312587, "header": 83.55789473680001, "applic": 17.1336067343, "empir": 2.8395635843299996, "prevent": 2.16117615029, "organ": 1.6387283237, "stemmer": 3175.2, "has": 3.1309492505999996, "enough": 2.2319696330700003, "replac": 1.5602948402899999, "use": 26.770607691879995, "experi": 5.61187698834, "resourc": 5.8974739970200005, "onlin": 2.6051854282900004, "out": 2.12033388982, "model": 12.5435870424, "cheapli": 86.28260869569999, "good": 4.55944859277, "alway": 4.13491340018, "concept": 2.65707112971, "number": 2.20285833218, "under": 1.0781663837, "sound": 3.11294117647, "translat": 2.85745140389, "decad": 2.1390460792200003, "around": 1.21394708671, "start": 5.06694326976, "reuter": 26.3720930233, "not": 13.20376175547, "someth": 3.28152128979, "involv": 2.8997260274000003, "porter": 36.0, "wrong": 5.478260869570001, "less": 5.87619135744, "happen": 5.92719805862, "broken": 4.46959459459, "pleas": 9.12938470385, "true": 5.11139729556, "slower": 17.1632432432, "metric": 22.235294117600002, "topicmodel": 3175.2, "english": 3.4865488086000003, "than": 1.03278688525, "problem": 5.30024482527, "index": 13.993829881, "they": 2.06034650574, "design": 1.45825296225, "anoth": 3.40930565496, "qualit": 84.44680851060001, "last": 1.2117234010100002, "let": 13.94466403164, "minimum": 24.11849601216, "pytorch": 1587.6, "github": 1587.6, "retriev": 2.16826003824, "construct": 1.9320920043799998, "away": 3.70285714286, "stem": 141.61690140844001, "featur": 6.10850327048, "practic": 1.70434782609, "simpl": 6.7962328767199995, "appear": 2.6429165972999997, "sens": 2.8365195640499996, "mine": 39.00737100735999, "worst": 6.653813914500001, "downstream": 83.778364116, "crude": 39.69, "either": 1.5830092731099998, "nonstandard": 233.470588235, "enrich": 178.6329113928, "such": 4.24605509496, "accuraci": 38.2861736334, "sever": 1.07241286139, "sentenc": 11.69072164948, "talk": 9.0910479099, "preval": 9.997481108310001, "transfer": 5.45098712446, "without": 1.29547123623, "lot": 4.40877534018, "some": 16.64587155968, "beyond": 2.54586273252, "small": 1.3594793629, "should": 14.978928608910001, "add": 13.83730389309, "result": 8.022812590240001, "document": 22.8687580026, "fetch": 58.583025830299995, "version": 2.0083491461099996, "supervis": 7.74061433447, "content": 3.5421686747, "inflect": 192.8259109311, "html": 71.1928251121, "stop": 32.67563117460001, "languag": 6.88464874242, "media": 2.59369384088, "caus": 1.38521943984, "ani": 1.13383802314, "question": 2.20408163265, "canada": 9.110175975510002, "from": 10.0056721497, "work": 6.69120539478, "num": 1.00031504001, "heurist": 115.043478261, "granular": 160.363636364, "synonym": 29.6747663552, "idea": 6.279235332900001, "miss": 3.53664513255, "for": 47.014806880470005, "depend": 22.411067193700003, "predict": 20.7393860222, "friend": 2.20194174757, "match": 7.1352808988800005, "book": 2.86829268292, "she": 2.16, "much": 4.7768918309199995, "steer": 18.0, "are": 19.56821277982, "topic": 38.2028188381, "shed": 16.835630965, "better": 6.01971688575, "meaning": 21.8076923077, "preprocess": 32973.23076921, "look": 7.634527530639999, "sparsiti": 4329.81818181, "root": 21.46855983774, "refer": 1.30024570025, "extract": 46.218340611360006, "surfac": 11.921902377959999, "lemmat": 15876.0, "combin": 3.39520958084, "pars": 145.651376147, "amount": 2.27027027027, "focus": 4.02025829324, "educ": 2.00733341763, "toward": 1.6303142329, "stopword": 7938.0, "compar": 3.7324556247800005, "remov": 60.17435249520001, "map": 32.582863006640004, "and": 52.00327559076, "doe": 3.4116256581, "lowercas": 1944.0, "convers": 3.3486606201200004, "give": 2.7306501548, "proper": 3.3388012618299996, "probabl": 5.29111814698, "train": 3.8731397901999998, "great": 1.26592775696, "goal": 3.28152128979, "framework": 8.200413223139998, "extrem": 2.36602086438, "librari": 2.68266306185, "note": 2.84899057874, "rulebas": 1587.6, "oppos": 2.51282051282, "analyz": 29.059182428310002, "scientist": 9.38852749852, "notebook": 80.3848101266, "into": 5.0751230739499995, "paper": 7.988594431410001, "relev": 13.877622377620002, "effect": 12.5667546174, "abov": 1.90382539873, "set": 2.37415881562, "canon": 43.2883435584, "gave": 1.85121268657, "what": 8.77404073896, "rememb": 9.77586206896, "allow": 1.2716059271100002, "particular": 1.3814827706200001, "help": 9.79740809313, "neuralnetwork": 1587.6, "larg": 3.55724848755, "find": 1.7294117647099998, "interfer": 8.52173913043, "instead": 1.59461631177, "consider": 2.29920347574, "even": 1.16461267606, "own": 1.17844418052, "base": 4.5851263538, "natur": 1.5392670157100001, "deal": 4.36693714758, "fact": 1.73375559681, "who": 3.18837863169, "decid": 1.9257641921400002, "bare": 17.630205441420003, "easiest": 64.8, "preserv": 3.1062414400300002, "post": 2.23826307627, "here": 24.2307692308, "buggi": 144.327272727, "ideal": 4.65571847507, "veri": 8.81160799239, "punctuat": 42.1114058355, "microsoft": 24.8450704225, "quantit": 55.60770577939999, "blog": 28.3753351206, "clean": 6.86975335353, "human": 1.8965476048299998, "section": 2.1284354471099998, "layer": 56.990769230779996, "spare": 9.997481108310001, "contain": 1.59814777532, "noisi": 186.7764705882, "about": 7.45402106113, "evid": 2.24872521246, "intuit": 27.7068062827, "which": 4.02076738, "neural": 59.4606741573, "mind": 3.5918552036199998, "context": 17.03890528576, "fake": 18.290322580599998, "just": 6.6790071518500005, "softwar": 10.2624434389, "tweet": 464.21052631599997, "solv": 7.26923076923, "correct": 10.989386248259999, "him": 1.63434218653, "simpli": 2.5192002538900002, "repost": 933.882352941, "gooood": 1587.6, "classifi": 10.5875291764, "usa": 8.114490161, "weak": 4.70539419087, "verb": 58.58302583020001, "narrow": 5.53363541304, "where": 11.73865698731, "overhead": 48.109090908999995, "weight": 4.878918254459999, "special": 4.464566929139999, "sms": 139.263157895, "few": 1.31729173581, "latter": 2.34159292035, "chunk": 81.0, "articl": 8.07220033048, "them": 3.29628347982, "then": 3.25973581548, "retweet": 1587.6, "dictionari": 15.68774703558, "keep": 4.08490930142, "systemat": 8.338235294119999, "order": 1.24625166811, "expand": 2.2260235558000003, "lose": 3.0851146521599997, "digit": 4.416133518780001, "quit": 8.65491550065, "health": 5.43140608964, "abl": 1.8208510150200001, "critic": 1.67010309278, "realiz": 9.78489984592, "over": 3.07575072651, "step": 14.139650872799997, "generat": 2.05275407292, "build": 1.6341739578, "analysi": 10.43558282208, "preestablish": 3175.2, "provid": 2.43105428374, "creativ": 6.52259654889, "algorithm": 111.8028169016, "unstructur": 214.54054054099998, "consist": 4.4704336399499995, "unfortun": 9.966101694919999, "same": 5.592897907399999, "sklearn": 1587.6, "show": 5.0681564246, "character": 4.563380281690001, "there": 5.20456333595, "scienc": 4.63939216832, "inform": 7.876562810100001, "nightmar": 31.625498008, "therebi": 6.47735618115, "known": 1.0859097127200001, "possibl": 1.4173734488, "messag": 14.90237797248, "top": 1.8387769284200002, "misspel": 132.3, "right": 2.8109065155799997, "custom": 7.269230769239999, "regex": 1587.6, "setup": 68.2838709678, "take": 4.55846672888, "spell": 6.75, "previous": 2.85693719632, "now": 3.4823426189999998, "strategi": 4.44208170118, "tri": 11.126737530659998, "dummi": 48.1090909091, "discov": 2.52160101652, "code": 23.28428257152, "blame": 6.47207501019, "high": 3.44331983805, "basic": 10.92072227, "hold": 1.6551292744, "aka": 20.834645669300002, "issu": 7.19608376395, "physician": 8.76158940397, "also": 8.11812080536, "typic": 2.2541530597799997, "person": 1.40520446097, "simplest": 28.0494699647, "everyth": 4.81967213115, "mean": 2.89813800658, "one": 8.05019965776, "notic": 8.73988439306, "task": 81.61468788249, "role": 1.55327267391, "technolog": 2.6034765496900003, "signific": 4.35874439463, "the": 90.0, "may": 7.3641243125099995, "becom": 2.24984057252, "piec": 3.24132298898, "noncrit": 1587.6, "ganesan": 3175.2, "peopl": 2.42640990372, "occurr": 13.805217391300001, "throw": 8.39555790587, "those": 1.19548192771, "machin": 16.09733840304, "want": 9.984905660399999, "convert": 3.2740771293099997, "peel": 36.8352668213, "system": 12.48658568559, "behind": 2.0845588235299997, "howev": 7.661633919309999, "essenti": 2.9280708225700005, "various": 2.6646525679799997, "although": 3.44905496415, "elimin": 3.67670217693, "actual": 7.49929145016, "were": 5.1229428848000005, "snippet": 271.384615384, "ident": 8.42377078176, "estim": 2.34991119005, "other": 6.05954198472, "appli": 4.5944147012, "techniqu": 3.7293868921800004, "might": 2.1561863370900003, "raw": 10.6478873239, "purpos": 2.23416830847, "eleg": 20.9169960474, "impact": 5.95052473764, "hope": 5.01769911504, "spellingcorrect": 1587.6, "see": 6.36210627555, "thumb": 66.705882353, "margin": 6.16783216783, "becaus": 4.5980739989999995, "choos": 4.17899447223, "signifi": 16.0363636364, "sourc": 5.09281437126, "webbas": 1587.6, "get": 14.2850073108, "like": 1.14918566775, "ebay": 140.495575221, "chines": 4.270037654649999, "imagin": 6.598503740650001, "file": 3.7710213776699995, "detect": 5.41288782816, "process": 6.78099305928, "transform": 13.68031021112, "analyt": 34.513043478200004, "that": 27.107569721249998, "includ": 4.076256499119999, "rule": 3.4831066257199996, "phrase": 18.55395403194, "this": 14.05311077394, "oov": 1587.6, "clinic": 20.2370936902, "decent": 35.5964125561, "variat": 14.111999999999998, "engin": 4.94271481942, "back": 1.26070038911, "perform": 4.59419311275, "project": 3.5069582505000003, "differ": 14.838538826999999, "embed": 101.01378579, "been": 1.0239277652399998, "statist": 4.24265098878, "most": 7.14675241161, "low": 2.13072070863, "hashtag": 496.125, "domain": 93.9408284024, "java": 31.625498008, "all": 12.13761467892, "charact": 10.06881243064, "input": 24.4058416602, "action": 5.45567010309, "written": 1.9573418813999999, "augment": 66.0811654528, "expertis": 20.0201765448, "program": 2.02139037433, "reserv": 3.24198488871, "verifi": 14.2258064516, "recent": 3.0881151527, "expans": 3.4770039421800005, "case": 8.90992423536, "onesizefitsal": 1587.6, "produc": 1.36932896326, "normal": 44.38283177102, "overlook": 34.2155172414, "unneed": 453.6, "architectur": 5.12790697674, "two": 1.01379310345, "more": 11.188877498699998, "improv": 10.218846549950001, "partofspeech": 3175.2, "list": 5.45285935084, "type": 6.084312723570001, "interfac": 20.9169960474, "ughh": 1587.6, "vocabulari": 23.2785923754, "these": 1.07415426252, "benefit": 6.13683803634, "keyword": 417.789473685, "general": 5.6091011871, "made": 2.14077669902, "expect": 2.20011086475, "semant": 39.1034482759, "befor": 3.30108123093, "comment": 9.17864713818, "none": 4.06555697823, "standard": 9.457881567950002, "can": 25.87775061124, "avail": 1.7288467821, "wordnet": 2442.46153846, "make": 2.1525320317200003, "queri": 168.8936170212, "way": 10.9716655149, "search": 42.30129124819, "onli": 3.0769429549800007, "each": 2.37949640288, "size": 2.49387370405, "situat": 2.06611140031, "bring": 4.07233551366, "thing": 4.813096862219999, "depth": 8.24299065421, "read": 2.3149606299200003, "pretti": 15.75, "advanc": 1.9997480791, "common": 14.025974025999998, "say": 3.5088960106, "data": 40.51722671208, "reduc": 3.97396745932, "address": 2.86157173756, "method": 2.5714285714300003, "compound": 8.09586945436, "level": 3.3088786994599997, "bio": 42.336000000000006, "come": 2.65662650602, "must": 3.844067796619999, "exampl": 30.0966824644, "kavita": 2646.0, "worth": 5.210370856580001, "text": 193.95310344834002, "lemma": 269.084745763, "run": 1.55692850838, "direct": 1.22226499346, "import": 6.700996116850001, "first": 1.00761614623, "mixedcas": 1587.6, "could": 7.226217569399999, "have": 9.134053570259999, "while": 4.176795580119999, "social": 3.9809428284800004, "except": 1.71948445792, "down": 1.35889754344, "learn": 20.90475493785, "garbageingarbageout": 1587.6, "when": 3.0623030926499997, "mention": 7.61682392451, "effici": 5.09335899904}, "logtfidf": {"after": 0.040981389296199995, "real": 0.824629060574, "food": 1.08565801008, "arguabl": 2.5594217052, "fall": 0.527402167952, "onc": 0.403765872355, "relat": 0.21310030165399999, "fit": 1.2151206268899999, "troubl": 11.25329045505, "form": 1.4406382102920001, "kind": 0.948031302717, "insuffici": 2.3898026343, "with": 0.0359247514017, "addit": 0.220218882972, "would": 0.3184705118588, "nois": 39.34072809248, "recogn": 0.935913859031, "etc": 1.4366730879700003, "dataset": 26.3292228332, "wikipedia": 3.70130197411, "class": 2.2493165697990003, "ecommerc": 7.2746685411000005, "outofvocabulari": 14.739957441820001, "python": 8.06131348592, "thought": 0.685867118283, "tag": 2.98296454472, "well": 0.1905433149468, "unlik": 0.885954358842, "done": 0.845975983129, "approach": 10.223270604134001, "user": 2.04258810688, "permiss": 1.8373800586400002, "reader": 1.8622111301800002, "know": 1.905839388796, "their": 0.061442020490800005, "abbrevi": 2.51171790724, "highlight": 1.76307432123, "similar": 0.637112184228, "noun": 6.8069350604, "origin": 0.643062187935, "bound": 1.68776042417, "creat": 0.445153637028, "wordnetbas": 7.369978720910001, "summar": 5.430132886059999, "how": 4.715669569300001, "test": 0.977224437103, "news": 2.93298029394, "fair": 2.32963016262, "format": 0.9287132518729999, "light": 0.64722859635, "inconsist": 5.3335495893000004, "spars": 3.04452243772, "had": 0.1394340732333, "especi": 0.511098609709, "troublinstead": 7.369978720910001, "end": 0.202953597236, "found": 0.215682248096, "gud": 7.369978720910001, "word": 31.63649844879, "interest": 0.9441435559639999, "deep": 5.1546938792, "sentiment": 2.29480490568, "will": 0.6083596047450001, "specif": 0.626980167541, "spoke": 1.7070182407700003, "realli": 6.2305633588, "lookup": 5.40988393686, "chop": 11.3097914415, "power": 0.292396282715, "updat": 1.7164374626899999, "tabl": 1.34049610661, "below": 0.813626591936, "near": 0.758562972102, "but": 0.0809618603595, "success": 0.27765441259199997, "surround": 0.915723999073, "need": 1.81370081721, "classif": 22.96569809919, "output": 6.11467973481, "sure": 2.0086865552, "off": 1.240585561164, "entail": 6.112997257540001, "capit": 1.79866805574, "header": 4.425539741740001, "applic": 6.15801964245, "empir": 1.04365037288, "prevent": 0.7706525875229999, "organ": 0.49392052866999997, "stemmer": 14.739957441820001, "has": 0.1281718345644, "enough": 0.802884439169, "replac": 0.444874803592, "use": 0.7594085130216001, "experi": 1.878818861799, "resourc": 2.16275388516, "onlin": 0.957503854357, "out": 0.1168527818386, "model": 4.424700438666001, "cheapli": 4.45762805629, "good": 1.2557682147209999, "alway": 1.452638409144, "concept": 0.977224437103, "number": 0.1932171568372, "under": 0.07526180538319999, "sound": 1.13556799519, "translat": 1.0499301100299998, "decad": 0.760359972282, "around": 0.19387710578200001, "start": 0.945773477164, "reuter": 3.2723063685900002, "not": 0.2021813690975, "someth": 1.18830712273, "involv": 0.742938157316, "porter": 5.7807435158, "wrong": 1.70078769102, "less": 1.5384578504, "happen": 2.17280883604, "broken": 1.49729770979, "pleas": 2.21149829955, "true": 1.876651259268, "slower": 2.84277007639, "metric": 3.1016808515599994, "topicmodel": 14.739957441820001, "english": 1.11153037267, "than": 0.0322608622182, "problem": 1.707422172819, "index": 3.89093865824, "they": 0.0594539895352, "design": 0.377239118022, "anoth": 0.38368908495599996, "qualit": 7.48594934102, "last": 0.19204364461100001, "let": 4.995210269119999, "minimum": 7.18673861764, "pytorch": 7.369978720910001, "github": 7.369978720910001, "retriev": 0.773925020223, "construct": 0.658603355972, "away": 1.231915083738, "stem": 38.165044548800005, "featur": 1.693549672568, "practic": 0.533182530867, "simpl": 2.4464425787799997, "appear": 0.557471796986, "sens": 1.04257779501, "mine": 12.67447269424, "worst": 1.89519021125, "downstream": 7.47005521764, "crude": 5.97590417248, "either": 0.459327638815, "nonstandard": 5.45305610873, "enrich": 24.847131072800003, "such": 0.238783911224, "accuraci": 7.6394296218, "sever": 0.06991112039689999, "sentenc": 3.5312966504400003, "talk": 3.32603368347, "preval": 2.3023331721, "transfer": 2.00529907094, "without": 0.258874517941, "lot": 1.4835969502500002, "some": 0.633176145032, "beyond": 0.934469583725, "small": 0.307101805059, "should": 4.584778890822, "add": 4.58626751139, "result": 0.954652358667, "document": 8.392924101446999, "fetch": 4.07044499302, "version": 0.697313064259, "supervis": 2.04648105583, "content": 1.26473915954, "inflect": 12.48952643181, "html": 4.26539204244, "stop": 11.678690624445, "languag": 2.492045473218, "media": 0.9530830530519999, "caus": 0.325858567406, "ani": 0.125608358366, "question": 0.790310929014, "canada": 3.3323392174800004, "from": 0.00567054168866, "work": 0.654207403638, "num": 0.00031499039539700004, "heurist": 4.74531012875, "granular": 5.0774439637699995, "synonym": 5.394299772899999, "idea": 2.21590776636, "miss": 1.2631785751200002, "for": 0.014804548583659002, "depend": 8.06969815, "predict": 6.5829609507599995, "friend": 0.7893395836239999, "match": 2.54380887748, "book": 0.7211395764, "she": 0.7701082216959999, "much": 0.7099829172040001, "steer": 2.8903717579, "are": 0.5598819980713, "topic": 11.878994087870002, "shed": 2.82349753127, "better": 2.0892838218, "meaning": 3.08226276571, "preprocess": 191.90559032387998, "look": 2.5855467744, "sparsiti": 21.824005623300003, "root": 7.64898037512, "refer": 0.262553246798, "extract": 12.249703398060001, "surfac": 4.1392948671, "lemmat": 73.69978720910001, "combin": 1.058436621502, "pars": 4.9812159316699995, "amount": 0.819898886199, "focus": 1.3963979441119998, "educ": 0.696807183384, "toward": 0.48877277716000006, "stopword": 36.849893604550005, "compar": 1.2478383618539999, "remov": 20.88146524764, "map": 11.23475947072, "and": 0.0032754873873072, "doe": 1.0680834594339998, "lowercas": 61.05115602276, "convers": 1.2085604509999999, "give": 0.622785104448, "proper": 1.2056118389200001, "probabl": 1.945764825826, "train": 1.321836625678, "great": 0.235805258079, "goal": 1.18830712273, "framework": 2.10418454607, "extrem": 0.8612095839370001, "librari": 0.986809980943, "note": 0.707635136166, "rulebas": 7.369978720910001, "oppos": 0.921405832541, "analyz": 6.812166705479999, "scientist": 3.09268256888, "notebook": 7.387356098, "into": 0.0745643161435, "paper": 2.938207618995, "relev": 3.8742609227999996, "effect": 3.0044720444219997, "abov": 0.643865229816, "set": 0.342992022578, "canon": 9.52635614304, "gave": 0.615840930592, "what": 1.581211077789, "rememb": 3.1735382252399997, "allow": 0.24028061118900002, "particular": 0.323157393804, "help": 2.353454049408, "neuralnetwork": 7.369978720910001, "larg": 0.511125181818, "find": 0.547781330288, "interfer": 2.1426204433000002, "instead": 0.46663315041500003, "consider": 0.8325627480600001, "even": 0.152388564834, "own": 0.164195077421, "base": 0.5460932091480001, "natur": 0.431306339292, "deal": 1.561829402506, "fact": 0.5502899207949999, "who": 0.1827006989577, "decid": 0.655322871893, "bare": 4.35293293746, "easiest": 4.17130560336, "preserv": 1.13341345513, "post": 0.8057001527009999, "here": 8.8503818837, "buggi": 4.97208344811, "ideal": 1.53809624363, "veri": 1.6111185526660001, "punctuat": 3.7403186264499997, "microsoft": 3.21265935953, "quantit": 6.650349208500001, "blog": 5.30474621118, "clean": 1.9271282036300001, "human": 0.640035183243, "section": 0.755387177948, "layer": 14.678854136450003, "spare": 2.3023331721, "contain": 0.468845318236, "noisi": 12.393900806220001, "about": 0.43990434232220005, "evid": 0.8103634834160001, "intuit": 3.3216780971900004, "which": 0.02071365538172, "neural": 4.0853151555, "mind": 1.2786688388299998, "context": 5.79681965768, "fake": 2.9063720992400004, "just": 1.447657170545, "softwar": 2.32849096333, "tweet": 22.654501287, "solv": 1.9836504770400003, "correct": 3.8949528954300003, "him": 0.49124039099699995, "simpli": 0.923941491586, "repost": 6.83935046985, "gooood": 7.369978720910001, "classifi": 3.3330592702999997, "usa": 2.80100838338, "weak": 1.5487095508000002, "verb": 6.754595624919999, "narrow": 1.71084499792, "where": 0.7149135262027001, "overhead": 6.36064795776, "weight": 1.58492352612, "special": 1.192679785803, "sms": 4.93636536551, "few": 0.275577913653, "latter": 0.850831432969, "chunk": 4.394449154669999, "articl": 2.808526958296, "them": 0.2825499807279, "then": 0.24910159569269996, "retweet": 7.369978720910001, "dictionari": 4.9628030261700005, "keep": 1.4283046893459999, "systemat": 2.12085159855, "order": 0.22014038079300002, "expand": 0.80021683492, "lose": 1.1265888210600001, "digit": 1.48526454375, "quit": 3.17854541052, "health": 1.9981017364640001, "abl": 0.599303982475, "critic": 0.512885356729, "realiz": 3.1753863695200004, "over": 0.0748101644871, "step": 5.1977252849, "generat": 0.719182341736, "build": 0.491137452091, "analysi": 3.7398273087600007, "preestablish": 14.739957441820001, "provid": 0.39035568865000003, "creativ": 1.87527254036, "algorithm": 13.32176958072, "unstructur": 5.3684987207, "consist": 1.196619379278, "unfortun": 2.29918950399, "same": 0.5602982480200001, "sklearn": 7.369978720910001, "show": 0.946731064052, "character": 1.51806363875, "there": 0.2004894646275, "scienc": 1.682872357782, "inform": 2.27226852331, "nightmar": 3.45396369421, "therebi": 1.86831243037, "known": 0.0824180805992, "possibl": 0.348805474891, "messag": 4.80872552106, "top": 0.609100637788, "misspel": 4.88507207112, "right": 0.68071970834, "custom": 2.5810065929599997, "regex": 7.369978720910001, "setup": 7.061052816639999, "take": 0.522767848788, "spell": 1.90954250488, "previous": 0.713205920126, "now": 0.44727883506300004, "strategi": 1.49112311818, "tri": 3.7055491749600002, "dummi": 3.87347115944, "discov": 0.924894023806, "code": 8.13611457582, "blame": 1.8674967696400002, "high": 0.41347135962000003, "basic": 4.0174709958, "hold": 0.503879117196, "aka": 3.03661725822, "issu": 1.82049521967, "physician": 2.1703773272999998, "also": 0.1172572624, "typic": 0.812774319158, "person": 0.34018281601800004, "simplest": 3.3339697356999998, "everyth": 1.57270590317, "mean": 0.74184256704, "one": 0.050042813164, "notic": 2.94949956336, "task": 28.507222938810003, "role": 0.44036410757399996, "technolog": 0.956847686355, "signific": 1.120715232996, "the": 0.0, "may": 0.3549699669908, "becom": 0.23542435297800002, "piec": 1.17598157639, "noncrit": 7.369978720910001, "ganesan": 14.739957441820001, "peopl": 0.386531156946, "occurr": 2.62504659255, "throw": 2.12770274524, "those": 0.17854939087299998, "machin": 5.56943832248, "want": 3.4581830312749995, "convert": 1.1860360368, "peel": 3.6064557238, "system": 2.946873110265, "behind": 0.7345572374320001, "howev": 0.6322058214325, "essenti": 1.07434378384, "various": 0.57385300014, "although": 0.418463944254, "elimin": 1.30201620283, "actual": 2.514056726592, "were": 0.12145571840549998, "snippet": 9.82077975822, "ident": 3.09733582695, "estim": 0.854377535975, "other": 0.05924848751856, "appli": 1.6633883796239999, "techniqu": 1.31624384807, "might": 0.7683410765340001, "raw": 2.36536149914, "purpos": 0.803869037322, "eleg": 3.0405620365099995, "impact": 2.18066445262, "hope": 1.83964860891, "spellingcorrect": 7.369978720910001, "see": 1.20460792746, "thumb": 7.014291919339999, "margin": 1.81934742575, "becaus": 0.5573726353, "choos": 1.43007066072, "signifi": 2.77485887077, "sourc": 1.587654932253, "webbas": 7.369978720910001, "get": 4.638152046256, "like": 0.139053576545, "ebay": 4.94517599519, "chines": 1.45162264562, "imagin": 1.88684291737, "file": 1.32734588723, "detect": 1.68878274493, "process": 2.1113167961, "transform": 4.91865290828, "analyt": 5.696380287719999, "that": 0.10735600625028, "includ": 0.075538725562, "rule": 1.109554847074, "phrase": 5.46621189909, "this": 0.053010286734999995, "oov": 7.369978720910001, "clinic": 4.62874012234, "decent": 3.5722448618800002, "variat": 4.6452396318, "engin": 1.809535116552, "back": 0.23166743089699998, "perform": 1.27854255174, "project": 1.123203771814, "differ": 2.5478534557440002, "embed": 16.94098518762, "been": 0.023645982368400004, "statist": 1.4451883070700002, "most": 0.14523527406919998, "low": 0.7564602833490001, "hashtag": 6.2068279111, "domain": 22.4008000599, "java": 3.45396369421, "all": 0.13683158517359997, "charact": 3.692593628956, "input": 5.00335067078, "action": 1.7941294952070002, "written": 0.671587369833, "augment": 11.218357619719999, "expertis": 2.99674059227, "program": 0.7037855787649999, "reserv": 1.1761857622, "verifi": 2.65505767096, "recent": 0.868827482576, "expans": 1.2461709868100002, "case": 2.372437613334, "onesizefitsal": 7.369978720910001, "produc": 0.314320812003, "normal": 16.313869439311, "overlook": 7.302200919030001, "unneed": 6.117215752409999, "architectur": 1.63469757919, "two": 0.0136988443582, "more": 0.18727424759999997, "improv": 3.5739790196599994, "partofspeech": 14.739957441820001, "list": 1.239383046024, "type": 2.121304456161, "interfac": 3.0405620365099995, "ughh": 7.369978720910001, "vocabulari": 3.14753415606, "these": 0.0715336194008, "benefit": 2.24232490232, "keyword": 14.809096096530002, "general": 0.5747628903149999, "made": 0.1360430521946, "expect": 0.78850775216, "semant": 3.6662106543, "befor": 0.2869133156385, "comment": 3.3548026036199996, "none": 1.40255075163, "standard": 3.1870525491, "can": 3.5715041206679996, "avail": 0.547454586289, "wordnet": 14.215228912879999, "make": 0.14699531564579998, "queri": 12.09197022888, "way": 1.7828235894150002, "search": 15.338287303169999, "onli": 0.0759728049873, "each": 0.347483378608, "size": 0.9138372060609999, "situat": 0.725668290015, "bring": 1.4221389811860001, "thing": 1.7563870693599999, "depth": 2.10936322154, "read": 0.83939268088, "pretti": 2.75684036527, "advanc": 0.6930212121780001, "common": 3.3832580527099996, "say": 1.124308561104, "data": 14.6018470176, "reduc": 1.373235550286, "address": 1.05137103247, "method": 0.944461608841, "compound": 2.09135398771, "level": 1.006924379886, "bio": 3.7456377879300002, "come": 0.5678198130600001, "must": 1.306767894776, "exampl": 8.173653499979999, "kavita": 14.37531432822, "worth": 1.65065103492, "text": 70.70988461937999, "lemma": 5.59502637, "run": 0.442714975539, "direct": 0.200705689496, "import": 1.46409138533, "first": 0.0075872898121599995, "mixedcas": 7.369978720910001, "could": 1.1157376337400002, "have": 0.1330650210708, "while": 0.17299993517520004, "social": 1.376743004522, "except": 0.54202451213, "down": 0.306673741186, "learn": 7.584768582704999, "garbageingarbageout": 7.369978720910001, "when": 0.0616649665752, "mention": 2.7952415590079998, "effici": 1.62793753414}, "logidf": {"after": 0.020490694648099998, "real": 0.824629060574, "food": 1.08565801008, "arguabl": 2.5594217052, "fall": 0.527402167952, "onc": 0.403765872355, "relat": 0.21310030165399999, "fit": 1.2151206268899999, "troubl": 1.60761292215, "form": 0.120053184191, "kind": 0.948031302717, "insuffici": 2.3898026343, "with": 0.00119749171339, "addit": 0.220218882972, "would": 0.0796176279647, "nois": 2.45879550578, "recogn": 0.935913859031, "etc": 1.4366730879700003, "dataset": 5.26584456664, "wikipedia": 3.70130197411, "class": 0.7497721899330001, "ecommerc": 7.2746685411000005, "outofvocabulari": 7.369978720910001, "python": 4.03065674296, "thought": 0.685867118283, "tag": 2.98296454472, "well": 0.0635144383156, "unlik": 0.885954358842, "done": 0.845975983129, "approach": 0.7302336145810001, "user": 2.04258810688, "permiss": 1.8373800586400002, "reader": 1.8622111301800002, "know": 0.952919694398, "their": 0.015360505122700001, "abbrevi": 2.51171790724, "highlight": 1.76307432123, "similar": 0.318556092114, "noun": 3.4034675302, "origin": 0.128612437587, "bound": 1.68776042417, "creat": 0.222576818514, "wordnetbas": 7.369978720910001, "summar": 2.7150664430299996, "how": 0.47156695693000006, "test": 0.977224437103, "news": 0.733245073485, "fair": 1.16481508131, "format": 0.9287132518729999, "light": 0.64722859635, "inconsist": 2.6667747946500002, "spars": 3.04452243772, "had": 0.0464780244111, "especi": 0.511098609709, "troublinstead": 7.369978720910001, "end": 0.101476798618, "found": 0.107841124048, "gud": 7.369978720910001, "word": 0.585861082385, "interest": 0.47207177798199995, "deep": 1.2886734698, "sentiment": 2.29480490568, "will": 0.202786534915, "specif": 0.626980167541, "spoke": 1.7070182407700003, "realli": 1.5576408397, "lookup": 5.40988393686, "chop": 3.7699304805000002, "power": 0.292396282715, "updat": 1.7164374626899999, "tabl": 1.34049610661, "below": 0.813626591936, "near": 0.252854324034, "but": 0.0161923720719, "success": 0.27765441259199997, "surround": 0.915723999073, "need": 0.362740163442, "classif": 2.08779073629, "output": 2.03822657827, "sure": 2.0086865552, "off": 0.41352852038800003, "entail": 3.0564986287700004, "capit": 0.89933402787, "header": 4.425539741740001, "applic": 1.23160392849, "empir": 1.04365037288, "prevent": 0.7706525875229999, "organ": 0.49392052866999997, "stemmer": 7.369978720910001, "has": 0.0427239448548, "enough": 0.802884439169, "replac": 0.444874803592, "use": 0.0292080197316, "experi": 0.626272953933, "resourc": 1.08137694258, "onlin": 0.957503854357, "out": 0.0584263909193, "model": 0.7374500731110001, "cheapli": 4.45762805629, "good": 0.418589404907, "alway": 0.726319204572, "concept": 0.977224437103, "number": 0.0966085784186, "under": 0.07526180538319999, "sound": 1.13556799519, "translat": 1.0499301100299998, "decad": 0.760359972282, "around": 0.19387710578200001, "start": 0.236443369291, "reuter": 3.2723063685900002, "not": 0.0155524130075, "someth": 1.18830712273, "involv": 0.371469078658, "porter": 2.8903717579, "wrong": 1.70078769102, "less": 0.3846144626, "happen": 1.08640441802, "broken": 1.49729770979, "pleas": 2.21149829955, "true": 0.938325629634, "slower": 2.84277007639, "metric": 3.1016808515599994, "topicmodel": 7.369978720910001, "english": 0.555765186335, "than": 0.0322608622182, "problem": 0.569140724273, "index": 1.94546932912, "they": 0.0297269947676, "design": 0.377239118022, "anoth": 0.127896361652, "qualit": 3.74297467051, "last": 0.19204364461100001, "let": 1.2488025672799998, "minimum": 1.79668465441, "pytorch": 7.369978720910001, "github": 7.369978720910001, "retriev": 0.773925020223, "construct": 0.658603355972, "away": 0.615957541869, "stem": 2.0086865552, "featur": 0.423387418142, "practic": 0.533182530867, "simpl": 1.2232212893899999, "appear": 0.278735898493, "sens": 1.04257779501, "mine": 1.58430908678, "worst": 1.89519021125, "downstream": 3.73502760882, "crude": 2.98795208624, "either": 0.459327638815, "nonstandard": 5.45305610873, "enrich": 3.1058913841000004, "such": 0.059695977806, "accuraci": 2.5464765406, "sever": 0.06991112039689999, "sentenc": 1.7656483252200001, "talk": 1.10867789449, "preval": 2.3023331721, "transfer": 1.00264953547, "without": 0.258874517941, "lot": 1.4835969502500002, "some": 0.0395735090645, "beyond": 0.934469583725, "small": 0.307101805059, "should": 0.509419876758, "add": 1.52875583713, "result": 0.136378908381, "document": 0.932547122383, "fetch": 4.07044499302, "version": 0.697313064259, "supervis": 2.04648105583, "content": 1.26473915954, "inflect": 4.16317547727, "html": 4.26539204244, "stop": 0.778579374963, "languag": 0.8306818244059999, "media": 0.9530830530519999, "caus": 0.325858567406, "ani": 0.125608358366, "question": 0.790310929014, "canada": 1.11077973916, "from": 0.000567054168866, "work": 0.109034567273, "num": 0.00031499039539700004, "heurist": 4.74531012875, "granular": 5.0774439637699995, "synonym": 2.6971498864499996, "idea": 0.73863592212, "miss": 1.2631785751200002, "for": 0.00031499039539700004, "depend": 0.806969815, "predict": 1.6457402376899999, "friend": 0.7893395836239999, "match": 1.27190443874, "book": 0.3605697882, "she": 0.7701082216959999, "much": 0.17749572930100002, "steer": 2.8903717579, "are": 0.0294674735827, "topic": 1.6969991554100001, "shed": 2.82349753127, "better": 0.6964279406, "meaning": 3.08226276571, "preprocess": 7.1076144564399995, "look": 0.6463866936, "sparsiti": 7.2746685411000005, "root": 1.27483006252, "refer": 0.262553246798, "extract": 2.04161723301, "surfac": 1.3797649557, "lemmat": 7.369978720910001, "combin": 0.529218310751, "pars": 4.9812159316699995, "amount": 0.819898886199, "focus": 0.6981989720559999, "educ": 0.696807183384, "toward": 0.48877277716000006, "stopword": 7.369978720910001, "compar": 0.6239191809269999, "remov": 0.6960488415880001, "map": 1.40434493384, "and": 6.29901420636e-05, "doe": 0.5340417297169999, "lowercas": 5.08759633523, "convers": 1.2085604509999999, "give": 0.311392552224, "proper": 1.2056118389200001, "probabl": 0.972882412913, "train": 0.660918312839, "great": 0.235805258079, "goal": 1.18830712273, "framework": 2.10418454607, "extrem": 0.8612095839370001, "librari": 0.986809980943, "note": 0.353817568083, "rulebas": 7.369978720910001, "oppos": 0.921405832541, "analyz": 2.2707222351599996, "scientist": 1.54634128444, "notebook": 3.693678049, "into": 0.0149128632287, "paper": 0.979402539665, "relev": 1.9371304613999998, "effect": 0.333830227158, "abov": 0.643865229816, "set": 0.171496011289, "canon": 2.38158903576, "gave": 0.615840930592, "what": 0.225887296827, "rememb": 1.5867691126199999, "allow": 0.24028061118900002, "particular": 0.323157393804, "help": 0.336207721344, "neuralnetwork": 7.369978720910001, "larg": 0.17037506060600002, "find": 0.547781330288, "interfer": 2.1426204433000002, "instead": 0.46663315041500003, "consider": 0.8325627480600001, "even": 0.152388564834, "own": 0.164195077421, "base": 0.13652330228700002, "natur": 0.431306339292, "deal": 0.780914701253, "fact": 0.5502899207949999, "who": 0.0609002329859, "decid": 0.655322871893, "bare": 2.17646646873, "easiest": 4.17130560336, "preserv": 1.13341345513, "post": 0.8057001527009999, "here": 0.8850381883700001, "buggi": 4.97208344811, "ideal": 1.53809624363, "veri": 0.230159793238, "punctuat": 3.7403186264499997, "microsoft": 3.21265935953, "quantit": 3.3251746042500003, "blog": 2.65237310559, "clean": 1.9271282036300001, "human": 0.640035183243, "section": 0.755387177948, "layer": 2.0969791623500003, "spare": 2.3023331721, "contain": 0.468845318236, "noisi": 4.1313002687400004, "about": 0.0628434774746, "evid": 0.8103634834160001, "intuit": 3.3216780971900004, "which": 0.00517841384543, "neural": 4.0853151555, "mind": 1.2786688388299998, "context": 1.44920491442, "fake": 2.9063720992400004, "just": 0.289531434109, "softwar": 2.32849096333, "tweet": 4.5309002574, "solv": 1.9836504770400003, "correct": 1.29831763181, "him": 0.49124039099699995, "simpli": 0.923941491586, "repost": 6.83935046985, "gooood": 7.369978720910001, "classifi": 1.6665296351499999, "usa": 1.40050419169, "weak": 1.5487095508000002, "verb": 3.3772978124599997, "narrow": 1.71084499792, "where": 0.0649921387457, "overhead": 3.18032397888, "weight": 1.58492352612, "special": 0.39755992860100003, "sms": 4.93636536551, "few": 0.275577913653, "latter": 0.850831432969, "chunk": 4.394449154669999, "articl": 0.702131739574, "them": 0.0941833269093, "then": 0.08303386523089999, "retweet": 7.369978720910001, "dictionari": 1.65426767539, "keep": 0.7141523446729999, "systemat": 2.12085159855, "order": 0.22014038079300002, "expand": 0.80021683492, "lose": 1.1265888210600001, "digit": 1.48526454375, "quit": 1.05951513684, "health": 0.9990508682320001, "abl": 0.599303982475, "critic": 0.512885356729, "realiz": 1.5876931847600002, "over": 0.0249367214957, "step": 1.03954505698, "generat": 0.719182341736, "build": 0.491137452091, "analysi": 1.2466091029200002, "preestablish": 7.369978720910001, "provid": 0.19517784432500002, "creativ": 1.87527254036, "algorithm": 3.33044239518, "unstructur": 5.3684987207, "consist": 0.398873126426, "unfortun": 2.29918950399, "same": 0.112059649604, "sklearn": 7.369978720910001, "show": 0.236682766013, "character": 1.51806363875, "there": 0.0400978929255, "scienc": 0.841436178891, "inform": 0.454453704662, "nightmar": 3.45396369421, "therebi": 1.86831243037, "known": 0.0824180805992, "possibl": 0.348805474891, "messag": 1.6029085070200002, "top": 0.609100637788, "misspel": 4.88507207112, "right": 0.34035985417, "custom": 1.2905032964799998, "regex": 7.369978720910001, "setup": 3.5305264083199996, "take": 0.130691962197, "spell": 1.90954250488, "previous": 0.356602960063, "now": 0.149092945021, "strategi": 1.49112311818, "tri": 0.61759152916, "dummi": 3.87347115944, "discov": 0.924894023806, "code": 1.35601909597, "blame": 1.8674967696400002, "high": 0.13782378654000002, "basic": 1.00436774895, "hold": 0.503879117196, "aka": 3.03661725822, "issu": 0.364099043934, "physician": 2.1703773272999998, "also": 0.0146571578, "typic": 0.812774319158, "person": 0.34018281601800004, "simplest": 3.3339697356999998, "everyth": 1.57270590317, "mean": 0.37092128352, "one": 0.0062553516455, "notic": 1.47474978168, "task": 1.35748680661, "role": 0.44036410757399996, "technolog": 0.956847686355, "signific": 0.373571744332, "the": 0.0, "may": 0.050709995284400004, "becom": 0.11771217648900001, "piec": 1.17598157639, "noncrit": 7.369978720910001, "ganesan": 7.369978720910001, "peopl": 0.193265578473, "occurr": 2.62504659255, "throw": 2.12770274524, "those": 0.17854939087299998, "machin": 1.39235958062, "want": 0.6916366062549999, "convert": 1.1860360368, "peel": 3.6064557238, "system": 0.327430345585, "behind": 0.7345572374320001, "howev": 0.0903151173475, "essenti": 1.07434378384, "various": 0.28692650007, "although": 0.139487981418, "elimin": 1.30201620283, "actual": 0.628514181648, "were": 0.024291143681099997, "snippet": 4.91038987911, "ident": 1.03244527565, "estim": 0.854377535975, "other": 0.00987474791976, "appli": 0.8316941898119999, "techniqu": 1.31624384807, "might": 0.7683410765340001, "raw": 2.36536149914, "purpos": 0.803869037322, "eleg": 3.0405620365099995, "impact": 1.09033222631, "hope": 0.919824304455, "spellingcorrect": 7.369978720910001, "see": 0.240921585492, "thumb": 3.5071459596699994, "margin": 1.81934742575, "becaus": 0.139343158825, "choos": 1.43007066072, "signifi": 2.77485887077, "sourc": 0.529218310751, "webbas": 7.369978720910001, "get": 0.579769005782, "like": 0.139053576545, "ebay": 4.94517599519, "chines": 1.45162264562, "imagin": 1.88684291737, "file": 1.32734588723, "detect": 1.68878274493, "process": 0.527829199025, "transform": 1.22966322707, "analyt": 2.8481901438599997, "that": 0.00397614837964, "includ": 0.0188846813905, "rule": 0.554777423537, "phrase": 1.82207063303, "this": 0.0037864490525, "oov": 7.369978720910001, "clinic": 2.31437006117, "decent": 3.5722448618800002, "variat": 1.5484132106, "engin": 0.904767558276, "back": 0.23166743089699998, "perform": 0.42618085058, "project": 0.561601885907, "differ": 0.212321121312, "embed": 2.82349753127, "been": 0.023645982368400004, "statist": 1.4451883070700002, "most": 0.020747896295599998, "low": 0.7564602833490001, "hashtag": 6.2068279111, "domain": 2.24008000599, "java": 3.45396369421, "all": 0.011402632097799998, "charact": 0.923148407239, "input": 2.50167533539, "action": 0.598043165069, "written": 0.671587369833, "augment": 2.8045894049299998, "expertis": 2.99674059227, "program": 0.7037855787649999, "reserv": 1.1761857622, "verifi": 2.65505767096, "recent": 0.434413741288, "expans": 1.2461709868100002, "case": 0.395406268889, "onesizefitsal": 7.369978720910001, "produc": 0.314320812003, "normal": 0.959639378783, "overlook": 2.43406697301, "unneed": 6.117215752409999, "architectur": 1.63469757919, "two": 0.0136988443582, "more": 0.017024931599999998, "improv": 0.7147958039319999, "partofspeech": 7.369978720910001, "list": 0.309845761506, "type": 0.707101485387, "interfac": 3.0405620365099995, "ughh": 7.369978720910001, "vocabulari": 3.14753415606, "these": 0.0715336194008, "benefit": 1.12116245116, "keyword": 4.93636536551, "general": 0.114952578063, "made": 0.0680215260973, "expect": 0.78850775216, "semant": 3.6662106543, "befor": 0.0956377718795, "comment": 1.11826753454, "none": 1.40255075163, "standard": 0.63741050982, "can": 0.162341096394, "avail": 0.547454586289, "wordnet": 7.1076144564399995, "make": 0.07349765782289999, "queri": 4.03065674296, "way": 0.19809150993500002, "search": 1.1798682540899998, "onli": 0.025324268329099998, "each": 0.173741689304, "size": 0.9138372060609999, "situat": 0.725668290015, "bring": 0.7110694905930001, "thing": 0.8781935346799999, "depth": 2.10936322154, "read": 0.83939268088, "pretti": 2.75684036527, "advanc": 0.6930212121780001, "common": 0.338325805271, "say": 0.562154280552, "data": 1.2168205848, "reduc": 0.686617775143, "address": 1.05137103247, "method": 0.944461608841, "compound": 2.09135398771, "level": 0.503462189943, "bio": 3.7456377879300002, "come": 0.28390990653000003, "must": 0.653383947388, "exampl": 0.40868267499899996, "kavita": 7.18765716411, "worth": 1.65065103492, "text": 1.14048200999, "lemma": 5.59502637, "run": 0.442714975539, "direct": 0.200705689496, "import": 0.292818277066, "first": 0.0075872898121599995, "mixedcas": 7.369978720910001, "could": 0.18595627229000003, "have": 0.0147850023412, "while": 0.04324998379380001, "social": 0.688371502261, "except": 0.54202451213, "down": 0.306673741186, "learn": 0.842752064745, "garbageingarbageout": 7.369978720910001, "when": 0.0205549888584, "mention": 0.931747186336, "effici": 1.62793753414}, "freq": {"after": 2, "real": 1, "food": 1, "arguabl": 1, "fall": 1, "onc": 1, "relat": 1, "fit": 1, "troubl": 7, "form": 12, "kind": 1, "insuffici": 1, "with": 30, "addit": 1, "would": 4, "nois": 16, "recogn": 1, "etc": 1, "dataset": 5, "wikipedia": 1, "class": 3, "ecommerc": 1, "outofvocabulari": 2, "python": 2, "thought": 1, "tag": 1, "well": 3, "unlik": 1, "done": 1, "approach": 14, "user": 1, "permiss": 1, "reader": 1, "know": 2, "their": 4, "abbrevi": 1, "highlight": 1, "similar": 2, "noun": 2, "origin": 5, "bound": 1, "creat": 2, "wordnetbas": 1, "summar": 2, "how": 10, "test": 1, "news": 4, "fair": 2, "format": 1, "light": 1, "inconsist": 2, "spars": 1, "had": 3, "especi": 1, "troublinstead": 1, "end": 2, "found": 2, "gud": 1, "word": 54, "interest": 2, "deep": 4, "sentiment": 1, "will": 3, "specif": 1, "spoke": 1, "realli": 4, "lookup": 1, "chop": 3, "power": 1, "updat": 1, "tabl": 1, "below": 1, "near": 3, "but": 5, "success": 1, "surround": 1, "need": 5, "classif": 11, "output": 3, "sure": 1, "off": 3, "entail": 2, "capit": 2, "header": 1, "applic": 5, "empir": 1, "prevent": 1, "organ": 1, "stemmer": 2, "has": 3, "enough": 1, "replac": 1, "use": 26, "experi": 3, "resourc": 2, "onlin": 1, "out": 2, "model": 6, "cheapli": 1, "good": 3, "alway": 2, "concept": 1, "number": 2, "under": 1, "sound": 1, "translat": 1, "decad": 1, "around": 1, "start": 4, "reuter": 1, "not": 13, "someth": 1, "involv": 2, "porter": 2, "wrong": 1, "less": 4, "happen": 2, "broken": 1, "pleas": 1, "true": 2, "slower": 1, "metric": 1, "topicmodel": 2, "english": 2, "than": 1, "problem": 3, "index": 2, "they": 2, "design": 1, "anoth": 3, "qualit": 2, "last": 1, "let": 4, "minimum": 4, "pytorch": 1, "github": 1, "retriev": 1, "construct": 1, "away": 2, "stem": 19, "featur": 4, "practic": 1, "simpl": 2, "appear": 2, "sens": 1, "mine": 8, "worst": 1, "downstream": 2, "crude": 2, "either": 1, "nonstandard": 1, "enrich": 8, "such": 4, "accuraci": 3, "sever": 1, "sentenc": 2, "talk": 3, "preval": 1, "transfer": 2, "without": 1, "lot": 1, "some": 16, "beyond": 1, "small": 1, "should": 9, "add": 3, "result": 7, "document": 9, "fetch": 1, "version": 1, "supervis": 1, "content": 1, "inflect": 3, "html": 1, "stop": 15, "languag": 3, "media": 1, "caus": 1, "ani": 1, "question": 1, "canada": 3, "from": 10, "work": 6, "num": 1, "heurist": 1, "granular": 1, "synonym": 2, "idea": 3, "miss": 1, "for": 47, "depend": 10, "predict": 4, "friend": 1, "match": 2, "book": 2, "she": 1, "much": 4, "steer": 1, "are": 19, "topic": 7, "shed": 1, "better": 3, "meaning": 1, "preprocess": 27, "look": 4, "sparsiti": 3, "root": 6, "refer": 1, "extract": 6, "surfac": 3, "lemmat": 10, "combin": 2, "pars": 1, "amount": 1, "focus": 2, "educ": 1, "toward": 1, "stopword": 5, "compar": 2, "remov": 30, "map": 8, "and": 52, "doe": 2, "lowercas": 12, "convers": 1, "give": 2, "proper": 1, "probabl": 2, "train": 2, "great": 1, "goal": 1, "framework": 1, "extrem": 1, "librari": 1, "note": 2, "rulebas": 1, "oppos": 1, "analyz": 3, "scientist": 2, "notebook": 2, "into": 5, "paper": 3, "relev": 2, "effect": 9, "abov": 1, "set": 2, "canon": 4, "gave": 1, "what": 7, "rememb": 2, "allow": 1, "particular": 1, "help": 7, "neuralnetwork": 1, "larg": 3, "find": 1, "interfer": 1, "instead": 1, "consider": 1, "even": 1, "own": 1, "base": 4, "natur": 1, "deal": 2, "fact": 1, "who": 3, "decid": 1, "bare": 2, "easiest": 1, "preserv": 1, "post": 1, "here": 10, "buggi": 1, "ideal": 1, "veri": 7, "punctuat": 1, "microsoft": 1, "quantit": 2, "blog": 2, "clean": 1, "human": 1, "section": 1, "layer": 7, "spare": 1, "contain": 1, "noisi": 3, "about": 7, "evid": 1, "intuit": 1, "which": 4, "neural": 1, "mind": 1, "context": 4, "fake": 1, "just": 5, "softwar": 1, "tweet": 5, "solv": 1, "correct": 3, "him": 1, "simpli": 1, "repost": 1, "gooood": 1, "classifi": 2, "usa": 2, "weak": 1, "verb": 2, "narrow": 1, "where": 11, "overhead": 2, "weight": 1, "special": 3, "sms": 1, "few": 1, "latter": 1, "chunk": 1, "articl": 4, "them": 3, "then": 3, "retweet": 1, "dictionari": 3, "keep": 2, "systemat": 1, "order": 1, "expand": 1, "lose": 1, "digit": 1, "quit": 3, "health": 2, "abl": 1, "critic": 1, "realiz": 2, "over": 3, "step": 5, "generat": 1, "build": 1, "analysi": 3, "preestablish": 2, "provid": 2, "creativ": 1, "algorithm": 4, "unstructur": 1, "consist": 3, "unfortun": 1, "same": 5, "sklearn": 1, "show": 4, "character": 1, "there": 5, "scienc": 2, "inform": 5, "nightmar": 1, "therebi": 1, "known": 1, "possibl": 1, "messag": 3, "top": 1, "misspel": 1, "right": 2, "custom": 2, "regex": 1, "setup": 2, "take": 4, "spell": 1, "previous": 2, "now": 3, "strategi": 1, "tri": 6, "dummi": 1, "discov": 1, "code": 6, "blame": 1, "high": 3, "basic": 4, "hold": 1, "aka": 1, "issu": 5, "physician": 1, "also": 8, "typic": 1, "person": 1, "simplest": 1, "everyth": 1, "mean": 2, "one": 8, "notic": 2, "task": 21, "role": 1, "technolog": 1, "signific": 3, "the": 90, "may": 7, "becom": 2, "piec": 1, "noncrit": 1, "ganesan": 2, "peopl": 2, "occurr": 1, "throw": 1, "those": 1, "machin": 4, "want": 5, "convert": 1, "peel": 1, "system": 9, "behind": 1, "howev": 7, "essenti": 1, "various": 2, "although": 3, "elimin": 1, "actual": 4, "were": 5, "snippet": 2, "ident": 3, "estim": 1, "other": 6, "appli": 2, "techniqu": 1, "might": 1, "raw": 1, "purpos": 1, "eleg": 1, "impact": 2, "hope": 2, "spellingcorrect": 1, "see": 5, "thumb": 2, "margin": 1, "becaus": 4, "choos": 1, "signifi": 1, "sourc": 3, "webbas": 1, "get": 8, "like": 1, "ebay": 1, "chines": 1, "imagin": 1, "file": 1, "detect": 1, "process": 4, "transform": 4, "analyt": 2, "that": 27, "includ": 4, "rule": 2, "phrase": 3, "this": 14, "oov": 1, "clinic": 2, "decent": 1, "variat": 3, "engin": 2, "back": 1, "perform": 3, "project": 2, "differ": 12, "embed": 6, "been": 1, "statist": 1, "most": 7, "low": 1, "hashtag": 1, "domain": 10, "java": 1, "all": 12, "charact": 4, "input": 2, "action": 3, "written": 1, "augment": 4, "expertis": 1, "program": 1, "reserv": 1, "verifi": 1, "recent": 2, "expans": 1, "case": 6, "onesizefitsal": 1, "produc": 1, "normal": 17, "overlook": 3, "unneed": 1, "architectur": 1, "two": 1, "more": 11, "improv": 5, "partofspeech": 2, "list": 4, "type": 3, "interfac": 1, "ughh": 1, "vocabulari": 1, "these": 1, "benefit": 2, "keyword": 3, "general": 5, "made": 2, "expect": 1, "semant": 1, "befor": 3, "comment": 3, "none": 1, "standard": 5, "can": 22, "avail": 1, "wordnet": 2, "make": 2, "queri": 3, "way": 9, "search": 13, "onli": 3, "each": 2, "size": 1, "situat": 1, "bring": 2, "thing": 2, "depth": 1, "read": 1, "pretti": 1, "advanc": 1, "common": 10, "say": 2, "data": 12, "reduc": 2, "address": 1, "method": 1, "compound": 1, "level": 2, "bio": 1, "come": 2, "must": 2, "exampl": 20, "kavita": 2, "worth": 1, "text": 62, "lemma": 1, "run": 1, "direct": 1, "import": 5, "first": 1, "mixedcas": 1, "could": 6, "have": 9, "while": 4, "social": 2, "except": 1, "down": 1, "learn": 9, "garbageingarbageout": 1, "when": 3, "mention": 3, "effici": 1}, "idf": {"after": 1.02070207021, "real": 2.28103448276, "food": 2.9613878007800003, "arguabl": 12.928338762200001, "fall": 1.6945244956799999, "onc": 1.4974533106999999, "relat": 1.23750876919, "fit": 3.37070063694, "troubl": 4.99088337001, "form": 1.12755681818, "kind": 2.5806241872599998, "insuffici": 10.911340206199998, "with": 1.0011982089899998, "addit": 1.24634950542, "would": 1.0828729281799998, "nois": 11.6907216495, "recogn": 2.54954231572, "etc": 4.2066772655, "dataset": 193.609756098, "wikipedia": 40.5, "class": 2.11651779763, "ecommerc": 1443.27272727, "outofvocabulari": 1587.6, "python": 56.2978723404, "thought": 1.9854927463699998, "tag": 19.7462686567, "well": 1.0655748708, "unlik": 2.42529789184, "done": 2.3302509907499998, "approach": 2.07556543339, "user": 7.71053909665, "permiss": 6.280063291139999, "reader": 6.437956204380001, "know": 2.59327017315, "their": 1.01547908405, "abbrevi": 12.326086956500001, "highlight": 5.83033419023, "similar": 1.37514075357, "noun": 30.068181818200003, "origin": 1.13724928367, "bound": 5.40735694823, "creat": 1.2492917847, "wordnetbas": 1587.6, "summar": 15.1056137012, "how": 1.60250328051, "test": 2.65707112971, "news": 2.08182533438, "fair": 3.20533010297, "format": 2.53125, "light": 1.9102394417, "inconsist": 14.3934723481, "spars": 21.0, "had": 1.0475750577399998, "especi": 1.66712170534, "troublinstead": 1587.6, "end": 1.10680423871, "found": 1.11387076405, "gud": 1587.6, "word": 1.7965372864099998, "interest": 1.60331246213, "deep": 3.6279707495399998, "sentiment": 9.9225, "will": 1.22481098596, "specif": 1.8719490626099997, "spoke": 5.5125, "realli": 4.7476076555, "lookup": 223.605633803, "chop": 43.3770491803, "power": 1.3396337861799998, "updat": 5.56466876972, "tabl": 3.82093862816, "below": 2.25607503197, "near": 1.28769567686, "but": 1.01632417899, "success": 1.32002993265, "surround": 2.49858356941, "need": 1.4372623574099999, "classif": 8.067073170730001, "output": 7.676982591880001, "sure": 7.453521126760001, "off": 1.5121440137200002, "entail": 21.2530120482, "capit": 2.45796562935, "header": 83.55789473680001, "applic": 3.42672134686, "empir": 2.8395635843299996, "prevent": 2.16117615029, "organ": 1.6387283237, "stemmer": 1587.6, "has": 1.0436497502, "enough": 2.2319696330700003, "replac": 1.5602948402899999, "use": 1.0296387573799999, "experi": 1.87062566278, "resourc": 2.9487369985100003, "onlin": 2.6051854282900004, "out": 1.06016694491, "model": 2.0905978404, "cheapli": 86.28260869569999, "good": 1.51981619759, "alway": 2.06745670009, "concept": 2.65707112971, "number": 1.10142916609, "under": 1.0781663837, "sound": 3.11294117647, "translat": 2.85745140389, "decad": 2.1390460792200003, "around": 1.21394708671, "start": 1.26673581744, "reuter": 26.3720930233, "not": 1.01567398119, "someth": 3.28152128979, "involv": 1.4498630137000001, "porter": 18.0, "wrong": 5.478260869570001, "less": 1.46904783936, "happen": 2.96359902931, "broken": 4.46959459459, "pleas": 9.12938470385, "true": 2.55569864778, "slower": 17.1632432432, "metric": 22.235294117600002, "topicmodel": 1587.6, "english": 1.7432744043000001, "than": 1.03278688525, "problem": 1.76674827509, "index": 6.9969149405, "they": 1.03017325287, "design": 1.45825296225, "anoth": 1.13643521832, "qualit": 42.223404255300004, "last": 1.2117234010100002, "let": 3.48616600791, "minimum": 6.02962400304, "pytorch": 1587.6, "github": 1587.6, "retriev": 2.16826003824, "construct": 1.9320920043799998, "away": 1.85142857143, "stem": 7.453521126760001, "featur": 1.52712581762, "practic": 1.70434782609, "simpl": 3.3981164383599998, "appear": 1.3214582986499999, "sens": 2.8365195640499996, "mine": 4.875921375919999, "worst": 6.653813914500001, "downstream": 41.889182058, "crude": 19.845, "either": 1.5830092731099998, "nonstandard": 233.470588235, "enrich": 22.3291139241, "such": 1.06151377374, "accuraci": 12.7620578778, "sever": 1.07241286139, "sentenc": 5.84536082474, "talk": 3.0303493033, "preval": 9.997481108310001, "transfer": 2.72549356223, "without": 1.29547123623, "lot": 4.40877534018, "some": 1.04036697248, "beyond": 2.54586273252, "small": 1.3594793629, "should": 1.6643254009900001, "add": 4.61243463103, "result": 1.14611608432, "document": 2.5409731114, "fetch": 58.583025830299995, "version": 2.0083491461099996, "supervis": 7.74061433447, "content": 3.5421686747, "inflect": 64.2753036437, "html": 71.1928251121, "stop": 2.1783754116400003, "languag": 2.29488291414, "media": 2.59369384088, "caus": 1.38521943984, "ani": 1.13383802314, "question": 2.20408163265, "canada": 3.0367253251700004, "from": 1.00056721497, "work": 1.11520089913, "num": 1.00031504001, "heurist": 115.043478261, "granular": 160.363636364, "synonym": 14.8373831776, "idea": 2.0930784443, "miss": 3.53664513255, "for": 1.00031504001, "depend": 2.2411067193700003, "predict": 5.18484650555, "friend": 2.20194174757, "match": 3.5676404494400002, "book": 1.43414634146, "she": 2.16, "much": 1.1942229577299999, "steer": 18.0, "are": 1.02990593578, "topic": 5.457545548300001, "shed": 16.835630965, "better": 2.0065722952500002, "meaning": 21.8076923077, "preprocess": 1221.23076923, "look": 1.9086318826599997, "sparsiti": 1443.27272727, "root": 3.57809330629, "refer": 1.30024570025, "extract": 7.703056768560001, "surfac": 3.97396745932, "lemmat": 1587.6, "combin": 1.69760479042, "pars": 145.651376147, "amount": 2.27027027027, "focus": 2.01012914662, "educ": 2.00733341763, "toward": 1.6303142329, "stopword": 1587.6, "compar": 1.8662278123900002, "remov": 2.0058117498400003, "map": 4.0728578758300005, "and": 1.00006299213, "doe": 1.70581282905, "lowercas": 162.0, "convers": 3.3486606201200004, "give": 1.3653250774, "proper": 3.3388012618299996, "probabl": 2.64555907349, "train": 1.9365698950999999, "great": 1.26592775696, "goal": 3.28152128979, "framework": 8.200413223139998, "extrem": 2.36602086438, "librari": 2.68266306185, "note": 1.42449528937, "rulebas": 1587.6, "oppos": 2.51282051282, "analyz": 9.68639414277, "scientist": 4.69426374926, "notebook": 40.1924050633, "into": 1.01502461479, "paper": 2.6628648104700003, "relev": 6.938811188810001, "effect": 1.3963060686000002, "abov": 1.90382539873, "set": 1.18707940781, "canon": 10.8220858896, "gave": 1.85121268657, "what": 1.25343439128, "rememb": 4.88793103448, "allow": 1.2716059271100002, "particular": 1.3814827706200001, "help": 1.39962972759, "neuralnetwork": 1587.6, "larg": 1.18574949585, "find": 1.7294117647099998, "interfer": 8.52173913043, "instead": 1.59461631177, "consider": 2.29920347574, "even": 1.16461267606, "own": 1.17844418052, "base": 1.14628158845, "natur": 1.5392670157100001, "deal": 2.18346857379, "fact": 1.73375559681, "who": 1.06279287723, "decid": 1.9257641921400002, "bare": 8.815102720710001, "easiest": 64.8, "preserv": 3.1062414400300002, "post": 2.23826307627, "here": 2.42307692308, "buggi": 144.327272727, "ideal": 4.65571847507, "veri": 1.25880114177, "punctuat": 42.1114058355, "microsoft": 24.8450704225, "quantit": 27.803852889699996, "blog": 14.1876675603, "clean": 6.86975335353, "human": 1.8965476048299998, "section": 2.1284354471099998, "layer": 8.14153846154, "spare": 9.997481108310001, "contain": 1.59814777532, "noisi": 62.2588235294, "about": 1.06486015159, "evid": 2.24872521246, "intuit": 27.7068062827, "which": 1.005191845, "neural": 59.4606741573, "mind": 3.5918552036199998, "context": 4.25972632144, "fake": 18.290322580599998, "just": 1.33580143037, "softwar": 10.2624434389, "tweet": 92.8421052632, "solv": 7.26923076923, "correct": 3.6631287494199998, "him": 1.63434218653, "simpli": 2.5192002538900002, "repost": 933.882352941, "gooood": 1587.6, "classifi": 5.2937645882, "usa": 4.0572450805, "weak": 4.70539419087, "verb": 29.291512915100004, "narrow": 5.53363541304, "where": 1.06715063521, "overhead": 24.054545454499998, "weight": 4.878918254459999, "special": 1.4881889763799998, "sms": 139.263157895, "few": 1.31729173581, "latter": 2.34159292035, "chunk": 81.0, "articl": 2.01805008262, "them": 1.09876115994, "then": 1.08657860516, "retweet": 1587.6, "dictionari": 5.2292490118599995, "keep": 2.04245465071, "systemat": 8.338235294119999, "order": 1.24625166811, "expand": 2.2260235558000003, "lose": 3.0851146521599997, "digit": 4.416133518780001, "quit": 2.8849718335500003, "health": 2.71570304482, "abl": 1.8208510150200001, "critic": 1.67010309278, "realiz": 4.89244992296, "over": 1.02525024217, "step": 2.8279301745599996, "generat": 2.05275407292, "build": 1.6341739578, "analysi": 3.47852760736, "preestablish": 1587.6, "provid": 1.21552714187, "creativ": 6.52259654889, "algorithm": 27.9507042254, "unstructur": 214.54054054099998, "consist": 1.4901445466499998, "unfortun": 9.966101694919999, "same": 1.11857958148, "sklearn": 1587.6, "show": 1.26703910615, "character": 4.563380281690001, "there": 1.04091266719, "scienc": 2.31969608416, "inform": 1.5753125620200001, "nightmar": 31.625498008, "therebi": 6.47735618115, "known": 1.0859097127200001, "possibl": 1.4173734488, "messag": 4.96745932416, "top": 1.8387769284200002, "misspel": 132.3, "right": 1.4054532577899999, "custom": 3.6346153846199996, "regex": 1587.6, "setup": 34.1419354839, "take": 1.13961668222, "spell": 6.75, "previous": 1.42846859816, "now": 1.160780873, "strategi": 4.44208170118, "tri": 1.8544562551099997, "dummi": 48.1090909091, "discov": 2.52160101652, "code": 3.8807137619199996, "blame": 6.47207501019, "high": 1.14777327935, "basic": 2.7301805675, "hold": 1.6551292744, "aka": 20.834645669300002, "issu": 1.43921675279, "physician": 8.76158940397, "also": 1.01476510067, "typic": 2.2541530597799997, "person": 1.40520446097, "simplest": 28.0494699647, "everyth": 4.81967213115, "mean": 1.44906900329, "one": 1.00627495722, "notic": 4.36994219653, "task": 3.88641370869, "role": 1.55327267391, "technolog": 2.6034765496900003, "signific": 1.4529147982100001, "the": 1.0, "may": 1.05201775893, "becom": 1.12492028626, "piec": 3.24132298898, "noncrit": 1587.6, "ganesan": 1587.6, "peopl": 1.21320495186, "occurr": 13.805217391300001, "throw": 8.39555790587, "those": 1.19548192771, "machin": 4.02433460076, "want": 1.99698113208, "convert": 3.2740771293099997, "peel": 36.8352668213, "system": 1.38739840951, "behind": 2.0845588235299997, "howev": 1.0945191313299998, "essenti": 2.9280708225700005, "various": 1.3323262839899999, "although": 1.14968498805, "elimin": 3.67670217693, "actual": 1.87482286254, "were": 1.02458857696, "snippet": 135.692307692, "ident": 2.80792359392, "estim": 2.34991119005, "other": 1.00992366412, "appli": 2.2972073506, "techniqu": 3.7293868921800004, "might": 2.1561863370900003, "raw": 10.6478873239, "purpos": 2.23416830847, "eleg": 20.9169960474, "impact": 2.97526236882, "hope": 2.50884955752, "spellingcorrect": 1587.6, "see": 1.27242125511, "thumb": 33.3529411765, "margin": 6.16783216783, "becaus": 1.1495184997499999, "choos": 4.17899447223, "signifi": 16.0363636364, "sourc": 1.69760479042, "webbas": 1587.6, "get": 1.78562591385, "like": 1.14918566775, "ebay": 140.495575221, "chines": 4.270037654649999, "imagin": 6.598503740650001, "file": 3.7710213776699995, "detect": 5.41288782816, "process": 1.69524826482, "transform": 3.42007755278, "analyt": 17.256521739100002, "that": 1.00398406375, "includ": 1.0190641247799999, "rule": 1.7415533128599998, "phrase": 6.18465134398, "this": 1.00379362671, "oov": 1587.6, "clinic": 10.1185468451, "decent": 35.5964125561, "variat": 4.704, "engin": 2.47135740971, "back": 1.26070038911, "perform": 1.5313977042500002, "project": 1.7534791252500002, "differ": 1.23654490225, "embed": 16.835630965, "been": 1.0239277652399998, "statist": 4.24265098878, "most": 1.02096463023, "low": 2.13072070863, "hashtag": 496.125, "domain": 9.39408284024, "java": 31.625498008, "all": 1.01146788991, "charact": 2.51720310766, "input": 12.2029208301, "action": 1.81855670103, "written": 1.9573418813999999, "augment": 16.5202913632, "expertis": 20.0201765448, "program": 2.02139037433, "reserv": 3.24198488871, "verifi": 14.2258064516, "recent": 1.54405757635, "expans": 3.4770039421800005, "case": 1.48498737256, "onesizefitsal": 1587.6, "produc": 1.36932896326, "normal": 2.61075481006, "overlook": 11.4051724138, "unneed": 453.6, "architectur": 5.12790697674, "two": 1.01379310345, "more": 1.0171706817, "improv": 2.04376930999, "partofspeech": 1587.6, "list": 1.36321483771, "type": 2.0281042411900003, "interfac": 20.9169960474, "ughh": 1587.6, "vocabulari": 23.2785923754, "these": 1.07415426252, "benefit": 3.06841901817, "keyword": 139.263157895, "general": 1.1218202374200001, "made": 1.07038834951, "expect": 2.20011086475, "semant": 39.1034482759, "befor": 1.10036041031, "comment": 3.05954904606, "none": 4.06555697823, "standard": 1.8915763135900003, "can": 1.17626139142, "avail": 1.7288467821, "wordnet": 1221.23076923, "make": 1.0762660158600001, "queri": 56.2978723404, "way": 1.2190739461, "search": 3.2539454806299997, "onli": 1.0256476516600002, "each": 1.18974820144, "size": 2.49387370405, "situat": 2.06611140031, "bring": 2.03616775683, "thing": 2.4065484311099996, "depth": 8.24299065421, "read": 2.3149606299200003, "pretti": 15.75, "advanc": 1.9997480791, "common": 1.4025974025999999, "say": 1.7544480053, "data": 3.37643555934, "reduc": 1.98698372966, "address": 2.86157173756, "method": 2.5714285714300003, "compound": 8.09586945436, "level": 1.6544393497299998, "bio": 42.336000000000006, "come": 1.32831325301, "must": 1.9220338983099996, "exampl": 1.50483412322, "kavita": 1323.0, "worth": 5.210370856580001, "text": 3.12827586207, "lemma": 269.084745763, "run": 1.55692850838, "direct": 1.22226499346, "import": 1.3401992233700002, "first": 1.00761614623, "mixedcas": 1587.6, "could": 1.2043695949, "have": 1.0148948411399998, "while": 1.0441988950299999, "social": 1.9904714142400002, "except": 1.71948445792, "down": 1.35889754344, "learn": 2.32275054865, "garbageingarbageout": 1587.6, "when": 1.02076769755, "mention": 2.53894130817, "effici": 5.09335899904}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  All you need to know about text preprocessing for NLP and Machine Learning</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb All you need to know about text preprocessing for NLP and Machine Learning Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/poll-data-science-machine-learning-methods-algorithms-use-2018-2019.html\" rel=\"prev\" title=\"Which Data Science / Machine Learning methods and algorithms did you use in 2018/2019 for a real-world application?\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/pivigo-s2ds-data-science-bootcamp.html\" rel=\"next\" title=\"S2DS, a 5-week data science bootcamp helping analytical PhDs transition from academia to industry\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=92644\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-92644 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 9-Apr, 2019  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/index.html\">Apr</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/tutorials.html\">Tutorials, Overviews</a> \u00bb All you need to know about text preprocessing for NLP and Machine Learning (\u00a0<a href=\"/2019/n14.html\">19:n14</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">All you need to know about text preprocessing for NLP and Machine Learning</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2019/04/poll-data-science-machine-learning-methods-algorithms-use-2018-2019.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/04/pivigo-s2ds-data-science-bootcamp.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/data-preprocessing\" rel=\"tag\">Data Preprocessing</a>, <a href=\"https://www.kdnuggets.com/tag/machine-learning\" rel=\"tag\">Machine Learning</a>, <a href=\"https://www.kdnuggets.com/tag/nlp\" rel=\"tag\">NLP</a>, <a href=\"https://www.kdnuggets.com/tag/python\" rel=\"tag\">Python</a>, <a href=\"https://www.kdnuggets.com/tag/text-analysis\" rel=\"tag\">Text Analysis</a>, <a href=\"https://www.kdnuggets.com/tag/text-mining\" rel=\"tag\">Text Mining</a></div>\n<br/>\n<p class=\"excerpt\">\n     We present a comprehensive introduction to text preprocessing, covering the different techniques including stemming, lemmatization, noise removal, normalization, with examples and explanations into when you should use each of them.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"http://kavita-ganesan.com/\">Kavita Ganesan</a>, Data Scientist</b>.</p>\n<p>Based on some recent conversations, I realized that text preprocessing is a severely overlooked topic. A few people I spoke to mentioned inconsistent results from their NLP applications only to realize that they were not preprocessing their text or were using the wrong kind of text preprocessing for their project.</p>\n<p>With that in mind, I thought of shedding some light around what text preprocessing really is, the different methods of text preprocessing, and a way to estimate how much preprocessing you may need. For those interested, I\u2019ve also made some\u00a0<a data-href=\"https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing\" href=\"https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing\">text preprocessing code snippets</a>\u00a0for you to try. Now, let\u2019s get started!</p>\n<h3>What is text preprocessing?</h3>\n<p>To preprocess your text simply means to bring your text into a form that is\u00a0<em><strong>predictable</strong></em><strong>\u00a0</strong>and\u00a0<em><strong>analyzable</strong></em><em>\u00a0</em>for your task. A task here is a combination of approach and domain. For example, extracting top keywords with TF-IDF (approach) from Tweets (domain) is an example of a\u00a0<em>Task</em>.</p>\n<blockquote><p><em>Task = approach +\u00a0domain</em></p></blockquote>\n<p>One task\u2019s ideal preprocessing can become another task\u2019s worst nightmare. So take note: text preprocessing is not directly transferable from task to task.</p>\n<p>Let\u2019s take a very simple example, let\u2019s say you are trying to discover commonly used words in a news dataset. If your pre-processing step involves removing\u00a0<a data-href=\"http://kavita-ganesan.com/what-are-stop-words/\" href=\"http://kavita-ganesan.com/what-are-stop-words/\">stop words</a>\u00a0because some other task used it, then you are probably going to miss out on some of the common words as you have ALREADY eliminated it. So really, it\u2019s not a one-size-fits-all approach.</p>\n<h3>Types of text preprocessing techniques</h3>\n<p>There are different ways to preprocess your text. Here are some of the approaches that you should know about and I will try to highlight the importance of each.</p>\n<h4>Lowercasing</h4>\n<p>Lowercasing ALL your text data, although commonly overlooked, is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output.</p>\n<p>Quite recently, one of my blog readers trained a\u00a0<a data-href=\"http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/\" href=\"http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/\">word embedding model for similarity lookups</a>. He found that different variation in input capitalization (e.g. \u2018Canada\u2019 vs. \u2018canada\u2019) gave him different types of output or no output at all. This was probably happening because the dataset had mixed-case occurrences of the word \u2018Canada\u2019 and there was insufficient evidence for the neural-network to effectively learn the weights for the less common version. This type of issue is bound to happen when your dataset is fairly small, and lowercasing is a great way to deal with sparsity issues.</p>\n<p>Here is an example of how lowercasing solves the sparsity issue, where the same words with different cases map to the same lowercase form:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*M8cSL8_1Qu8HNk3eaIX_-w.png\" width=\"100%\"><br>\n<strong>Word with different cases all map to the same lowercase form</strong></br></img></p>\n<p>Another example where lowercasing is very useful is for search. Imagine, you are looking for documents containing \u201cusa\u201d. However, no results were showing up because \u201cusa\u201d was indexed as\u00a0<strong>\u201cUSA\u201d.</strong>\u00a0Now, who should we blame? The U.I. designer who set-up the interface or the engineer who set-up the search index?</p>\n<p>While lowercasing should be standard practice, I\u2019ve also had situations where preserving the capitalization was important. For example, in predicting the programming language of a source code file. The word\u00a0System\u00a0in Java is quite different from\u00a0system\u00a0in python. Lowercasing the two makes them identical, causing the classifier to lose important predictive features. While lowercasing\u00a0<em>is generally\u00a0</em>helpful, it may not be applicable for all tasks.</p>\n<h4>Stemming</h4>\n<p>Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The \u201croot\u201d in this case may not be a real root word, but just a canonical form of the original word.</p>\n<p>Stemming uses a crude heuristic process that chops off the ends of words in the hope of correctly transforming words into its root form. So the words \u201ctrouble\u201d, \u201ctroubled\u201d and \u201ctroubles\u201d might actually be converted to\u00a0troublinstead of\u00a0trouble\u00a0because the ends were just chopped off (ughh, how crude!).</p>\n<p>There are different algorithms for stemming. The most common algorithm, which is also known to be empirically effective for English, is\u00a0<a data-href=\"https://tartarus.org/martin/PorterStemmer/\" href=\"https://tartarus.org/martin/PorterStemmer/\">Porters Algorithm</a>. Here is an example of stemming in action with Porter Stemmer:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*9KLZmtSh-t6SfEGX_nbznA.png\" width=\"100%\"/></p>\n<p><strong>Effects of stemming inflected words</strong></p>\n<p>Stemming is useful for dealing with sparsity issues as well as standardizing vocabulary. I\u2019ve had success with stemming in search applications in particular. The idea is that, if say you search for \u201cdeep learning classes\u201d, you also want to surface documents that mention \u201cdeep learning\u00a0<em><strong>class</strong></em>\u201d as well as \u201cdeep\u00a0<em><strong>learn</strong></em>\u00a0classes\u201d, although the latter doesn\u2019t sound right. But you get where we are going with this. You want to match all variations of a word to bring up the most relevant documents.</p>\n<p><span data-creator-ids=\"anon\">In most of my previous text classification work however, stemming only marginally helped improved classification accuracy as opposed to using better engineered features and text enrichment approaches such as using word embeddings.</span></p>\n<h4>Lemmatization</h4>\n<p>Lemmatization on the surface is very similar to stemming, where the goal is to remove inflections and map a word to its root form. The only difference is that, lemmatization tries to do it the proper way. It doesn\u2019t just chop things off, it actually transforms words to the actual root. For example, the word \u201cbetter\u201d would map to \u201cgood\u201d. It may use a dictionary such as\u00a0<a data-href=\"https://www.nltk.org/_modules/nltk/stem/wordnet.html\" href=\"https://www.nltk.org/_modules/nltk/stem/wordnet.html\">WordNet for mappings</a>\u00a0or some special\u00a0<a data-href=\"https://www.semanticscholar.org/paper/A-Rule-based-Approach-to-Word-Lemmatization-Plisson-Lavrac/5319539616e81b02637b1bf90fb667ca2066cf14\" href=\"https://www.semanticscholar.org/paper/A-Rule-based-Approach-to-Word-Lemmatization-Plisson-Lavrac/5319539616e81b02637b1bf90fb667ca2066cf14\">rule-based approaches</a>. Here is an example of lemmatization in action using a WordNet-based approach:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*z4f7My5peI28lNpZdHk_Iw.png\" width=\"100%\"/></p>\n<p>Effects of Lemmatization with\u00a0WordNet</p>\n<p>In my experience, lemmatization provides no significant benefit over stemming for search and text classification purposes. In fact, depending on the algorithm you choose, it could be much slower compared to using a very basic stemmer and you may have to know the part-of-speech of the word in question in order to get a correct lemma.\u00a0<a data-href=\"https://arxiv.org/pdf/1707.01780.pdf\" href=\"https://arxiv.org/pdf/1707.01780.pdf\">This paper</a>\u00a0finds that lemmatization has no significant impact on accuracy for text classification with neural architectures.</p>\n<p>I would personally use lemmatization sparingly. The additional overhead may or may not be worth it. But you could always try it to see the impact it has on your performance metric.</p>\n<h4>Stopword Removal</h4>\n<p>Stop words are a set of commonly used words in a language. Examples of stop words in English are \u201ca\u201d, \u201cthe\u201d, \u201cis\u201d, \u201care\u201d and etc. The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead.</p>\n<p>For example, in the context of a search system, if your search query is<em>\u00a0\u201cwhat is text preprocessing?\u201d</em>, you want the search system to focus on surfacing documents that talk about\u00a0text preprocessing\u00a0over documents that talk about\u00a0what is. This can be done by preventing all words from your stop word list from being analyzed. Stop words are commonly applied in search systems, text classification applications, topic modeling, topic extraction and others.</p>\n<p>In my experience, stop word removal, while effective in search and topic extraction systems, showed to be non-critical in classification systems. However, it does help reduce the number of features in consideration which helps keep your models decently sized.</p>\n<p>Here is an example of stop word removal in action. All stop words are replaced with a dummy character,\u00a0<strong>W</strong>:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*lGARQmFv13hj504Lua0AeA.png\" width=\"100%\"/></p>\n<p><strong>Sentence before and after stop word\u00a0removal</strong></p>\n<p><a data-href=\"http://kavita-ganesan.com/what-are-stop-words/\" href=\"http://kavita-ganesan.com/what-are-stop-words/\">Stop word lists</a>\u00a0can come from pre-established sets or you can create a\u00a0<a data-href=\"http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/\" href=\"http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/\">custom one for your domain</a>. Some libraries (e.g. sklearn) allow you to remove words that appeared in X% of your documents, which can also give you a stop word removal effect.</p>\n<h4>Normalization</h4>\n<p>A highly overlooked preprocessing step is text normalization. Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word \u201cgooood\u201d and \u201cgud\u201d can be transformed to \u201cgood\u201d, its canonical form. Another example is mapping of near identical words such as \u201cstopwords\u201d, \u201cstop-words\u201d and \u201cstop words\u201d to just \u201cstopwords\u201d.</p>\n<p>Text normalization is important for noisy texts such as social media comments, text messages and comments to blog posts where abbreviations, misspellings and use of out-of-vocabulary words (oov) are prevalent.\u00a0<a data-href=\"https://sentic.net/microtext-normalization.pdf\" href=\"https://sentic.net/microtext-normalization.pdf\">This paper</a>\u00a0showed that by using a text normalization strategy for Tweets, they were able to improve sentiment classification accuracy by ~4%.</p>\n<p>Here\u2019s an example of words before and after normalization:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*e7Y-Rbxky5i2U6awCQILVQ.png\" width=\"100%\"/></p>\n<p><strong>Effects of Text Normalization</strong></p>\n<p>Notice how the variations, map to the same canonical form.</p>\n<p>In my experience, text normalization has even been effective for analyzing\u00a0<a data-href=\"http://kavita-ganesan.com/general-supervised-approach-segmentation-clinical-texts/\" href=\"http://kavita-ganesan.com/general-supervised-approach-segmentation-clinical-texts/\">highly unstructured clinical texts</a>\u00a0where physicians take notes in non-standard ways. I\u2019ve also found it useful for\u00a0<a data-href=\"https://githubengineering.com/topics/\" href=\"https://githubengineering.com/topics/\">topic extraction</a>\u00a0where near synonyms and spelling differences are common (e.g. topic modelling, topic modeling, topic-modeling, topic-modelling).</p>\n<p>Unfortunately, unlike stemming and lemmatization, there isn\u2019t a standard way to normalize texts. It typically depends on the task. For example, the way you would normalize clinical texts would arguably be different from how you normalize sms text messages.</p>\n<p>Some common approaches to text normalization include dictionary mappings (easiest), statistical machine translation (SMT) and spelling-correction based approaches.\u00a0<a data-href=\"https://nlp.stanford.edu/courses/cs224n/2009/fp/27.pdf\" href=\"https://nlp.stanford.edu/courses/cs224n/2009/fp/27.pdf\">This interesting article</a>\u00a0compares the use of a dictionary based approach and a SMT approach for normalizing text messages.</p>\n<h4>Noise Removal</h4>\n<p>Noise removal is about removing\u00a0characters\u00a0digits\u00a0and\u00a0pieces of text that can interfere with your text analysis. Noise removal is one of the most essential text preprocessing steps. It is also highly domain dependent.</p>\n<p>For example, in Tweets, noise could be all special characters except hashtags as it signifies concepts that can characterize a Tweet. The problem with noise is that it can produce results that are inconsistent in your downstream tasks. Let\u2019s take the example below:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*5WeDzBXnCmInni9PfKFM_g.png\" width=\"100%\"/></p>\n<p><strong>Stemming\u00a0without\u00a0Noise\u00a0Removal</strong></p>\n<p>Notice that all the raw words above have some surrounding noise in them. If you stem these words, you can see that the stemmed result does not look very pretty. None of them have a correct stem. However, with some cleaning as applied in\u00a0<a href=\"https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/text-pre-processing/Text%20Pre-Processing%20Examples.ipynb\">this notebook</a>, the results now look much better:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*481aP2Cf-nQ-u8g60PfjUg.png\" width=\"100%\"/></p>\n<p>Stemming\u00a0<strong>with</strong>\u00a0Noise\u00a0Removal</p>\n<p>Noise removal is one of the first things you should be looking into when it comes to Text Mining and NLP. There are various ways to remove noise. This includes\u00a0<em>punctuation removal</em>,\u00a0<em>special character removal</em>,\u00a0<em>numbers removal, html formatting removal, domain specific keyword removal</em>\u00a0<em>(e.g. \u2018RT\u2019 for retweet), source code removal, header removal</em>\u00a0and more. It all depends on which domain you are working in and what entails noise for your task. The\u00a0<a data-href=\"https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing\" href=\"https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing\">code snippet in my notebook</a>\u00a0shows how to do some basic noise removal.</p>\n<h4>Text Enrichment / Augmentation</h4>\n<p>Text enrichment involves augmenting your original text data with information that you did not previously have. Text enrichment provides more semantics to your original text, thereby improving its predictive power and the depth of analysis you can perform on your data.</p>\n<p>In an information retrieval example, expanding a user\u2019s query to improve the matching of keywords is a form of augmentation. A query like\u00a0text mining could become\u00a0text document mining analysis. While this doesn\u2019t make sense to a human, it can help fetch documents that are more relevant.</p>\n<p>You can get really creative with how you enrich your text. You can use\u00a0<a data-href=\"https://en.wikipedia.org/wiki/Part-of-speech_tagging\" href=\"https://en.wikipedia.org/wiki/Part-of-speech_tagging\"><strong>part-of-speech tagging</strong></a>\u00a0to get more granular information about the words in your text.</p>\n<p>For example, in a document classification problem, the appearance of the word\u00a0<strong>book</strong>\u00a0as a\u00a0<strong>noun</strong>\u00a0could result in a different classification than\u00a0<strong>book</strong>\u00a0as a\u00a0<strong>verb</strong>\u00a0as one is used in the context of reading and the other is used in the context of reserving something.\u00a0<a data-href=\"http://www.iapr-tc11.org/archive/icdar2011/fileup/PDF/4520a920.pdf\" href=\"http://www.iapr-tc11.org/archive/icdar2011/fileup/PDF/4520a920.pdf\">This article</a>\u00a0talks about how Chinese text classification is improved with a combination of nouns and verbs as input features.</p>\n<p>With the availability of large amounts texts however, people have started using\u00a0<a data-href=\"https://en.wikipedia.org/wiki/Word_embedding\" href=\"https://en.wikipedia.org/wiki/Word_embedding\">embeddings</a>\u00a0to enrich the meaning of words, phrases and sentences for classification, search, summarization and text generation in general. This is especially true in deep learning based NLP approaches where a\u00a0<a data-href=\"https://keras.io/layers/embeddings/\" href=\"https://keras.io/layers/embeddings/\">word level embedding layer</a>\u00a0is quite common. You can either start with\u00a0<a data-href=\"https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\" href=\"https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\">pre-established embeddings</a>\u00a0or create your own and use it in downstream tasks.</p>\n<p>Other ways to enrich your text data include\u00a0<a data-href=\"http://kavita-ganesan.com/how-to-incorporate-phrases-into-word2vec-a-text-mining-approach/#.XHCcJ1xKg2w\" href=\"http://kavita-ganesan.com/how-to-incorporate-phrases-into-word2vec-a-text-mining-approach/#.XHCcJ1xKg2w\">phrase extraction</a>, where you recognize compound words as one (aka chunking),\u00a0<a data-href=\"http://aclweb.org/anthology/R09-1073\" href=\"http://aclweb.org/anthology/R09-1073\">expansion with synonyms</a>and\u00a0<a data-href=\"http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/slides/15-DP.pdf\" href=\"http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/slides/15-DP.pdf\">dependency parsing</a>.</p>\n<h3>Do you need it\u00a0all?</h3>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*_VjLxnzsMajCB8HlWdv6cQ.jpeg\" width=\"100%\"/></p>\n<p>Not really, but you do have to do some of it for sure if you want good, consistent results. To give you an idea of what the bare minimum should be, I\u2019ve broken it down to\u00a0<em><strong>Must Do</strong></em>,\u00a0<em><strong>Should Do</strong></em>\u00a0and\u00a0<em><strong>Task Dependent</strong></em>. Everything that falls under task dependent can be quantitatively or qualitatively tested before deciding you actually need it.</p>\n<p>Remember, less is more and you want to keep your approach as elegant as possible. The more overhead you add, the more layers you will have to peel back when you run into issues.</p>\n<h3>Must Do:</h3>\n<ul>\n<li>Noise removal</li>\n<li>Lowercasing (can be task dependent in some cases)</li>\n</ul>\n<h3>Should Do:</h3>\n<ul>\n<li>Simple normalization\u200a\u2014\u200a(e.g. standardize near identical words)</li>\n</ul>\n<h3>Task Dependent:</h3>\n<ol>\n<li>Advanced normalization (e.g. addressing out-of-vocabulary words)</li>\n<li>Stop-word removal</li>\n<li>Stemming / lemmatization</li>\n<li>Text enrichment / augmentation</li>\n</ol>\n<p>So, for any task, the minimum you should do is try to lowercase your text and remove noise. What entails noise depends on your domain (see section on Noise Removal). You can also do some basic normalization steps for more consistency and then systematically add other layers as you see fit.</p>\n<h3>General Rule of\u00a0Thumb</h3>\n<p>Not all tasks need the same level of preprocessing. For some tasks, you can get away with the minimum. However, for others, the dataset is so noisy that, if you don\u2019t preprocess enough, it\u2019s going to be garbage-in-garbage-out.</p>\n<p>Here\u2019s a general rule of thumb. This will not always hold true, but works for most cases. If you have a lot of well written texts to work with in a fairly general domain, then preprocessing is not extremely critical; you can get away with the bare minimum (e.g. training a word embedding model using all of Wikipedia texts or Reuters news articles).</p>\n<p>However, if you are working in a very narrow domain (e.g. Tweets about health foods) and data is sparse and noisy, you could benefit from more preprocessing layers, although each layer you add (e.g. stop word removal, stemming, normalization) needs to be quantitatively or qualitatively verified as a meaningful layer. Here\u2019s a table that summarizes how much preprocessing you should be performing on your text data</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*VRgrWGLBJJDXsQE72QyI3g.png\" width=\"100%\"/></p>\n<p>I hope the ideas here steer you towards the right preprocessing steps for your projects. Remember,\u00a0<em>less is more</em>. A friend of mine once mentioned to me how he made a large e-commerce search system more efficient and less buggy just by throwing out layers of\u00a0<em>unneeded</em>\u00a0preprocessing.</p>\n<h3>Resources</h3>\n<ul>\n<li><a data-href=\"https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/text-pre-processing/Text%20Preprocessing%20Examples.ipynb\" href=\"https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/text-pre-processing/Text%20Preprocessing%20Examples.ipynb\">Python code for basic text preprocessing using NLTK and regex</a></li>\n<li><a data-href=\"http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/\" href=\"http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/\">Constructing custom stop word lists</a></li>\n<li><a data-href=\"https://kavgan.github.io/phrase-at-scale/\" href=\"https://kavgan.github.io/phrase-at-scale/\">Source code for phrase extraction</a></li>\n</ul>\n<h3>References</h3>\n<ul>\n<li>For an updated list of papers, please see\u00a0<a data-href=\"http://kavita-ganesan.com/text-preprocessing-tutorial/#Relevant-Papers\" href=\"http://kavita-ganesan.com/text-preprocessing-tutorial/#Relevant-Papers\">my original article</a></li>\n</ul>\n<p>Bio: <a href=\"http://kavita-ganesan.com/about-me/\">Kavita Ganesan</a> is a Data Scientist with expertise in Natural Language Processing, Text Mining, Search and Machine Learning. Over the last decade, she worked for various technology organizations including GitHub (Microsoft), 3M Health Information Systems and eBay.</p>\n<p><a href=\"https://medium.freecodecamp.org/all-you-need-to-know-about-text-preprocessing-for-nlp-and-machine-learning-bc1c5765ff67\">Original</a>. Reposted with permission.</p>\n<p><strong>Resources:</strong></p>\n<ul>\n<li><a href=\"https://www.kdnuggets.com/education/online.html\">On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning education</a></li>\n<li><a href=\"https://www.kdnuggets.com/software/index.html\">Software for Analytics, Data Science, Data Mining, and Machine Learning</a></li>\n</ul>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"https://www.kdnuggets.com/2019/04/nlp-pytorch.html\">Getting started with NLP using the PyTorch framework</a></li>\n<li><a href=\"https://www.kdnuggets.com/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html\">Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision</a></li>\n<li><a href=\"https://www.kdnuggets.com/2019/03/beyond-news-contents-role-of-social-context-for-fake-news-detection.html\">Beyond news contents: the role of social context for fake news detection</a></li>\n</ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets'; \n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2019/04/poll-data-science-machine-learning-methods-algorithms-use-2018-2019.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/04/pivigo-s2ds-data-science-bootcamp.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/index.html\">Apr</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/tutorials.html\">Tutorials, Overviews</a> \u00bb All you need to know about text preprocessing for NLP and Machine Learning (\u00a0<a href=\"/2019/n14.html\">19:n14</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556327597\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.707 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 21:13:17 -->\n<!-- Compression = gzip -->", "content_tokenized": ["comment", "kavita", "ganesan", "data", "scientist", "base", "some", "recent", "convers", "realiz", "that", "text", "preprocess", "sever", "overlook", "topic", "few", "peopl", "spoke", "mention", "inconsist", "result", "from", "their", "applic", "onli", "realiz", "that", "they", "were", "not", "preprocess", "their", "text", "were", "use", "the", "wrong", "kind", "text", "preprocess", "for", "their", "project", "with", "that", "mind", "thought", "shed", "some", "light", "around", "what", "text", "preprocess", "realli", "the", "differ", "method", "text", "preprocess", "and", "way", "estim", "how", "much", "preprocess", "may", "need", "for", "those", "interest", "also", "made", "some", "text", "preprocess", "code", "snippet", "for", "tri", "now", "let", "get", "start", "what", "text", "preprocess", "preprocess", "text", "simpli", "mean", "bring", "text", "into", "form", "that", "predict", "and", "analyz", "for", "task", "task", "here", "combin", "approach", "and", "domain", "for", "exampl", "extract", "top", "keyword", "with", "approach", "from", "tweet", "domain", "exampl", "task", "task", "approach", "domain", "one", "task", "ideal", "preprocess", "can", "becom", "anoth", "task", "worst", "nightmar", "take", "note", "text", "preprocess", "not", "direct", "transfer", "from", "task", "task", "let", "take", "veri", "simpl", "exampl", "let", "say", "are", "tri", "discov", "common", "use", "word", "news", "dataset", "preprocess", "step", "involv", "remov", "stop", "word", "becaus", "some", "other", "task", "use", "then", "are", "probabl", "miss", "out", "some", "the", "common", "word", "have", "elimin", "realli", "not", "onesizefitsal", "approach", "type", "text", "preprocess", "techniqu", "there", "are", "differ", "way", "preprocess", "text", "here", "are", "some", "the", "approach", "that", "should", "know", "about", "and", "will", "tri", "highlight", "the", "import", "each", "lowercas", "lowercas", "text", "data", "although", "common", "overlook", "one", "the", "simplest", "and", "most", "effect", "form", "text", "preprocess", "applic", "most", "text", "mine", "and", "problem", "and", "can", "help", "case", "where", "dataset", "not", "veri", "larg", "and", "signific", "help", "with", "consist", "expect", "output", "quit", "recent", "one", "blog", "reader", "train", "word", "embed", "model", "for", "similar", "lookup", "found", "that", "differ", "variat", "input", "capit", "canada", "canada", "gave", "him", "differ", "type", "output", "output", "all", "this", "probabl", "happen", "becaus", "the", "dataset", "had", "mixedcas", "occurr", "the", "word", "canada", "and", "there", "insuffici", "evid", "for", "the", "neuralnetwork", "effect", "learn", "the", "weight", "for", "the", "less", "common", "version", "this", "type", "issu", "bound", "happen", "when", "dataset", "fair", "small", "and", "lowercas", "great", "way", "deal", "with", "sparsiti", "issu", "here", "exampl", "how", "lowercas", "solv", "the", "sparsiti", "issu", "where", "the", "same", "word", "with", "differ", "case", "map", "the", "same", "lowercas", "form", "word", "with", "differ", "case", "all", "map", "the", "same", "lowercas", "form", "anoth", "exampl", "where", "lowercas", "veri", "use", "for", "search", "imagin", "are", "look", "for", "document", "contain", "usa", "howev", "result", "were", "show", "becaus", "usa", "index", "now", "who", "should", "blame", "the", "design", "who", "setup", "the", "interfac", "the", "engin", "who", "setup", "the", "search", "index", "while", "lowercas", "should", "standard", "practic", "also", "had", "situat", "where", "preserv", "the", "capit", "import", "for", "exampl", "predict", "the", "program", "languag", "sourc", "code", "file", "the", "word", "system", "java", "quit", "differ", "from", "system", "python", "lowercas", "the", "two", "make", "them", "ident", "caus", "the", "classifi", "lose", "import", "predict", "featur", "while", "lowercas", "general", "help", "may", "not", "applic", "for", "all", "task", "stem", "stem", "the", "process", "reduc", "inflect", "word", "troubl", "troubl", "their", "root", "form", "troubl", "the", "root", "this", "case", "may", "not", "real", "root", "word", "but", "just", "canon", "form", "the", "origin", "word", "stem", "use", "crude", "heurist", "process", "that", "chop", "off", "the", "end", "word", "the", "hope", "correct", "transform", "word", "into", "root", "form", "the", "word", "troubl", "troubl", "and", "troubl", "might", "actual", "convert", "troublinstead", "troubl", "becaus", "the", "end", "were", "just", "chop", "off", "ughh", "how", "crude", "there", "are", "differ", "algorithm", "for", "stem", "the", "most", "common", "algorithm", "which", "also", "known", "empir", "effect", "for", "english", "porter", "algorithm", "here", "exampl", "stem", "action", "with", "porter", "stemmer", "effect", "stem", "inflect", "word", "stem", "use", "for", "deal", "with", "sparsiti", "issu", "well", "standard", "vocabulari", "had", "success", "with", "stem", "search", "applic", "particular", "the", "idea", "that", "say", "search", "for", "deep", "learn", "class", "also", "want", "surfac", "document", "that", "mention", "deep", "learn", "class", "well", "deep", "learn", "class", "although", "the", "latter", "sound", "right", "but", "get", "where", "are", "with", "this", "want", "match", "all", "variat", "word", "bring", "the", "most", "relev", "document", "most", "previous", "text", "classif", "work", "howev", "stem", "onli", "margin", "help", "improv", "classif", "accuraci", "oppos", "use", "better", "engin", "featur", "and", "text", "enrich", "approach", "such", "use", "word", "embed", "lemmat", "lemmat", "the", "surfac", "veri", "similar", "stem", "where", "the", "goal", "remov", "inflect", "and", "map", "word", "root", "form", "the", "onli", "differ", "that", "lemmat", "tri", "the", "proper", "way", "just", "chop", "thing", "off", "actual", "transform", "word", "the", "actual", "root", "for", "exampl", "the", "word", "better", "would", "map", "good", "may", "use", "dictionari", "such", "wordnet", "for", "map", "some", "special", "rulebas", "approach", "here", "exampl", "lemmat", "action", "use", "wordnetbas", "approach", "effect", "lemmat", "with", "wordnet", "experi", "lemmat", "provid", "signific", "benefit", "over", "stem", "for", "search", "and", "text", "classif", "purpos", "fact", "depend", "the", "algorithm", "choos", "could", "much", "slower", "compar", "use", "veri", "basic", "stemmer", "and", "may", "have", "know", "the", "partofspeech", "the", "word", "question", "order", "get", "correct", "lemma", "this", "paper", "find", "that", "lemmat", "has", "signific", "impact", "accuraci", "for", "text", "classif", "with", "neural", "architectur", "would", "person", "use", "lemmat", "spare", "the", "addit", "overhead", "may", "may", "not", "worth", "but", "could", "alway", "tri", "see", "the", "impact", "has", "perform", "metric", "stopword", "remov", "stop", "word", "are", "set", "common", "use", "word", "languag", "exampl", "stop", "word", "english", "are", "the", "are", "and", "etc", "the", "intuit", "behind", "use", "stop", "word", "that", "remov", "low", "inform", "word", "from", "text", "can", "focus", "the", "import", "word", "instead", "for", "exampl", "the", "context", "search", "system", "search", "queri", "what", "text", "preprocess", "want", "the", "search", "system", "focus", "surfac", "document", "that", "talk", "about", "text", "preprocess", "over", "document", "that", "talk", "about", "what", "this", "can", "done", "prevent", "all", "word", "from", "stop", "word", "list", "from", "analyz", "stop", "word", "are", "common", "appli", "search", "system", "text", "classif", "applic", "topic", "model", "topic", "extract", "and", "other", "experi", "stop", "word", "remov", "while", "effect", "search", "and", "topic", "extract", "system", "show", "noncrit", "classif", "system", "howev", "doe", "help", "reduc", "the", "number", "featur", "consider", "which", "help", "keep", "model", "decent", "size", "here", "exampl", "stop", "word", "remov", "action", "all", "stop", "word", "are", "replac", "with", "dummi", "charact", "sentenc", "befor", "and", "after", "stop", "word", "remov", "stop", "word", "list", "can", "come", "from", "preestablish", "set", "can", "creat", "custom", "one", "for", "domain", "some", "librari", "sklearn", "allow", "remov", "word", "that", "appear", "document", "which", "can", "also", "give", "stop", "word", "remov", "effect", "normal", "high", "overlook", "preprocess", "step", "text", "normal", "text", "normal", "the", "process", "transform", "text", "into", "canon", "standard", "form", "for", "exampl", "the", "word", "gooood", "and", "gud", "can", "transform", "good", "canon", "form", "anoth", "exampl", "map", "near", "ident", "word", "such", "stopword", "stopword", "and", "stop", "word", "just", "stopword", "text", "normal", "import", "for", "noisi", "text", "such", "social", "media", "comment", "text", "messag", "and", "comment", "blog", "post", "where", "abbrevi", "misspel", "and", "use", "outofvocabulari", "word", "oov", "are", "preval", "this", "paper", "show", "that", "use", "text", "normal", "strategi", "for", "tweet", "they", "were", "abl", "improv", "sentiment", "classif", "accuraci", "num", "here", "exampl", "word", "befor", "and", "after", "normal", "effect", "text", "normal", "notic", "how", "the", "variat", "map", "the", "same", "canon", "form", "experi", "text", "normal", "has", "even", "been", "effect", "for", "analyz", "high", "unstructur", "clinic", "text", "where", "physician", "take", "note", "nonstandard", "way", "also", "found", "use", "for", "topic", "extract", "where", "near", "synonym", "and", "spell", "differ", "are", "common", "topic", "model", "topic", "model", "topicmodel", "topicmodel", "unfortun", "unlik", "stem", "and", "lemmat", "there", "standard", "way", "normal", "text", "typic", "depend", "the", "task", "for", "exampl", "the", "way", "would", "normal", "clinic", "text", "would", "arguabl", "differ", "from", "how", "normal", "sms", "text", "messag", "some", "common", "approach", "text", "normal", "includ", "dictionari", "map", "easiest", "statist", "machin", "translat", "and", "spellingcorrect", "base", "approach", "this", "interest", "articl", "compar", "the", "use", "dictionari", "base", "approach", "and", "approach", "for", "normal", "text", "messag", "nois", "remov", "nois", "remov", "about", "remov", "charact", "digit", "and", "piec", "text", "that", "can", "interfer", "with", "text", "analysi", "nois", "remov", "one", "the", "most", "essenti", "text", "preprocess", "step", "also", "high", "domain", "depend", "for", "exampl", "tweet", "nois", "could", "all", "special", "charact", "except", "hashtag", "signifi", "concept", "that", "can", "character", "tweet", "the", "problem", "with", "nois", "that", "can", "produc", "result", "that", "are", "inconsist", "downstream", "task", "let", "take", "the", "exampl", "below", "stem", "without", "nois", "remov", "notic", "that", "all", "the", "raw", "word", "abov", "have", "some", "surround", "nois", "them", "stem", "these", "word", "can", "see", "that", "the", "stem", "result", "doe", "not", "look", "veri", "pretti", "none", "them", "have", "correct", "stem", "howev", "with", "some", "clean", "appli", "this", "notebook", "the", "result", "now", "look", "much", "better", "stem", "with", "nois", "remov", "nois", "remov", "one", "the", "first", "thing", "should", "look", "into", "when", "come", "text", "mine", "and", "there", "are", "various", "way", "remov", "nois", "this", "includ", "punctuat", "remov", "special", "charact", "remov", "number", "remov", "html", "format", "remov", "domain", "specif", "keyword", "remov", "for", "retweet", "sourc", "code", "remov", "header", "remov", "and", "more", "all", "depend", "which", "domain", "are", "work", "and", "what", "entail", "nois", "for", "task", "the", "code", "snippet", "notebook", "show", "how", "some", "basic", "nois", "remov", "text", "enrich", "augment", "text", "enrich", "involv", "augment", "origin", "text", "data", "with", "inform", "that", "not", "previous", "have", "text", "enrich", "provid", "more", "semant", "origin", "text", "therebi", "improv", "predict", "power", "and", "the", "depth", "analysi", "can", "perform", "data", "inform", "retriev", "exampl", "expand", "user", "queri", "improv", "the", "match", "keyword", "form", "augment", "queri", "like", "text", "mine", "could", "becom", "text", "document", "mine", "analysi", "while", "this", "make", "sens", "human", "can", "help", "fetch", "document", "that", "are", "more", "relev", "can", "get", "realli", "creativ", "with", "how", "enrich", "text", "can", "use", "partofspeech", "tag", "get", "more", "granular", "inform", "about", "the", "word", "text", "for", "exampl", "document", "classif", "problem", "the", "appear", "the", "word", "book", "noun", "could", "result", "differ", "classif", "than", "book", "verb", "one", "use", "the", "context", "read", "and", "the", "other", "use", "the", "context", "reserv", "someth", "this", "articl", "talk", "about", "how", "chines", "text", "classif", "improv", "with", "combin", "noun", "and", "verb", "input", "featur", "with", "the", "avail", "larg", "amount", "text", "howev", "peopl", "have", "start", "use", "embed", "enrich", "the", "mean", "word", "phrase", "and", "sentenc", "for", "classif", "search", "summar", "and", "text", "generat", "general", "this", "especi", "true", "deep", "learn", "base", "approach", "where", "word", "level", "embed", "layer", "quit", "common", "can", "either", "start", "with", "preestablish", "embed", "creat", "own", "and", "use", "downstream", "task", "other", "way", "enrich", "text", "data", "includ", "phrase", "extract", "where", "recogn", "compound", "word", "one", "aka", "chunk", "expans", "with", "synonym", "and", "depend", "pars", "need", "all", "not", "realli", "but", "have", "some", "for", "sure", "want", "good", "consist", "result", "give", "idea", "what", "the", "bare", "minimum", "should", "broken", "down", "must", "should", "and", "task", "depend", "everyth", "that", "fall", "under", "task", "depend", "can", "quantit", "qualit", "test", "befor", "decid", "actual", "need", "rememb", "less", "more", "and", "want", "keep", "approach", "eleg", "possibl", "the", "more", "overhead", "add", "the", "more", "layer", "will", "have", "peel", "back", "when", "run", "into", "issu", "must", "nois", "remov", "lowercas", "can", "task", "depend", "some", "case", "should", "simpl", "normal", "standard", "near", "ident", "word", "task", "depend", "advanc", "normal", "address", "outofvocabulari", "word", "stopword", "remov", "stem", "lemmat", "text", "enrich", "augment", "for", "ani", "task", "the", "minimum", "should", "tri", "lowercas", "text", "and", "remov", "nois", "what", "entail", "nois", "depend", "domain", "see", "section", "nois", "remov", "can", "also", "some", "basic", "normal", "step", "for", "more", "consist", "and", "then", "systemat", "add", "other", "layer", "see", "fit", "general", "rule", "thumb", "not", "all", "task", "need", "the", "same", "level", "preprocess", "for", "some", "task", "can", "get", "away", "with", "the", "minimum", "howev", "for", "other", "the", "dataset", "noisi", "that", "preprocess", "enough", "garbageingarbageout", "here", "general", "rule", "thumb", "this", "will", "not", "alway", "hold", "true", "but", "work", "for", "most", "case", "have", "lot", "well", "written", "text", "work", "with", "fair", "general", "domain", "then", "preprocess", "not", "extrem", "critic", "can", "get", "away", "with", "the", "bare", "minimum", "train", "word", "embed", "model", "use", "all", "wikipedia", "text", "reuter", "news", "articl", "howev", "are", "work", "veri", "narrow", "domain", "tweet", "about", "health", "food", "and", "data", "spars", "and", "noisi", "could", "benefit", "from", "more", "preprocess", "layer", "although", "each", "layer", "add", "stop", "word", "remov", "stem", "normal", "need", "quantit", "qualit", "verifi", "meaning", "layer", "here", "tabl", "that", "summar", "how", "much", "preprocess", "should", "perform", "text", "data", "hope", "the", "idea", "here", "steer", "toward", "the", "right", "preprocess", "step", "for", "project", "rememb", "less", "more", "friend", "mine", "onc", "mention", "how", "made", "larg", "ecommerc", "search", "system", "more", "effici", "and", "less", "buggi", "just", "throw", "out", "layer", "unneed", "preprocess", "resourc", "python", "code", "for", "basic", "text", "preprocess", "use", "and", "regex", "construct", "custom", "stop", "word", "list", "sourc", "code", "for", "phrase", "extract", "refer", "for", "updat", "list", "paper", "pleas", "see", "origin", "articl", "bio", "kavita", "ganesan", "data", "scientist", "with", "expertis", "natur", "languag", "process", "text", "mine", "search", "and", "machin", "learn", "over", "the", "last", "decad", "she", "work", "for", "various", "technolog", "organ", "includ", "github", "microsoft", "health", "inform", "system", "and", "ebay", "origin", "repost", "with", "permiss", "resourc", "onlin", "and", "webbas", "analyt", "data", "mine", "data", "scienc", "machin", "learn", "educ", "softwar", "for", "analyt", "data", "scienc", "data", "mine", "and", "machin", "learn", "relat", "get", "start", "with", "use", "the", "pytorch", "framework", "build", "classifi", "cheapli", "with", "transfer", "learn", "and", "weak", "supervis", "beyond", "news", "content", "the", "role", "social", "context", "for", "fake", "news", "detect"], "timestamp_scraper": 1556376163.873595, "title": "All you need to know about text preprocessing for NLP and Machine Learning", "read_time": 763.1999999999999, "content_html": "<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"http://kavita-ganesan.com/\">Kavita Ganesan</a>, Data Scientist</b>.</p>\n<p>Based on some recent conversations, I realized that text preprocessing is a severely overlooked topic. A few people I spoke to mentioned inconsistent results from their NLP applications only to realize that they were not preprocessing their text or were using the wrong kind of text preprocessing for their project.</p>\n<p>With that in mind, I thought of shedding some light around what text preprocessing really is, the different methods of text preprocessing, and a way to estimate how much preprocessing you may need. For those interested, I\u2019ve also made some\u00a0<a data-href=\"https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing\" href=\"https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing\">text preprocessing code snippets</a>\u00a0for you to try. Now, let\u2019s get started!</p>\n<h3>What is text preprocessing?</h3>\n<p>To preprocess your text simply means to bring your text into a form that is\u00a0<em><strong>predictable</strong></em><strong>\u00a0</strong>and\u00a0<em><strong>analyzable</strong></em><em>\u00a0</em>for your task. A task here is a combination of approach and domain. For example, extracting top keywords with TF-IDF (approach) from Tweets (domain) is an example of a\u00a0<em>Task</em>.</p>\n<blockquote><p><em>Task = approach +\u00a0domain</em></p></blockquote>\n<p>One task\u2019s ideal preprocessing can become another task\u2019s worst nightmare. So take note: text preprocessing is not directly transferable from task to task.</p>\n<p>Let\u2019s take a very simple example, let\u2019s say you are trying to discover commonly used words in a news dataset. If your pre-processing step involves removing\u00a0<a data-href=\"http://kavita-ganesan.com/what-are-stop-words/\" href=\"http://kavita-ganesan.com/what-are-stop-words/\">stop words</a>\u00a0because some other task used it, then you are probably going to miss out on some of the common words as you have ALREADY eliminated it. So really, it\u2019s not a one-size-fits-all approach.</p>\n<h3>Types of text preprocessing techniques</h3>\n<p>There are different ways to preprocess your text. Here are some of the approaches that you should know about and I will try to highlight the importance of each.</p>\n<h4>Lowercasing</h4>\n<p>Lowercasing ALL your text data, although commonly overlooked, is one of the simplest and most effective form of text preprocessing. It is applicable to most text mining and NLP problems and can help in cases where your dataset is not very large and significantly helps with consistency of expected output.</p>\n<p>Quite recently, one of my blog readers trained a\u00a0<a data-href=\"http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/\" href=\"http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/\">word embedding model for similarity lookups</a>. He found that different variation in input capitalization (e.g. \u2018Canada\u2019 vs. \u2018canada\u2019) gave him different types of output or no output at all. This was probably happening because the dataset had mixed-case occurrences of the word \u2018Canada\u2019 and there was insufficient evidence for the neural-network to effectively learn the weights for the less common version. This type of issue is bound to happen when your dataset is fairly small, and lowercasing is a great way to deal with sparsity issues.</p>\n<p>Here is an example of how lowercasing solves the sparsity issue, where the same words with different cases map to the same lowercase form:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*M8cSL8_1Qu8HNk3eaIX_-w.png\" width=\"100%\"><br>\n<strong>Word with different cases all map to the same lowercase form</strong></br></img></p>\n<p>Another example where lowercasing is very useful is for search. Imagine, you are looking for documents containing \u201cusa\u201d. However, no results were showing up because \u201cusa\u201d was indexed as\u00a0<strong>\u201cUSA\u201d.</strong>\u00a0Now, who should we blame? The U.I. designer who set-up the interface or the engineer who set-up the search index?</p>\n<p>While lowercasing should be standard practice, I\u2019ve also had situations where preserving the capitalization was important. For example, in predicting the programming language of a source code file. The word\u00a0System\u00a0in Java is quite different from\u00a0system\u00a0in python. Lowercasing the two makes them identical, causing the classifier to lose important predictive features. While lowercasing\u00a0<em>is generally\u00a0</em>helpful, it may not be applicable for all tasks.</p>\n<h4>Stemming</h4>\n<p>Stemming is the process of reducing inflection in words (e.g. troubled, troubles) to their root form (e.g. trouble). The \u201croot\u201d in this case may not be a real root word, but just a canonical form of the original word.</p>\n<p>Stemming uses a crude heuristic process that chops off the ends of words in the hope of correctly transforming words into its root form. So the words \u201ctrouble\u201d, \u201ctroubled\u201d and \u201ctroubles\u201d might actually be converted to\u00a0troublinstead of\u00a0trouble\u00a0because the ends were just chopped off (ughh, how crude!).</p>\n<p>There are different algorithms for stemming. The most common algorithm, which is also known to be empirically effective for English, is\u00a0<a data-href=\"https://tartarus.org/martin/PorterStemmer/\" href=\"https://tartarus.org/martin/PorterStemmer/\">Porters Algorithm</a>. Here is an example of stemming in action with Porter Stemmer:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*9KLZmtSh-t6SfEGX_nbznA.png\" width=\"100%\"/></p>\n<p><strong>Effects of stemming inflected words</strong></p>\n<p>Stemming is useful for dealing with sparsity issues as well as standardizing vocabulary. I\u2019ve had success with stemming in search applications in particular. The idea is that, if say you search for \u201cdeep learning classes\u201d, you also want to surface documents that mention \u201cdeep learning\u00a0<em><strong>class</strong></em>\u201d as well as \u201cdeep\u00a0<em><strong>learn</strong></em>\u00a0classes\u201d, although the latter doesn\u2019t sound right. But you get where we are going with this. You want to match all variations of a word to bring up the most relevant documents.</p>\n<p><span data-creator-ids=\"anon\">In most of my previous text classification work however, stemming only marginally helped improved classification accuracy as opposed to using better engineered features and text enrichment approaches such as using word embeddings.</span></p>\n<h4>Lemmatization</h4>\n<p>Lemmatization on the surface is very similar to stemming, where the goal is to remove inflections and map a word to its root form. The only difference is that, lemmatization tries to do it the proper way. It doesn\u2019t just chop things off, it actually transforms words to the actual root. For example, the word \u201cbetter\u201d would map to \u201cgood\u201d. It may use a dictionary such as\u00a0<a data-href=\"https://www.nltk.org/_modules/nltk/stem/wordnet.html\" href=\"https://www.nltk.org/_modules/nltk/stem/wordnet.html\">WordNet for mappings</a>\u00a0or some special\u00a0<a data-href=\"https://www.semanticscholar.org/paper/A-Rule-based-Approach-to-Word-Lemmatization-Plisson-Lavrac/5319539616e81b02637b1bf90fb667ca2066cf14\" href=\"https://www.semanticscholar.org/paper/A-Rule-based-Approach-to-Word-Lemmatization-Plisson-Lavrac/5319539616e81b02637b1bf90fb667ca2066cf14\">rule-based approaches</a>. Here is an example of lemmatization in action using a WordNet-based approach:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*z4f7My5peI28lNpZdHk_Iw.png\" width=\"100%\"/></p>\n<p>Effects of Lemmatization with\u00a0WordNet</p>\n<p>In my experience, lemmatization provides no significant benefit over stemming for search and text classification purposes. In fact, depending on the algorithm you choose, it could be much slower compared to using a very basic stemmer and you may have to know the part-of-speech of the word in question in order to get a correct lemma.\u00a0<a data-href=\"https://arxiv.org/pdf/1707.01780.pdf\" href=\"https://arxiv.org/pdf/1707.01780.pdf\">This paper</a>\u00a0finds that lemmatization has no significant impact on accuracy for text classification with neural architectures.</p>\n<p>I would personally use lemmatization sparingly. The additional overhead may or may not be worth it. But you could always try it to see the impact it has on your performance metric.</p>\n<h4>Stopword Removal</h4>\n<p>Stop words are a set of commonly used words in a language. Examples of stop words in English are \u201ca\u201d, \u201cthe\u201d, \u201cis\u201d, \u201care\u201d and etc. The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead.</p>\n<p>For example, in the context of a search system, if your search query is<em>\u00a0\u201cwhat is text preprocessing?\u201d</em>, you want the search system to focus on surfacing documents that talk about\u00a0text preprocessing\u00a0over documents that talk about\u00a0what is. This can be done by preventing all words from your stop word list from being analyzed. Stop words are commonly applied in search systems, text classification applications, topic modeling, topic extraction and others.</p>\n<p>In my experience, stop word removal, while effective in search and topic extraction systems, showed to be non-critical in classification systems. However, it does help reduce the number of features in consideration which helps keep your models decently sized.</p>\n<p>Here is an example of stop word removal in action. All stop words are replaced with a dummy character,\u00a0<strong>W</strong>:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*lGARQmFv13hj504Lua0AeA.png\" width=\"100%\"/></p>\n<p><strong>Sentence before and after stop word\u00a0removal</strong></p>\n<p><a data-href=\"http://kavita-ganesan.com/what-are-stop-words/\" href=\"http://kavita-ganesan.com/what-are-stop-words/\">Stop word lists</a>\u00a0can come from pre-established sets or you can create a\u00a0<a data-href=\"http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/\" href=\"http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/\">custom one for your domain</a>. Some libraries (e.g. sklearn) allow you to remove words that appeared in X% of your documents, which can also give you a stop word removal effect.</p>\n<h4>Normalization</h4>\n<p>A highly overlooked preprocessing step is text normalization. Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word \u201cgooood\u201d and \u201cgud\u201d can be transformed to \u201cgood\u201d, its canonical form. Another example is mapping of near identical words such as \u201cstopwords\u201d, \u201cstop-words\u201d and \u201cstop words\u201d to just \u201cstopwords\u201d.</p>\n<p>Text normalization is important for noisy texts such as social media comments, text messages and comments to blog posts where abbreviations, misspellings and use of out-of-vocabulary words (oov) are prevalent.\u00a0<a data-href=\"https://sentic.net/microtext-normalization.pdf\" href=\"https://sentic.net/microtext-normalization.pdf\">This paper</a>\u00a0showed that by using a text normalization strategy for Tweets, they were able to improve sentiment classification accuracy by ~4%.</p>\n<p>Here\u2019s an example of words before and after normalization:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*e7Y-Rbxky5i2U6awCQILVQ.png\" width=\"100%\"/></p>\n<p><strong>Effects of Text Normalization</strong></p>\n<p>Notice how the variations, map to the same canonical form.</p>\n<p>In my experience, text normalization has even been effective for analyzing\u00a0<a data-href=\"http://kavita-ganesan.com/general-supervised-approach-segmentation-clinical-texts/\" href=\"http://kavita-ganesan.com/general-supervised-approach-segmentation-clinical-texts/\">highly unstructured clinical texts</a>\u00a0where physicians take notes in non-standard ways. I\u2019ve also found it useful for\u00a0<a data-href=\"https://githubengineering.com/topics/\" href=\"https://githubengineering.com/topics/\">topic extraction</a>\u00a0where near synonyms and spelling differences are common (e.g. topic modelling, topic modeling, topic-modeling, topic-modelling).</p>\n<p>Unfortunately, unlike stemming and lemmatization, there isn\u2019t a standard way to normalize texts. It typically depends on the task. For example, the way you would normalize clinical texts would arguably be different from how you normalize sms text messages.</p>\n<p>Some common approaches to text normalization include dictionary mappings (easiest), statistical machine translation (SMT) and spelling-correction based approaches.\u00a0<a data-href=\"https://nlp.stanford.edu/courses/cs224n/2009/fp/27.pdf\" href=\"https://nlp.stanford.edu/courses/cs224n/2009/fp/27.pdf\">This interesting article</a>\u00a0compares the use of a dictionary based approach and a SMT approach for normalizing text messages.</p>\n<h4>Noise Removal</h4>\n<p>Noise removal is about removing\u00a0characters\u00a0digits\u00a0and\u00a0pieces of text that can interfere with your text analysis. Noise removal is one of the most essential text preprocessing steps. It is also highly domain dependent.</p>\n<p>For example, in Tweets, noise could be all special characters except hashtags as it signifies concepts that can characterize a Tweet. The problem with noise is that it can produce results that are inconsistent in your downstream tasks. Let\u2019s take the example below:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*5WeDzBXnCmInni9PfKFM_g.png\" width=\"100%\"/></p>\n<p><strong>Stemming\u00a0without\u00a0Noise\u00a0Removal</strong></p>\n<p>Notice that all the raw words above have some surrounding noise in them. If you stem these words, you can see that the stemmed result does not look very pretty. None of them have a correct stem. However, with some cleaning as applied in\u00a0<a href=\"https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/text-pre-processing/Text%20Pre-Processing%20Examples.ipynb\">this notebook</a>, the results now look much better:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*481aP2Cf-nQ-u8g60PfjUg.png\" width=\"100%\"/></p>\n<p>Stemming\u00a0<strong>with</strong>\u00a0Noise\u00a0Removal</p>\n<p>Noise removal is one of the first things you should be looking into when it comes to Text Mining and NLP. There are various ways to remove noise. This includes\u00a0<em>punctuation removal</em>,\u00a0<em>special character removal</em>,\u00a0<em>numbers removal, html formatting removal, domain specific keyword removal</em>\u00a0<em>(e.g. \u2018RT\u2019 for retweet), source code removal, header removal</em>\u00a0and more. It all depends on which domain you are working in and what entails noise for your task. The\u00a0<a data-href=\"https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing\" href=\"https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/text-pre-processing\">code snippet in my notebook</a>\u00a0shows how to do some basic noise removal.</p>\n<h4>Text Enrichment / Augmentation</h4>\n<p>Text enrichment involves augmenting your original text data with information that you did not previously have. Text enrichment provides more semantics to your original text, thereby improving its predictive power and the depth of analysis you can perform on your data.</p>\n<p>In an information retrieval example, expanding a user\u2019s query to improve the matching of keywords is a form of augmentation. A query like\u00a0text mining could become\u00a0text document mining analysis. While this doesn\u2019t make sense to a human, it can help fetch documents that are more relevant.</p>\n<p>You can get really creative with how you enrich your text. You can use\u00a0<a data-href=\"https://en.wikipedia.org/wiki/Part-of-speech_tagging\" href=\"https://en.wikipedia.org/wiki/Part-of-speech_tagging\"><strong>part-of-speech tagging</strong></a>\u00a0to get more granular information about the words in your text.</p>\n<p>For example, in a document classification problem, the appearance of the word\u00a0<strong>book</strong>\u00a0as a\u00a0<strong>noun</strong>\u00a0could result in a different classification than\u00a0<strong>book</strong>\u00a0as a\u00a0<strong>verb</strong>\u00a0as one is used in the context of reading and the other is used in the context of reserving something.\u00a0<a data-href=\"http://www.iapr-tc11.org/archive/icdar2011/fileup/PDF/4520a920.pdf\" href=\"http://www.iapr-tc11.org/archive/icdar2011/fileup/PDF/4520a920.pdf\">This article</a>\u00a0talks about how Chinese text classification is improved with a combination of nouns and verbs as input features.</p>\n<p>With the availability of large amounts texts however, people have started using\u00a0<a data-href=\"https://en.wikipedia.org/wiki/Word_embedding\" href=\"https://en.wikipedia.org/wiki/Word_embedding\">embeddings</a>\u00a0to enrich the meaning of words, phrases and sentences for classification, search, summarization and text generation in general. This is especially true in deep learning based NLP approaches where a\u00a0<a data-href=\"https://keras.io/layers/embeddings/\" href=\"https://keras.io/layers/embeddings/\">word level embedding layer</a>\u00a0is quite common. You can either start with\u00a0<a data-href=\"https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\" href=\"https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\">pre-established embeddings</a>\u00a0or create your own and use it in downstream tasks.</p>\n<p>Other ways to enrich your text data include\u00a0<a data-href=\"http://kavita-ganesan.com/how-to-incorporate-phrases-into-word2vec-a-text-mining-approach/#.XHCcJ1xKg2w\" href=\"http://kavita-ganesan.com/how-to-incorporate-phrases-into-word2vec-a-text-mining-approach/#.XHCcJ1xKg2w\">phrase extraction</a>, where you recognize compound words as one (aka chunking),\u00a0<a data-href=\"http://aclweb.org/anthology/R09-1073\" href=\"http://aclweb.org/anthology/R09-1073\">expansion with synonyms</a>and\u00a0<a data-href=\"http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/slides/15-DP.pdf\" href=\"http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/slides/15-DP.pdf\">dependency parsing</a>.</p>\n<h3>Do you need it\u00a0all?</h3>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*_VjLxnzsMajCB8HlWdv6cQ.jpeg\" width=\"100%\"/></p>\n<p>Not really, but you do have to do some of it for sure if you want good, consistent results. To give you an idea of what the bare minimum should be, I\u2019ve broken it down to\u00a0<em><strong>Must Do</strong></em>,\u00a0<em><strong>Should Do</strong></em>\u00a0and\u00a0<em><strong>Task Dependent</strong></em>. Everything that falls under task dependent can be quantitatively or qualitatively tested before deciding you actually need it.</p>\n<p>Remember, less is more and you want to keep your approach as elegant as possible. The more overhead you add, the more layers you will have to peel back when you run into issues.</p>\n<h3>Must Do:</h3>\n<ul>\n<li>Noise removal</li>\n<li>Lowercasing (can be task dependent in some cases)</li>\n</ul>\n<h3>Should Do:</h3>\n<ul>\n<li>Simple normalization\u200a\u2014\u200a(e.g. standardize near identical words)</li>\n</ul>\n<h3>Task Dependent:</h3>\n<ol>\n<li>Advanced normalization (e.g. addressing out-of-vocabulary words)</li>\n<li>Stop-word removal</li>\n<li>Stemming / lemmatization</li>\n<li>Text enrichment / augmentation</li>\n</ol>\n<p>So, for any task, the minimum you should do is try to lowercase your text and remove noise. What entails noise depends on your domain (see section on Noise Removal). You can also do some basic normalization steps for more consistency and then systematically add other layers as you see fit.</p>\n<h3>General Rule of\u00a0Thumb</h3>\n<p>Not all tasks need the same level of preprocessing. For some tasks, you can get away with the minimum. However, for others, the dataset is so noisy that, if you don\u2019t preprocess enough, it\u2019s going to be garbage-in-garbage-out.</p>\n<p>Here\u2019s a general rule of thumb. This will not always hold true, but works for most cases. If you have a lot of well written texts to work with in a fairly general domain, then preprocessing is not extremely critical; you can get away with the bare minimum (e.g. training a word embedding model using all of Wikipedia texts or Reuters news articles).</p>\n<p>However, if you are working in a very narrow domain (e.g. Tweets about health foods) and data is sparse and noisy, you could benefit from more preprocessing layers, although each layer you add (e.g. stop word removal, stemming, normalization) needs to be quantitatively or qualitatively verified as a meaningful layer. Here\u2019s a table that summarizes how much preprocessing you should be performing on your text data</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/1600/1*VRgrWGLBJJDXsQE72QyI3g.png\" width=\"100%\"/></p>\n<p>I hope the ideas here steer you towards the right preprocessing steps for your projects. Remember,\u00a0<em>less is more</em>. A friend of mine once mentioned to me how he made a large e-commerce search system more efficient and less buggy just by throwing out layers of\u00a0<em>unneeded</em>\u00a0preprocessing.</p>\n<h3>Resources</h3>\n<ul>\n<li><a data-href=\"https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/text-pre-processing/Text%20Preprocessing%20Examples.ipynb\" href=\"https://github.com/kavgan/nlp-text-mining-working-examples/blob/master/text-pre-processing/Text%20Preprocessing%20Examples.ipynb\">Python code for basic text preprocessing using NLTK and regex</a></li>\n<li><a data-href=\"http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/\" href=\"http://kavita-ganesan.com/tips-for-constructing-custom-stop-word-lists/\">Constructing custom stop word lists</a></li>\n<li><a data-href=\"https://kavgan.github.io/phrase-at-scale/\" href=\"https://kavgan.github.io/phrase-at-scale/\">Source code for phrase extraction</a></li>\n</ul>\n<h3>References</h3>\n<ul>\n<li>For an updated list of papers, please see\u00a0<a data-href=\"http://kavita-ganesan.com/text-preprocessing-tutorial/#Relevant-Papers\" href=\"http://kavita-ganesan.com/text-preprocessing-tutorial/#Relevant-Papers\">my original article</a></li>\n</ul>\n<p>Bio: <a href=\"http://kavita-ganesan.com/about-me/\">Kavita Ganesan</a> is a Data Scientist with expertise in Natural Language Processing, Text Mining, Search and Machine Learning. Over the last decade, she worked for various technology organizations including GitHub (Microsoft), 3M Health Information Systems and eBay.</p>\n<p><a href=\"https://medium.freecodecamp.org/all-you-need-to-know-about-text-preprocessing-for-nlp-and-machine-learning-bc1c5765ff67\">Original</a>. Reposted with permission.</p>\n<p><strong>Resources:</strong></p>\n<ul>\n<li><a href=\"https://www.kdnuggets.com/education/online.html\">On-line and web-based: Analytics, Data Mining, Data Science, Machine Learning education</a></li>\n<li><a href=\"https://www.kdnuggets.com/software/index.html\">Software for Analytics, Data Science, Data Mining, and Machine Learning</a></li>\n</ul>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"https://www.kdnuggets.com/2019/04/nlp-pytorch.html\">Getting started with NLP using the PyTorch framework</a></li>\n<li><a href=\"https://www.kdnuggets.com/2019/03/building-nlp-classifiers-cheaply-transfer-learning-weak-supervision.html\">Building NLP Classifiers Cheaply With Transfer Learning and Weak Supervision</a></li>\n<li><a href=\"https://www.kdnuggets.com/2019/03/beyond-news-contents-role-of-social-context-for-fake-news-detection.html\">Beyond news contents: the role of social context for fake news detection</a></li>\n</ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets'; \n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}