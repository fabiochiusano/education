{"content": "comments By William Falcon , PhD Researcher, AI researcher and AI writer for Forbes Adding attention to your neural networks is a bit like wanting to take an afternoon nap at work. You know it\u2019s better for you, everyone wants to do it, but everyone\u2019s too scared to . My goal today is to assume nothing, explain the details with animations, and make math great again (MMGA? ugh\u2026) Here we\u2019ll cover: Short RNN review. Short sequence to sequence model review. Attention in RNN's. Improvements to attention. Transformer network introduction. \u00a0 Recurrent Neural Networks (RNN) \u00a0 RNNs let us model sequences in neural networks. While there are other ways of modeling sequences, RNNs are particularly useful. RNNs come in two flavors, LSTM's (Hochreiter et al, 1997) and GRUs (Cho et al, 2014) . For a deep tutorial, check out Chris Colah\u2019s tutorial . Let\u2019s look at machine translation for a concrete example of an RNN. Imagine we have an RNN with 56 hidden units. \r \r rnn_cell = \r \r We have a word \u201cNYU\u201d which is represented by the integer 12 meaning it\u2019s the 12th word in the vocab I created. \r \r # 'NYU' is the 12th word in my vocab\r word = 'NYU'\r word = VOCAB[word]\r \r \r # 11\r \r Except we don\u2019t feed an integer into the RNN, we use a higher dimensional representation which we currently obtain through embeddings . An embedding lets us map a sequence of discrete tokens into continuous space (Bengio et al, 2003) . \r \r embedding_layer = \r \r # project our word to 10 dimensions\r x = \r \r An RNN cell takes in two inputs, a word x , and a hidden state from the previous time step h . At every time step, it outputs a new h .", "title_html": "<h1 id=\"title\">Attention Craving RNNS: Building Up To Transformer Networks</h1> ", "url": "https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html", "tfidf": {"tfidf": {"token": 33.7070063694, "current": 1.5325803649, "review": 4.419821826280001, "nap": 126.0, "too": 1.81585268215, "feed": 7.77853993141, "math": 22.0806675939, "detail": 2.26186066391, "other": 1.00992366412, "introduct": 2.7808723068799996, "new": 1.0178880554, "attent": 8.43122676579, "integ": 92.03478260860001, "numth": 2.3502590673599997, "previous": 1.42846859816, "noth": 3.46410648047, "network": 10.37477536352, "like": 1.14918566775, "neural": 178.3820224719, "through": 1.07074930869, "phd": 22.3605633803, "imagin": 6.598503740650001, "except": 1.71948445792, "rnncell": 132.3, "work": 1.11520089913, "transform": 3.42007755278, "state": 1.0477133240899998, "cell": 7.1033557047, "forb": 17.237785016300002, "higher": 2.1218925421, "know": 2.59327017315, "check": 6.50655737705, "two": 2.0275862069, "afternoon": 10.3696930111, "cho": 132.3, "and": 4.00025196852, "discret": 15.0056710775, "particular": 1.3814827706200001, "colah": 132.3, "bit": 8.33385826772, "creat": 1.2492917847, "great": 1.26592775696, "project": 1.7534791252500002, "num": 7.00220528007, "embed": 33.67126193, "here": 2.42307692308, "cover": 1.69380134429, "william": 1.75483585719, "hochreit": 132.3, "word": 14.372298291279998, "for": 4.00126016004, "explain": 2.60049140049, "falcon": 23.381443299, "output": 7.676982591880001, "everyon": 12.7929089444, "today": 1.74961428257, "with": 2.0023964179799996, "deep": 3.6279707495399998, "assum": 2.9575260804799997, "are": 2.05981187156, "step": 5.655860349119999, "vocab": 264.6, "better": 2.0065722952500002, "chris": 5.979661016950001, "use": 2.0592775147599998, "translat": 2.85745140389, "look": 1.9086318826599997, "improv": 2.04376930999, "but": 1.01632417899, "rnns": 396.90000000000003, "obtain": 2.68629441624, "our": 2.35758835759, "there": 1.04091266719, "writer": 2.75816539263, "space": 2.39818731118, "flavor": 32.33401222, "scare": 24.5758513932, "comment": 3.05954904606, "again": 1.50883862384, "repres": 1.46972782818, "have": 2.0297896822799997, "take": 2.27923336444, "make": 1.0762660158600001, "way": 1.2190739461, "out": 1.06016694491, "model": 6.2717935212, "represent": 5.928304705, "ugh\u2026": 132.3, "goal": 3.28152128979, "unit": 1.15394679459, "time": 2.02254920696, "tutori": 118.9213483146, "continu": 1.13928955867, "hidden": 15.62598425196, "embeddinglay": 132.3, "while": 1.0441988950299999, "mean": 1.44906900329, "short": 2.82591669634, "sequenc": 30.35564053535, "anim": 2.8395635843299996, "concret": 10.0100882724, "come": 1.32831325301, "research": 3.8840366972400004, "exampl": 1.50483412322, "dimens": 8.25585023401, "the": 7.0, "dimension": 54.1843003413, "grus": 132.3, "from": 1.00056721497, "everi": 1.47917637194, "input": 12.2029208301, "bengio": 132.3, "map": 4.0728578758300005, "machin": 4.02433460076, "want": 3.99396226416, "which": 2.01038369, "into": 2.03004922958, "recurr": 35.5964125561, "let": 10.45849802373}, "logtfidf": {"token": 3.5177057198900004, "current": 0.42695282784500005, "review": 1.5859044078420002, "nap": 4.83628190695, "too": 0.5965551547219999, "feed": 2.05136865109, "math": 3.09470245618, "detail": 0.816187777173, "other": 0.00987474791976, "introduct": 1.02276465794, "new": 0.0177299468511, "attent": 3.09998998974, "integ": 7.658038793739999, "numth": 0.322756765898, "previous": 0.356602960063, "noth": 1.24245472939, "network": 3.8123322122079997, "like": 0.139053576545, "neural": 12.2559454665, "through": 0.0683586918849, "phd": 3.10729884387, "imagin": 1.88684291737, "except": 0.54202451213, "rnncell": 4.88507207112, "work": 0.109034567273, "transform": 1.22966322707, "state": 0.0466100027668, "cell": 1.9605673068599998, "forb": 2.8471037776499997, "higher": 0.752308398995, "know": 0.952919694398, "check": 1.87281049562, "two": 0.0273976887164, "afternoon": 2.3388874182499997, "cho": 4.88507207112, "and": 0.0002519605682544, "discret": 2.70842820148, "particular": 0.323157393804, "colah": 4.88507207112, "bit": 2.12032652634, "creat": 0.222576818514, "great": 0.235805258079, "project": 0.561601885907, "num": 0.0022049327677790003, "embed": 5.64699506254, "here": 0.8850381883700001, "cover": 0.526975319156, "william": 0.562375323877, "hochreit": 4.88507207112, "word": 4.68688865908, "for": 0.0012599615815880002, "explain": 0.955700427358, "falcon": 3.15194268634, "output": 2.03822657827, "everyon": 3.7114876962800003, "today": 0.559395353679, "with": 0.00239498342678, "deep": 1.2886734698, "assum": 1.08435313525, "are": 0.0589349471654, "step": 2.07909011396, "vocab": 9.77014414224, "better": 0.6964279406, "chris": 1.78836388023, "use": 0.0584160394632, "translat": 1.0499301100299998, "look": 0.6463866936, "improv": 0.7147958039319999, "but": 0.0161923720719, "rnns": 14.65521621336, "obtain": 0.988162703503, "our": 0.8576392141820001, "there": 0.0400978929255, "writer": 1.0145657459, "space": 0.874713164972, "flavor": 3.47611968611, "scare": 3.20176431012, "comment": 1.11826753454, "again": 0.411340231612, "repres": 0.38507723275, "have": 0.0295700046824, "take": 0.261383924394, "make": 0.07349765782289999, "way": 0.19809150993500002, "out": 0.0584263909193, "model": 2.2123502193330005, "represent": 1.7797382876499999, "ugh\u2026": 4.88507207112, "goal": 1.18830712273, "unit": 0.143188061817, "time": 0.0224230377252, "tutori": 8.170630311, "continu": 0.13040487398700001, "hidden": 4.1115760104, "embeddinglay": 4.88507207112, "while": 0.04324998379380001, "mean": 0.37092128352, "short": 0.691371251358, "sequenc": 9.017722187, "anim": 1.04365037288, "concret": 2.3035934117099996, "come": 0.28390990653000003, "research": 1.327455636276, "exampl": 0.40868267499899996, "dimens": 2.11092206831, "the": 0.0, "dimension": 3.99239120489, "grus": 4.88507207112, "from": 0.000567054168866, "everi": 0.391485427421, "input": 2.50167533539, "bengio": 4.88507207112, "map": 1.40434493384, "machin": 1.39235958062, "want": 1.3832732125099998, "which": 0.01035682769086, "into": 0.0298257264574, "recurr": 3.5722448618800002, "let": 3.7464077018399995}, "logidf": {"token": 3.5177057198900004, "current": 0.42695282784500005, "review": 0.7929522039210001, "nap": 4.83628190695, "too": 0.5965551547219999, "feed": 2.05136865109, "math": 3.09470245618, "detail": 0.816187777173, "other": 0.00987474791976, "introduct": 1.02276465794, "new": 0.0177299468511, "attent": 1.03332999658, "integ": 3.8290193968699997, "numth": 0.161378382949, "previous": 0.356602960063, "noth": 1.24245472939, "network": 0.9530830530519999, "like": 0.139053576545, "neural": 4.0853151555, "through": 0.0683586918849, "phd": 3.10729884387, "imagin": 1.88684291737, "except": 0.54202451213, "rnncell": 4.88507207112, "work": 0.109034567273, "transform": 1.22966322707, "state": 0.0466100027668, "cell": 1.9605673068599998, "forb": 2.8471037776499997, "higher": 0.752308398995, "know": 0.952919694398, "check": 1.87281049562, "two": 0.0136988443582, "afternoon": 2.3388874182499997, "cho": 4.88507207112, "and": 6.29901420636e-05, "discret": 2.70842820148, "particular": 0.323157393804, "colah": 4.88507207112, "bit": 2.12032652634, "creat": 0.222576818514, "great": 0.235805258079, "project": 0.561601885907, "num": 0.00031499039539700004, "embed": 2.82349753127, "here": 0.8850381883700001, "cover": 0.526975319156, "william": 0.562375323877, "hochreit": 4.88507207112, "word": 0.585861082385, "for": 0.00031499039539700004, "explain": 0.955700427358, "falcon": 3.15194268634, "output": 2.03822657827, "everyon": 1.8557438481400002, "today": 0.559395353679, "with": 0.00119749171339, "deep": 1.2886734698, "assum": 1.08435313525, "are": 0.0294674735827, "step": 1.03954505698, "vocab": 4.88507207112, "better": 0.6964279406, "chris": 1.78836388023, "use": 0.0292080197316, "translat": 1.0499301100299998, "look": 0.6463866936, "improv": 0.7147958039319999, "but": 0.0161923720719, "rnns": 4.88507207112, "obtain": 0.988162703503, "our": 0.8576392141820001, "there": 0.0400978929255, "writer": 1.0145657459, "space": 0.874713164972, "flavor": 3.47611968611, "scare": 3.20176431012, "comment": 1.11826753454, "again": 0.411340231612, "repres": 0.38507723275, "have": 0.0147850023412, "take": 0.130691962197, "make": 0.07349765782289999, "way": 0.19809150993500002, "out": 0.0584263909193, "model": 0.7374500731110001, "represent": 1.7797382876499999, "ugh\u2026": 4.88507207112, "goal": 1.18830712273, "unit": 0.143188061817, "time": 0.0112115188626, "tutori": 4.0853151555, "continu": 0.13040487398700001, "hidden": 2.0557880052, "embeddinglay": 4.88507207112, "while": 0.04324998379380001, "mean": 0.37092128352, "short": 0.345685625679, "sequenc": 1.8035444374, "anim": 1.04365037288, "concret": 2.3035934117099996, "come": 0.28390990653000003, "research": 0.663727818138, "exampl": 0.40868267499899996, "dimens": 2.11092206831, "the": 0.0, "dimension": 3.99239120489, "grus": 4.88507207112, "from": 0.000567054168866, "everi": 0.391485427421, "input": 2.50167533539, "bengio": 4.88507207112, "map": 1.40434493384, "machin": 1.39235958062, "want": 0.6916366062549999, "which": 0.00517841384543, "into": 0.0149128632287, "recurr": 3.5722448618800002, "let": 1.2488025672799998}, "freq": {"token": 1, "current": 1, "review": 2, "nap": 1, "too": 1, "feed": 1, "math": 1, "detail": 1, "other": 1, "introduct": 1, "new": 1, "attent": 3, "integ": 2, "numth": 2, "previous": 1, "noth": 1, "network": 4, "like": 1, "neural": 3, "through": 1, "phd": 1, "imagin": 1, "except": 1, "rnncell": 1, "work": 1, "transform": 1, "state": 1, "cell": 1, "forb": 1, "higher": 1, "know": 1, "check": 1, "two": 2, "afternoon": 1, "cho": 1, "and": 4, "discret": 1, "particular": 1, "colah": 1, "bit": 1, "creat": 1, "great": 1, "project": 1, "num": 7, "embed": 2, "here": 1, "cover": 1, "william": 1, "hochreit": 1, "word": 8, "for": 4, "explain": 1, "falcon": 1, "output": 1, "everyon": 2, "today": 1, "with": 2, "deep": 1, "assum": 1, "are": 2, "step": 2, "vocab": 2, "better": 1, "chris": 1, "use": 2, "translat": 1, "look": 1, "improv": 1, "but": 1, "rnns": 3, "obtain": 1, "our": 1, "there": 1, "writer": 1, "space": 1, "flavor": 1, "scare": 1, "comment": 1, "again": 1, "repres": 1, "have": 2, "take": 2, "make": 1, "way": 1, "out": 1, "model": 3, "represent": 1, "ugh\u2026": 1, "goal": 1, "unit": 1, "time": 2, "tutori": 2, "continu": 1, "hidden": 2, "embeddinglay": 1, "while": 1, "mean": 1, "short": 2, "sequenc": 5, "anim": 1, "concret": 1, "come": 1, "research": 2, "exampl": 1, "dimens": 1, "the": 7, "dimension": 1, "grus": 1, "from": 1, "everi": 1, "input": 1, "bengio": 1, "map": 1, "machin": 1, "want": 2, "which": 2, "into": 2, "recurr": 1, "let": 3}, "idf": {"token": 33.7070063694, "current": 1.5325803649, "review": 2.2099109131400003, "nap": 126.0, "too": 1.81585268215, "feed": 7.77853993141, "math": 22.0806675939, "detail": 2.26186066391, "other": 1.00992366412, "introduct": 2.7808723068799996, "new": 1.0178880554, "attent": 2.81040892193, "integ": 46.017391304300006, "numth": 1.1751295336799998, "previous": 1.42846859816, "noth": 3.46410648047, "network": 2.59369384088, "like": 1.14918566775, "neural": 59.4606741573, "through": 1.07074930869, "phd": 22.3605633803, "imagin": 6.598503740650001, "except": 1.71948445792, "rnncell": 132.3, "work": 1.11520089913, "transform": 3.42007755278, "state": 1.0477133240899998, "cell": 7.1033557047, "forb": 17.237785016300002, "higher": 2.1218925421, "know": 2.59327017315, "check": 6.50655737705, "two": 1.01379310345, "afternoon": 10.3696930111, "cho": 132.3, "and": 1.00006299213, "discret": 15.0056710775, "particular": 1.3814827706200001, "colah": 132.3, "bit": 8.33385826772, "creat": 1.2492917847, "great": 1.26592775696, "project": 1.7534791252500002, "num": 1.00031504001, "embed": 16.835630965, "here": 2.42307692308, "cover": 1.69380134429, "william": 1.75483585719, "hochreit": 132.3, "word": 1.7965372864099998, "for": 1.00031504001, "explain": 2.60049140049, "falcon": 23.381443299, "output": 7.676982591880001, "everyon": 6.3964544722, "today": 1.74961428257, "with": 1.0011982089899998, "deep": 3.6279707495399998, "assum": 2.9575260804799997, "are": 1.02990593578, "step": 2.8279301745599996, "vocab": 132.3, "better": 2.0065722952500002, "chris": 5.979661016950001, "use": 1.0296387573799999, "translat": 2.85745140389, "look": 1.9086318826599997, "improv": 2.04376930999, "but": 1.01632417899, "rnns": 132.3, "obtain": 2.68629441624, "our": 2.35758835759, "there": 1.04091266719, "writer": 2.75816539263, "space": 2.39818731118, "flavor": 32.33401222, "scare": 24.5758513932, "comment": 3.05954904606, "again": 1.50883862384, "repres": 1.46972782818, "have": 1.0148948411399998, "take": 1.13961668222, "make": 1.0762660158600001, "way": 1.2190739461, "out": 1.06016694491, "model": 2.0905978404, "represent": 5.928304705, "ugh\u2026": 132.3, "goal": 3.28152128979, "unit": 1.15394679459, "time": 1.01127460348, "tutori": 59.4606741573, "continu": 1.13928955867, "hidden": 7.81299212598, "embeddinglay": 132.3, "while": 1.0441988950299999, "mean": 1.44906900329, "short": 1.41295834817, "sequenc": 6.07112810707, "anim": 2.8395635843299996, "concret": 10.0100882724, "come": 1.32831325301, "research": 1.9420183486200002, "exampl": 1.50483412322, "dimens": 8.25585023401, "the": 1.0, "dimension": 54.1843003413, "grus": 132.3, "from": 1.00056721497, "everi": 1.47917637194, "input": 12.2029208301, "bengio": 132.3, "map": 4.0728578758300005, "machin": 4.02433460076, "want": 1.99698113208, "which": 1.005191845, "into": 1.01502461479, "recurr": 35.5964125561, "let": 3.48616600791}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Attention Craving RNNS: Building Up To Transformer Networks</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Attention Craving RNNS: Building Up To Transformer Networks Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\" rel=\"prev\" title=\"Fors Marsh Group: Lead Data Scientist [Arlington, VA]\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\" rel=\"next\" title=\"Generative Adversarial Networks \u2013 Key Milestones and State of the Art\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=93093\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-93093 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 24-Apr, 2019  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/index.html\">Apr</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/tutorials.html\">Tutorials, Overviews</a> \u00bb Attention Craving RNNS: Building Up To Transformer Networks (\u00a0<a href=\"/2019/n17.html\">19:n17</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Attention Craving RNNS: Building Up To Transformer Networks</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/ai\" rel=\"tag\">AI</a>, <a href=\"https://www.kdnuggets.com/tag/data-science\" rel=\"tag\">Data Science</a>, <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/recurrent-neural-networks\" rel=\"tag\">Recurrent Neural Networks</a></div>\n<br/>\n<p class=\"excerpt\">\n     RNNs let us model sequences in neural networks. While there are other ways of modeling sequences, RNNs are particularly useful. RNNs come in two flavors, LSTMs (Hochreiter et al, 1997) and GRUs (Cho et al, 2014)\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"/2019/04/attention-craving-rnn-building-transformer-networks.html?page=2#comments\">comments</a></div>\n<p><b>By <a href=\"https://www.williamfalcon.com/\" rel=\"noopener noreferrer\" target=\"_blank\">William Falcon</a>, PhD Researcher, AI researcher and AI writer for Forbes</b></p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://i.ibb.co/G7nfLsS/parks-and-recreation.gif\" width=\"40%\"/></div>\n<p>Adding attention to your neural networks is a bit like wanting to take an afternoon nap at work. You know it\u2019s better for you, everyone wants to do it, but <b>everyone\u2019s too scared to</b>.</p>\n<p>My goal today is to assume nothing, explain the details with animations, and make math great again (MMGA? ugh\u2026)</p>\n<p><strong>Here we\u2019ll cover:</strong></p>\n<ol>\n<li>Short RNN review.</li>\n<li>Short sequence to sequence model review.</li>\n<li>Attention in RNN's.</li>\n<li>Improvements to attention.</li>\n<li>Transformer network introduction.</li>\n</ol>\n<p>\u00a0</p>\n<h3>Recurrent Neural Networks (RNN) </h3>\n<p>\u00a0 </p>\n<p>RNNs let us model sequences in neural networks. While there are other ways of modeling sequences, RNNs are particularly useful. RNNs come in two flavors, LSTM's <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&amp;rep=rep1&amp;type=pdf\" rel=\"noopener noreferrer\" target=\"_blank\">(Hochreiter et al, 1997)</a> and GRUs <a href=\"https://arxiv.org/pdf/1406.1078.pdf\" rel=\"nopener noreferrer\" target=\"_blank\">(Cho et al, 2014)</a>. For a deep tutorial, check out <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noopener noreferrer\" target=\"_blank\">Chris Colah\u2019s tutorial</a>.</p>\n<p><strong>Let\u2019s look at machine translation for a concrete example of an RNN.</strong></p>\n<ol>\n<li>Imagine we have an RNN with 56 hidden units.</li>\n<pre>\r\n<code>\r\nrnn_cell = rnn_cell(input_dim=100, output_dim=56)\r\n</code>\r\n</pre>\n<li>We have a word \u201cNYU\u201d which is represented by the integer 12 meaning it\u2019s the 12th word in the vocab I created.</li>\n<pre>\r\n<code>\r\n# 'NYU' is the 12th word in my vocab\r\nword = 'NYU'\r\nword = VOCAB[word]\r\n\r\nprint(word)\r\n# 11\r\n</code>\r\n</pre>\n</ol>\n<p>Except we don\u2019t feed an integer into the RNN, we use a higher dimensional representation which we currently obtain through <a href=\"https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\" rel=\"nopener noreferrer\" target=\"_blank\">embeddings</a>. An embedding lets us map a sequence of discrete tokens into continuous space <a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">(Bengio et al, 2003)</a>.</p>\n<pre>\r\n<code>\r\nembedding_layer = Embedding(vocab_size=120, embedding_dim=10)\r\n\r\n# project our word to 10 dimensions\r\nx = embedding_layer(x)\r\n</code>\r\n</pre>\n<p>An RNN cell takes in two inputs, a word <b>x</b>, and a hidden state from the previous time step <b>h</b>. At every time step, it outputs a new <b>h</b>.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://i.ibb.co/CB3zRyY/rnn-cell.gif\" width=\"50%\"/><font size=\"-1\"></font></div></div></div></div></div></div></body></html>\n<div class=\"caption\"><strong>RNN CELL: next_h= f(x, prev_h).</strong></div>\n<p></p>\n\n<p><em>*Tip: For the first step h is normally just zeros.</em></p>\n<pre>\r\n<code>\r\n# 1 word, RNN has 56 hidden units\r\nh_0 = np.zeros(1, 56)\r\n</code>\r\n</pre>\n<p>This is important: RNN <em>cell</em> is <b>DIFFERENT</b> from an RNN.</p>\n<p>There\u2019s a <b>MAJOR</b> point of confusion in RNN terminology. In deep learning frameworks like <a href=\"https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNNCell\" rel=\"nopener noreferrer\" target=\"_blank\">Pytorch</a> and <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell\" rel=\"noopener noreferrer\" target=\"_blank\">Tensorflow</a>, the RNN CELL is the unit that performs this computation:</p>\n<pre>\r\n<code>\r\nh1 = rnn_cell(x, h0)\r\n</code>\r\n</pre>\n<p>the RNN <em>NETWORK</em> for loops the cell over the time steps</p>\n<div style=\"width:98%;border:1px solid #ccc;overflow:auto;padding-left:10px;padding-top:10px\">\n<pre>\r\ndef RNN(sentence):\r\n  prev_h = h_0\r\n\r\n  all_h = []\r\n  for word in sentence:\r\n    # use the RNN CELL at each time step\r\n    current_h = rnn_cell(embed(word), prev_h)\r\n    all_h.append(current_h)\r\n\r\n  # RNNs output a hidden vector h at each time step\r\n  return all_h\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p><strong>Here\u2019s an illustration of an RNN moving the same RNN cell over time:</strong></p>\n<div style=\"text-align:center\"><img alt=\"rnn-move-rnn-cel\" src=\"https://i.ibb.co/LdL7CM5/rnn-move-rnn-cell.gif\" width=\"99%\"/><font size=\"-1\"></font></div>\n<div class=\"caption\"><strong>The RNN moves the RNN cell over time. For attention, we\u2019ll use ALL the h\u2019s produced at each timestep</strong></div>\n<p></p>\n\n<p>\u00a0</p>\n<h3>Sequence To Sequence Models (Seq2Seq) </h3>\n<p>\u00a0 </p>\n<p>Now you\u2019re a pro at RNNs, but let\u2019s take it easy for a minute.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://i.ibb.co/sjyyvJ8/anna-kendrick.gif\" width=\"40%\"/><font size=\"-1\"></font></div>\n<div class=\"caption\">Chill</div>\n<p></p>\n\n<p>RNNs can be used as blocks into larger deep learning systems.</p>\n<p>One such system is a Seq2Seq model introduced by Bengio\u2019s group <a href=\"https://arxiv.org/pdf/1406.1078.pdf\" rel=\"nopener noreferrer\" target=\"_blank\">(Cho et al, 2014)</a>  and Google <a href=\"\" rel=\"nopener noreferrer\" target=\"_blank\">(Sutskever et al, 2014)</a>, which can be used to translate a sequence to another. You can frame a lot of problems as translation:</p>\n<ol>\n<li>Translate English to Spanish.</li>\n<li>Translate a video sequence into another sequence.</li>\n<li>Translate a sequence of instructions into programming code.</li>\n<li>Translate user behavior into future user behavior</li>\n<li>\u2026</li>\n<li>The only limit is your creativity!</li>\n</ol>\n<p>A seq2seq model is nothing more than 2 RNNs, an encoder (E), and decoder (D).</p>\n<div style=\"width:98%;border:1px solid #ccc;overflow:auto;padding-left:10px;padding-bottom:10px;padding-top:10px\">\n<pre>\r\nclass Seq2Seq(object):\r\n\r\n  def __init__():\r\n      self.encoder = RNN(...)\r\n      self.decoder = RNN(...)\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The seq2seq model has 2 major steps:</p>\n<p><b>Step 1: <em>Encode</em> a sequence:</b></p>\n<pre>\r\n<code>\r\nsentence = [\"NYU\", \"NLP\", \"rocks\", \"!\"]\r\nall_h = Seq2Seq.encoder(sentence)\r\n\r\n# all_h now has 4 h (activations)\r\n</code>\r\n</pre>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://i.ibb.co/LdL7CM5/rnn-move-rnn-cell.gif\" width=\"99%\"/><font size=\"-1\"></font></div>\n<div class=\"caption\">Encoding</div>\n<p></p>\n\n<p><strong>Step 2: Decode to generate a\u201ctranslation.\u201d</strong></p>\n<p>This part gets really involved. The encoder in the previous step processed the full sequence at once (ie: it was a vanilla RNN).</p>\n<p>In this second step, we run the decoder RNN one step at a time to generate predictions autoregressively (this is fancy for using the output of the previous step as the input to the next step).</p>\n<p>There are two major ways of doing the decoding:</p>\n<p><b>Option 1: Greedy Decoding</b></p>\n<ol>\n<li>Run 1 step of the decoder.</li>\n<li>Pick the highest probability output.</li>\n<li>Use this output as the input to the next step</li>\n</ol>\n<div style=\"width:98%;border:1px solid #ccc;overflow:auto;padding-left:10px;padding-bottom:10px;padding-top:10px\">\n<pre>\r\n# you have to seed the first x since there are no predictions yet\r\n# SOS means start of sentence\r\ncurrent_X_token = ''\r\n\r\n# we also use the last hidden output of the encoder (or set to zero)\r\nh_option_1 = hs[-1]\r\nh_option_2 = zeros(...)\r\n\r\n# let's use option 1 where it's the last h produced by the encoder\r\ndec_h = h_option_1\r\n\r\n# run greedy search until the RNN generates an End-of-Sentence token\r\nwhile current_X_token != 'EOS':\r\n\r\n   # keep the output h for next step\r\n   next_h = decoder(dec_h, current_X_token)\r\n\r\n   # use new h to find most probable next word using classifier\r\n   next_token = max(softmax(fully_connected_layer(next_h)))\r\n\r\n   # *KEY* prepare for next pass by updating pointers\r\n   current_X_token = next_token\r\n   dec_h = next_h\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>It\u2019s called greedy because we always go with the highest probability next word.</p>\n<p><b>Option 2: Beam Search</b></p>\n<p>There\u2019s a better technique called Beam Search, which considers multiple paths through the decoding process. Colloquially, a beam search of width 5 means we consider 5 possible sequences with the maximum log likelihood (math talk for 5 most probable sequences).</p>\n<p>At a high-level, instead of taking the highest probability prediction, we keep the top k (beam size = k). Notice below, at each step we have 5 options (5 with the highest probability).</p>\n<div style=\"text-align:center\"><img alt=\"beam-search\" src=\"https://i.ibb.co/fS8K2Xn/beam-search.png\" width=\"99%\"/><font size=\"-1\"></font></div>\n<div class=\"caption\">\nBeam search figure found <a href=\"https://www.researchgate.net/publication/317377611_Retrosynthetic_Reaction_Prediction_Using_Neural_Sequence-to-Sequence_Models/figures?lo=1&amp;utm_source=google&amp;utm_medium=organic\" rel=\"nopener noreferrer\" target=\"_blank\">here</a> </div>\n<p></p>\n\n<p><a href=\"https://www.youtube.com/watch?v=UXW6Cs82UKo\" rel=\"noopener noreferrer\" target=\"_blank\">This youtube video</a> has a detailed beam search tutorial!</p>\n<p>So, the full seq2seq process with greedy decoding as an animation to translate <em>\u201cNYU NLP is awesome\u201d</em> into Spanish looks like this:</p>\n<div style=\"text-align:center\"><img alt=\"seq-2-seq-rnn-encoder-decoder\" src=\"https://i.ibb.co/PCW8Wb8/seq-2-seq-rnn-encoder-decoder.gif\" width=\"99%\"/><font size=\"-1\"></font></div>\n<div class=\"caption\"><strong>Seq2Seq is made up of 2 RNNs an encoder and decoder</strong></div>\n<p></p>\n\n<p><b>This model has various parts:</b></p>\n<ol>\n<li>Blue RNN is the encoder.</li>\n<li>Red RNN is the decoder</li>\n<li>The blue rectangle on top of the decoder is a fully connected layer with a softmax. This picks the most likely next word.</li>\n</ol>\n\n<div class=\"page-link\"><p>Pages: 1 <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html/2\">2</a></p></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end-->\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/index.html\">Apr</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/tutorials.html\">Tutorials, Overviews</a> \u00bb Attention Craving RNNS: Building Up To Transformer Networks (\u00a0<a href=\"/2019/n17.html\">19:n17</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end-->\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556324557\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper-->\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n\n\n<!-- Dynamic page generated in 0.704 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 20:22:37 -->\n<!-- Compression = gzip -->", "content_tokenized": ["comment", "william", "falcon", "phd", "research", "research", "and", "writer", "for", "forb", "attent", "neural", "network", "bit", "like", "want", "take", "afternoon", "nap", "work", "know", "better", "for", "everyon", "want", "but", "everyon", "too", "scare", "goal", "today", "assum", "noth", "explain", "the", "detail", "with", "anim", "and", "make", "math", "great", "again", "ugh\u2026", "here", "cover", "short", "review", "short", "sequenc", "sequenc", "model", "review", "attent", "improv", "attent", "transform", "network", "introduct", "recurr", "neural", "network", "rnns", "let", "model", "sequenc", "neural", "network", "while", "there", "are", "other", "way", "model", "sequenc", "rnns", "are", "particular", "use", "rnns", "come", "two", "flavor", "hochreit", "num", "and", "grus", "cho", "num", "for", "deep", "tutori", "check", "out", "chris", "colah", "tutori", "let", "look", "machin", "translat", "for", "concret", "exampl", "imagin", "have", "with", "num", "hidden", "unit", "rnncell", "have", "word", "which", "repres", "the", "integ", "num", "mean", "the", "numth", "word", "the", "vocab", "creat", "the", "numth", "word", "vocab", "word", "word", "word", "num", "except", "feed", "integ", "into", "the", "use", "higher", "dimension", "represent", "which", "current", "obtain", "through", "embed", "embed", "let", "map", "sequenc", "discret", "token", "into", "continu", "space", "bengio", "num", "embeddinglay", "project", "our", "word", "num", "dimens", "cell", "take", "two", "input", "word", "and", "hidden", "state", "from", "the", "previous", "time", "step", "everi", "time", "step", "output", "new"], "timestamp_scraper": 1556366538.129458, "title": "Attention Craving RNNS: Building Up To Transformer Networks", "read_time": 90.6, "content_html": "<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"/2019/04/attention-craving-rnn-building-transformer-networks.html?page=2#comments\">comments</a></div>\n<p><b>By <a href=\"https://www.williamfalcon.com/\" rel=\"noopener noreferrer\" target=\"_blank\">William Falcon</a>, PhD Researcher, AI researcher and AI writer for Forbes</b></p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://i.ibb.co/G7nfLsS/parks-and-recreation.gif\" width=\"40%\"/></div>\n<p>Adding attention to your neural networks is a bit like wanting to take an afternoon nap at work. You know it\u2019s better for you, everyone wants to do it, but <b>everyone\u2019s too scared to</b>.</p>\n<p>My goal today is to assume nothing, explain the details with animations, and make math great again (MMGA? ugh\u2026)</p>\n<p><strong>Here we\u2019ll cover:</strong></p>\n<ol>\n<li>Short RNN review.</li>\n<li>Short sequence to sequence model review.</li>\n<li>Attention in RNN's.</li>\n<li>Improvements to attention.</li>\n<li>Transformer network introduction.</li>\n</ol>\n<p>\u00a0</p>\n<h3>Recurrent Neural Networks (RNN) </h3>\n<p>\u00a0 </p>\n<p>RNNs let us model sequences in neural networks. While there are other ways of modeling sequences, RNNs are particularly useful. RNNs come in two flavors, LSTM's <a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&amp;rep=rep1&amp;type=pdf\" rel=\"noopener noreferrer\" target=\"_blank\">(Hochreiter et al, 1997)</a> and GRUs <a href=\"https://arxiv.org/pdf/1406.1078.pdf\" rel=\"nopener noreferrer\" target=\"_blank\">(Cho et al, 2014)</a>. For a deep tutorial, check out <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noopener noreferrer\" target=\"_blank\">Chris Colah\u2019s tutorial</a>.</p>\n<p><strong>Let\u2019s look at machine translation for a concrete example of an RNN.</strong></p>\n<ol>\n<li>Imagine we have an RNN with 56 hidden units.</li>\n<pre>\r\n<code>\r\nrnn_cell = rnn_cell(input_dim=100, output_dim=56)\r\n</code>\r\n</pre>\n<li>We have a word \u201cNYU\u201d which is represented by the integer 12 meaning it\u2019s the 12th word in the vocab I created.</li>\n<pre>\r\n<code>\r\n# 'NYU' is the 12th word in my vocab\r\nword = 'NYU'\r\nword = VOCAB[word]\r\n\r\nprint(word)\r\n# 11\r\n</code>\r\n</pre>\n</ol>\n<p>Except we don\u2019t feed an integer into the RNN, we use a higher dimensional representation which we currently obtain through <a href=\"https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\" rel=\"nopener noreferrer\" target=\"_blank\">embeddings</a>. An embedding lets us map a sequence of discrete tokens into continuous space <a href=\"https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">(Bengio et al, 2003)</a>.</p>\n<pre>\r\n<code>\r\nembedding_layer = Embedding(vocab_size=120, embedding_dim=10)\r\n\r\n# project our word to 10 dimensions\r\nx = embedding_layer(x)\r\n</code>\r\n</pre>\n<p>An RNN cell takes in two inputs, a word <b>x</b>, and a hidden state from the previous time step <b>h</b>. At every time step, it outputs a new <b>h</b>.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://i.ibb.co/CB3zRyY/rnn-cell.gif\" width=\"50%\"/><font size=\"-1\"></font></div></div> ", "website": "kdnuggets"}