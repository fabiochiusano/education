{"content": "By Chaitanya Sagar, Perceptive Analytics . It All Starts With The Text There is so much of information lying in the text posts made by you and me and all others about all the trending topics today. Being in our respective firms, big or small, each of us collect some data related to our respective businesses and store it to analyze for various projects. At the same time, we all need this \u2018unstructured data\u2019 to know and understand more about our clients, customers and the state of our company in the world today. However, working with this data is not easy. The data is not structured, every piece does not have all the information and each part is unique. This is how textual data is. It needs to be processed first and converted in a form that is suitable for analysis. This is very similar to our own databases which we create except that they cannot be used directly and the amount of data is very large. This article opens up the world of text mining in a simple and intuitive way and provides great tips to get started with text mining. Tip #1: Think First The mammoth of text mining can become a simple task if you work on it with a plan in mind. Think what you need to do with text before going all out on it. What is your objective behind text mining? What sources of data do you want to use? How much data do you need for it to be sufficient? How do you plan to present your results from the data? It is all about getting curious about your problem and break it into small fragments. Thinking through the problem also opens up your mind towards the various situations you may encounter and ways to tackle those situations. You can then chart out a workflow and start pursuing the task. Tip #2: R or Python.. Or Something Else? There is no gold standard procedure for text mining. You have to choose the method which is most convenient for text mining. This is where factors such as efficiency,effectiveness type of problem and other factors come into play and helps you decide the best candidate for your problem. After having decided your chosen path, you need to build your knowledge and skills in developing skills in that language. I find the text mining techniques more intuitive in Python than in R but R has some handy functions to do tasks such as word counting and is richer in terms of packages available for text mining. Tip #3: Start Early and Collect Your Data The usual process of text mining involves the following steps: Collect data; either from social media such as twitter or other websites. Write your code that can adjust to the specific type of text you collect and store it Convert your data into readable text Remove special characters from the text (such as hashtags). You can add a hashtag count feature if that is required Removing numbers from the text data (unless the problem requires numbers) Deciding whether to keep all the data or remove some of it such as all non-English text Converting all the text to uppercase or lowercase only to ease analysis Removing stop words.. Words that have no use in your analysis. This includes articles, conjunctions, etc. Using word stemming and grouping similar words such as \u2018keep\u2019 and \u2018keeping\u2019 are same words used in different tense form. Final analysis of the processed stemmed words and visualize results The steps are short and simple but they all depend on the first step executed well. You need to collect your data so that text mining can be performed on it. There are many ways to collect data. One of the most popular sources to collect data from is Twitter. Twitter has exposed some APIs so that tweets can be mined using both R and Python. Besides twitter, one can capture data from any website today including e-commerce websites, movie websites, song websites, etc. Some websites also contain preformatted repositories of text data such as project gutenberg, corpora, etc. Google trends and yahoo also offer some analysis online. Tip #4: Find and Use The Best Way to Convert Text to Data Based on the tools and your project objective, you may use a different approach to convert your collected text to data. If you are using R, packages such as twitteR, tm and stringr are what you may be using for most of the preprocessing. The nltk library and Tweepy package are the equivalent packages in Python. Whichever language and package you use, make sure that you have enough resources and memory to handle the data. Text mining can be cumbersome just because of the irrelavant text lying around in your data even after removing stop words. Using a good method to prepare data will give you a lot of useful information when you apply modelling techniques on the data. Tip #5: Explore and Play Around You need to know your data before preprocessing it. Without the knowledge of how your data looks like, you might carelessly remove text which might have been useful in your analysis. There are many standard methods and dictionaries of removing stopwords and assigning importance to words but they may or may not apply to your data. For example, data about the government may include a lot of words such as \u2018rule\u2019, \u2018govern\u2019 and \u2018politics\u2019 which you may deem unnecessary and want to remove. Reviews may include lots of \u2018hi\u2019 in the beginning but may not be useful for a review dataset. It is always a good step to look at the source of data and go through some of the text to know how the process you defined for analysis is working to transform it correctly into useful information. Other ways specific to exploring text data is by creating a document term matrix. A document term matrix is a m*n matrix where the number of columns denote the total number of unique words in the entire dataset and the number of rows denote the total data points. Each cell thus represents the count of the particular word in that datapoint. This is a very large matrix and is later collapsed into term-frequency. From this document term matrix, one can count the total occurrences of each word in the dataset and that is exactly what term-frequency matrix stores. Other uses of document term matrix include knowing correlation between words, drawing a word cloud using term-frequency or predicting patterns using modelling techniques. This exploration will further give you confidence on the best way to move forward with textual data analysis. Tip #6: Dive Deep and Get Your Hands Dirty The primary objective or every machine learning and data science project is to find patterns in the data that are otherwise hard to find. You need to look for those interesting patterns and are not a true data scientist if you\u2019re scared of this step. It can be as simple as fitting a simple classifier to classify data points and see its performance. This will set a benchmark while giving you an idea of the predicting ability of the data. At times, the data may be biased or have a poor predictive power and data quality checks can help define this. For example, If I am collecting twitter data on the basis of hashtags, I can divide my collected data into train and test datasets keeping the hashtags as the dependent feature. If my prediction performance is not up to the mark, I need to go back a few steps and find out the cause of this low performance and then check how I am collecting data or how I am cleaning my data as the case may be. Other ways of getting patterns involve associations. For example, some data points may be related to each other while others may have a similar or opposite pattern. If tweets are being used for text mining, there can be duplicate tweets because of retweeting or debates going for or against a remark. Working with data also exposes problems such as dealing with sarcasm or comments that convey mixed expressions. Without brushing through the data, it will be difficult to know how much of your data is affected by these problems and whether you should drop such data or use some technique to handle the situation. Tip #7: Rework and Repeat The problem you are trying to solve may or may not be the first text mining problem in your company but it is certainly not the first text mining problem in the world today. There are several data scientists out there who have worked on either the same or similar problem as the one you are working on and knowing what methods they followed and what they did differently will help you take your problem solving to the next level. Though not as frequent as other domains, there are several analysis and projects being done on text mining which include finding the trending topics, sentiment analysis on the trending topics, identifying remarks about your firm or product, identifying grievances and appreciations and the like. With the same data, there can be more than one problem that can be solved. Complex problems which can be explored also include NLP and topic modelling. I read about a fairly recent project in which some students predicted the next topic which a group of people will discuss based on the current conversation. There can be many such new projects which can be thought of and pursued in the area of text mining but since it is a new and hot field to work on, always refer other similar data and resources to further compliment your analysis and come up with strong insights. Tip #8: Presenting Text Visually As mentioned earlier, there can be a lot of problems which can be pursued using text mining and more than one problem can be solved from the same data. With so much to present, it is a good practice to come up with ways to present the results in a way that would seem attractive to people. This is why most of the text mining results are already visualized in the form of word clouds, sentiment studies and figures. There are a packages and libraries for each such task which include wordcloud, ggplot2, igraph, text2vec, networkD3 and plotly in R and Networkx, matplotlib, plotly in Python. You can also use other sophisticated tools just for visualization such as Tableau or Power BI which can help visualize your data in many more ways. Conclusion: A Roadmap Visualizing results is not the end step in text mining projects. Since text is captured from online sources, it is constantly changing and so is the data that is captured. With the changing data comes changing insights and hence, when the project is completed and accepted, it should be continuously updated with new data and new insights. These insights can be further enriched with the rate of change. With time, the change can also be captured and used as a metric of progression. This becomes another longitudinal problem to be solved. Apart from the problems which can be pursued with text data, text mining is no easy feat. When you create a roadmap of collecting, cleaning and analyzing data, there may be several obstacles that will come your way. They can be situations when you have to decide whether to work with a single word frequency in document term matrix or use groups of words (known as n-grams) or building your own visualization method to present your results or memory management. At the same time, new projects are coming up in the area of text mining. The best way to learn is to face the problem hands on and learn from the experience of working on the problem. Hope this article provides motivation to head to the world of text and start mining insightful nuggets of information. \u00a0 Bio: Chaitanya Sagar is the Founder and CEO of Perceptive Analytics . Perceptive Analytics has been chosen as one of the top 10 analytics companies to watch out for by Analytics India Magazine. It works on Marketing Analytics for e-commerce, Retail and Pharma companies. Related: Top 10 Machine Learning with R Videos Learn Generalized Linear Models (GLM) using R A Solution to Missing Data: Imputation Using R", "title_html": "<h1 id=\"title\">Tips for Getting Started with Text Mining in R and Python</h1> ", "url": "https://www.kdnuggets.com/2017/11/getting-started-text-mining-r-python.html", "tfidf": {"tfidf": {"after": 2.04140414042, "mark": 1.5079787234, "watch": 3.92581602374, "song": 3.4333910034599997, "googl": 11.388809182200001, "equival": 4.09175257732, "longitudin": 46.557184750699996, "fit": 3.37070063694, "plot": 10.767039674460001, "basi": 2.42122922068, "magazin": 2.4312404287900002, "form": 3.38267045454, "collaps": 4.26659500134, "new": 5.0894402770000005, "assign": 3.83663605607, "dirti": 18.2482758621, "number": 5.50714583045, "etc": 12.6200317965, "dataset": 774.439024392, "function": 2.495441685, "ecommerc": 2886.54545454, "client": 14.1371326803, "conjunct": 9.43315508021, "progress": 2.44697903822, "python": 281.489361702, "thought": 1.9854927463699998, "well": 1.0655748708, "product": 1.62264922322, "done": 2.3302509907499998, "tableau": 244.246153846, "complet": 1.24021560816, "approach": 2.07556543339, "path": 4.6421052631599995, "toward": 1.6303142329, "know": 15.559621038900001, "motiv": 5.01611374408, "preformat": 1587.6, "particular": 1.3814827706200001, "creat": 3.7478753541, "express": 1.9120799710900003, "readabl": 74.8867924528, "how": 12.82002624408, "test": 2.65707112971, "present": 6.27758007115, "obstacl": 13.963060685999999, "than": 3.0983606557499996, "special": 1.4881889763799998, "would": 1.0828729281799998, "end": 1.10680423871, "correl": 13.1860465116, "word": 32.337671155379994, "interest": 1.60331246213, "deep": 3.6279707495399998, "depend": 4.4822134387400006, "will": 8.57367690172, "associ": 1.3263157894700002, "open": 2.49113447356, "compliment": 50.2405063291, "otherwis": 3.72151898734, "updat": 5.56466876972, "next": 2.9901120632800002, "perform": 6.125590817000001, "but": 6.09794507394, "certain": 1.8077886586200003, "need": 12.935361216689998, "our": 11.78794178795, "uppercas": 305.307692308, "final": 1.34008609775, "point": 3.7797000238200003, "pharma": 407.07692307699995, "review": 4.419821826280001, "richer": 47.25, "level": 1.6544393497299998, "has": 3.1309492505999996, "enough": 2.2319696330700003, "besid": 5.1362018764199995, "also": 7.10335570469, "textnumvec": 1587.6, "adjust": 7.112903225810001, "use": 27.800246449259998, "experi": 1.87062566278, "govern": 3.01882487164, "resourc": 5.8974739970200005, "networkx": 1587.6, "out": 5.30083472455, "model": 8.3623913616, "good": 4.55944859277, "alway": 4.13491340018, "yahoo": 39.2, "count": 13.92631578948, "field": 1.7790228597, "store": 10.34042553192, "short": 1.41295834817, "process": 6.78099305928, "not": 12.18808777428, "strong": 1.6439888163999998, "involv": 2.8997260274000003, "henc": 5.390831918509999, "founder": 4.033536585369999, "appreci": 8.11241696474, "respect": 3.2886587260400004, "specif": 3.7438981252199994, "easi": 10.5875291764, "metric": 22.235294117600002, "handl": 7.845811712380001, "though": 1.36076112111, "then": 2.17315721032, "they": 6.18103951722, "anoth": 1.13643521832, "wordcloud": 1587.6, "effect": 1.3963060686000002, "choos": 4.17899447223, "world": 4.45360824744, "scienc": 2.31969608416, "sentiment": 19.845, "chosen": 7.18533604888, "attract": 2.53326950694, "stem": 14.907042253520002, "draw": 2.97247706422, "featur": 3.05425163524, "suffici": 4.3117870722400005, "careless": 56.7, "percept": 25.04100946371, "practic": 1.70434782609, "grievanc": 37.6208530806, "alreadi": 1.9551724137900002, "dive": 16.085106383, "provid": 2.43105428374, "twitter": 199.28033472780004, "offer": 1.53896859248, "either": 3.1660185462199997, "develop": 1.1955719557200002, "should": 3.3286508019800003, "cell": 7.1033557047, "pattern": 18.958681633650002, "sever": 3.2172385841699995, "hand": 3.2304405331200003, "enrich": 22.3291139241, "explor": 13.58374331552, "stopword": 1587.6, "lie": 6.4314360948000004, "without": 2.59094247246, "lot": 17.63510136072, "some": 10.4036697248, "sourc": 6.79041916168, "small": 2.7189587258, "collect": 19.69319826336, "add": 4.61243463103, "may": 16.83228414288, "result": 6.87669650592, "document": 12.704865557, "confid": 6.327620565959999, "duplic": 19.7955112219, "repositori": 44.974504249300004, "india": 3.92387543253, "miss": 3.53664513255, "media": 2.59369384088, "caus": 1.38521943984, "ani": 1.13383802314, "sophist": 10.0037807183, "divid": 2.3169877408099997, "from": 11.00623936467, "firm": 7.415226529660001, "num": 10.003150400100001, "market": 2.36602086438, "els": 5.44444444444, "idea": 2.0930784443, "movi": 4.00403530895, "for": 20.006300800200002, "difficult": 2.48957189901, "predict": 25.92423252775, "remark": 7.703056768560001, "with": 19.022765970809996, "handi": 102.425806452, "stop": 4.3567508232800005, "are": 17.50840090826, "solut": 4.7278141751, "poor": 2.42196796339, "compani": 6.2094455852, "current": 1.5325803649, "frequent": 2.10501193317, "preprocess": 2442.46153846, "look": 5.725895647979999, "popular": 1.50769230769, "conveni": 9.85474860335, "video": 3.29719626168, "rework": 28.0, "tweepi": 1587.6, "veri": 3.77640342531, "task": 15.54565483476, "whichev": 76.3269230769, "think": 8.721479582490002, "trend": 21.72562435852, "unless": 5.44818119423, "true": 2.55569864778, "convey": 12.297443842, "networkdnum": 1587.6, "later": 1.08650424309, "scare": 24.5758513932, "column": 7.078020508250001, "nugget": 113.4, "tackl": 19.8698372966, "remov": 16.046493998720003, "group": 3.62990625714, "websit": 15.12960609912, "doe": 1.70581282905, "lowercas": 162.0, "convers": 3.3486606201200004, "sagar": 705.6, "give": 4.095975232200001, "check": 13.0131147541, "train": 1.9365698950999999, "great": 1.26592775696, "step": 19.79551122192, "total": 4.638036809819999, "move": 1.29125660838, "big": 2.7400759406299997, "librari": 5.3653261237, "accept": 1.7377408056, "ggplotnum": 1587.6, "analyz": 19.37278828554, "scientist": 9.38852749852, "machin": 8.04866920152, "into": 6.09014768874, "bias": 13.7335640138, "ngram": 1587.6, "through": 3.21224792607, "constant": 3.6589075823900004, "requir": 3.05689804564, "where": 2.13430127042, "figur": 2.0343413634, "face": 1.80327124035, "earlier": 1.86776470588, "social": 1.9904714142400002, "head": 1.57781753131, "against": 1.2902072328299998, "workflow": 369.209302326, "help": 5.59851891036, "affect": 2.4794627518400003, "larg": 2.3714989917, "datapoint": 1587.6, "abil": 2.70875277256, "conclus": 4.84615384615, "even": 1.16461267606, "own": 2.35688836104, "base": 2.2925631769, "relat": 3.71252630757, "deal": 2.18346857379, "who": 1.06279287723, "decid": 7.703056768560001, "hot": 4.6178010471199995, "start": 6.3336790872, "fragment": 8.628260869570001, "post": 2.23826307627, "factor": 5.78255326898, "pursu": 16.6153846154, "clean": 13.73950670706, "apart": 3.1032056294, "onlin": 5.210370856580001, "debat": 3.2294548413300004, "much": 4.7768918309199995, "tens": 22.7124463519, "contain": 1.59814777532, "about": 7.45402106113, "problem": 35.3349655018, "articl": 6.054150247859999, "chart": 8.45367412141, "mind": 7.1837104072399995, "sure": 7.453521126760001, "just": 2.67160286074, "entir": 1.59365589239, "understand": 2.96858638743, "tweet": 278.5263157896, "correct": 3.6631287494199998, "igraph": 1587.6, "type": 4.056208482380001, "frequenc": 8.8102108768, "classifi": 10.5875291764, "continu": 1.13928955867, "defin": 5.45660766454, "fair": 3.20533010297, "cumbersom": 79.38, "complex": 2.34021226415, "tool": 9.99433427762, "few": 1.31729173581, "intuit": 55.4136125654, "insight": 59.01858736050001, "dictionari": 5.2292490118599995, "simpl": 16.990582191799998, "keep": 8.16981860284, "studi": 1.53184098804, "execut": 2.2363713199, "singl": 1.60948905109, "chaitanya": 1176.0, "whi": 3.2566153846200003, "usual": 1.72508964468, "benchmark": 51.8823529412, "mani": 4.17707031508, "refer": 1.30024570025, "the": 101.0, "build": 3.2683479156, "analysi": 38.26380368096, "mine": 112.14619164615998, "unstructur": 214.54054054099998, "linear": 13.8776223776, "earli": 1.12468121281, "same": 6.71147748888, "exact": 3.46864758575, "there": 13.53186467347, "imput": 178.38202247200002, "inform": 7.876562810100001, "primari": 2.2373167981999997, "mammoth": 74.1869158879, "brush": 25.2802547771, "best": 6.331405782640001, "exampl": 4.51450236966, "gutenberg": 45.7521613833, "custom": 3.6346153846199996, "termfrequ": 4762.799999999999, "knowledg": 6.7962328767199995, "take": 1.13961668222, "visual": 36.59269015479, "follow": 2.09280253098, "might": 4.312372674180001, "further": 4.0854348945, "code": 3.8807137619199996, "hard": 2.73253012048, "tip": 84.79762611273, "object": 7.04660452731, "play": 2.92780082988, "power": 2.6792675723599997, "languag": 4.58976582828, "break": 2.42863698944, "cloud": 21.2387959866, "around": 2.42789417342, "such": 14.86119283236, "textual": 82.9033942558, "seem": 2.29123971713, "row": 5.549108703250001, "plan": 3.0713871155000003, "encount": 4.13976531943, "uniqu": 6.03191489362, "area": 2.7762525137800003, "becom": 2.24984057252, "candid": 4.51279135873, "peopl": 2.42640990372, "occurr": 13.805217391300001, "polit": 1.76851954996, "those": 2.39096385542, "want": 3.99396226416, "captur": 11.52104499276, "convert": 16.370385646549998, "sinc": 2.16737201366, "nonenglish": 1587.6, "behind": 2.0845588235299997, "howev": 1.0945191313299998, "various": 2.6646525679799997, "piec": 3.24132298898, "work": 11.1520089913, "deem": 6.49059689289, "which": 13.067493984999999, "databas": 8.24727272727, "skill": 7.3979496738199995, "term": 8.37121012392, "thus": 1.6463756092500001, "other": 11.10916030532, "appli": 4.5944147012, "techniqu": 14.917547568720002, "procedur": 5.8691312384500005, "one": 7.04392470054, "known": 1.0859097127200001, "hope": 2.50884955752, "begin": 1.3305397251100002, "see": 1.27242125511, "suitabl": 6.23811394892, "becaus": 2.2990369994999997, "unnecessari": 17.4845814978, "get": 7.1425036554, "like": 2.2983713355, "text": 128.25931034487002, "manag": 1.6448404475799998, "drop": 2.4594887684, "except": 1.71948445792, "tri": 1.8544562551099997, "forward": 3.66566612792, "both": 1.05215720061, "transform": 3.42007755278, "analyt": 103.53913043460001, "memori": 5.14785992218, "includ": 8.152512998239999, "structur": 2.0580762250499998, "rule": 1.7415533128599998, "repeat": 2.8771293947099994, "this": 17.06449165407, "find": 10.376470588259998, "top": 3.6775538568400004, "time": 4.04509841392, "api": 84.44680851060001, "part": 1.04330682789, "back": 1.26070038911, "chang": 5.9044927105, "similar": 6.87570376785, "project": 17.5347912525, "differ": 3.7096347067499997, "been": 2.0478555304799997, "most": 4.08385852092, "between": 1.03453668708, "low": 2.13072070863, "hashtag": 1984.5, "what": 8.77404073896, "domain": 9.39408284024, "expos": 10.07360406092, "sarcasm": 203.53846153799998, "all": 11.12614678901, "charact": 2.51720310766, "busi": 2.05541170378, "today": 6.99845713028, "nltk": 1587.6, "situat": 8.26444560124, "someth": 3.28152128979, "recent": 1.54405757635, "solv": 36.34615384615, "feat": 17.070967741900002, "case": 1.48498737256, "that": 17.06772908375, "rate": 2.14048806795, "more": 5.085853408499999, "and": 75.00472440975, "discuss": 2.19676214197, "these": 2.14830852504, "prepar": 2.43012398592, "amount": 2.27027027027, "general": 1.1218202374200001, "made": 1.07038834951, "befor": 2.20072082062, "comment": 3.05954904606, "matrix": 180.9230769232, "state": 1.0477133240899998, "standard": 3.7831526271800007, "can": 32.93531895976, "avail": 1.7288467821, "topic": 27.287727741500003, "make": 1.0762660158600001, "set": 1.18707940781, "way": 14.6288873532, "onli": 1.0256476516600002, "each": 7.13848920864, "eas": 9.04615384615, "retail": 8.78097345133, "student": 2.47174217655, "denot": 20.3930635838, "read": 2.3149606299200003, "packag": 46.970414201160004, "data": 202.5861335604, "qualiti": 2.9329392204, "method": 12.857142857150002, "whether": 6.62051709759, "repres": 1.46972782818, "bio": 42.336000000000006, "come": 7.969879518059999, "matplotlib": 1587.6, "mix": 2.7852631578900002, "corpora": 481.09090909099996, "opposit": 2.4663663197099996, "identifi": 4.60374075686, "direct": 1.22226499346, "import": 1.3401992233700002, "first": 5.03808073115, "everi": 2.95835274388, "irrelav": 1587.6, "curious": 23.381443299, "have": 10.1489484114, "while": 2.0883977900599997, "roadmap": 360.818181818, "write": 2.0575427682700003, "retweet": 1587.6, "stringr": 1587.6, "learn": 11.61375274325, "gold": 3.3642720915400006, "when": 4.0830707902, "mention": 2.53894130817, "effici": 5.09335899904}, "logtfidf": {"after": 0.040981389296199995, "mark": 0.410770160338, "watch": 1.36757423376, "song": 1.23354840355, "googl": 2.43263122258, "equival": 1.40897338129, "longitudin": 3.8406813366199994, "fit": 1.2151206268899999, "plot": 3.36668481018, "basi": 0.884275353639, "magazin": 0.888401591632, "form": 0.36015955257300003, "collaps": 1.4508160855599999, "new": 0.08864973425549999, "assign": 1.3445959556, "dirti": 2.90407060225, "number": 0.483042892093, "etc": 4.310019263910001, "dataset": 21.06337826656, "function": 0.914465741594, "ecommerc": 14.549337082200001, "client": 2.6488048591599997, "conjunct": 2.2442306197099997, "progress": 0.894854218108, "python": 20.153283714799997, "thought": 0.685867118283, "well": 0.0635144383156, "product": 0.484060136536, "done": 0.845975983129, "tableau": 5.4981765440100006, "complet": 0.215285242047, "approach": 0.7302336145810001, "path": 1.5351679838499999, "toward": 0.48877277716000006, "know": 5.7175181663879995, "motiv": 1.61265547932, "preformat": 7.369978720910001, "particular": 0.323157393804, "creat": 0.667730455542, "express": 0.648191639641, "readabl": 4.31597753923, "how": 3.7725356554400005, "test": 0.977224437103, "present": 1.137733273995, "obstacl": 2.63641532015, "than": 0.0967825866546, "special": 0.39755992860100003, "would": 0.0796176279647, "end": 0.101476798618, "correl": 2.57915918803, "word": 10.54549948293, "interest": 0.47207177798199995, "deep": 1.2886734698, "depend": 1.61393963, "will": 1.419505744405, "associ": 0.28240501535100004, "open": 0.439182076058, "compliment": 3.9168216003199996, "otherwis": 1.3141319148700001, "updat": 1.7164374626899999, "next": 0.804327370998, "perform": 1.70472340232, "but": 0.0971542324314, "certain": 0.592104362781, "need": 3.264661470978, "our": 4.288196070910001, "uppercas": 5.721320095319999, "final": 0.292733863948, "point": 0.6930970770989999, "pharma": 6.009002167769999, "review": 1.5859044078420002, "richer": 3.85545265394, "level": 0.503462189943, "has": 0.1281718345644, "enough": 0.802884439169, "besid": 1.63631387177, "also": 0.1026001046, "textnumvec": 7.369978720910001, "adjust": 1.9619104904, "use": 0.7886165327532, "experi": 0.626272953933, "govern": 0.823440919508, "resourc": 2.16275388516, "networkx": 7.369978720910001, "out": 0.2921319545965, "model": 2.9498002924440003, "good": 1.2557682147209999, "alway": 1.452638409144, "yahoo": 3.6686767468, "count": 4.98994364556, "field": 0.5760642583510001, "store": 3.7123462005600008, "short": 0.345685625679, "process": 2.1113167961, "not": 0.18662895609, "strong": 0.49712549393600003, "involv": 0.742938157316, "henc": 1.68469971782, "founder": 1.3946435557299999, "appreci": 2.09339584651, "respect": 0.99466523808, "specif": 1.253960335082, "easi": 3.3330592702999997, "metric": 3.1016808515599994, "handl": 2.73366533806, "though": 0.308044191079, "then": 0.16606773046179998, "they": 0.1783619686056, "anoth": 0.127896361652, "wordcloud": 7.369978720910001, "effect": 0.333830227158, "choos": 1.43007066072, "world": 0.429680994484, "scienc": 0.841436178891, "sentiment": 4.58960981136, "chosen": 2.55779021754, "attract": 0.929510763678, "stem": 4.0173731104, "draw": 1.0893956335600001, "featur": 0.846774836284, "suffici": 1.4613524521099999, "careless": 4.03777421073, "percept": 6.365707712400001, "practic": 0.533182530867, "grievanc": 3.6275584998699997, "alreadi": 0.670478380747, "dive": 2.7778937744700003, "provid": 0.39035568865000003, "twitter": 21.017718488459998, "offer": 0.431112446902, "either": 0.91865527763, "develop": 0.178624694913, "should": 1.018839753516, "cell": 1.9605673068599998, "pattern": 6.6641202394, "sever": 0.20973336119069996, "hand": 0.958942670672, "enrich": 3.1058913841000004, "explor": 4.89031748872, "stopword": 7.369978720910001, "lie": 2.33610135128, "without": 0.517749035882, "lot": 5.934387801000001, "some": 0.395735090645, "sourc": 2.116873243004, "small": 0.614203610118, "collect": 5.94439992624, "add": 1.52875583713, "may": 0.8113599245504001, "result": 0.8182734502860001, "document": 4.662735611915, "confid": 1.8449242675400002, "duplic": 2.98545520604, "repositori": 3.8060957569699996, "india": 1.36707979618, "miss": 1.2631785751200002, "media": 0.9530830530519999, "caus": 0.325858567406, "ani": 0.125608358366, "sophist": 2.30296309338, "divid": 0.8402679544589999, "from": 0.006237595857525999, "firm": 2.6207766895, "num": 0.0031499039539700006, "market": 0.8612095839370001, "els": 1.6945957207700002, "idea": 0.73863592212, "movi": 1.3873026798299999, "for": 0.006299807907940001, "difficult": 0.912110767588, "predict": 8.22870118845, "remark": 2.6969401049, "with": 0.02275234255441, "handi": 4.62913869698, "stop": 1.557158749926, "are": 0.5009470509059, "solut": 1.55346297627, "poor": 0.8845804177050001, "compani": 1.759109012388, "current": 0.42695282784500005, "frequent": 0.7443211360850001, "preprocess": 14.215228912879999, "look": 1.9391600808, "popular": 0.41058020877499996, "conveni": 2.28795343073, "video": 1.19307248967, "rework": 3.33220451018, "tweepi": 7.369978720910001, "veri": 0.6904793797140001, "task": 5.42994722644, "whichev": 4.3350257342, "think": 3.2015298352499997, "trend": 6.7687921951600005, "unless": 1.69528182715, "true": 0.938325629634, "convey": 2.50939142306, "networkdnum": 7.369978720910001, "later": 0.0829654259878, "scare": 3.20176431012, "column": 1.95699427938, "nugget": 4.73092139129, "tackl": 2.98920286814, "remov": 5.5683907327040005, "group": 0.5717836043910001, "websit": 5.5493641428360005, "doe": 0.5340417297169999, "lowercas": 5.08759633523, "convers": 1.2085604509999999, "sagar": 11.73180264826, "give": 0.9341776566719999, "check": 3.74562099124, "train": 0.660918312839, "great": 0.235805258079, "step": 7.27681539886, "total": 1.307036660115, "move": 0.255615859253, "big": 1.00798563557, "librari": 1.973619961886, "accept": 0.552585882007, "ggplotnum": 7.369978720910001, "analyz": 4.541444470319999, "scientist": 3.09268256888, "machin": 2.78471916124, "into": 0.0894771793722, "bias": 2.61984276467, "ngram": 7.369978720910001, "through": 0.20507607565469999, "constant": 1.2971646281, "requir": 0.84850702135, "where": 0.1299842774914, "figur": 0.7101721121600001, "face": 0.589602371257, "earlier": 0.624742371425, "social": 0.688371502261, "head": 0.456042582852, "against": 0.254802851078, "workflow": 5.91136369821, "help": 1.344830885376, "affect": 0.908041904384, "larg": 0.34075012121200005, "datapoint": 7.369978720910001, "abil": 0.996488297427, "conclus": 1.57818536893, "even": 0.152388564834, "own": 0.328390154842, "base": 0.27304660457400004, "relat": 0.639300904962, "deal": 0.780914701253, "who": 0.0609002329859, "decid": 2.621291487572, "hot": 1.52991862796, "start": 1.182216846455, "fragment": 2.1550429633, "post": 0.8057001527009999, "factor": 2.12339629324, "pursu": 5.6961387564, "clean": 3.8542564072600003, "apart": 1.1324356512, "onlin": 1.915007708714, "debat": 1.1723133432200001, "much": 0.7099829172040001, "tens": 3.1229130716699998, "contain": 0.468845318236, "about": 0.43990434232220005, "problem": 11.382814485459999, "articl": 2.106395218722, "chart": 2.13460115413, "mind": 2.5573376776599996, "sure": 2.0086865552, "just": 0.579062868218, "entir": 0.46603068026999994, "understand": 1.0880858756799998, "tweet": 13.5927007722, "correct": 1.29831763181, "igraph": 7.369978720910001, "type": 1.414202970774, "frequenc": 2.1759113757299997, "classifi": 3.3330592702999997, "continu": 0.13040487398700001, "defin": 2.0073602185, "fair": 1.16481508131, "cumbersom": 4.37424644735, "complex": 0.8502416364309999, "tool": 3.21774235926, "few": 0.275577913653, "intuit": 6.643356194380001, "insight": 12.342072609350001, "dictionari": 1.65426767539, "simpl": 6.116106446949999, "keep": 2.8566093786919997, "studi": 0.426470272221, "execut": 0.804854605864, "singl": 0.475916769059, "chaitanya": 12.7534538958, "whi": 1.18068843047, "usual": 0.545279017064, "benchmark": 3.9489787119499997, "mani": 0.1732630324884, "refer": 0.262553246798, "the": 0.0, "build": 0.982274904182, "analysi": 13.712700132120002, "mine": 36.43910899594, "unstructur": 5.3684987207, "linear": 2.63027764196, "earli": 0.117499629108, "same": 0.672357897624, "exact": 1.2437647732500001, "there": 0.5212726080315, "imput": 5.18392744417, "inform": 2.27226852331, "primari": 0.805277289914, "mammoth": 4.30658779888, "brush": 3.23002364743, "best": 1.836911731788, "exampl": 1.2260480249969998, "gutenberg": 3.8232390339599998, "custom": 1.2905032964799998, "termfrequ": 22.10993616273, "knowledg": 2.4464425787799997, "take": 0.130691962197, "visual": 11.57756844202, "follow": 0.09071382218839999, "might": 1.5366821530680002, "further": 0.926447685891, "code": 1.35601909597, "hard": 1.00522796406, "tip": 20.187386740080004, "object": 2.561800754409, "play": 0.7622087812839999, "power": 0.58479256543, "languag": 1.6613636488119998, "break": 0.88733019029, "cloud": 4.72536465616, "around": 0.38775421156400003, "such": 0.835743689284, "textual": 7.4490576494399985, "seem": 0.829093032276, "row": 1.71363732085, "plan": 0.857964216294, "encount": 1.4206391000999998, "uniqu": 2.2078346818, "area": 0.655909642244, "becom": 0.23542435297800002, "candid": 1.50691588861, "peopl": 0.386531156946, "occurr": 2.62504659255, "polit": 0.570142784146, "those": 0.35709878174599996, "want": 1.3832732125099998, "captur": 4.231524004840001, "convert": 5.930180184, "sinc": 0.1607363989154, "nonenglish": 7.369978720910001, "behind": 0.7345572374320001, "howev": 0.0903151173475, "various": 0.57385300014, "piec": 1.17598157639, "work": 1.09034567273, "deem": 1.8703544976599997, "which": 0.06731937999059, "databas": 2.10988256718, "skill": 2.6161114203, "term": 1.9982339012760002, "thus": 0.49857627139300004, "other": 0.10862222711736, "appli": 1.6633883796239999, "techniqu": 5.26497539228, "procedur": 1.76970662262, "one": 0.0437874615185, "known": 0.0824180805992, "hope": 0.919824304455, "begin": 0.285584668268, "see": 0.240921585492, "suitabl": 1.83067788492, "becaus": 0.27868631765, "unnecessari": 2.8613194352999995, "get": 2.319076023128, "like": 0.27810715309, "text": 46.759762409589996, "manag": 0.497643387158, "drop": 0.8999535106219999, "except": 0.54202451213, "tri": 0.61759152916, "forward": 1.29901007269, "both": 0.050842533389300004, "transform": 1.22966322707, "analyt": 17.089140863159997, "memori": 1.8908677973199999, "includ": 0.151077451124, "structur": 0.7217716751350001, "rule": 0.554777423537, "repeat": 1.0567930591299999, "this": 0.0643696338925, "find": 3.286687981728, "top": 1.218201275576, "time": 0.0448460754504, "api": 4.43612185107, "part": 0.04239531098280001, "back": 0.23166743089699998, "chang": 0.83137812529, "similar": 1.59278046057, "project": 5.6160188590699995, "differ": 0.6369633639360001, "been": 0.04729196473680001, "most": 0.08299158518239999, "between": 0.033953681165299995, "low": 0.7564602833490001, "hashtag": 24.8273116444, "what": 1.581211077789, "domain": 2.24008000599, "expos": 3.2335427258599996, "sarcasm": 5.31585498721, "all": 0.1254289530758, "charact": 0.923148407239, "busi": 0.720476170355, "today": 2.237581414716, "nltk": 7.369978720910001, "situat": 2.90267316006, "someth": 1.18830712273, "recent": 0.434413741288, "solv": 9.918252385200002, "feat": 2.8373792277599996, "case": 0.395406268889, "that": 0.06759452245388, "rate": 0.761033872166, "more": 0.08512465799999999, "and": 0.00472426065477, "discuss": 0.78698452262, "these": 0.1430672388016, "prepar": 0.8879422790620001, "amount": 0.819898886199, "general": 0.114952578063, "made": 0.0680215260973, "befor": 0.191275543759, "comment": 1.11826753454, "matrix": 24.949043279039998, "state": 0.0466100027668, "standard": 1.27482101964, "can": 4.545550699032, "avail": 0.547454586289, "topic": 8.48499577705, "make": 0.07349765782289999, "set": 0.171496011289, "way": 2.3770981192200002, "onli": 0.025324268329099998, "each": 1.042450135824, "eas": 2.202339678, "retail": 2.17258727295, "student": 0.904923236645, "denot": 4.6440952841400005, "read": 0.83939268088, "packag": 12.346550695140003, "data": 73.009235088, "qualiti": 1.07600506711, "method": 4.7223080442050005, "whether": 2.3746835689409997, "repres": 0.38507723275, "bio": 3.7456377879300002, "come": 1.7034594391800002, "matplotlib": 7.369978720910001, "mix": 1.02434236008, "corpora": 6.17605625244, "opposit": 0.90274594185, "identifi": 1.667444000944, "direct": 0.200705689496, "import": 0.292818277066, "first": 0.0379364490608, "everi": 0.782970854842, "irrelav": 7.369978720910001, "curious": 3.15194268634, "have": 0.14785002341200001, "while": 0.08649996758760002, "roadmap": 10.39045399884, "write": 0.721512439877, "retweet": 7.369978720910001, "stringr": 7.369978720910001, "learn": 4.213760323724999, "gold": 1.21321162218, "when": 0.0822199554336, "mention": 0.931747186336, "effici": 1.62793753414}, "logidf": {"after": 0.020490694648099998, "mark": 0.410770160338, "watch": 1.36757423376, "song": 1.23354840355, "googl": 2.43263122258, "equival": 1.40897338129, "longitudin": 3.8406813366199994, "fit": 1.2151206268899999, "plot": 1.68334240509, "basi": 0.884275353639, "magazin": 0.888401591632, "form": 0.120053184191, "collaps": 1.4508160855599999, "new": 0.0177299468511, "assign": 1.3445959556, "dirti": 2.90407060225, "number": 0.0966085784186, "etc": 1.4366730879700003, "dataset": 5.26584456664, "function": 0.914465741594, "ecommerc": 7.2746685411000005, "client": 2.6488048591599997, "conjunct": 2.2442306197099997, "progress": 0.894854218108, "python": 4.03065674296, "thought": 0.685867118283, "well": 0.0635144383156, "product": 0.484060136536, "done": 0.845975983129, "tableau": 5.4981765440100006, "complet": 0.215285242047, "approach": 0.7302336145810001, "path": 1.5351679838499999, "toward": 0.48877277716000006, "know": 0.952919694398, "motiv": 1.61265547932, "preformat": 7.369978720910001, "particular": 0.323157393804, "creat": 0.222576818514, "express": 0.648191639641, "readabl": 4.31597753923, "how": 0.47156695693000006, "test": 0.977224437103, "present": 0.227546654799, "obstacl": 2.63641532015, "than": 0.0322608622182, "special": 0.39755992860100003, "would": 0.0796176279647, "end": 0.101476798618, "correl": 2.57915918803, "word": 0.585861082385, "interest": 0.47207177798199995, "deep": 1.2886734698, "depend": 0.806969815, "will": 0.202786534915, "associ": 0.28240501535100004, "open": 0.219591038029, "compliment": 3.9168216003199996, "otherwis": 1.3141319148700001, "updat": 1.7164374626899999, "next": 0.402163685499, "perform": 0.42618085058, "but": 0.0161923720719, "certain": 0.592104362781, "need": 0.362740163442, "our": 0.8576392141820001, "uppercas": 5.721320095319999, "final": 0.292733863948, "point": 0.23103235903299998, "pharma": 6.009002167769999, "review": 0.7929522039210001, "richer": 3.85545265394, "level": 0.503462189943, "has": 0.0427239448548, "enough": 0.802884439169, "besid": 1.63631387177, "also": 0.0146571578, "textnumvec": 7.369978720910001, "adjust": 1.9619104904, "use": 0.0292080197316, "experi": 0.626272953933, "govern": 0.411720459754, "resourc": 1.08137694258, "networkx": 7.369978720910001, "out": 0.0584263909193, "model": 0.7374500731110001, "good": 0.418589404907, "alway": 0.726319204572, "yahoo": 3.6686767468, "count": 1.24748591139, "field": 0.5760642583510001, "store": 1.2374487335200002, "short": 0.345685625679, "process": 0.527829199025, "not": 0.0155524130075, "strong": 0.49712549393600003, "involv": 0.371469078658, "henc": 1.68469971782, "founder": 1.3946435557299999, "appreci": 2.09339584651, "respect": 0.49733261904, "specif": 0.626980167541, "easi": 1.6665296351499999, "metric": 3.1016808515599994, "handl": 1.36683266903, "though": 0.308044191079, "then": 0.08303386523089999, "they": 0.0297269947676, "anoth": 0.127896361652, "wordcloud": 7.369978720910001, "effect": 0.333830227158, "choos": 1.43007066072, "world": 0.107420248621, "scienc": 0.841436178891, "sentiment": 2.29480490568, "chosen": 1.27889510877, "attract": 0.929510763678, "stem": 2.0086865552, "draw": 1.0893956335600001, "featur": 0.423387418142, "suffici": 1.4613524521099999, "careless": 4.03777421073, "percept": 2.1219025708, "practic": 0.533182530867, "grievanc": 3.6275584998699997, "alreadi": 0.670478380747, "dive": 2.7778937744700003, "provid": 0.19517784432500002, "twitter": 3.50295308141, "offer": 0.431112446902, "either": 0.459327638815, "develop": 0.178624694913, "should": 0.509419876758, "cell": 1.9605673068599998, "pattern": 1.33282404788, "sever": 0.06991112039689999, "hand": 0.479471335336, "enrich": 3.1058913841000004, "explor": 1.22257937218, "stopword": 7.369978720910001, "lie": 1.16805067564, "without": 0.258874517941, "lot": 1.4835969502500002, "some": 0.0395735090645, "sourc": 0.529218310751, "small": 0.307101805059, "collect": 0.49536666052, "add": 1.52875583713, "may": 0.050709995284400004, "result": 0.136378908381, "document": 0.932547122383, "confid": 1.8449242675400002, "duplic": 2.98545520604, "repositori": 3.8060957569699996, "india": 1.36707979618, "miss": 1.2631785751200002, "media": 0.9530830530519999, "caus": 0.325858567406, "ani": 0.125608358366, "sophist": 2.30296309338, "divid": 0.8402679544589999, "from": 0.000567054168866, "firm": 1.31038834475, "num": 0.00031499039539700004, "market": 0.8612095839370001, "els": 1.6945957207700002, "idea": 0.73863592212, "movi": 1.3873026798299999, "for": 0.00031499039539700004, "difficult": 0.912110767588, "predict": 1.6457402376899999, "remark": 1.34847005245, "with": 0.00119749171339, "handi": 4.62913869698, "stop": 0.778579374963, "are": 0.0294674735827, "solut": 1.55346297627, "poor": 0.8845804177050001, "compani": 0.439777253097, "current": 0.42695282784500005, "frequent": 0.7443211360850001, "preprocess": 7.1076144564399995, "look": 0.6463866936, "popular": 0.41058020877499996, "conveni": 2.28795343073, "video": 1.19307248967, "rework": 3.33220451018, "tweepi": 7.369978720910001, "veri": 0.230159793238, "task": 1.35748680661, "whichev": 4.3350257342, "think": 1.06717661175, "trend": 1.6921980487900001, "unless": 1.69528182715, "true": 0.938325629634, "convey": 2.50939142306, "networkdnum": 7.369978720910001, "later": 0.0829654259878, "scare": 3.20176431012, "column": 1.95699427938, "nugget": 4.73092139129, "tackl": 2.98920286814, "remov": 0.6960488415880001, "group": 0.190594534797, "websit": 0.924894023806, "doe": 0.5340417297169999, "lowercas": 5.08759633523, "convers": 1.2085604509999999, "sagar": 5.86590132413, "give": 0.311392552224, "check": 1.87281049562, "train": 0.660918312839, "great": 0.235805258079, "step": 1.03954505698, "total": 0.43567888670500005, "move": 0.255615859253, "big": 1.00798563557, "librari": 0.986809980943, "accept": 0.552585882007, "ggplotnum": 7.369978720910001, "analyz": 2.2707222351599996, "scientist": 1.54634128444, "machin": 1.39235958062, "into": 0.0149128632287, "bias": 2.61984276467, "ngram": 7.369978720910001, "through": 0.0683586918849, "constant": 1.2971646281, "requir": 0.424253510675, "where": 0.0649921387457, "figur": 0.7101721121600001, "face": 0.589602371257, "earlier": 0.624742371425, "social": 0.688371502261, "head": 0.456042582852, "against": 0.254802851078, "workflow": 5.91136369821, "help": 0.336207721344, "affect": 0.908041904384, "larg": 0.17037506060600002, "datapoint": 7.369978720910001, "abil": 0.996488297427, "conclus": 1.57818536893, "even": 0.152388564834, "own": 0.164195077421, "base": 0.13652330228700002, "relat": 0.21310030165399999, "deal": 0.780914701253, "who": 0.0609002329859, "decid": 0.655322871893, "hot": 1.52991862796, "start": 0.236443369291, "fragment": 2.1550429633, "post": 0.8057001527009999, "factor": 1.06169814662, "pursu": 1.4240346891, "clean": 1.9271282036300001, "apart": 1.1324356512, "onlin": 0.957503854357, "debat": 1.1723133432200001, "much": 0.17749572930100002, "tens": 3.1229130716699998, "contain": 0.468845318236, "about": 0.0628434774746, "problem": 0.569140724273, "articl": 0.702131739574, "chart": 2.13460115413, "mind": 1.2786688388299998, "sure": 2.0086865552, "just": 0.289531434109, "entir": 0.46603068026999994, "understand": 1.0880858756799998, "tweet": 4.5309002574, "correct": 1.29831763181, "igraph": 7.369978720910001, "type": 0.707101485387, "frequenc": 2.1759113757299997, "classifi": 1.6665296351499999, "continu": 0.13040487398700001, "defin": 1.00368010925, "fair": 1.16481508131, "cumbersom": 4.37424644735, "complex": 0.8502416364309999, "tool": 1.60887117963, "few": 0.275577913653, "intuit": 3.3216780971900004, "insight": 2.46841452187, "dictionari": 1.65426767539, "simpl": 1.2232212893899999, "keep": 0.7141523446729999, "studi": 0.426470272221, "execut": 0.804854605864, "singl": 0.475916769059, "chaitanya": 6.3767269479, "whi": 1.18068843047, "usual": 0.545279017064, "benchmark": 3.9489787119499997, "mani": 0.0433157581221, "refer": 0.262553246798, "the": 0.0, "build": 0.491137452091, "analysi": 1.2466091029200002, "mine": 1.58430908678, "unstructur": 5.3684987207, "linear": 2.63027764196, "earli": 0.117499629108, "same": 0.112059649604, "exact": 1.2437647732500001, "there": 0.0400978929255, "imput": 5.18392744417, "inform": 0.454453704662, "primari": 0.805277289914, "mammoth": 4.30658779888, "brush": 3.23002364743, "best": 0.459227932947, "exampl": 0.40868267499899996, "gutenberg": 3.8232390339599998, "custom": 1.2905032964799998, "termfrequ": 7.369978720910001, "knowledg": 1.2232212893899999, "take": 0.130691962197, "visual": 1.6539383488600001, "follow": 0.045356911094199995, "might": 0.7683410765340001, "further": 0.308815895297, "code": 1.35601909597, "hard": 1.00522796406, "tip": 2.2430429711200004, "object": 0.853933584803, "play": 0.38110439064199997, "power": 0.292396282715, "languag": 0.8306818244059999, "break": 0.88733019029, "cloud": 2.36268232808, "around": 0.19387710578200001, "such": 0.059695977806, "textual": 3.7245288247199992, "seem": 0.829093032276, "row": 1.71363732085, "plan": 0.428982108147, "encount": 1.4206391000999998, "uniqu": 1.1039173409, "area": 0.327954821122, "becom": 0.11771217648900001, "candid": 1.50691588861, "peopl": 0.193265578473, "occurr": 2.62504659255, "polit": 0.570142784146, "those": 0.17854939087299998, "want": 0.6916366062549999, "captur": 1.0578810012100002, "convert": 1.1860360368, "sinc": 0.0803681994577, "nonenglish": 7.369978720910001, "behind": 0.7345572374320001, "howev": 0.0903151173475, "various": 0.28692650007, "piec": 1.17598157639, "work": 0.109034567273, "deem": 1.8703544976599997, "which": 0.00517841384543, "databas": 2.10988256718, "skill": 1.30805571015, "term": 0.33303898354600003, "thus": 0.49857627139300004, "other": 0.00987474791976, "appli": 0.8316941898119999, "techniqu": 1.31624384807, "procedur": 1.76970662262, "one": 0.0062553516455, "known": 0.0824180805992, "hope": 0.919824304455, "begin": 0.285584668268, "see": 0.240921585492, "suitabl": 1.83067788492, "becaus": 0.139343158825, "unnecessari": 2.8613194352999995, "get": 0.579769005782, "like": 0.139053576545, "text": 1.14048200999, "manag": 0.497643387158, "drop": 0.8999535106219999, "except": 0.54202451213, "tri": 0.61759152916, "forward": 1.29901007269, "both": 0.050842533389300004, "transform": 1.22966322707, "analyt": 2.8481901438599997, "memori": 0.9454338986599999, "includ": 0.0188846813905, "structur": 0.7217716751350001, "rule": 0.554777423537, "repeat": 1.0567930591299999, "this": 0.0037864490525, "find": 0.547781330288, "top": 0.609100637788, "time": 0.0112115188626, "api": 4.43612185107, "part": 0.04239531098280001, "back": 0.23166743089699998, "chang": 0.166275625058, "similar": 0.318556092114, "project": 0.561601885907, "differ": 0.212321121312, "been": 0.023645982368400004, "most": 0.020747896295599998, "between": 0.033953681165299995, "low": 0.7564602833490001, "hashtag": 6.2068279111, "what": 0.225887296827, "domain": 2.24008000599, "expos": 1.6167713629299998, "sarcasm": 5.31585498721, "all": 0.011402632097799998, "charact": 0.923148407239, "busi": 0.720476170355, "today": 0.559395353679, "nltk": 7.369978720910001, "situat": 0.725668290015, "someth": 1.18830712273, "recent": 0.434413741288, "solv": 1.9836504770400003, "feat": 2.8373792277599996, "case": 0.395406268889, "that": 0.00397614837964, "rate": 0.761033872166, "more": 0.017024931599999998, "and": 6.29901420636e-05, "discuss": 0.78698452262, "these": 0.0715336194008, "prepar": 0.8879422790620001, "amount": 0.819898886199, "general": 0.114952578063, "made": 0.0680215260973, "befor": 0.0956377718795, "comment": 1.11826753454, "matrix": 3.1186304098799997, "state": 0.0466100027668, "standard": 0.63741050982, "can": 0.162341096394, "avail": 0.547454586289, "topic": 1.6969991554100001, "make": 0.07349765782289999, "set": 0.171496011289, "way": 0.19809150993500002, "onli": 0.025324268329099998, "each": 0.173741689304, "eas": 2.202339678, "retail": 2.17258727295, "student": 0.904923236645, "denot": 2.3220476420700003, "read": 0.83939268088, "packag": 2.0577584491900005, "data": 1.2168205848, "qualiti": 1.07600506711, "method": 0.944461608841, "whether": 0.791561189647, "repres": 0.38507723275, "bio": 3.7456377879300002, "come": 0.28390990653000003, "matplotlib": 7.369978720910001, "mix": 1.02434236008, "corpora": 6.17605625244, "opposit": 0.90274594185, "identifi": 0.833722000472, "direct": 0.200705689496, "import": 0.292818277066, "first": 0.0075872898121599995, "everi": 0.391485427421, "irrelav": 7.369978720910001, "curious": 3.15194268634, "have": 0.0147850023412, "while": 0.04324998379380001, "roadmap": 5.19522699942, "write": 0.721512439877, "retweet": 7.369978720910001, "stringr": 7.369978720910001, "learn": 0.842752064745, "gold": 1.21321162218, "when": 0.0205549888584, "mention": 0.931747186336, "effici": 1.62793753414}, "freq": {"after": 2, "mark": 1, "watch": 1, "song": 1, "googl": 1, "equival": 1, "longitudin": 1, "fit": 1, "plot": 2, "basi": 1, "magazin": 1, "form": 3, "collaps": 1, "new": 5, "assign": 1, "dirti": 1, "number": 5, "etc": 3, "dataset": 4, "function": 1, "ecommerc": 2, "client": 1, "conjunct": 1, "progress": 1, "python": 5, "thought": 1, "well": 1, "product": 1, "done": 1, "tableau": 1, "complet": 1, "approach": 1, "path": 1, "toward": 1, "know": 6, "motiv": 1, "preformat": 1, "particular": 1, "creat": 3, "express": 1, "readabl": 1, "how": 8, "test": 1, "present": 5, "obstacl": 1, "than": 3, "special": 1, "would": 1, "end": 1, "correl": 1, "word": 18, "interest": 1, "deep": 1, "depend": 2, "will": 7, "associ": 1, "open": 2, "compliment": 1, "otherwis": 1, "updat": 1, "next": 2, "perform": 4, "but": 6, "certain": 1, "need": 9, "our": 5, "uppercas": 1, "final": 1, "point": 3, "pharma": 1, "review": 2, "richer": 1, "level": 1, "has": 3, "enough": 1, "besid": 1, "also": 7, "textnumvec": 1, "adjust": 1, "use": 27, "experi": 1, "govern": 2, "resourc": 2, "networkx": 1, "out": 5, "model": 4, "good": 3, "alway": 2, "yahoo": 1, "count": 4, "field": 1, "store": 3, "short": 1, "process": 4, "not": 12, "strong": 1, "involv": 2, "henc": 1, "founder": 1, "appreci": 1, "respect": 2, "specif": 2, "easi": 2, "metric": 1, "handl": 2, "though": 1, "then": 2, "they": 6, "anoth": 1, "wordcloud": 1, "effect": 1, "choos": 1, "world": 4, "scienc": 1, "sentiment": 2, "chosen": 2, "attract": 1, "stem": 2, "draw": 1, "featur": 2, "suffici": 1, "careless": 1, "percept": 3, "practic": 1, "grievanc": 1, "alreadi": 1, "dive": 1, "provid": 2, "twitter": 6, "offer": 1, "either": 2, "develop": 1, "should": 2, "cell": 1, "pattern": 5, "sever": 3, "hand": 2, "enrich": 1, "explor": 4, "stopword": 1, "lie": 2, "without": 2, "lot": 4, "some": 10, "sourc": 4, "small": 2, "collect": 12, "add": 1, "may": 16, "result": 6, "document": 5, "confid": 1, "duplic": 1, "repositori": 1, "india": 1, "miss": 1, "media": 1, "caus": 1, "ani": 1, "sophist": 1, "divid": 1, "from": 11, "firm": 2, "num": 10, "market": 1, "els": 1, "idea": 1, "movi": 1, "for": 20, "difficult": 1, "predict": 5, "remark": 2, "with": 19, "handi": 1, "stop": 2, "are": 17, "solut": 1, "poor": 1, "compani": 4, "current": 1, "frequent": 1, "preprocess": 2, "look": 3, "popular": 1, "conveni": 1, "video": 1, "rework": 1, "tweepi": 1, "veri": 3, "task": 4, "whichev": 1, "think": 3, "trend": 4, "unless": 1, "true": 1, "convey": 1, "networkdnum": 1, "later": 1, "scare": 1, "column": 1, "nugget": 1, "tackl": 1, "remov": 8, "group": 3, "websit": 6, "doe": 1, "lowercas": 1, "convers": 1, "sagar": 2, "give": 3, "check": 2, "train": 1, "great": 1, "step": 7, "total": 3, "move": 1, "big": 1, "librari": 2, "accept": 1, "ggplotnum": 1, "analyz": 2, "scientist": 2, "machin": 2, "into": 6, "bias": 1, "ngram": 1, "through": 3, "constant": 1, "requir": 2, "where": 2, "figur": 1, "face": 1, "earlier": 1, "social": 1, "head": 1, "against": 1, "workflow": 1, "help": 4, "affect": 1, "larg": 2, "datapoint": 1, "abil": 1, "conclus": 1, "even": 1, "own": 2, "base": 2, "relat": 3, "deal": 1, "who": 1, "decid": 4, "hot": 1, "start": 5, "fragment": 1, "post": 1, "factor": 2, "pursu": 4, "clean": 2, "apart": 1, "onlin": 2, "debat": 1, "much": 4, "tens": 1, "contain": 1, "about": 7, "problem": 20, "articl": 3, "chart": 1, "mind": 2, "sure": 1, "just": 2, "entir": 1, "understand": 1, "tweet": 3, "correct": 1, "igraph": 1, "type": 2, "frequenc": 1, "classifi": 2, "continu": 1, "defin": 2, "fair": 1, "cumbersom": 1, "complex": 1, "tool": 2, "few": 1, "intuit": 2, "insight": 5, "dictionari": 1, "simpl": 5, "keep": 4, "studi": 1, "execut": 1, "singl": 1, "chaitanya": 2, "whi": 1, "usual": 1, "benchmark": 1, "mani": 4, "refer": 1, "the": 101, "build": 2, "analysi": 11, "mine": 23, "unstructur": 1, "linear": 1, "earli": 1, "same": 6, "exact": 1, "there": 13, "imput": 1, "inform": 5, "primari": 1, "mammoth": 1, "brush": 1, "best": 4, "exampl": 3, "gutenberg": 1, "custom": 1, "termfrequ": 3, "knowledg": 2, "take": 1, "visual": 7, "follow": 2, "might": 2, "further": 3, "code": 1, "hard": 1, "tip": 9, "object": 3, "play": 2, "power": 2, "languag": 2, "break": 1, "cloud": 2, "around": 2, "such": 14, "textual": 2, "seem": 1, "row": 1, "plan": 2, "encount": 1, "uniqu": 2, "area": 2, "becom": 2, "candid": 1, "peopl": 2, "occurr": 1, "polit": 1, "those": 2, "want": 2, "captur": 4, "convert": 5, "sinc": 2, "nonenglish": 1, "behind": 1, "howev": 1, "various": 2, "piec": 1, "work": 10, "deem": 1, "which": 13, "databas": 1, "skill": 2, "term": 6, "thus": 1, "other": 11, "appli": 2, "techniqu": 4, "procedur": 1, "one": 7, "known": 1, "hope": 1, "begin": 1, "see": 1, "suitabl": 1, "becaus": 2, "unnecessari": 1, "get": 4, "like": 2, "text": 41, "manag": 1, "drop": 1, "except": 1, "tri": 1, "forward": 1, "both": 1, "transform": 1, "analyt": 6, "memori": 2, "includ": 8, "structur": 1, "rule": 1, "repeat": 1, "this": 17, "find": 6, "top": 2, "time": 4, "api": 1, "part": 1, "back": 1, "chang": 5, "similar": 5, "project": 10, "differ": 3, "been": 2, "most": 4, "between": 1, "low": 1, "hashtag": 4, "what": 7, "domain": 1, "expos": 2, "sarcasm": 1, "all": 11, "charact": 1, "busi": 1, "today": 4, "nltk": 1, "situat": 4, "someth": 1, "recent": 1, "solv": 5, "feat": 1, "case": 1, "that": 17, "rate": 1, "more": 5, "and": 75, "discuss": 1, "these": 2, "prepar": 1, "amount": 1, "general": 1, "made": 1, "befor": 2, "comment": 1, "matrix": 8, "state": 1, "standard": 2, "can": 28, "avail": 1, "topic": 5, "make": 1, "set": 1, "way": 12, "onli": 1, "each": 6, "eas": 1, "retail": 1, "student": 1, "denot": 2, "read": 1, "packag": 6, "data": 60, "qualiti": 1, "method": 5, "whether": 3, "repres": 1, "bio": 1, "come": 6, "matplotlib": 1, "mix": 1, "corpora": 1, "opposit": 1, "identifi": 2, "direct": 1, "import": 1, "first": 5, "everi": 2, "irrelav": 1, "curious": 1, "have": 10, "while": 2, "roadmap": 2, "write": 1, "retweet": 1, "stringr": 1, "learn": 5, "gold": 1, "when": 4, "mention": 1, "effici": 1}, "idf": {"after": 1.02070207021, "mark": 1.5079787234, "watch": 3.92581602374, "song": 3.4333910034599997, "googl": 11.388809182200001, "equival": 4.09175257732, "longitudin": 46.557184750699996, "fit": 3.37070063694, "plot": 5.383519837230001, "basi": 2.42122922068, "magazin": 2.4312404287900002, "form": 1.12755681818, "collaps": 4.26659500134, "new": 1.0178880554, "assign": 3.83663605607, "dirti": 18.2482758621, "number": 1.10142916609, "etc": 4.2066772655, "dataset": 193.609756098, "function": 2.495441685, "ecommerc": 1443.27272727, "client": 14.1371326803, "conjunct": 9.43315508021, "progress": 2.44697903822, "python": 56.2978723404, "thought": 1.9854927463699998, "well": 1.0655748708, "product": 1.62264922322, "done": 2.3302509907499998, "tableau": 244.246153846, "complet": 1.24021560816, "approach": 2.07556543339, "path": 4.6421052631599995, "toward": 1.6303142329, "know": 2.59327017315, "motiv": 5.01611374408, "preformat": 1587.6, "particular": 1.3814827706200001, "creat": 1.2492917847, "express": 1.9120799710900003, "readabl": 74.8867924528, "how": 1.60250328051, "test": 2.65707112971, "present": 1.25551601423, "obstacl": 13.963060685999999, "than": 1.03278688525, "special": 1.4881889763799998, "would": 1.0828729281799998, "end": 1.10680423871, "correl": 13.1860465116, "word": 1.7965372864099998, "interest": 1.60331246213, "deep": 3.6279707495399998, "depend": 2.2411067193700003, "will": 1.22481098596, "associ": 1.3263157894700002, "open": 1.24556723678, "compliment": 50.2405063291, "otherwis": 3.72151898734, "updat": 5.56466876972, "next": 1.4950560316400001, "perform": 1.5313977042500002, "but": 1.01632417899, "certain": 1.8077886586200003, "need": 1.4372623574099999, "our": 2.35758835759, "uppercas": 305.307692308, "final": 1.34008609775, "point": 1.25990000794, "pharma": 407.07692307699995, "review": 2.2099109131400003, "richer": 47.25, "level": 1.6544393497299998, "has": 1.0436497502, "enough": 2.2319696330700003, "besid": 5.1362018764199995, "also": 1.01476510067, "textnumvec": 1587.6, "adjust": 7.112903225810001, "use": 1.0296387573799999, "experi": 1.87062566278, "govern": 1.50941243582, "resourc": 2.9487369985100003, "networkx": 1587.6, "out": 1.06016694491, "model": 2.0905978404, "good": 1.51981619759, "alway": 2.06745670009, "yahoo": 39.2, "count": 3.48157894737, "field": 1.7790228597, "store": 3.44680851064, "short": 1.41295834817, "process": 1.69524826482, "not": 1.01567398119, "strong": 1.6439888163999998, "involv": 1.4498630137000001, "henc": 5.390831918509999, "founder": 4.033536585369999, "appreci": 8.11241696474, "respect": 1.6443293630200002, "specif": 1.8719490626099997, "easi": 5.2937645882, "metric": 22.235294117600002, "handl": 3.9229058561900003, "though": 1.36076112111, "then": 1.08657860516, "they": 1.03017325287, "anoth": 1.13643521832, "wordcloud": 1587.6, "effect": 1.3963060686000002, "choos": 4.17899447223, "world": 1.11340206186, "scienc": 2.31969608416, "sentiment": 9.9225, "chosen": 3.59266802444, "attract": 2.53326950694, "stem": 7.453521126760001, "draw": 2.97247706422, "featur": 1.52712581762, "suffici": 4.3117870722400005, "careless": 56.7, "percept": 8.34700315457, "practic": 1.70434782609, "grievanc": 37.6208530806, "alreadi": 1.9551724137900002, "dive": 16.085106383, "provid": 1.21552714187, "twitter": 33.213389121300004, "offer": 1.53896859248, "either": 1.5830092731099998, "develop": 1.1955719557200002, "should": 1.6643254009900001, "cell": 7.1033557047, "pattern": 3.79173632673, "sever": 1.07241286139, "hand": 1.6152202665600002, "enrich": 22.3291139241, "explor": 3.39593582888, "stopword": 1587.6, "lie": 3.2157180474000002, "without": 1.29547123623, "lot": 4.40877534018, "some": 1.04036697248, "sourc": 1.69760479042, "small": 1.3594793629, "collect": 1.64109985528, "add": 4.61243463103, "may": 1.05201775893, "result": 1.14611608432, "document": 2.5409731114, "confid": 6.327620565959999, "duplic": 19.7955112219, "repositori": 44.974504249300004, "india": 3.92387543253, "miss": 3.53664513255, "media": 2.59369384088, "caus": 1.38521943984, "ani": 1.13383802314, "sophist": 10.0037807183, "divid": 2.3169877408099997, "from": 1.00056721497, "firm": 3.7076132648300004, "num": 1.00031504001, "market": 2.36602086438, "els": 5.44444444444, "idea": 2.0930784443, "movi": 4.00403530895, "for": 1.00031504001, "difficult": 2.48957189901, "predict": 5.18484650555, "remark": 3.8515283842800003, "with": 1.0011982089899998, "handi": 102.425806452, "stop": 2.1783754116400003, "are": 1.02990593578, "solut": 4.7278141751, "poor": 2.42196796339, "compani": 1.5523613963, "current": 1.5325803649, "frequent": 2.10501193317, "preprocess": 1221.23076923, "look": 1.9086318826599997, "popular": 1.50769230769, "conveni": 9.85474860335, "video": 3.29719626168, "rework": 28.0, "tweepi": 1587.6, "veri": 1.25880114177, "task": 3.88641370869, "whichev": 76.3269230769, "think": 2.90715986083, "trend": 5.43140608963, "unless": 5.44818119423, "true": 2.55569864778, "convey": 12.297443842, "networkdnum": 1587.6, "later": 1.08650424309, "scare": 24.5758513932, "column": 7.078020508250001, "nugget": 113.4, "tackl": 19.8698372966, "remov": 2.0058117498400003, "group": 1.20996875238, "websit": 2.52160101652, "doe": 1.70581282905, "lowercas": 162.0, "convers": 3.3486606201200004, "sagar": 352.8, "give": 1.3653250774, "check": 6.50655737705, "train": 1.9365698950999999, "great": 1.26592775696, "step": 2.8279301745599996, "total": 1.5460122699399999, "move": 1.29125660838, "big": 2.7400759406299997, "librari": 2.68266306185, "accept": 1.7377408056, "ggplotnum": 1587.6, "analyz": 9.68639414277, "scientist": 4.69426374926, "machin": 4.02433460076, "into": 1.01502461479, "bias": 13.7335640138, "ngram": 1587.6, "through": 1.07074930869, "constant": 3.6589075823900004, "requir": 1.52844902282, "where": 1.06715063521, "figur": 2.0343413634, "face": 1.80327124035, "earlier": 1.86776470588, "social": 1.9904714142400002, "head": 1.57781753131, "against": 1.2902072328299998, "workflow": 369.209302326, "help": 1.39962972759, "affect": 2.4794627518400003, "larg": 1.18574949585, "datapoint": 1587.6, "abil": 2.70875277256, "conclus": 4.84615384615, "even": 1.16461267606, "own": 1.17844418052, "base": 1.14628158845, "relat": 1.23750876919, "deal": 2.18346857379, "who": 1.06279287723, "decid": 1.9257641921400002, "hot": 4.6178010471199995, "start": 1.26673581744, "fragment": 8.628260869570001, "post": 2.23826307627, "factor": 2.89127663449, "pursu": 4.15384615385, "clean": 6.86975335353, "apart": 3.1032056294, "onlin": 2.6051854282900004, "debat": 3.2294548413300004, "much": 1.1942229577299999, "tens": 22.7124463519, "contain": 1.59814777532, "about": 1.06486015159, "problem": 1.76674827509, "articl": 2.01805008262, "chart": 8.45367412141, "mind": 3.5918552036199998, "sure": 7.453521126760001, "just": 1.33580143037, "entir": 1.59365589239, "understand": 2.96858638743, "tweet": 92.8421052632, "correct": 3.6631287494199998, "igraph": 1587.6, "type": 2.0281042411900003, "frequenc": 8.8102108768, "classifi": 5.2937645882, "continu": 1.13928955867, "defin": 2.72830383227, "fair": 3.20533010297, "cumbersom": 79.38, "complex": 2.34021226415, "tool": 4.99716713881, "few": 1.31729173581, "intuit": 27.7068062827, "insight": 11.8037174721, "dictionari": 5.2292490118599995, "simpl": 3.3981164383599998, "keep": 2.04245465071, "studi": 1.53184098804, "execut": 2.2363713199, "singl": 1.60948905109, "chaitanya": 588.0, "whi": 3.2566153846200003, "usual": 1.72508964468, "benchmark": 51.8823529412, "mani": 1.04426757877, "refer": 1.30024570025, "the": 1.0, "build": 1.6341739578, "analysi": 3.47852760736, "mine": 4.875921375919999, "unstructur": 214.54054054099998, "linear": 13.8776223776, "earli": 1.12468121281, "same": 1.11857958148, "exact": 3.46864758575, "there": 1.04091266719, "imput": 178.38202247200002, "inform": 1.5753125620200001, "primari": 2.2373167981999997, "mammoth": 74.1869158879, "brush": 25.2802547771, "best": 1.5828514456600002, "exampl": 1.50483412322, "gutenberg": 45.7521613833, "custom": 3.6346153846199996, "termfrequ": 1587.6, "knowledg": 3.3981164383599998, "take": 1.13961668222, "visual": 5.22752716497, "follow": 1.04640126549, "might": 2.1561863370900003, "further": 1.3618116315, "code": 3.8807137619199996, "hard": 2.73253012048, "tip": 9.42195845697, "object": 2.3488681757700003, "play": 1.46390041494, "power": 1.3396337861799998, "languag": 2.29488291414, "break": 2.42863698944, "cloud": 10.6193979933, "around": 1.21394708671, "such": 1.06151377374, "textual": 41.4516971279, "seem": 2.29123971713, "row": 5.549108703250001, "plan": 1.5356935577500002, "encount": 4.13976531943, "uniqu": 3.01595744681, "area": 1.3881262568900001, "becom": 1.12492028626, "candid": 4.51279135873, "peopl": 1.21320495186, "occurr": 13.805217391300001, "polit": 1.76851954996, "those": 1.19548192771, "want": 1.99698113208, "captur": 2.88026124819, "convert": 3.2740771293099997, "sinc": 1.08368600683, "nonenglish": 1587.6, "behind": 2.0845588235299997, "howev": 1.0945191313299998, "various": 1.3323262839899999, "piec": 3.24132298898, "work": 1.11520089913, "deem": 6.49059689289, "which": 1.005191845, "databas": 8.24727272727, "skill": 3.6989748369099997, "term": 1.39520168732, "thus": 1.6463756092500001, "other": 1.00992366412, "appli": 2.2972073506, "techniqu": 3.7293868921800004, "procedur": 5.8691312384500005, "one": 1.00627495722, "known": 1.0859097127200001, "hope": 2.50884955752, "begin": 1.3305397251100002, "see": 1.27242125511, "suitabl": 6.23811394892, "becaus": 1.1495184997499999, "unnecessari": 17.4845814978, "get": 1.78562591385, "like": 1.14918566775, "text": 3.12827586207, "manag": 1.6448404475799998, "drop": 2.4594887684, "except": 1.71948445792, "tri": 1.8544562551099997, "forward": 3.66566612792, "both": 1.05215720061, "transform": 3.42007755278, "analyt": 17.256521739100002, "memori": 2.57392996109, "includ": 1.0190641247799999, "structur": 2.0580762250499998, "rule": 1.7415533128599998, "repeat": 2.8771293947099994, "this": 1.00379362671, "find": 1.7294117647099998, "top": 1.8387769284200002, "time": 1.01127460348, "api": 84.44680851060001, "part": 1.04330682789, "back": 1.26070038911, "chang": 1.1808985421, "similar": 1.37514075357, "project": 1.7534791252500002, "differ": 1.23654490225, "been": 1.0239277652399998, "most": 1.02096463023, "between": 1.03453668708, "low": 2.13072070863, "hashtag": 496.125, "what": 1.25343439128, "domain": 9.39408284024, "expos": 5.03680203046, "sarcasm": 203.53846153799998, "all": 1.01146788991, "charact": 2.51720310766, "busi": 2.05541170378, "today": 1.74961428257, "nltk": 1587.6, "situat": 2.06611140031, "someth": 3.28152128979, "recent": 1.54405757635, "solv": 7.26923076923, "feat": 17.070967741900002, "case": 1.48498737256, "that": 1.00398406375, "rate": 2.14048806795, "more": 1.0171706817, "and": 1.00006299213, "discuss": 2.19676214197, "these": 1.07415426252, "prepar": 2.43012398592, "amount": 2.27027027027, "general": 1.1218202374200001, "made": 1.07038834951, "befor": 1.10036041031, "comment": 3.05954904606, "matrix": 22.6153846154, "state": 1.0477133240899998, "standard": 1.8915763135900003, "can": 1.17626139142, "avail": 1.7288467821, "topic": 5.457545548300001, "make": 1.0762660158600001, "set": 1.18707940781, "way": 1.2190739461, "onli": 1.0256476516600002, "each": 1.18974820144, "eas": 9.04615384615, "retail": 8.78097345133, "student": 2.47174217655, "denot": 10.1965317919, "read": 2.3149606299200003, "packag": 7.828402366860001, "data": 3.37643555934, "qualiti": 2.9329392204, "method": 2.5714285714300003, "whether": 2.20683903253, "repres": 1.46972782818, "bio": 42.336000000000006, "come": 1.32831325301, "matplotlib": 1587.6, "mix": 2.7852631578900002, "corpora": 481.09090909099996, "opposit": 2.4663663197099996, "identifi": 2.30187037843, "direct": 1.22226499346, "import": 1.3401992233700002, "first": 1.00761614623, "everi": 1.47917637194, "irrelav": 1587.6, "curious": 23.381443299, "have": 1.0148948411399998, "while": 1.0441988950299999, "roadmap": 180.409090909, "write": 2.0575427682700003, "retweet": 1587.6, "stringr": 1587.6, "learn": 2.32275054865, "gold": 3.3642720915400006, "when": 1.02076769755, "mention": 2.53894130817, "effici": 5.09335899904}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Tips for Getting Started with Text Mining in R and Python</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/11/getting-started-text-mining-r-python.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Tips for Getting Started with Text Mining in R and Python Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/n43.html\" rel=\"prev\" title=\"KDnuggets\u2122 News 17:n43, Nov 8: Peak Demand for Data Scientists/Machine Learning Experts \u2013 When? Advice For New and Junior Data Scientists\"/>\n<link href=\"https://www.kdnuggets.com/2017/11/7-super-simple-steps-idea-successful-data-science-project.html\" rel=\"next\" title=\"7 Super Simple Steps From Idea To Successful Data Science Project\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2017/11/getting-started-text-mining-r-python.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=74177\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2017/11/getting-started-text-mining-r-python.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-74177 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 8-Nov, 2017  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/11/index.html\">Nov</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/11/tutorials.html\">Tutorials, Overviews</a> \u00bb Tips for Getting Started with Text Mining in R and Python (\u00a0<a href=\"/2017/n44.html\">17:n44</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Tips for Getting Started with Text Mining in R and Python</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/n43.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/11/7-super-simple-steps-idea-successful-data-science-project.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/python\" rel=\"tag\">Python</a>, <a href=\"https://www.kdnuggets.com/tag/r\" rel=\"tag\">R</a>, <a href=\"https://www.kdnuggets.com/tag/text-mining\" rel=\"tag\">Text Mining</a></div>\n<br/>\n<p class=\"excerpt\">\n     This article opens up the world of text mining in a simple and intuitive way and provides great tips to get started with text mining.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><b>By Chaitanya Sagar, <a href=\"http://www.perceptive-analytics.com/\" target=\"_blank\">Perceptive Analytics</a>.</b></p>\n<p><b>It All Starts With The Text</b></p>\n<p>There is so much of information lying in the text posts made by you and me and all others about all the trending topics today. Being in our respective firms, big or small, each of us collect some data related to our respective businesses and store it to analyze for various projects. At the same time, we all need this \u2018unstructured data\u2019 to know and understand more about our clients, customers and the state of our company in the world today. However, working with this data is not easy. The data is not structured, every piece does not have all the information and each part is unique. This is how textual data is. It needs to be processed first and converted in a form that is suitable for analysis. This is very similar to our own databases which we create except that they cannot be used directly and the amount of data is very large. This article opens up the world of text mining in a simple and intuitive way and provides great tips to get started with text mining.</p>\n<p><img alt=\"\" class=\"aligncenter\" src=\"/wp-content/uploads/documents.jpg\" width=\"95%\"/></p>\n<p><b>Tip #1: Think First</b></p>\n<p>The mammoth of text mining can become a simple task if you work on it with a plan in mind. Think what you need to do with text before going all out on it. What is your objective behind text mining? What sources of data do you want to use? How much data do you need for it to be sufficient? How do you plan to present your results from the data? It is all about getting curious about your problem and break it into small fragments. Thinking through the problem also opens up your mind towards the various situations you may encounter and ways to tackle those situations. You can then chart out a workflow and start pursuing the task.</p>\n<p><b>Tip #2: R or Python.. Or Something Else?</b></p>\n<p>There is no gold standard procedure for text mining. You have to choose the method which is most convenient for text mining. This is where factors such as efficiency,effectiveness type of problem and other factors come into play and helps you decide the best candidate for your problem. After having decided your chosen path, you need to build your knowledge and skills in developing skills in that language. I find the text mining techniques more intuitive in Python than in R but R has some handy functions to do tasks such as word counting and is richer in terms of packages available for text mining.</p>\n<p><b>Tip #3: Start Early and Collect Your Data</b></p>\n<ul>\n<li>The usual process of text mining involves the following steps:\n<li>Collect data; either from social media such as twitter or other websites. Write your code that can adjust to the specific type of text you collect and store it\n<li>Convert your data into readable text\n<li>Remove special characters from the text (such as hashtags). You can add a hashtag count feature if that is required\n<li>Removing numbers from the text data (unless the problem requires numbers)\n<li>Deciding whether to keep all the data or remove some of it such as all non-English text\n<li>Converting all the text to uppercase or lowercase only to ease analysis\n<li>Removing stop words.. Words that have no use in your analysis. This includes articles, conjunctions, etc.\n<li>Using word stemming and grouping similar words such as \u2018keep\u2019 and \u2018keeping\u2019 are same words used in different tense form.\n<li>Final analysis of the processed stemmed words and visualize results\n</li></li></li></li></li></li></li></li></li></li></ul>\n<p>The steps are short and simple but they all depend on the first step executed well. You need to collect your data so that text mining can be performed on it. There are many ways to collect data. One of the most popular sources to collect data from is Twitter. Twitter has exposed some APIs so that tweets can be mined using both R and Python. Besides twitter, one can capture data from any website today including e-commerce websites, movie websites, song websites, etc. Some websites also contain preformatted repositories of text data such as project gutenberg, corpora, etc. Google trends and yahoo also offer some analysis online.</p>\n<p><b>Tip #4: Find and Use The Best Way to Convert Text to Data</b></p>\n<p>Based on the tools and your project objective, you may use a different approach to convert your collected text to data. If you are using R, packages such as twitteR, tm and stringr are what you may be using for most of the preprocessing. The nltk library and Tweepy package are the equivalent packages in Python. Whichever language and package you use, make sure that you have enough resources and memory to handle the data. Text mining can be cumbersome just because of the irrelavant text lying around in your data even after removing stop words. Using a good method to prepare data will give you a lot of useful information when you apply modelling techniques on the data.</p>\n<p><b>Tip #5: Explore and Play Around</b></p>\n<p>You need to know your data before preprocessing it. Without the knowledge of how your data looks like, you might carelessly remove text which might have been useful in your analysis. There are many standard methods and dictionaries of removing stopwords and assigning importance to words but they may or may not apply to your data. For example, data about the government may include a lot of words such as \u2018rule\u2019, \u2018govern\u2019 and \u2018politics\u2019 which you may deem unnecessary and want to remove. Reviews may include lots of \u2018hi\u2019 in the beginning but may not be useful for a review dataset. It is always a good step to look at the source of data and go through some of the text to know how the process you defined for analysis is working to transform it correctly into useful information. Other ways specific to exploring text data is by creating a document term matrix. A document term matrix is a m*n matrix where the number of columns denote the total number of unique words in the entire dataset and the number of rows denote the total data points. Each cell thus represents the count of the particular word in that datapoint. This is a very large matrix and is later collapsed into term-frequency. From this document term matrix, one can count the total occurrences of each word in the dataset and that is exactly what term-frequency matrix stores. Other uses of document term matrix include knowing correlation between words, drawing a word cloud using term-frequency or predicting patterns using modelling techniques. This exploration will further give you confidence on the best way to move forward with textual data analysis.</p>\n<p><b>Tip #6: Dive Deep and Get Your Hands Dirty</b></p>\n<p>The primary objective or every machine learning and data science project is to find patterns in the data that are otherwise hard to find. You need to look for those interesting patterns and are not a true data scientist if you\u2019re scared of this step. It can be as simple as fitting a simple classifier to classify data points and see its performance. This will set a benchmark while giving you an idea of the predicting ability of the data. At times, the data may be biased or have a poor predictive power and data quality checks can help define this. For example, If I am collecting twitter data on the basis of hashtags, I can divide my collected data into train and test datasets keeping the hashtags as the dependent feature. If my prediction performance is not up to the mark, I need to go back a few steps and find out the cause of this low performance and then check how I am collecting data or how I am cleaning my data as the case may be. Other ways of getting patterns involve associations. For example, some data points may be related to each other while others may have a similar or opposite pattern. If tweets are being used for text mining, there can be duplicate tweets because of retweeting or debates going for or against a remark. Working with data also exposes problems such as dealing with sarcasm or comments that convey mixed expressions. Without brushing through the data, it will be difficult to know how much of your data is affected by these problems and whether you should drop such data or use some technique to handle the situation.</p>\n<p><b>Tip #7: Rework and Repeat</b></p>\n<p>The problem you are trying to solve may or may not be the first text mining problem in your company but it is certainly not the first text mining problem in the world today. There are several data scientists out there who have worked on either the same or similar problem as the one you are working on and knowing what methods they followed and what they did differently will help you take your problem solving to the next level. Though not as frequent as other domains, there are several analysis and projects being done on text mining which include finding the trending topics, sentiment analysis on the trending topics, identifying remarks about your firm or product, identifying grievances and appreciations and the like. With the same data, there can be more than one problem that can be solved. Complex problems which can be explored also include NLP and topic modelling. I read about a fairly recent project in which some students predicted the next topic which a group of people will discuss based on the current conversation. There can be many such new projects which can be thought of and pursued in the area of text mining but since it is a new and hot field to work on, always refer other similar data and resources to further compliment your analysis and come up with strong insights.</p>\n<p><b>Tip #8: Presenting Text Visually</b></p>\n<p>As mentioned earlier, there can be a lot of problems which can be pursued using text mining and more than one problem can be solved from the same data. With so much to present, it is a good practice to come up with ways to present the results in a way that would seem attractive to people. This is why most of the text mining results are already visualized in the form of word clouds, sentiment studies and figures. There are a packages and libraries for each such task which include wordcloud, ggplot2, igraph, text2vec, networkD3 and plotly in R and Networkx, matplotlib, plotly in Python. You can also use other sophisticated tools just for visualization such as Tableau or Power BI which can help visualize your data in many more ways.</p>\n<p><b>Conclusion: A Roadmap</b></p>\n<p>Visualizing results is not the end step in text mining projects. Since text is captured from online sources, it is constantly changing and so is the data that is captured. With the changing data comes changing insights and hence, when the project is completed and accepted, it should be continuously updated with new data and new insights. These insights can be further enriched with the rate of change. With time, the change can also be captured and used as a metric of progression. This becomes another longitudinal problem to be solved. Apart from the problems which can be pursued with text data, text mining is no easy feat. When you create a roadmap of collecting, cleaning and analyzing data, there may be several obstacles that will come your way. They can be situations when you have to decide whether to work with a single word frequency in document term matrix or use groups of words (known as n-grams) or building your own visualization method to present your results or memory management. At the same time, new projects are coming up in the area of text mining. The best way to learn is to face the problem hands on and learn from the experience of working on the problem. Hope this article provides motivation to head to the world of text and start mining insightful nuggets of information.</p>\n<p>\u00a0<br>\n<b>Bio: <a href=\"https://www.linkedin.com/in/chaitanyasagar/\" target=\"_blank\">Chaitanya Sagar</a></b> is the Founder and CEO of <a href=\"http://www.perceptive-analytics.com/\" target=\"_blank\">Perceptive Analytics</a>. Perceptive Analytics has been chosen as one of the top 10 analytics companies to watch out for by Analytics India Magazine. It works on Marketing Analytics for e-commerce, Retail and Pharma companies.</br></p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/10/top-10-machine-learning-r-videos.html\">Top 10 Machine Learning with R Videos</a>\n<li><a href=\"/2017/10/learn-generalized-linear-models-glm-r.html\">Learn Generalized Linear Models (GLM) using R</a>\n<li><a href=\"/2017/09/missing-data-imputation-using-r.html\">A Solution to Missing Data: Imputation Using R</a>\n</li></li></li></ul>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/n43.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/11/7-super-simple-steps-idea-successful-data-science-project.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/11/index.html\">Nov</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/11/tutorials.html\">Tutorials, Overviews</a> \u00bb Tips for Getting Started with Text Mining in R and Python (\u00a0<a href=\"/2017/n44.html\">17:n44</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556379306\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.681 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 11:35:06 -->\n<!-- Compression = gzip -->", "content_tokenized": ["chaitanya", "sagar", "percept", "analyt", "all", "start", "with", "the", "text", "there", "much", "inform", "lie", "the", "text", "post", "made", "and", "and", "all", "other", "about", "all", "the", "trend", "topic", "today", "our", "respect", "firm", "big", "small", "each", "collect", "some", "data", "relat", "our", "respect", "busi", "and", "store", "analyz", "for", "various", "project", "the", "same", "time", "all", "need", "this", "unstructur", "data", "know", "and", "understand", "more", "about", "our", "client", "custom", "and", "the", "state", "our", "compani", "the", "world", "today", "howev", "work", "with", "this", "data", "not", "easi", "the", "data", "not", "structur", "everi", "piec", "doe", "not", "have", "all", "the", "inform", "and", "each", "part", "uniqu", "this", "how", "textual", "data", "need", "process", "first", "and", "convert", "form", "that", "suitabl", "for", "analysi", "this", "veri", "similar", "our", "own", "databas", "which", "creat", "except", "that", "they", "can", "not", "use", "direct", "and", "the", "amount", "data", "veri", "larg", "this", "articl", "open", "the", "world", "text", "mine", "simpl", "and", "intuit", "way", "and", "provid", "great", "tip", "get", "start", "with", "text", "mine", "tip", "num", "think", "first", "the", "mammoth", "text", "mine", "can", "becom", "simpl", "task", "work", "with", "plan", "mind", "think", "what", "need", "with", "text", "befor", "all", "out", "what", "object", "behind", "text", "mine", "what", "sourc", "data", "want", "use", "how", "much", "data", "need", "for", "suffici", "how", "plan", "present", "result", "from", "the", "data", "all", "about", "get", "curious", "about", "problem", "and", "break", "into", "small", "fragment", "think", "through", "the", "problem", "also", "open", "mind", "toward", "the", "various", "situat", "may", "encount", "and", "way", "tackl", "those", "situat", "can", "then", "chart", "out", "workflow", "and", "start", "pursu", "the", "task", "tip", "num", "python", "someth", "els", "there", "gold", "standard", "procedur", "for", "text", "mine", "have", "choos", "the", "method", "which", "most", "conveni", "for", "text", "mine", "this", "where", "factor", "such", "effici", "effect", "type", "problem", "and", "other", "factor", "come", "into", "play", "and", "help", "decid", "the", "best", "candid", "for", "problem", "after", "have", "decid", "chosen", "path", "need", "build", "knowledg", "and", "skill", "develop", "skill", "that", "languag", "find", "the", "text", "mine", "techniqu", "more", "intuit", "python", "than", "but", "has", "some", "handi", "function", "task", "such", "word", "count", "and", "richer", "term", "packag", "avail", "for", "text", "mine", "tip", "num", "start", "earli", "and", "collect", "data", "the", "usual", "process", "text", "mine", "involv", "the", "follow", "step", "collect", "data", "either", "from", "social", "media", "such", "twitter", "other", "websit", "write", "code", "that", "can", "adjust", "the", "specif", "type", "text", "collect", "and", "store", "convert", "data", "into", "readabl", "text", "remov", "special", "charact", "from", "the", "text", "such", "hashtag", "can", "add", "hashtag", "count", "featur", "that", "requir", "remov", "number", "from", "the", "text", "data", "unless", "the", "problem", "requir", "number", "decid", "whether", "keep", "all", "the", "data", "remov", "some", "such", "all", "nonenglish", "text", "convert", "all", "the", "text", "uppercas", "lowercas", "onli", "eas", "analysi", "remov", "stop", "word", "word", "that", "have", "use", "analysi", "this", "includ", "articl", "conjunct", "etc", "use", "word", "stem", "and", "group", "similar", "word", "such", "keep", "and", "keep", "are", "same", "word", "use", "differ", "tens", "form", "final", "analysi", "the", "process", "stem", "word", "and", "visual", "result", "the", "step", "are", "short", "and", "simpl", "but", "they", "all", "depend", "the", "first", "step", "execut", "well", "need", "collect", "data", "that", "text", "mine", "can", "perform", "there", "are", "mani", "way", "collect", "data", "one", "the", "most", "popular", "sourc", "collect", "data", "from", "twitter", "twitter", "has", "expos", "some", "api", "that", "tweet", "can", "mine", "use", "both", "and", "python", "besid", "twitter", "one", "can", "captur", "data", "from", "ani", "websit", "today", "includ", "ecommerc", "websit", "movi", "websit", "song", "websit", "etc", "some", "websit", "also", "contain", "preformat", "repositori", "text", "data", "such", "project", "gutenberg", "corpora", "etc", "googl", "trend", "and", "yahoo", "also", "offer", "some", "analysi", "onlin", "tip", "num", "find", "and", "use", "the", "best", "way", "convert", "text", "data", "base", "the", "tool", "and", "project", "object", "may", "use", "differ", "approach", "convert", "collect", "text", "data", "are", "use", "packag", "such", "twitter", "and", "stringr", "are", "what", "may", "use", "for", "most", "the", "preprocess", "the", "nltk", "librari", "and", "tweepi", "packag", "are", "the", "equival", "packag", "python", "whichev", "languag", "and", "packag", "use", "make", "sure", "that", "have", "enough", "resourc", "and", "memori", "handl", "the", "data", "text", "mine", "can", "cumbersom", "just", "becaus", "the", "irrelav", "text", "lie", "around", "data", "even", "after", "remov", "stop", "word", "use", "good", "method", "prepar", "data", "will", "give", "lot", "use", "inform", "when", "appli", "model", "techniqu", "the", "data", "tip", "num", "explor", "and", "play", "around", "need", "know", "data", "befor", "preprocess", "without", "the", "knowledg", "how", "data", "look", "like", "might", "careless", "remov", "text", "which", "might", "have", "been", "use", "analysi", "there", "are", "mani", "standard", "method", "and", "dictionari", "remov", "stopword", "and", "assign", "import", "word", "but", "they", "may", "may", "not", "appli", "data", "for", "exampl", "data", "about", "the", "govern", "may", "includ", "lot", "word", "such", "rule", "govern", "and", "polit", "which", "may", "deem", "unnecessari", "and", "want", "remov", "review", "may", "includ", "lot", "the", "begin", "but", "may", "not", "use", "for", "review", "dataset", "alway", "good", "step", "look", "the", "sourc", "data", "and", "through", "some", "the", "text", "know", "how", "the", "process", "defin", "for", "analysi", "work", "transform", "correct", "into", "use", "inform", "other", "way", "specif", "explor", "text", "data", "creat", "document", "term", "matrix", "document", "term", "matrix", "matrix", "where", "the", "number", "column", "denot", "the", "total", "number", "uniqu", "word", "the", "entir", "dataset", "and", "the", "number", "row", "denot", "the", "total", "data", "point", "each", "cell", "thus", "repres", "the", "count", "the", "particular", "word", "that", "datapoint", "this", "veri", "larg", "matrix", "and", "later", "collaps", "into", "termfrequ", "from", "this", "document", "term", "matrix", "one", "can", "count", "the", "total", "occurr", "each", "word", "the", "dataset", "and", "that", "exact", "what", "termfrequ", "matrix", "store", "other", "use", "document", "term", "matrix", "includ", "know", "correl", "between", "word", "draw", "word", "cloud", "use", "termfrequ", "predict", "pattern", "use", "model", "techniqu", "this", "explor", "will", "further", "give", "confid", "the", "best", "way", "move", "forward", "with", "textual", "data", "analysi", "tip", "num", "dive", "deep", "and", "get", "hand", "dirti", "the", "primari", "object", "everi", "machin", "learn", "and", "data", "scienc", "project", "find", "pattern", "the", "data", "that", "are", "otherwis", "hard", "find", "need", "look", "for", "those", "interest", "pattern", "and", "are", "not", "true", "data", "scientist", "scare", "this", "step", "can", "simpl", "fit", "simpl", "classifi", "classifi", "data", "point", "and", "see", "perform", "this", "will", "set", "benchmark", "while", "give", "idea", "the", "predict", "abil", "the", "data", "time", "the", "data", "may", "bias", "have", "poor", "predict", "power", "and", "data", "qualiti", "check", "can", "help", "defin", "this", "for", "exampl", "collect", "twitter", "data", "the", "basi", "hashtag", "can", "divid", "collect", "data", "into", "train", "and", "test", "dataset", "keep", "the", "hashtag", "the", "depend", "featur", "predict", "perform", "not", "the", "mark", "need", "back", "few", "step", "and", "find", "out", "the", "caus", "this", "low", "perform", "and", "then", "check", "how", "collect", "data", "how", "clean", "data", "the", "case", "may", "other", "way", "get", "pattern", "involv", "associ", "for", "exampl", "some", "data", "point", "may", "relat", "each", "other", "while", "other", "may", "have", "similar", "opposit", "pattern", "tweet", "are", "use", "for", "text", "mine", "there", "can", "duplic", "tweet", "becaus", "retweet", "debat", "for", "against", "remark", "work", "with", "data", "also", "expos", "problem", "such", "deal", "with", "sarcasm", "comment", "that", "convey", "mix", "express", "without", "brush", "through", "the", "data", "will", "difficult", "know", "how", "much", "data", "affect", "these", "problem", "and", "whether", "should", "drop", "such", "data", "use", "some", "techniqu", "handl", "the", "situat", "tip", "num", "rework", "and", "repeat", "the", "problem", "are", "tri", "solv", "may", "may", "not", "the", "first", "text", "mine", "problem", "compani", "but", "certain", "not", "the", "first", "text", "mine", "problem", "the", "world", "today", "there", "are", "sever", "data", "scientist", "out", "there", "who", "have", "work", "either", "the", "same", "similar", "problem", "the", "one", "are", "work", "and", "know", "what", "method", "they", "follow", "and", "what", "they", "differ", "will", "help", "take", "problem", "solv", "the", "next", "level", "though", "not", "frequent", "other", "domain", "there", "are", "sever", "analysi", "and", "project", "done", "text", "mine", "which", "includ", "find", "the", "trend", "topic", "sentiment", "analysi", "the", "trend", "topic", "identifi", "remark", "about", "firm", "product", "identifi", "grievanc", "and", "appreci", "and", "the", "like", "with", "the", "same", "data", "there", "can", "more", "than", "one", "problem", "that", "can", "solv", "complex", "problem", "which", "can", "explor", "also", "includ", "and", "topic", "model", "read", "about", "fair", "recent", "project", "which", "some", "student", "predict", "the", "next", "topic", "which", "group", "peopl", "will", "discuss", "base", "the", "current", "convers", "there", "can", "mani", "such", "new", "project", "which", "can", "thought", "and", "pursu", "the", "area", "text", "mine", "but", "sinc", "new", "and", "hot", "field", "work", "alway", "refer", "other", "similar", "data", "and", "resourc", "further", "compliment", "analysi", "and", "come", "with", "strong", "insight", "tip", "num", "present", "text", "visual", "mention", "earlier", "there", "can", "lot", "problem", "which", "can", "pursu", "use", "text", "mine", "and", "more", "than", "one", "problem", "can", "solv", "from", "the", "same", "data", "with", "much", "present", "good", "practic", "come", "with", "way", "present", "the", "result", "way", "that", "would", "seem", "attract", "peopl", "this", "whi", "most", "the", "text", "mine", "result", "are", "alreadi", "visual", "the", "form", "word", "cloud", "sentiment", "studi", "and", "figur", "there", "are", "packag", "and", "librari", "for", "each", "such", "task", "which", "includ", "wordcloud", "ggplotnum", "igraph", "textnumvec", "networkdnum", "and", "plot", "and", "networkx", "matplotlib", "plot", "python", "can", "also", "use", "other", "sophist", "tool", "just", "for", "visual", "such", "tableau", "power", "which", "can", "help", "visual", "data", "mani", "more", "way", "conclus", "roadmap", "visual", "result", "not", "the", "end", "step", "text", "mine", "project", "sinc", "text", "captur", "from", "onlin", "sourc", "constant", "chang", "and", "the", "data", "that", "captur", "with", "the", "chang", "data", "come", "chang", "insight", "and", "henc", "when", "the", "project", "complet", "and", "accept", "should", "continu", "updat", "with", "new", "data", "and", "new", "insight", "these", "insight", "can", "further", "enrich", "with", "the", "rate", "chang", "with", "time", "the", "chang", "can", "also", "captur", "and", "use", "metric", "progress", "this", "becom", "anoth", "longitudin", "problem", "solv", "apart", "from", "the", "problem", "which", "can", "pursu", "with", "text", "data", "text", "mine", "easi", "feat", "when", "creat", "roadmap", "collect", "clean", "and", "analyz", "data", "there", "may", "sever", "obstacl", "that", "will", "come", "way", "they", "can", "situat", "when", "have", "decid", "whether", "work", "with", "singl", "word", "frequenc", "document", "term", "matrix", "use", "group", "word", "known", "ngram", "build", "own", "visual", "method", "present", "result", "memori", "manag", "the", "same", "time", "new", "project", "are", "come", "the", "area", "text", "mine", "the", "best", "way", "learn", "face", "the", "problem", "hand", "and", "learn", "from", "the", "experi", "work", "the", "problem", "hope", "this", "articl", "provid", "motiv", "head", "the", "world", "text", "and", "start", "mine", "insight", "nugget", "inform", "bio", "chaitanya", "sagar", "the", "founder", "and", "percept", "analyt", "percept", "analyt", "has", "been", "chosen", "one", "the", "top", "num", "analyt", "compani", "watch", "out", "for", "analyt", "india", "magazin", "work", "market", "analyt", "for", "ecommerc", "retail", "and", "pharma", "compani", "relat", "top", "num", "machin", "learn", "with", "video", "learn", "general", "linear", "model", "use", "solut", "miss", "data", "imput", "use"], "timestamp_scraper": 1556379307.039756, "title": "Tips for Getting Started with Text Mining in R and Python", "read_time": 621.9, "content_html": "<div class=\"post\" id=\"post-\">\n<p><b>By Chaitanya Sagar, <a href=\"http://www.perceptive-analytics.com/\" target=\"_blank\">Perceptive Analytics</a>.</b></p>\n<p><b>It All Starts With The Text</b></p>\n<p>There is so much of information lying in the text posts made by you and me and all others about all the trending topics today. Being in our respective firms, big or small, each of us collect some data related to our respective businesses and store it to analyze for various projects. At the same time, we all need this \u2018unstructured data\u2019 to know and understand more about our clients, customers and the state of our company in the world today. However, working with this data is not easy. The data is not structured, every piece does not have all the information and each part is unique. This is how textual data is. It needs to be processed first and converted in a form that is suitable for analysis. This is very similar to our own databases which we create except that they cannot be used directly and the amount of data is very large. This article opens up the world of text mining in a simple and intuitive way and provides great tips to get started with text mining.</p>\n<p><img alt=\"\" class=\"aligncenter\" src=\"/wp-content/uploads/documents.jpg\" width=\"95%\"/></p>\n<p><b>Tip #1: Think First</b></p>\n<p>The mammoth of text mining can become a simple task if you work on it with a plan in mind. Think what you need to do with text before going all out on it. What is your objective behind text mining? What sources of data do you want to use? How much data do you need for it to be sufficient? How do you plan to present your results from the data? It is all about getting curious about your problem and break it into small fragments. Thinking through the problem also opens up your mind towards the various situations you may encounter and ways to tackle those situations. You can then chart out a workflow and start pursuing the task.</p>\n<p><b>Tip #2: R or Python.. Or Something Else?</b></p>\n<p>There is no gold standard procedure for text mining. You have to choose the method which is most convenient for text mining. This is where factors such as efficiency,effectiveness type of problem and other factors come into play and helps you decide the best candidate for your problem. After having decided your chosen path, you need to build your knowledge and skills in developing skills in that language. I find the text mining techniques more intuitive in Python than in R but R has some handy functions to do tasks such as word counting and is richer in terms of packages available for text mining.</p>\n<p><b>Tip #3: Start Early and Collect Your Data</b></p>\n<ul>\n<li>The usual process of text mining involves the following steps:\n<li>Collect data; either from social media such as twitter or other websites. Write your code that can adjust to the specific type of text you collect and store it\n<li>Convert your data into readable text\n<li>Remove special characters from the text (such as hashtags). You can add a hashtag count feature if that is required\n<li>Removing numbers from the text data (unless the problem requires numbers)\n<li>Deciding whether to keep all the data or remove some of it such as all non-English text\n<li>Converting all the text to uppercase or lowercase only to ease analysis\n<li>Removing stop words.. Words that have no use in your analysis. This includes articles, conjunctions, etc.\n<li>Using word stemming and grouping similar words such as \u2018keep\u2019 and \u2018keeping\u2019 are same words used in different tense form.\n<li>Final analysis of the processed stemmed words and visualize results\n</li></li></li></li></li></li></li></li></li></li></ul>\n<p>The steps are short and simple but they all depend on the first step executed well. You need to collect your data so that text mining can be performed on it. There are many ways to collect data. One of the most popular sources to collect data from is Twitter. Twitter has exposed some APIs so that tweets can be mined using both R and Python. Besides twitter, one can capture data from any website today including e-commerce websites, movie websites, song websites, etc. Some websites also contain preformatted repositories of text data such as project gutenberg, corpora, etc. Google trends and yahoo also offer some analysis online.</p>\n<p><b>Tip #4: Find and Use The Best Way to Convert Text to Data</b></p>\n<p>Based on the tools and your project objective, you may use a different approach to convert your collected text to data. If you are using R, packages such as twitteR, tm and stringr are what you may be using for most of the preprocessing. The nltk library and Tweepy package are the equivalent packages in Python. Whichever language and package you use, make sure that you have enough resources and memory to handle the data. Text mining can be cumbersome just because of the irrelavant text lying around in your data even after removing stop words. Using a good method to prepare data will give you a lot of useful information when you apply modelling techniques on the data.</p>\n<p><b>Tip #5: Explore and Play Around</b></p>\n<p>You need to know your data before preprocessing it. Without the knowledge of how your data looks like, you might carelessly remove text which might have been useful in your analysis. There are many standard methods and dictionaries of removing stopwords and assigning importance to words but they may or may not apply to your data. For example, data about the government may include a lot of words such as \u2018rule\u2019, \u2018govern\u2019 and \u2018politics\u2019 which you may deem unnecessary and want to remove. Reviews may include lots of \u2018hi\u2019 in the beginning but may not be useful for a review dataset. It is always a good step to look at the source of data and go through some of the text to know how the process you defined for analysis is working to transform it correctly into useful information. Other ways specific to exploring text data is by creating a document term matrix. A document term matrix is a m*n matrix where the number of columns denote the total number of unique words in the entire dataset and the number of rows denote the total data points. Each cell thus represents the count of the particular word in that datapoint. This is a very large matrix and is later collapsed into term-frequency. From this document term matrix, one can count the total occurrences of each word in the dataset and that is exactly what term-frequency matrix stores. Other uses of document term matrix include knowing correlation between words, drawing a word cloud using term-frequency or predicting patterns using modelling techniques. This exploration will further give you confidence on the best way to move forward with textual data analysis.</p>\n<p><b>Tip #6: Dive Deep and Get Your Hands Dirty</b></p>\n<p>The primary objective or every machine learning and data science project is to find patterns in the data that are otherwise hard to find. You need to look for those interesting patterns and are not a true data scientist if you\u2019re scared of this step. It can be as simple as fitting a simple classifier to classify data points and see its performance. This will set a benchmark while giving you an idea of the predicting ability of the data. At times, the data may be biased or have a poor predictive power and data quality checks can help define this. For example, If I am collecting twitter data on the basis of hashtags, I can divide my collected data into train and test datasets keeping the hashtags as the dependent feature. If my prediction performance is not up to the mark, I need to go back a few steps and find out the cause of this low performance and then check how I am collecting data or how I am cleaning my data as the case may be. Other ways of getting patterns involve associations. For example, some data points may be related to each other while others may have a similar or opposite pattern. If tweets are being used for text mining, there can be duplicate tweets because of retweeting or debates going for or against a remark. Working with data also exposes problems such as dealing with sarcasm or comments that convey mixed expressions. Without brushing through the data, it will be difficult to know how much of your data is affected by these problems and whether you should drop such data or use some technique to handle the situation.</p>\n<p><b>Tip #7: Rework and Repeat</b></p>\n<p>The problem you are trying to solve may or may not be the first text mining problem in your company but it is certainly not the first text mining problem in the world today. There are several data scientists out there who have worked on either the same or similar problem as the one you are working on and knowing what methods they followed and what they did differently will help you take your problem solving to the next level. Though not as frequent as other domains, there are several analysis and projects being done on text mining which include finding the trending topics, sentiment analysis on the trending topics, identifying remarks about your firm or product, identifying grievances and appreciations and the like. With the same data, there can be more than one problem that can be solved. Complex problems which can be explored also include NLP and topic modelling. I read about a fairly recent project in which some students predicted the next topic which a group of people will discuss based on the current conversation. There can be many such new projects which can be thought of and pursued in the area of text mining but since it is a new and hot field to work on, always refer other similar data and resources to further compliment your analysis and come up with strong insights.</p>\n<p><b>Tip #8: Presenting Text Visually</b></p>\n<p>As mentioned earlier, there can be a lot of problems which can be pursued using text mining and more than one problem can be solved from the same data. With so much to present, it is a good practice to come up with ways to present the results in a way that would seem attractive to people. This is why most of the text mining results are already visualized in the form of word clouds, sentiment studies and figures. There are a packages and libraries for each such task which include wordcloud, ggplot2, igraph, text2vec, networkD3 and plotly in R and Networkx, matplotlib, plotly in Python. You can also use other sophisticated tools just for visualization such as Tableau or Power BI which can help visualize your data in many more ways.</p>\n<p><b>Conclusion: A Roadmap</b></p>\n<p>Visualizing results is not the end step in text mining projects. Since text is captured from online sources, it is constantly changing and so is the data that is captured. With the changing data comes changing insights and hence, when the project is completed and accepted, it should be continuously updated with new data and new insights. These insights can be further enriched with the rate of change. With time, the change can also be captured and used as a metric of progression. This becomes another longitudinal problem to be solved. Apart from the problems which can be pursued with text data, text mining is no easy feat. When you create a roadmap of collecting, cleaning and analyzing data, there may be several obstacles that will come your way. They can be situations when you have to decide whether to work with a single word frequency in document term matrix or use groups of words (known as n-grams) or building your own visualization method to present your results or memory management. At the same time, new projects are coming up in the area of text mining. The best way to learn is to face the problem hands on and learn from the experience of working on the problem. Hope this article provides motivation to head to the world of text and start mining insightful nuggets of information.</p>\n<p>\u00a0<br>\n<b>Bio: <a href=\"https://www.linkedin.com/in/chaitanyasagar/\" target=\"_blank\">Chaitanya Sagar</a></b> is the Founder and CEO of <a href=\"http://www.perceptive-analytics.com/\" target=\"_blank\">Perceptive Analytics</a>. Perceptive Analytics has been chosen as one of the top 10 analytics companies to watch out for by Analytics India Magazine. It works on Marketing Analytics for e-commerce, Retail and Pharma companies.</br></p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/10/top-10-machine-learning-r-videos.html\">Top 10 Machine Learning with R Videos</a>\n<li><a href=\"/2017/10/learn-generalized-linear-models-glm-r.html\">Learn Generalized Linear Models (GLM) using R</a>\n<li><a href=\"/2017/09/missing-data-imputation-using-r.html\">A Solution to Missing Data: Imputation Using R</a>\n</li></li></li></ul>\n</div> ", "website": "kdnuggets"}