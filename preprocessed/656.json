{"content": "By Matthew Mayo , KDnuggets. comments In a pair of previous posts, we first discussed a framework for approaching textual data science tasks , and followed that up with a discussion on a general approach to preprocessing text data . This post will serve as a practical walkthrough of a text data preprocessing task using some common Python tools. Preprocessing, in the context of the textual data science framework. Our goal is to go from what we will describe as a chunk of text (not to be confused with text chunking), a lengthy, unprocessed single string, and end up with a list (or several lists) of cleaned tokens that would be useful for further text mining and/or natural language processing tasks. First we start with our imports. Beyond the standard Python libraries, we are also using the following: NLTK - The Natural Language ToolKit is one of the best-known and most-used NLP libraries in the Python ecosystem, useful for all sorts of tasks from tokenization, to stemming, to part of speech tagging, and beyond BeautifulSoup - BeautifulSoup is a useful library for extracting data from HTML and XML documents Inflect - This is a simple library for accomplishing the natural language related tasks of generating plurals, singular nouns, ordinals, and indefinite articles, and (of most interest to us) converting numbers to words Contractions - Another simple library, solely for expanding contractions If you have NLTK installed, yet require the download of its any additional data, see here . We need some sample text. We'll start with something very small and artificial in order to easily see the results of what we are doing step by step. A toy dataset indeed, but make no mistake; the steps we are taking here to preprocessing this data are fully transferable. The text data preprocessing framework. \u00a0 Noise Removal \u00a0 Let's loosely define noise removal as text-specific normalization tasks which often take place prior to tokenization. I would argue that, while the other 2 major steps of the preprocessing framework (tokenization and normalization) are basically task-independent, noise removal is much more task-specific. Sample noise removal tasks could include: removing text file headers, footers removing HTML, XML, etc. markup and metadata extracting valuable data from other formats, such as JSON As you can imagine, the boundary between noise removal and data collection and assembly, on the one hand, is a fuzzy one, while the line between noise removal and normalization is blurred on the other. Given its close relationship with specific texts and their collection and assembly, many denoising tasks, such as parsing a JSON structure, would obviously need to be implemented prior to tokenization. In our data preprocessing pipeline, we will strip away HTML markup with the help of the BeautifulSoup library, and use regular expressions to remove open and close double brackets and anything in between them (we assume this is necessary based on our sample text). While not mandatory to do at this stage prior to tokenization (you'll find that this statement is the norm for the relatively flexible ordering of text data preprocessing tasks), replacing contractions with their expansions can be beneficial at this point, since our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\" It's not impossible to remedy this tokenization at a later stage, but doing so prior makes it easier and more straightforward. And here is the result of de-noising on our sample text. Title Goes Here\r Bolded Text\r Italicized Text\r \r But this will still be here!\r \r I run. He ran. She is running. Will they stop running?\r \r I talked. She was talking. They talked to them about running. Who ran to the talking runner?\r \r \r \r \u00a1Sebasti\u00e1n, Nicol\u00e1s, Alejandro and J\u00e9ronimo are going to the store tomorrow morning!\r \r something... is!  with.,; this :: sentence.\r \r I cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?\r \r My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\r \r do not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\r \r \r \r John: \"Well, well, well.\"\r James: \"There, there. There, there.\"\r \r \r \r There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\r I have to go get 2 tutus from 2 different stores, too.\r \r 22 45 1067 445\r \r {{Here is some stuff inside of double curly braces.}}\r {Here is more stuff in single curly braces.} \u00a0 Tokenization \u00a0 Tokenization is a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized. Tokenization is also referred to as text segmentation or lexical analysis. Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words. For our task, we will tokenize our sample text into a list of words. This is done using NTLK's  function. And here are our word tokens: ['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'But', 'this', 'will', 'still',\r 'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', \r 'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', \r 'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', '\u00a1Sebasti\u00e1n', ',', \r 'Nicol\u00e1s', ',', 'Alejandro', 'and', 'J\u00e9ronimo', 'are', 'going', 'tot', 'he', 'store', 'tomorrow', \r 'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', \r 'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', \r 'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', \r 'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Star', 'Wars', ';', 'Marvel', \r 'Cinematic', 'Universe', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'do', 'not', \r 'do', 'it', '...', '.', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', \r 'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', \r ':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', \r 'There', ',', 'there', '.', \"''\", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', \r '.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', \r 'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', \r 'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', \r 'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', \r '.', '}'] \u00a0 Normalization \u00a0 Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly. Normalizing text can mean performing a number of tasks, but for our framework we will approach normalization in 3 distinct steps: (1) stemming, (2) lemmatization, and (3) everything else. For specifics on what these distinct steps may be, see this post . Remember, after tokenization, we are no longer working at a text level, but now at a word level. Our normalization functions, shown below, reflect this. Function names and comments should provide the necessary insight into what each does. After calling the normalization function: ['title', 'goes', 'bolded', 'text', 'italicized', 'text', 'still', 'run', 'ran', 'running', 'stop', \r 'running', 'talked', 'talking', 'talked', 'running', 'ran', 'talking', 'runner', 'sebastian', 'nicolas', \r 'alejandro', 'jeronimo', 'going', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', \r 'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchises', 'order', \r 'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'wars', 'back', 'future', 'harry', \r 'potter', 'billy', 'know', 'great', 'little', 'house', 'got', 'john', 'well', 'well', 'well', 'james', \r 'lot', 'reasons', 'one hundred and one', 'reasons', 'one million', 'reasons', 'actually', 'go', 'get', \r 'two', 'tutus', 'two', 'different', 'stores', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', \r 'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'braces', 'stuff', 'single', \r 'curly', 'braces'] Calling the stemming and lemming functions are done as below: This results in a return of 2 new lists: one of stemmed tokens, and another of lemmatized tokens with respect to verbs. Depending on your upcoming NLP task or preference, one of these may be more appropriate than the other. See here for a discussion on lemmatization vs. stemming . Stemmed:\r ['titl', 'goe', 'bold', 'text', 'it', 'text', 'stil', 'run', 'ran', 'run', 'stop', 'run', 'talk', \r 'talk', 'talk', 'run', 'ran', 'talk', 'run', 'sebast', 'nicola', 'alejandro', 'jeronimo', 'going', \r 'stor', 'tomorrow', 'morn', 'someth', 'wrong', 'sent', 'anym', 'know', 'could', 'din', 'resta', \r 'favorit', 'movy', 'franch', 'ord', 'indian', 'jon', 'marvel', 'cinem', 'univers', 'star', 'war', 'back', \r 'fut', 'harry', 'pot', 'bil', 'know', 'gre', 'littl', 'hous', 'got', 'john', 'wel', 'wel', 'wel', 'jam', \r 'lot', 'reason', 'one hundred and on', 'reason', 'one million', 'reason', 'act', 'go', 'get', 'two', \r 'tut', 'two', 'diff', 'stor', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred \r and forty-five', 'stuff', 'insid', 'doubl', 'cur', 'brac', 'stuff', 'singl', 'cur', 'brac']\r \r Lemmatized:\r ['title', 'go', 'bolded', 'text', 'italicize', 'text', 'still', 'run', 'run', 'run', 'stop', 'run', \r 'talk', 'talk', 'talk', 'run', 'run', 'talk', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', \r 'go', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', \r 'dinner', 'restaurant', 'favorite', 'movie', 'franchise', 'order', 'indiana', 'jones', 'marvel', \r 'cinematic', 'universe', 'star', 'war', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', \r 'little', 'house', 'get', 'john', 'well', 'well', 'well', 'jam', 'lot', 'reason', 'one hundred and one', \r 'reason', 'one million', 'reason', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'store', \r 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', \r 'inside', 'double', 'curly', 'brace', 'stuff', 'single', 'curly', 'brace']\r And there you have a walkthrough of a simple text data preprocessing process using Python on a sample piece of text. I would encourage you to perform these tasks on some additional texts to verify the results. We will use this same process to clean the text data for our next task, in which we will undertake some actual NLP task, as opposed to spending time preparing our data for such an actual task. \u00a0 Related : A General Approach to Preprocessing Text Data A Framework for Approaching Textual Data Science Tasks Natural Language Processing Key Terms, Explained", "title_html": "<h1 id=\"title\"><img align=\"right\" alt=\"Gold Blog\" src=\"/images/tkb-1803-g.png\" width=\"94\"/>Text Data Preprocessing: A Walkthrough in Python</h1> ", "url": "https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html", "tfidf": {"tfidf": {"after": 3.06210621063, "hand": 1.6152202665600002, "matthew": 6.908616187989999, "equival": 4.09175257732, "harri": 19.394087466400002, "too": 3.6317053643, "relat": 4.95003507676, "stage": 4.1663823645199995, "new": 1.0178880554, "\u00a1sebasti\u00e1n": 3175.2, "addit": 2.49269901084, "alejandro": 354.375, "would": 4.331491712719999, "nois": 70.144329897, "boundari": 4.85801713586, "been": 1.0239277652399998, "etc": 8.413354531, "dataset": 193.609756098, "franchis": 44.2845188284, "potter": 88.8167832168, "undertak": 10.8, "python": 225.1914893616, "tut": 441.0, "well": 12.786898449599999, "done": 4.6605019814999995, "four": 3.62852354106, "assembl": 6.002268431, "approach": 10.37782716695, "toy": 15.383720930199999, "lemmat": 6350.4, "know": 25.9327017315, "bil": 588.0, "their": 3.0464372521500005, "equal": 2.542193755, "noun": 30.068181818200003, "sebastian": 53.6351351352, "john": 6.7776639344500005, "express": 1.9120799710900003, "markup": 538.169491526, "strip": 5.50485436893, "million": 5.183717892900001, "chunk": 324.0, "lengthi": 12.165517241400002, "jame": 5.794160583929999, "hundr": 14.818917237059997, "end": 1.10680423871, "given": 1.35426085473, "word": 19.761910150509998, "interest": 1.60331246213, "depend": 2.2411067193700003, "will": 14.69773183152, "specif": 3.7438981252199994, "open": 1.24556723678, "confus": 4.1451697127900005, "ran": 27.932263030559998, "twentytwo": 4762.799999999999, "next": 1.4950560316400001, "below": 4.51215006394, "perform": 4.59419311275, "but": 6.09794507394, "need": 2.8745247148199997, "our": 30.64864864867, "metadata": 211.68, "denois": 3175.2, "titl": 9.36305732485, "dinner": 50.9254210104, "point": 1.25990000794, "header": 83.55789473680001, "key": 2.28005170185, "level": 4.96331804919, "has": 1.0436497502, "have": 8.119158729119999, "replac": 1.5602948402899999, "resta": 1221.23076923, "use": 10.296387573799999, "mistak": 8.71350164654, "format": 2.53125, "toolkit": 189.0, "number": 3.30428749827, "field": 1.7790228597, "store": 27.57446808512, "not": 17.266457680229998, "reserv": 3.24198488871, "lem": 196.0, "respect": 1.6443293630200002, "wrong": 21.913043478280002, "implement": 3.57648118946, "anym": 1587.6, "breakdown": 30.0966824644, "indiana": 42.7348586812, "shown": 2.76923076923, "stuff": 233.127753304, "function": 12.477208424999999, "franch": 1323.0, "anoth": 2.27287043664, "instal": 3.78721374046, "case": 1.48498737256, "let": 3.48616600791, "scienc": 6.959088252479999, "kdnugget": 1587.6, "mostus": 1587.6, "away": 1.85142857143, "stem": 44.721126760560004, "ord": 143.027027027, "practic": 1.70434782609, "simpl": 10.19434931508, "normal": 26.1075481006, "provid": 1.21552714187, "sort": 5.188235294119999, "got": 14.47879616964, "order": 7.4775100086599995, "such": 3.18454132122, "sever": 1.07241286139, "sentenc": 40.91752577318, "talk": 60.606986066, "remedi": 18.0614334471, "lot": 22.0438767009, "some": 6.2422018348799995, "beyond": 5.09172546504, "small": 1.3594793629, "should": 1.6643254009900001, "put": 3.31613577024, "step": 19.79551122192, "result": 5.7305804216, "document": 2.5409731114, "univ": 32.869565217399995, "inflect": 64.2753036437, "languag": 9.17953165656, "futur": 7.430844839680001, "ani": 1.13383802314, "two": 6.0827586207, "from": 6.00340328982, "ecosystem": 26.111842105300003, "verifi": 14.2258064516, "allow": 1.2716059271100002, "work": 1.11520089913, "num": 22.00693088022, "els": 5.44444444444, "favorit": 40.582822085900006, "upcom": 13.853403141400001, "movi": 20.02017654475, "for": 15.004725600150001, "brac": 907.2, "cur": 980.0, "paragraph": 20.6181818182, "with": 11.013180298889997, "stop": 10.891877058200002, "assum": 2.9575260804799997, "are": 16.47849497248, "split": 6.9418452120600005, "marvel": 100.100882724, "preprocess": 12212.3076923, "sent": 2.32683570277, "gre": 793.8, "nicol\u00e1": 3175.2, "veri": 1.25880114177, "extract": 15.406113537120001, "fut": 756.0, "pars": 145.651376147, "textspecif": 1587.6, "later": 1.08650424309, "imagin": 6.598503740650001, "general": 4.487280949680001, "remov": 20.0581174984, "singular": 16.8, "doe": 1.70581282905, "foot": 5.65384615385, "place": 1.1004366812200002, "great": 5.06371102784, "goal": 3.28152128979, "framework": 49.20247933883999, "comment": 6.11909809212, "oppos": 2.51282051282, "encourag": 2.7975330396499998, "into": 7.10517230353, "prior": 8.71230621484, "unprocess": 378.0, "download": 14.6457564576, "requir": 1.52844902282, "meant": 3.20597738288, "what": 7.520606347679999, "rememb": 4.88793103448, "restaur": 29.170418006440002, "jeronimo": 3402.0, "relationship": 2.39132399458, "j\u00e9ronimo": 3175.2, "flexibl": 9.68639414277, "help": 1.39962972759, "larg": 1.18574949585, "ordin": 15.3391304348, "jon": 10.4173228346, "line": 1.4182597820299998, "doubl": 17.82818641212, "taskspecif": 1587.6, "base": 1.14628158845, "natur": 6.1570680628400005, "nicola": 46.7399411187, "who": 2.12558575446, "univers": 4.99559471364, "bracket": 32.466257668699996, "post": 6.71478922881, "here": 36.3461538462, "lower": 2.10055570257, "larger": 4.481580804519999, "littl": 7.749682710149999, "punctuat": 42.1114058355, "upper": 3.41052631579, "clean": 13.73950670706, "much": 1.1942229577299999, "about": 2.12972030318, "norm": 11.299644128099999, "articl": 2.01805008262, "which": 4.02076738, "distinct": 4.567318757200001, "context": 4.25972632144, "just": 2.67160286074, "war": 7.0660494925999995, "fuzzi": 113.4, "billi": 36.3295194508, "she": 8.64, "standard": 1.8915763135900003, "statement": 3.42228928648, "segment": 15.11280342694, "defin": 2.72830383227, "sole": 4.04175152749, "term": 1.39520168732, "jam": 30.9775609756, "pot": 21.2530120482, "tool": 4.99716713881, "start": 2.53347163488, "run": 35.80935569274, "them": 5.4938057997000005, "verb": 29.291512915100004, "insight": 11.8037174721, "they": 4.12069301148, "seri": 1.46511627907, "cinemat": 166.67716535440002, "explain": 2.60049140049, "singl": 9.65693430654, "expand": 2.2260235558000003, "whi": 6.513230769240001, "indian": 3.5046357615900003, "mani": 1.04426757877, "stor": 3175.2, "string": 16.75567282322, "the": 39.0, "anymor": 140.4955752212, "indefinit": 17.0160771704, "mine": 4.875921375919999, "taskindepend": 1587.6, "anyth": 4.58843930636, "there": 13.53186467347, "blur": 29.7303370787, "sebast": 1134.0, "imposs": 4.96125, "spend": 4.15928739848, "loos": 7.065420560750001, "lexic": 74.1869158879, "hous": 7.312085482699999, "take": 2.27923336444, "follow": 2.09280253098, "now": 1.160780873, "further": 2.723623263, "name": 1.10211732037, "brace": 288.0, "than": 2.0655737705, "basic": 2.7301805675, "play": 1.46390041494, "goe": 17.006963042320002, "wel": 1443.2727272729999, "also": 2.02953020134, "stil": 1587.6, "everyth": 4.81967213115, "mean": 1.44906900329, "textual": 124.3550913837, "fulli": 2.79015817223, "task": 69.95544675642, "major": 1.14852058164, "accomplish": 5.17302052786, "may": 2.10403551786, "mayo": 49.7680250784, "fortyf": 9525.599999999999, "tag": 19.7462686567, "thousand": 7.430265210600001, "tot": 162.0, "previous": 1.42846859816, "convert": 9.82223138793, "act": 1.4318181818200002, "sixtyseven": 4762.799999999999, "analysi": 3.47852760736, "get": 10.7137554831, "piec": 12.96529195592, "actual": 11.24893717524, "token": 707.8471337574, "easier": 7.84, "uniform": 5.7231434751300005, "appropri": 8.62826086956, "plural": 14.2898289829, "other": 4.03969465648, "refer": 3.9007371007500002, "benefici": 18.269275028800003, "one": 16.10039931552, "prefer": 3.0216977540900003, "see": 5.08968502044, "andor": 690.260869565, "tomorrow": 106.8371467025, "bestknown": 1587.6, "mandatori": 14.5785123967, "still": 4.746543089919999, "like": 1.14918566775, "morn": 20.790990047150004, "collect": 3.28219971056, "file": 3.7710213776699995, "beautifulsoup": 4762.799999999999, "process": 11.86673785374, "yet": 2.1258703802900003, "includ": 1.0190641247799999, "structur": 2.0580762250499998, "straightforward": 27.7552447552, "necessari": 5.684210526319999, "this": 23.08725341433, "transfer": 2.72549356223, "sampl": 43.39681093392, "time": 1.01127460348, "part": 1.04330682789, "back": 6.30350194555, "runner": 64.1454545456, "smaller": 2.59369384088, "contract": 9.049591487729998, "differ": 4.946179609, "same": 2.23715916296, "valuabl": 7.46754468485, "most": 1.02096463023, "between": 3.1036100612399995, "return": 1.39532431007, "obvious": 6.44841592201, "all": 4.04587155964, "reason": 25.8510638298, "someth": 19.68912773874, "footer": 793.8, "expans": 3.4770039421800005, "longer": 4.04638715432, "that": 4.015936255, "pair": 4.36873968079, "walkthrough": 1587.6, "call": 2.1353059852, "regular": 2.09418282548, "more": 5.085853408499999, "and": 40.0025196852, "list": 5.45285935084, "discuss": 6.59028642591, "these": 3.22246278756, "prepar": 2.43012398592, "jone": 19.89473684212, "artifici": 8.31639601886, "could": 7.226217569399999, "librari": 16.0959783711, "diff": 756.0, "close": 2.5697636775599997, "serv": 1.4668760972, "can": 8.23382973994, "describ": 1.47027227264, "make": 2.1525320317200003, "each": 1.18974820144, "speech": 3.8227787141800005, "find": 1.7294117647099998, "common": 1.4025974025999999, "din": 65.8755186722, "data": 57.39940450878, "cinem": 1587.6, "sinc": 1.08368600683, "sometim": 1.7126213592200001, "bold": 66.70588235299999, "reflect": 2.3443591258099996, "text": 115.74620689659001, "insid": 13.698015530649998, "italic": 1587.6, "import": 1.3401992233700002, "first": 2.01523229246, "proceed": 3.4333910034599997, "tutus": 6350.4, "generat": 2.05275407292, "while": 4.176795580119999, "argu": 2.67768595041, "exclus": 3.40906162766, "easili": 3.6938110749199997, "often": 1.29452054795, "inde": 4.43092380687, "pipelin": 32.1376518219, "star": 12.225473586950002}, "logtfidf": {"after": 0.061472083944299996, "hand": 0.479471335336, "matthew": 1.9327693554900003, "equival": 1.40897338129, "harri": 6.7776516870500005, "too": 1.1931103094439999, "relat": 0.8524012066159999, "stage": 1.467801880474, "new": 0.0177299468511, "\u00a1sebasti\u00e1n": 14.739957441820001, "addit": 0.440437765944, "alejandro": 21.30458881025, "would": 0.3184705118588, "nois": 14.75277303468, "boundari": 1.58063035792, "been": 0.023645982368400004, "etc": 2.8733461759400005, "dataset": 5.26584456664, "franchis": 9.617363170960001, "potter": 12.40112508484, "undertak": 2.37954613413, "python": 16.12262697184, "tut": 6.08904487545, "well": 0.7621732597872, "done": 1.691951966258, "four": 0.5706406166069999, "assembl": 2.1979805781, "approach": 3.6511680729050005, "toy": 2.73330986786, "lemmat": 29.479914883640003, "know": 9.529196943979999, "bil": 6.3767269479, "their": 0.046081515368100005, "equal": 0.933027391343, "noun": 3.4034675302, "sebastian": 6.5781143580400006, "john": 1.5209728885099998, "express": 0.648191639641, "markup": 11.19005274, "strip": 1.7056303155, "million": 1.640730750282, "chunk": 17.577796618679997, "lengthi": 2.49860549415, "jame": 1.974714977559, "hundr": 5.4248705222760005, "end": 0.101476798618, "given": 0.303255810831, "word": 6.444471906235, "interest": 0.47207177798199995, "depend": 0.806969815, "will": 2.4334384189800002, "specif": 1.253960335082, "open": 0.219591038029, "confus": 1.4219437317299999, "ran": 10.00272688064, "twentytwo": 22.10993616273, "next": 0.402163685499, "below": 1.627253183872, "perform": 1.27854255174, "but": 0.0971542324314, "need": 0.725480326884, "our": 11.149309784366, "metadata": 5.35507570037, "denois": 14.739957441820001, "titl": 3.1366698099499994, "dinner": 10.17627147288, "point": 0.23103235903299998, "header": 4.425539741740001, "key": 0.82419811896, "level": 1.510386569829, "has": 0.0427239448548, "have": 0.1182800187296, "replac": 0.444874803592, "resta": 7.1076144564399995, "use": 0.292080197316, "mistak": 2.1648737360799997, "format": 0.9287132518729999, "toolkit": 5.24174701506, "number": 0.2898257352558, "field": 0.5760642583510001, "store": 9.899589868160001, "not": 0.2643910211275, "reserv": 1.1761857622, "lem": 5.278114659230001, "respect": 0.49733261904, "wrong": 6.80315076408, "implement": 1.27437940907, "anym": 7.369978720910001, "breakdown": 5.422535535980001, "indiana": 9.47488235452, "shown": 1.01856958099, "stuff": 31.490015077499997, "function": 4.57232870797, "franch": 7.18765716411, "anoth": 0.255792723304, "instal": 1.3316305879, "case": 0.395406268889, "let": 1.2488025672799998, "scienc": 2.524308536673, "kdnugget": 7.369978720910001, "mostus": 7.369978720910001, "away": 0.615957541869, "stem": 12.0521193312, "ord": 4.96303361259, "practic": 0.533182530867, "simpl": 3.66966386817, "normal": 9.59639378783, "provid": 0.19517784432500002, "sort": 1.64639361896, "got": 5.1455635397199995, "order": 1.3208422847580001, "such": 0.179087933418, "sever": 0.06991112039689999, "sentenc": 12.35953827654, "talk": 22.1735578898, "remedi": 2.8937789162199996, "lot": 7.417984751250001, "some": 0.23744105438700003, "beyond": 1.86893916745, "small": 0.307101805059, "should": 0.509419876758, "put": 1.011305999708, "step": 7.27681539886, "result": 0.681894541905, "document": 0.932547122383, "univ": 3.4925471602499996, "inflect": 4.16317547727, "languag": 3.3227272976239997, "futur": 2.477380790796, "ani": 0.125608358366, "two": 0.08219306614920001, "from": 0.0034023250131959997, "ecosystem": 3.26238893194, "verifi": 2.65505767096, "allow": 0.24028061118900002, "work": 0.109034567273, "num": 0.006929788698734001, "els": 1.6945957207700002, "favorit": 10.469534816549999, "upcom": 2.62853091663, "movi": 6.93651339915, "for": 0.004724855930955001, "brac": 12.234431504819998, "cur": 45.84967478669999, "paragraph": 3.0261732990599994, "with": 0.01317240884729, "stop": 3.892896874815, "assum": 1.08435313525, "are": 0.4714795773232, "split": 2.48884087864, "marvel": 14.983702961350001, "preprocess": 71.0761445644, "sent": 0.844509277088, "gre": 6.676831540349999, "nicol\u00e1": 14.739957441820001, "veri": 0.230159793238, "extract": 4.08323446602, "fut": 6.6280413761800006, "pars": 4.9812159316699995, "textspecif": 7.369978720910001, "later": 0.0829654259878, "imagin": 1.88684291737, "general": 0.459810312252, "remov": 6.9604884158800004, "singular": 2.82137888641, "doe": 0.5340417297169999, "foot": 1.73233604876, "place": 0.0957070839572, "great": 0.943221032316, "goal": 1.18830712273, "framework": 12.62510727642, "comment": 2.23653506908, "oppos": 0.921405832541, "encourag": 1.02873797155, "into": 0.1043900426009, "prior": 3.113768690084, "unprocess": 5.934894195619999, "download": 2.6841506319, "requir": 0.424253510675, "meant": 1.16501699954, "what": 1.355323780962, "rememb": 1.5867691126199999, "restaur": 7.947443010680001, "jeronimo": 21.100519452869996, "relationship": 0.871847185184, "j\u00e9ronimo": 14.739957441820001, "flexibl": 2.2707222351599996, "help": 0.336207721344, "larg": 0.17037506060600002, "ordin": 2.7304071082, "jon": 2.3434700776599997, "line": 0.349430614452, "doubl": 6.534127451699999, "taskspecif": 7.369978720910001, "base": 0.13652330228700002, "natur": 1.725225357168, "nicola": 8.237960342040001, "who": 0.1218004659718, "univers": 0.889048422744, "bracket": 3.4802013244300003, "post": 2.4171004581029996, "here": 13.275572825550002, "lower": 0.742201929994, "larger": 1.613657323556, "littl": 2.19106994733, "punctuat": 3.7403186264499997, "upper": 1.22686662419, "clean": 3.8542564072600003, "much": 0.17749572930100002, "about": 0.1256869549492, "norm": 2.4247712321400003, "articl": 0.702131739574, "which": 0.02071365538172, "distinct": 1.651558293916, "context": 1.44920491442, "just": 0.579062868218, "war": 1.7293182040550001, "fuzzi": 4.73092139129, "billi": 8.825345030760001, "she": 3.0804328867839996, "standard": 0.63741050982, "statement": 1.2303097091500002, "segment": 4.04479022612, "defin": 1.00368010925, "sole": 1.3966781444299998, "term": 0.33303898354600003, "jam": 5.48023184466, "pot": 3.0564986287700004, "tool": 1.60887117963, "start": 0.472886738582, "run": 10.182444437397, "them": 0.47091663454649996, "verb": 3.3772978124599997, "insight": 2.46841452187, "they": 0.1189079790704, "seri": 0.38193461069799994, "cinemat": 14.91905775512, "explain": 0.955700427358, "singl": 2.855500614354, "expand": 0.80021683492, "whi": 2.36137686094, "indian": 1.25408659543, "mani": 0.0433157581221, "stor": 14.739957441820001, "string": 4.251179392780001, "the": 0.0, "anymor": 14.235526536279998, "indefinit": 2.83415861306, "mine": 1.58430908678, "taskindepend": 7.369978720910001, "anyth": 1.52353994585, "there": 0.5212726080315, "blur": 3.39216797494, "sebast": 7.033506484289999, "imposs": 1.60165772512, "spend": 1.42534376116, "loos": 1.9552125417200001, "lexic": 4.30658779888, "hous": 1.9004530619399997, "take": 0.261383924394, "follow": 0.09071382218839999, "now": 0.149092945021, "further": 0.617631790594, "name": 0.09723316638430002, "brace": 28.668151507679998, "than": 0.0645217244364, "basic": 1.00436774895, "play": 0.38110439064199997, "goe": 5.789313959199999, "wel": 18.52816875732, "also": 0.0293143156, "stil": 7.369978720910001, "everyth": 1.57270590317, "mean": 0.37092128352, "textual": 11.173586474159997, "fulli": 1.02609828678, "task": 24.43476251898, "major": 0.138474663439, "accomplish": 1.64345675928, "may": 0.10141999056880001, "mayo": 3.90737271112, "fortyf": 44.21987232546, "tag": 2.98296454472, "thousand": 2.720847791964, "tot": 5.08759633523, "previous": 0.356602960063, "convert": 3.5581081104, "act": 0.358945092473, "sixtyseven": 22.10993616273, "analysi": 1.2466091029200002, "get": 3.478614034692, "piec": 4.70392630556, "actual": 3.771085089888, "token": 73.87182011769, "easier": 2.05923883436, "uniform": 1.74451821303, "appropri": 2.9237915654799997, "plural": 2.65954802426, "other": 0.03949899167904, "refer": 0.787659740394, "benefici": 2.90522068864, "one": 0.100085626328, "prefer": 1.10581884366, "see": 0.963686341968, "andor": 6.5370695979699995, "tomorrow": 15.309338845950002, "bestknown": 7.369978720910001, "mandatori": 2.67954869097, "still": 0.6844888857160001, "like": 0.139053576545, "morn": 7.125409048100001, "collect": 0.99073332104, "file": 1.32734588723, "beautifulsoup": 22.10993616273, "process": 3.694804393175, "yet": 0.754181309241, "includ": 0.0188846813905, "structur": 0.7217716751350001, "straightforward": 3.3234248225200003, "necessari": 2.0890901347999997, "this": 0.0870883282075, "transfer": 1.00264953547, "sampl": 11.87175893034, "time": 0.0112115188626, "part": 0.04239531098280001, "back": 1.1583371544849999, "runner": 11.09943548308, "smaller": 0.9530830530519999, "contract": 3.31232198565, "differ": 0.849284485248, "same": 0.224119299208, "valuabl": 2.010566255, "most": 0.020747896295599998, "between": 0.10186104349589999, "return": 0.333126868592, "obvious": 1.86383450716, "all": 0.04561052839119999, "reason": 8.16452329443, "someth": 7.12984273638, "footer": 6.676831540349999, "expans": 1.2461709868100002, "longer": 1.4093544835499998, "that": 0.01590459351856, "pair": 1.47447456495, "walkthrough": 13.353663080699999, "call": 0.1309255488976, "regular": 0.739163417847, "more": 0.08512465799999999, "and": 0.002519605682544, "list": 1.239383046024, "discuss": 2.3609535678599998, "these": 0.2146008582024, "prepar": 0.8879422790620001, "jone": 6.41664342132, "artifici": 2.11822899018, "could": 1.1157376337400002, "librari": 5.920859885658, "diff": 6.6280413761800006, "close": 0.501333519728, "serv": 0.383135035608, "can": 1.136387674758, "describ": 0.385447603125, "make": 0.14699531564579998, "each": 0.173741689304, "speech": 1.3409775702700002, "find": 0.547781330288, "common": 0.338325805271, "din": 4.18776688041, "data": 20.6859499416, "cinem": 7.369978720910001, "sinc": 0.0803681994577, "sometim": 0.538025155343, "bold": 12.954276139000001, "reflect": 0.85201207065, "text": 42.197834369629994, "insid": 5.03906529065, "italic": 23.93473743916, "import": 0.292818277066, "first": 0.015174579624319999, "proceed": 1.23354840355, "tutus": 29.479914883640003, "generat": 0.719182341736, "while": 0.17299993517520004, "argu": 0.984952970196, "exclus": 1.22643707092, "easili": 1.3066587367, "often": 0.258140393351, "inde": 1.4886080966, "pipelin": 3.47002829672, "star": 4.47041930697}, "logidf": {"after": 0.020490694648099998, "hand": 0.479471335336, "matthew": 1.9327693554900003, "equival": 1.40897338129, "harri": 1.35553033741, "too": 0.5965551547219999, "relat": 0.21310030165399999, "stage": 0.733900940237, "new": 0.0177299468511, "\u00a1sebasti\u00e1n": 7.369978720910001, "addit": 0.220218882972, "alejandro": 4.26091776205, "would": 0.0796176279647, "nois": 2.45879550578, "boundari": 1.58063035792, "been": 0.023645982368400004, "etc": 1.4366730879700003, "dataset": 5.26584456664, "franchis": 2.4043407927400002, "potter": 3.10028127121, "undertak": 2.37954613413, "python": 4.03065674296, "tut": 6.08904487545, "well": 0.0635144383156, "done": 0.845975983129, "four": 0.190213538869, "assembl": 1.09899028905, "approach": 0.7302336145810001, "toy": 2.73330986786, "lemmat": 7.369978720910001, "know": 0.952919694398, "bil": 6.3767269479, "their": 0.015360505122700001, "equal": 0.933027391343, "noun": 3.4034675302, "sebastian": 3.2890571790200003, "john": 0.304194577702, "express": 0.648191639641, "markup": 5.59502637, "strip": 1.7056303155, "million": 0.5469102500940001, "chunk": 4.394449154669999, "lengthi": 2.49860549415, "jame": 0.658238325853, "hundr": 0.904145087046, "end": 0.101476798618, "given": 0.303255810831, "word": 0.585861082385, "interest": 0.47207177798199995, "depend": 0.806969815, "will": 0.202786534915, "specif": 0.626980167541, "open": 0.219591038029, "confus": 1.4219437317299999, "ran": 1.25034086008, "twentytwo": 7.369978720910001, "next": 0.402163685499, "below": 0.813626591936, "perform": 0.42618085058, "but": 0.0161923720719, "need": 0.362740163442, "our": 0.8576392141820001, "metadata": 5.35507570037, "denois": 7.369978720910001, "titl": 0.6273339619899999, "dinner": 2.54406786822, "point": 0.23103235903299998, "header": 4.425539741740001, "key": 0.82419811896, "level": 0.503462189943, "has": 0.0427239448548, "have": 0.0147850023412, "replac": 0.444874803592, "resta": 7.1076144564399995, "use": 0.0292080197316, "mistak": 2.1648737360799997, "format": 0.9287132518729999, "toolkit": 5.24174701506, "number": 0.0966085784186, "field": 0.5760642583510001, "store": 1.2374487335200002, "not": 0.0155524130075, "reserv": 1.1761857622, "lem": 5.278114659230001, "respect": 0.49733261904, "wrong": 1.70078769102, "implement": 1.27437940907, "anym": 7.369978720910001, "breakdown": 2.7112677679900004, "indiana": 2.36872058863, "shown": 1.01856958099, "stuff": 3.1490015077499995, "function": 0.914465741594, "franch": 7.18765716411, "anoth": 0.127896361652, "instal": 1.3316305879, "case": 0.395406268889, "let": 1.2488025672799998, "scienc": 0.841436178891, "kdnugget": 7.369978720910001, "mostus": 7.369978720910001, "away": 0.615957541869, "stem": 2.0086865552, "ord": 4.96303361259, "practic": 0.533182530867, "simpl": 1.2232212893899999, "normal": 0.959639378783, "provid": 0.19517784432500002, "sort": 1.64639361896, "got": 1.2863908849299999, "order": 0.22014038079300002, "such": 0.059695977806, "sever": 0.06991112039689999, "sentenc": 1.7656483252200001, "talk": 1.10867789449, "remedi": 2.8937789162199996, "lot": 1.4835969502500002, "some": 0.0395735090645, "beyond": 0.934469583725, "small": 0.307101805059, "should": 0.509419876758, "put": 0.505652999854, "step": 1.03954505698, "result": 0.136378908381, "document": 0.932547122383, "univ": 3.4925471602499996, "inflect": 4.16317547727, "languag": 0.8306818244059999, "futur": 0.619345197699, "ani": 0.125608358366, "two": 0.0136988443582, "from": 0.000567054168866, "ecosystem": 3.26238893194, "verifi": 2.65505767096, "allow": 0.24028061118900002, "work": 0.109034567273, "num": 0.00031499039539700004, "els": 1.6945957207700002, "favorit": 2.09390696331, "upcom": 2.62853091663, "movi": 1.3873026798299999, "for": 0.00031499039539700004, "brac": 6.117215752409999, "cur": 4.584967478669999, "paragraph": 3.0261732990599994, "with": 0.00119749171339, "stop": 0.778579374963, "assum": 1.08435313525, "are": 0.0294674735827, "split": 1.24442043932, "marvel": 2.99674059227, "preprocess": 7.1076144564399995, "sent": 0.844509277088, "gre": 6.676831540349999, "nicol\u00e1": 7.369978720910001, "veri": 0.230159793238, "extract": 2.04161723301, "fut": 6.6280413761800006, "pars": 4.9812159316699995, "textspecif": 7.369978720910001, "later": 0.0829654259878, "imagin": 1.88684291737, "general": 0.114952578063, "remov": 0.6960488415880001, "singular": 2.82137888641, "doe": 0.5340417297169999, "foot": 1.73233604876, "place": 0.0957070839572, "great": 0.235805258079, "goal": 1.18830712273, "framework": 2.10418454607, "comment": 1.11826753454, "oppos": 0.921405832541, "encourag": 1.02873797155, "into": 0.0149128632287, "prior": 0.778442172521, "unprocess": 5.934894195619999, "download": 2.6841506319, "requir": 0.424253510675, "meant": 1.16501699954, "what": 0.225887296827, "rememb": 1.5867691126199999, "restaur": 1.9868607526700002, "jeronimo": 7.033506484289999, "relationship": 0.871847185184, "j\u00e9ronimo": 7.369978720910001, "flexibl": 2.2707222351599996, "help": 0.336207721344, "larg": 0.17037506060600002, "ordin": 2.7304071082, "jon": 2.3434700776599997, "line": 0.349430614452, "doubl": 1.0890212419499998, "taskspecif": 7.369978720910001, "base": 0.13652330228700002, "natur": 0.431306339292, "nicola": 2.74598678068, "who": 0.0609002329859, "univers": 0.222262105686, "bracket": 3.4802013244300003, "post": 0.8057001527009999, "here": 0.8850381883700001, "lower": 0.742201929994, "larger": 0.806828661778, "littl": 0.438213989466, "punctuat": 3.7403186264499997, "upper": 1.22686662419, "clean": 1.9271282036300001, "much": 0.17749572930100002, "about": 0.0628434774746, "norm": 2.4247712321400003, "articl": 0.702131739574, "which": 0.00517841384543, "distinct": 0.825779146958, "context": 1.44920491442, "just": 0.289531434109, "war": 0.345863640811, "fuzzi": 4.73092139129, "billi": 2.2063362576900003, "she": 0.7701082216959999, "standard": 0.63741050982, "statement": 1.2303097091500002, "segment": 2.02239511306, "defin": 1.00368010925, "sole": 1.3966781444299998, "term": 0.33303898354600003, "jam": 2.74011592233, "pot": 3.0564986287700004, "tool": 1.60887117963, "start": 0.236443369291, "run": 0.442714975539, "them": 0.0941833269093, "verb": 3.3772978124599997, "insight": 2.46841452187, "they": 0.0297269947676, "seri": 0.38193461069799994, "cinemat": 3.72976443878, "explain": 0.955700427358, "singl": 0.475916769059, "expand": 0.80021683492, "whi": 1.18068843047, "indian": 1.25408659543, "mani": 0.0433157581221, "stor": 7.369978720910001, "string": 2.1255896963900005, "the": 0.0, "anymor": 3.5588816340699996, "indefinit": 2.83415861306, "mine": 1.58430908678, "taskindepend": 7.369978720910001, "anyth": 1.52353994585, "there": 0.0400978929255, "blur": 3.39216797494, "sebast": 7.033506484289999, "imposs": 1.60165772512, "spend": 1.42534376116, "loos": 1.9552125417200001, "lexic": 4.30658779888, "hous": 0.38009061238799996, "take": 0.130691962197, "follow": 0.045356911094199995, "now": 0.149092945021, "further": 0.308815895297, "name": 0.09723316638430002, "brace": 3.5835189384599997, "than": 0.0322608622182, "basic": 1.00436774895, "play": 0.38110439064199997, "goe": 1.4473284897999998, "wel": 6.17605625244, "also": 0.0146571578, "stil": 7.369978720910001, "everyth": 1.57270590317, "mean": 0.37092128352, "textual": 3.7245288247199992, "fulli": 1.02609828678, "task": 1.35748680661, "major": 0.138474663439, "accomplish": 1.64345675928, "may": 0.050709995284400004, "mayo": 3.90737271112, "fortyf": 7.369978720910001, "tag": 2.98296454472, "thousand": 0.906949263988, "tot": 5.08759633523, "previous": 0.356602960063, "convert": 1.1860360368, "act": 0.358945092473, "sixtyseven": 7.369978720910001, "analysi": 1.2466091029200002, "get": 0.579769005782, "piec": 1.17598157639, "actual": 0.628514181648, "token": 3.5177057198900004, "easier": 2.05923883436, "uniform": 1.74451821303, "appropri": 1.4618957827399999, "plural": 2.65954802426, "other": 0.00987474791976, "refer": 0.262553246798, "benefici": 2.90522068864, "one": 0.0062553516455, "prefer": 1.10581884366, "see": 0.240921585492, "andor": 6.5370695979699995, "tomorrow": 3.0618677691900005, "bestknown": 7.369978720910001, "mandatori": 2.67954869097, "still": 0.17112222142900002, "like": 0.139053576545, "morn": 1.4250818096200002, "collect": 0.49536666052, "file": 1.32734588723, "beautifulsoup": 7.369978720910001, "process": 0.527829199025, "yet": 0.754181309241, "includ": 0.0188846813905, "structur": 0.7217716751350001, "straightforward": 3.3234248225200003, "necessari": 1.0445450673999999, "this": 0.0037864490525, "transfer": 1.00264953547, "sampl": 1.9786264883900002, "time": 0.0112115188626, "part": 0.04239531098280001, "back": 0.23166743089699998, "runner": 2.77485887077, "smaller": 0.9530830530519999, "contract": 1.10410732855, "differ": 0.212321121312, "same": 0.112059649604, "valuabl": 2.010566255, "most": 0.020747896295599998, "between": 0.033953681165299995, "return": 0.333126868592, "obvious": 1.86383450716, "all": 0.011402632097799998, "reason": 0.544301552962, "someth": 1.18830712273, "footer": 6.676831540349999, "expans": 1.2461709868100002, "longer": 0.7046772417749999, "that": 0.00397614837964, "pair": 1.47447456495, "walkthrough": 6.676831540349999, "call": 0.0654627744488, "regular": 0.739163417847, "more": 0.017024931599999998, "and": 6.29901420636e-05, "list": 0.309845761506, "discuss": 0.78698452262, "these": 0.0715336194008, "prepar": 0.8879422790620001, "jone": 1.60416085533, "artifici": 2.11822899018, "could": 0.18595627229000003, "librari": 0.986809980943, "diff": 6.6280413761800006, "close": 0.250666759864, "serv": 0.383135035608, "can": 0.162341096394, "describ": 0.385447603125, "make": 0.07349765782289999, "each": 0.173741689304, "speech": 1.3409775702700002, "find": 0.547781330288, "common": 0.338325805271, "din": 4.18776688041, "data": 1.2168205848, "cinem": 7.369978720910001, "sinc": 0.0803681994577, "sometim": 0.538025155343, "bold": 2.5908552278, "reflect": 0.85201207065, "text": 1.14048200999, "insid": 1.00781305813, "italic": 5.98368435979, "import": 0.292818277066, "first": 0.0075872898121599995, "proceed": 1.23354840355, "tutus": 7.369978720910001, "generat": 0.719182341736, "while": 0.04324998379380001, "argu": 0.984952970196, "exclus": 1.22643707092, "easili": 1.3066587367, "often": 0.258140393351, "inde": 1.4886080966, "pipelin": 3.47002829672, "star": 0.8940838613940001}, "freq": {"after": 3, "hand": 1, "matthew": 1, "equival": 1, "harri": 5, "too": 2, "relat": 4, "stage": 2, "new": 1, "\u00a1sebasti\u00e1n": 2, "addit": 2, "alejandro": 5, "would": 4, "nois": 6, "boundari": 1, "been": 1, "etc": 2, "dataset": 1, "franchis": 4, "potter": 4, "undertak": 1, "python": 4, "tut": 1, "well": 12, "done": 2, "four": 3, "assembl": 2, "approach": 5, "toy": 1, "lemmat": 4, "know": 10, "bil": 1, "their": 3, "equal": 1, "noun": 1, "sebastian": 2, "john": 5, "express": 1, "markup": 2, "strip": 1, "million": 3, "chunk": 4, "lengthi": 1, "jame": 3, "hundr": 6, "end": 1, "given": 1, "word": 11, "interest": 1, "depend": 1, "will": 12, "specif": 2, "open": 1, "confus": 1, "ran": 8, "twentytwo": 3, "next": 1, "below": 2, "perform": 3, "but": 6, "need": 2, "our": 13, "metadata": 1, "denois": 2, "titl": 5, "dinner": 4, "point": 1, "header": 1, "key": 1, "level": 3, "has": 1, "have": 8, "replac": 1, "resta": 1, "use": 10, "mistak": 1, "format": 1, "toolkit": 1, "number": 3, "field": 1, "store": 8, "not": 17, "reserv": 1, "lem": 1, "respect": 1, "wrong": 4, "implement": 1, "anym": 1, "breakdown": 2, "indiana": 4, "shown": 1, "stuff": 10, "function": 5, "franch": 1, "anoth": 2, "instal": 1, "case": 1, "let": 1, "scienc": 3, "kdnugget": 1, "mostus": 1, "away": 1, "stem": 6, "ord": 1, "practic": 1, "simpl": 3, "normal": 10, "provid": 1, "sort": 1, "got": 4, "order": 6, "such": 3, "sever": 1, "sentenc": 7, "talk": 20, "remedi": 1, "lot": 5, "some": 6, "beyond": 2, "small": 1, "should": 1, "put": 2, "step": 7, "result": 5, "document": 1, "univ": 1, "inflect": 1, "languag": 4, "futur": 4, "ani": 1, "two": 6, "from": 6, "ecosystem": 1, "verifi": 1, "allow": 1, "work": 1, "num": 22, "els": 1, "favorit": 5, "upcom": 1, "movi": 5, "for": 15, "brac": 2, "cur": 10, "paragraph": 1, "with": 11, "stop": 5, "assum": 1, "are": 16, "split": 2, "marvel": 5, "preprocess": 10, "sent": 1, "gre": 1, "nicol\u00e1": 2, "veri": 1, "extract": 2, "fut": 1, "pars": 1, "textspecif": 1, "later": 1, "imagin": 1, "general": 4, "remov": 10, "singular": 1, "doe": 1, "foot": 1, "place": 1, "great": 4, "goal": 1, "framework": 6, "comment": 2, "oppos": 1, "encourag": 1, "into": 7, "prior": 4, "unprocess": 1, "download": 1, "requir": 1, "meant": 1, "what": 6, "rememb": 1, "restaur": 4, "jeronimo": 3, "relationship": 1, "j\u00e9ronimo": 2, "flexibl": 1, "help": 1, "larg": 1, "ordin": 1, "jon": 1, "line": 1, "doubl": 6, "taskspecif": 1, "base": 1, "natur": 4, "nicola": 3, "who": 2, "univers": 4, "bracket": 1, "post": 3, "here": 15, "lower": 1, "larger": 2, "littl": 5, "punctuat": 1, "upper": 1, "clean": 2, "much": 1, "about": 2, "norm": 1, "articl": 1, "which": 4, "distinct": 2, "context": 1, "just": 2, "war": 5, "fuzzi": 1, "billi": 4, "she": 4, "standard": 1, "statement": 1, "segment": 2, "defin": 1, "sole": 1, "term": 1, "jam": 2, "pot": 1, "tool": 1, "start": 2, "run": 23, "them": 5, "verb": 1, "insight": 1, "they": 4, "seri": 1, "cinemat": 4, "explain": 1, "singl": 6, "expand": 1, "whi": 2, "indian": 1, "mani": 1, "stor": 2, "string": 2, "the": 39, "anymor": 4, "indefinit": 1, "mine": 1, "taskindepend": 1, "anyth": 1, "there": 13, "blur": 1, "sebast": 1, "imposs": 1, "spend": 1, "loos": 1, "lexic": 1, "hous": 5, "take": 2, "follow": 2, "now": 1, "further": 2, "name": 1, "brace": 8, "than": 2, "basic": 1, "play": 1, "goe": 4, "wel": 3, "also": 2, "stil": 1, "everyth": 1, "mean": 1, "textual": 3, "fulli": 1, "task": 18, "major": 1, "accomplish": 1, "may": 2, "mayo": 1, "fortyf": 6, "tag": 1, "thousand": 3, "tot": 1, "previous": 1, "convert": 3, "act": 1, "sixtyseven": 3, "analysi": 1, "get": 6, "piec": 4, "actual": 6, "token": 21, "easier": 1, "uniform": 1, "appropri": 2, "plural": 1, "other": 4, "refer": 3, "benefici": 1, "one": 16, "prefer": 1, "see": 4, "andor": 1, "tomorrow": 5, "bestknown": 1, "mandatori": 1, "still": 4, "like": 1, "morn": 5, "collect": 2, "file": 1, "beautifulsoup": 3, "process": 7, "yet": 1, "includ": 1, "structur": 1, "straightforward": 1, "necessari": 2, "this": 23, "transfer": 1, "sampl": 6, "time": 1, "part": 1, "back": 5, "runner": 4, "smaller": 1, "contract": 3, "differ": 4, "same": 2, "valuabl": 1, "most": 1, "between": 3, "return": 1, "obvious": 1, "all": 4, "reason": 15, "someth": 6, "footer": 1, "expans": 1, "longer": 2, "that": 4, "pair": 1, "walkthrough": 2, "call": 2, "regular": 1, "more": 5, "and": 40, "list": 4, "discuss": 3, "these": 3, "prepar": 1, "jone": 4, "artifici": 1, "could": 6, "librari": 6, "diff": 1, "close": 2, "serv": 1, "can": 7, "describ": 1, "make": 2, "each": 1, "speech": 1, "find": 1, "common": 1, "din": 1, "data": 17, "cinem": 1, "sinc": 1, "sometim": 1, "bold": 5, "reflect": 1, "text": 37, "insid": 5, "italic": 4, "import": 1, "first": 2, "proceed": 1, "tutus": 4, "generat": 1, "while": 4, "argu": 1, "exclus": 1, "easili": 1, "often": 1, "inde": 1, "pipelin": 1, "star": 5}, "idf": {"after": 1.02070207021, "hand": 1.6152202665600002, "matthew": 6.908616187989999, "equival": 4.09175257732, "harri": 3.87881749328, "too": 1.81585268215, "relat": 1.23750876919, "stage": 2.0831911822599998, "new": 1.0178880554, "\u00a1sebasti\u00e1n": 1587.6, "addit": 1.24634950542, "alejandro": 70.875, "would": 1.0828729281799998, "nois": 11.6907216495, "boundari": 4.85801713586, "been": 1.0239277652399998, "etc": 4.2066772655, "dataset": 193.609756098, "franchis": 11.0711297071, "potter": 22.2041958042, "undertak": 10.8, "python": 56.2978723404, "tut": 441.0, "well": 1.0655748708, "done": 2.3302509907499998, "four": 1.20950784702, "assembl": 3.0011342155, "approach": 2.07556543339, "toy": 15.383720930199999, "lemmat": 1587.6, "know": 2.59327017315, "bil": 588.0, "their": 1.01547908405, "equal": 2.542193755, "noun": 30.068181818200003, "sebastian": 26.8175675676, "john": 1.35553278689, "express": 1.9120799710900003, "markup": 269.084745763, "strip": 5.50485436893, "million": 1.7279059643, "chunk": 81.0, "lengthi": 12.165517241400002, "jame": 1.9313868613099998, "hundr": 2.4698195395099996, "end": 1.10680423871, "given": 1.35426085473, "word": 1.7965372864099998, "interest": 1.60331246213, "depend": 2.2411067193700003, "will": 1.22481098596, "specif": 1.8719490626099997, "open": 1.24556723678, "confus": 4.1451697127900005, "ran": 3.4915328788199997, "twentytwo": 1587.6, "next": 1.4950560316400001, "below": 2.25607503197, "perform": 1.5313977042500002, "but": 1.01632417899, "need": 1.4372623574099999, "our": 2.35758835759, "metadata": 211.68, "denois": 1587.6, "titl": 1.87261146497, "dinner": 12.7313552526, "point": 1.25990000794, "header": 83.55789473680001, "key": 2.28005170185, "level": 1.6544393497299998, "has": 1.0436497502, "have": 1.0148948411399998, "replac": 1.5602948402899999, "resta": 1221.23076923, "use": 1.0296387573799999, "mistak": 8.71350164654, "format": 2.53125, "toolkit": 189.0, "number": 1.10142916609, "field": 1.7790228597, "store": 3.44680851064, "not": 1.01567398119, "reserv": 3.24198488871, "lem": 196.0, "respect": 1.6443293630200002, "wrong": 5.478260869570001, "implement": 3.57648118946, "anym": 1587.6, "breakdown": 15.0483412322, "indiana": 10.6837146703, "shown": 2.76923076923, "stuff": 23.3127753304, "function": 2.495441685, "franch": 1323.0, "anoth": 1.13643521832, "instal": 3.78721374046, "case": 1.48498737256, "let": 3.48616600791, "scienc": 2.31969608416, "kdnugget": 1587.6, "mostus": 1587.6, "away": 1.85142857143, "stem": 7.453521126760001, "ord": 143.027027027, "practic": 1.70434782609, "simpl": 3.3981164383599998, "normal": 2.61075481006, "provid": 1.21552714187, "sort": 5.188235294119999, "got": 3.61969904241, "order": 1.24625166811, "such": 1.06151377374, "sever": 1.07241286139, "sentenc": 5.84536082474, "talk": 3.0303493033, "remedi": 18.0614334471, "lot": 4.40877534018, "some": 1.04036697248, "beyond": 2.54586273252, "small": 1.3594793629, "should": 1.6643254009900001, "put": 1.65806788512, "step": 2.8279301745599996, "result": 1.14611608432, "document": 2.5409731114, "univ": 32.869565217399995, "inflect": 64.2753036437, "languag": 2.29488291414, "futur": 1.8577112099200002, "ani": 1.13383802314, "two": 1.01379310345, "from": 1.00056721497, "ecosystem": 26.111842105300003, "verifi": 14.2258064516, "allow": 1.2716059271100002, "work": 1.11520089913, "num": 1.00031504001, "els": 5.44444444444, "favorit": 8.116564417180001, "upcom": 13.853403141400001, "movi": 4.00403530895, "for": 1.00031504001, "brac": 453.6, "cur": 98.0, "paragraph": 20.6181818182, "with": 1.0011982089899998, "stop": 2.1783754116400003, "assum": 2.9575260804799997, "are": 1.02990593578, "split": 3.4709226060300002, "marvel": 20.0201765448, "preprocess": 1221.23076923, "sent": 2.32683570277, "gre": 793.8, "nicol\u00e1": 1587.6, "veri": 1.25880114177, "extract": 7.703056768560001, "fut": 756.0, "pars": 145.651376147, "textspecif": 1587.6, "later": 1.08650424309, "imagin": 6.598503740650001, "general": 1.1218202374200001, "remov": 2.0058117498400003, "singular": 16.8, "doe": 1.70581282905, "foot": 5.65384615385, "place": 1.1004366812200002, "great": 1.26592775696, "goal": 3.28152128979, "framework": 8.200413223139998, "comment": 3.05954904606, "oppos": 2.51282051282, "encourag": 2.7975330396499998, "into": 1.01502461479, "prior": 2.17807655371, "unprocess": 378.0, "download": 14.6457564576, "requir": 1.52844902282, "meant": 3.20597738288, "what": 1.25343439128, "rememb": 4.88793103448, "restaur": 7.2926045016100005, "jeronimo": 1134.0, "relationship": 2.39132399458, "j\u00e9ronimo": 1587.6, "flexibl": 9.68639414277, "help": 1.39962972759, "larg": 1.18574949585, "ordin": 15.3391304348, "jon": 10.4173228346, "line": 1.4182597820299998, "doubl": 2.9713644020200003, "taskspecif": 1587.6, "base": 1.14628158845, "natur": 1.5392670157100001, "nicola": 15.5799803729, "who": 1.06279287723, "univers": 1.24889867841, "bracket": 32.466257668699996, "post": 2.23826307627, "here": 2.42307692308, "lower": 2.10055570257, "larger": 2.2407904022599996, "littl": 1.5499365420299998, "punctuat": 42.1114058355, "upper": 3.41052631579, "clean": 6.86975335353, "much": 1.1942229577299999, "about": 1.06486015159, "norm": 11.299644128099999, "articl": 2.01805008262, "which": 1.005191845, "distinct": 2.2836593786000003, "context": 4.25972632144, "just": 1.33580143037, "war": 1.41320989852, "fuzzi": 113.4, "billi": 9.0823798627, "she": 2.16, "standard": 1.8915763135900003, "statement": 3.42228928648, "segment": 7.55640171347, "defin": 2.72830383227, "sole": 4.04175152749, "term": 1.39520168732, "jam": 15.4887804878, "pot": 21.2530120482, "tool": 4.99716713881, "start": 1.26673581744, "run": 1.55692850838, "them": 1.09876115994, "verb": 29.291512915100004, "insight": 11.8037174721, "they": 1.03017325287, "seri": 1.46511627907, "cinemat": 41.669291338600004, "explain": 2.60049140049, "singl": 1.60948905109, "expand": 2.2260235558000003, "whi": 3.2566153846200003, "indian": 3.5046357615900003, "mani": 1.04426757877, "stor": 1587.6, "string": 8.37783641161, "the": 1.0, "anymor": 35.1238938053, "indefinit": 17.0160771704, "mine": 4.875921375919999, "taskindepend": 1587.6, "anyth": 4.58843930636, "there": 1.04091266719, "blur": 29.7303370787, "sebast": 1134.0, "imposs": 4.96125, "spend": 4.15928739848, "loos": 7.065420560750001, "lexic": 74.1869158879, "hous": 1.4624170965399999, "take": 1.13961668222, "follow": 1.04640126549, "now": 1.160780873, "further": 1.3618116315, "name": 1.10211732037, "brace": 36.0, "than": 1.03278688525, "basic": 2.7301805675, "play": 1.46390041494, "goe": 4.251740760580001, "wel": 481.09090909099996, "also": 1.01476510067, "stil": 1587.6, "everyth": 4.81967213115, "mean": 1.44906900329, "textual": 41.4516971279, "fulli": 2.79015817223, "task": 3.88641370869, "major": 1.14852058164, "accomplish": 5.17302052786, "may": 1.05201775893, "mayo": 49.7680250784, "fortyf": 1587.6, "tag": 19.7462686567, "thousand": 2.4767550702000003, "tot": 162.0, "previous": 1.42846859816, "convert": 3.2740771293099997, "act": 1.4318181818200002, "sixtyseven": 1587.6, "analysi": 3.47852760736, "get": 1.78562591385, "piec": 3.24132298898, "actual": 1.87482286254, "token": 33.7070063694, "easier": 7.84, "uniform": 5.7231434751300005, "appropri": 4.31413043478, "plural": 14.2898289829, "other": 1.00992366412, "refer": 1.30024570025, "benefici": 18.269275028800003, "one": 1.00627495722, "prefer": 3.0216977540900003, "see": 1.27242125511, "andor": 690.260869565, "tomorrow": 21.3674293405, "bestknown": 1587.6, "mandatori": 14.5785123967, "still": 1.1866357724799999, "like": 1.14918566775, "morn": 4.15819800943, "collect": 1.64109985528, "file": 3.7710213776699995, "beautifulsoup": 1587.6, "process": 1.69524826482, "yet": 2.1258703802900003, "includ": 1.0190641247799999, "structur": 2.0580762250499998, "straightforward": 27.7552447552, "necessari": 2.8421052631599997, "this": 1.00379362671, "transfer": 2.72549356223, "sampl": 7.23280182232, "time": 1.01127460348, "part": 1.04330682789, "back": 1.26070038911, "runner": 16.0363636364, "smaller": 2.59369384088, "contract": 3.0165304959099997, "differ": 1.23654490225, "same": 1.11857958148, "valuabl": 7.46754468485, "most": 1.02096463023, "between": 1.03453668708, "return": 1.39532431007, "obvious": 6.44841592201, "all": 1.01146788991, "reason": 1.72340425532, "someth": 3.28152128979, "footer": 793.8, "expans": 3.4770039421800005, "longer": 2.02319357716, "that": 1.00398406375, "pair": 4.36873968079, "walkthrough": 793.8, "call": 1.0676529926, "regular": 2.09418282548, "more": 1.0171706817, "and": 1.00006299213, "list": 1.36321483771, "discuss": 2.19676214197, "these": 1.07415426252, "prepar": 2.43012398592, "jone": 4.97368421053, "artifici": 8.31639601886, "could": 1.2043695949, "librari": 2.68266306185, "diff": 756.0, "close": 1.2848818387799998, "serv": 1.4668760972, "can": 1.17626139142, "describ": 1.47027227264, "make": 1.0762660158600001, "each": 1.18974820144, "speech": 3.8227787141800005, "find": 1.7294117647099998, "common": 1.4025974025999999, "din": 65.8755186722, "data": 3.37643555934, "cinem": 1587.6, "sinc": 1.08368600683, "sometim": 1.7126213592200001, "bold": 13.341176470599999, "reflect": 2.3443591258099996, "text": 3.12827586207, "insid": 2.7396031061299997, "italic": 396.9, "import": 1.3401992233700002, "first": 1.00761614623, "proceed": 3.4333910034599997, "tutus": 1587.6, "generat": 2.05275407292, "while": 1.0441988950299999, "argu": 2.67768595041, "exclus": 3.40906162766, "easili": 3.6938110749199997, "often": 1.29452054795, "inde": 4.43092380687, "pipelin": 32.1376518219, "star": 2.4450947173900004}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Text Data Preprocessing: A Walkthrough in Python</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Text Data Preprocessing: A Walkthrough in Python Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/new-poll-where-applied-analytics-data-science.html\" rel=\"prev\" title=\"New KDnuggets Poll: Where did you apply Analytics, Data Science, Machine Learning methods in 2017?\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/deep-learning-frameworks.html\" rel=\"next\" title=\"Comparing Deep Learning Frameworks: A Rosetta Stone Approach\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=79318\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-79318 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 26-Mar, 2018  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/index.html\">Mar</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/tutorials.html\">Tutorials, Overviews</a> \u00bb Text Data Preprocessing: A Walkthrough in Python (\u00a0<a href=\"/2018/n13.html\">18:n13</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\"><img align=\"right\" alt=\"Gold Blog\" src=\"/images/tkb-1803-g.png\" width=\"94\"/>Text Data Preprocessing: A Walkthrough in Python</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/03/new-poll-where-applied-analytics-data-science.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/03/deep-learning-frameworks.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/data-preparation\" rel=\"tag\">Data Preparation</a>, <a href=\"https://www.kdnuggets.com/tag/data-preprocessing\" rel=\"tag\">Data Preprocessing</a>, <a href=\"https://www.kdnuggets.com/tag/nlp\" rel=\"tag\">NLP</a>, <a href=\"https://www.kdnuggets.com/tag/python\" rel=\"tag\">Python</a>, <a href=\"https://www.kdnuggets.com/tag/text-analytics\" rel=\"tag\">Text Analytics</a>, <a href=\"https://www.kdnuggets.com/tag/text-mining\" rel=\"tag\">Text Mining</a></div>\n<br/>\n<p class=\"excerpt\">\n     This post will serve as a practical walkthrough of a text data preprocessing task using some common Python tools.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/matt-mayo\" rel=\"author\" title=\"Posts by Matthew Mayo\">Matthew Mayo</a>, KDnuggets.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p>In a pair of previous posts, we first discussed a <a href=\"/2017/11/framework-approaching-textual-data-tasks.html\" rel=\"noopener\" target=\"_blank\">framework for approaching textual data science tasks</a>, and followed that up with a discussion on a <a href=\"/2017/12/general-approach-preprocessing-text-data.html\" rel=\"noopener\" target=\"_blank\">general approach to preprocessing text data</a>. This post will serve as a practical walkthrough of a text data preprocessing task using some common Python tools.</p>\n<p><center><img alt=\"Data preprocessing\" src=\"/wp-content/uploads/text-data-task-framework-preprocessing.png\" width=\"99%\"><br>\n<font size=\"-1\">Preprocessing, in the context of the textual data science framework.</font></br></img></center></p>\n<p>Our goal is to go from what we will describe as a chunk of text (not to be confused with text chunking), a lengthy, unprocessed single string, and end up with a list (or several lists) of cleaned tokens that would be useful for further text mining and/or natural language processing tasks.</p>\n<p>First we start with our imports.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/4c42d7ad13de9c2bdc340750a75db8c1.js\"></script></p>\n<p>Beyond the standard Python libraries, we are also using the following:</p>\n<ul>\n<li><a href=\"http://www.nltk.org/\" rel=\"noopener\" target=\"_blank\">NLTK</a> - The Natural Language ToolKit is one of the best-known and most-used NLP libraries in the Python ecosystem, useful for all sorts of tasks from tokenization, to stemming, to part of speech tagging, and beyond\n<li><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" rel=\"noopener\" target=\"_blank\">BeautifulSoup</a> - BeautifulSoup is a useful library for extracting data from HTML and XML documents\n<li><a href=\"https://pypi.python.org/pypi/inflect\" rel=\"noopener\" target=\"_blank\">Inflect</a> - This is a simple library for accomplishing the natural language related tasks of generating plurals, singular nouns, ordinals, and indefinite articles, and (of most interest to us) converting numbers to words\n<li><a href=\"https://github.com/kootenpv/contractions\" rel=\"noopener\" target=\"_blank\">Contractions</a> - Another simple library, solely for expanding contractions\n</li></li></li></li></ul>\n<p>If you have NLTK installed, yet require the download of its any additional data, <a href=\"https://www.nltk.org/data.html\" rel=\"noopener\" target=\"_blank\">see here</a>.</p>\n<p>We need some sample text. We'll start with something very small and artificial in order to easily see the results of what we are doing step by step.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/ad2abd5a3527f75540c443f31b46c39d.js\"></script></p>\n<p>A toy dataset indeed, but make no mistake; the steps we are taking here to preprocessing this data are fully transferable.</p>\n<p><center><img alt=\"Preprocessing framework\" src=\"/wp-content/uploads/text-preprocessing-framework-2.png\" width=\"60%\"/><br>\n<font size=\"-1\">The text data preprocessing framework.</font></br></center></p>\n<p>\u00a0</p>\n<h3>Noise Removal</h3>\n<p>\u00a0<br>\nLet's loosely define <b>noise removal</b> as text-specific normalization tasks which often take place prior to tokenization. I would argue that, while the other 2 major steps of the preprocessing framework (tokenization and normalization) are basically task-independent, noise removal is much more task-specific.</br></p>\n<p>Sample noise removal tasks could include:</p>\n<ul>\n<li>removing text file headers, footers\n<li>removing HTML, XML, etc. markup and metadata\n<li>extracting valuable data from other formats, such as JSON\n</li></li></li></ul>\n<p>As you can imagine, the boundary between noise removal and data collection and assembly, on the one hand, is a fuzzy one, while the line between noise removal and normalization is blurred on the other. Given its close relationship with specific texts and their collection and assembly, many denoising tasks, such as parsing a JSON structure, would obviously need to be implemented prior to tokenization.</p>\n<p>In our data preprocessing pipeline, we will strip away HTML markup with the help of the BeautifulSoup library, and use regular expressions to remove open and close double brackets and anything in between them (we assume this is necessary based on our sample text).</p>\n<p><script src=\"https://gist.github.com/mmmayo13/a14fad3d4a3571bf91552146ea8cea6d.js\"></script></p>\n<p>While not mandatory to do at this stage prior to tokenization (you'll find that this statement is the norm for the relatively flexible ordering of text data preprocessing tasks), replacing contractions with their expansions can be beneficial at this point, since our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\" It's not impossible to remedy this tokenization at a later stage, but doing so prior makes it easier and more straightforward.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/d7458bfab7a2c10bec9a5199f6711d1a.js\"></script></p>\n<p>And here is the result of de-noising on our sample text.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Title Goes Here\r\nBolded Text\r\nItalicized Text\r\n\r\nBut this will still be here!\r\n\r\nI run. He ran. She is running. Will they stop running?\r\n\r\nI talked. She was talking. They talked to them about running. Who ran to the talking runner?\r\n\r\n\r\n\r\n\u00a1Sebasti\u00e1n, Nicol\u00e1s, Alejandro and J\u00e9ronimo are going to the store tomorrow morning!\r\n\r\nsomething... is! wrong() with.,; this :: sentence.\r\n\r\nI cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?\r\n\r\nMy favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\r\n\r\ndo not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\r\n\r\n\r\n\r\nJohn: \"Well, well, well.\"\r\nJames: \"There, there. There, there.\"\r\n\r\n  \r\n\r\nThere are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\r\nI have to go get 2 tutus from 2 different stores, too.\r\n\r\n22    45   1067   445\r\n\r\n{{Here is some stuff inside of double curly braces.}}\r\n{Here is more stuff in single curly braces.}</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>\u00a0</p>\n<h3>Tokenization</h3>\n<p>\u00a0<br/>\nTokenization is a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized. Tokenization is also referred to as text segmentation or lexical analysis. Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words.</p>\n<p>For our task, we will tokenize our sample text into a list of words. This is done using NTLK's <code>word_tokenize()</code> function.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/8fe5d0f3f314e22a0364d754f2257678.js\"></script></p>\n<p>And here are our word tokens:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'But', 'this', 'will', 'still',\r\n'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', \r\n'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', \r\n'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', '\u00a1Sebasti\u00e1n', ',', \r\n'Nicol\u00e1s', ',', 'Alejandro', 'and', 'J\u00e9ronimo', 'are', 'going', 'tot', 'he', 'store', 'tomorrow', \r\n'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', \r\n'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', \r\n'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', \r\n'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Star', 'Wars', ';', 'Marvel', \r\n'Cinematic', 'Universe', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'do', 'not', \r\n'do', 'it', '...', '.', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', \r\n'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', \r\n':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', \r\n'There', ',', 'there', '.', \"''\", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', \r\n'.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', \r\n'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', \r\n'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', \r\n'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', \r\n'.', '}']</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>\u00a0</p>\n<h3>Normalization</h3>\n<p>\u00a0<br/>\nNormalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.</p>\n<p>Normalizing text can mean performing a number of tasks, but for our framework we will approach normalization in 3 distinct steps: (1) stemming, (2) lemmatization, and (3) everything else. For specifics on what these distinct steps may be, <a href=\"/2017/12/general-approach-preprocessing-text-data.html\" rel=\"noopener\" target=\"_blank\">see this post</a>.</p>\n<p>Remember, after tokenization, we are no longer working at a text level, but now at a word level. Our normalization functions, shown below, reflect this. Function names and comments should provide the necessary insight into what each does.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/07252b4eb27e5495b6032888b38e5333.js\"></script></p>\n<p>After calling the normalization function:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>['title', 'goes', 'bolded', 'text', 'italicized', 'text', 'still', 'run', 'ran', 'running', 'stop', \r\n'running', 'talked', 'talking', 'talked', 'running', 'ran', 'talking', 'runner', 'sebastian', 'nicolas', \r\n'alejandro', 'jeronimo', 'going', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', \r\n'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchises', 'order', \r\n'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'wars', 'back', 'future', 'harry', \r\n'potter', 'billy', 'know', 'great', 'little', 'house', 'got', 'john', 'well', 'well', 'well', 'james', \r\n'lot', 'reasons', 'one hundred and one', 'reasons', 'one million', 'reasons', 'actually', 'go', 'get', \r\n'two', 'tutus', 'two', 'different', 'stores', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', \r\n'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'braces', 'stuff', 'single', \r\n'curly', 'braces']</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Calling the stemming and lemming functions are done as below:</p>\n<p><script src=\"https://gist.github.com/mmmayo13/aa19c8fd5f4e85cdbb06ddde15de70e7.js\"></script></p>\n<p>This results in a return of 2 new lists: one of stemmed tokens, and another of lemmatized tokens with respect to verbs. Depending on your upcoming NLP task or preference, one of these may be more appropriate than the other. See here for a <a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" rel=\"noopener\" target=\"_blank\">discussion on lemmatization vs. stemming</a>.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Stemmed:\r\n ['titl', 'goe', 'bold', 'text', 'it', 'text', 'stil', 'run', 'ran', 'run', 'stop', 'run', 'talk', \r\n'talk', 'talk', 'run', 'ran', 'talk', 'run', 'sebast', 'nicola', 'alejandro', 'jeronimo', 'going', \r\n'stor', 'tomorrow', 'morn', 'someth', 'wrong', 'sent', 'anym', 'know', 'could', 'din', 'resta', \r\n'favorit', 'movy', 'franch', 'ord', 'indian', 'jon', 'marvel', 'cinem', 'univers', 'star', 'war', 'back', \r\n'fut', 'harry', 'pot', 'bil', 'know', 'gre', 'littl', 'hous', 'got', 'john', 'wel', 'wel', 'wel', 'jam', \r\n'lot', 'reason', 'one hundred and on', 'reason', 'one million', 'reason', 'act', 'go', 'get', 'two', \r\n'tut', 'two', 'diff', 'stor', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred \r\nand forty-five', 'stuff', 'insid', 'doubl', 'cur', 'brac', 'stuff', 'singl', 'cur', 'brac']\r\n\r\nLemmatized:\r\n ['title', 'go', 'bolded', 'text', 'italicize', 'text', 'still', 'run', 'run', 'run', 'stop', 'run', \r\n'talk', 'talk', 'talk', 'run', 'run', 'talk', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', \r\n'go', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', \r\n'dinner', 'restaurant', 'favorite', 'movie', 'franchise', 'order', 'indiana', 'jones', 'marvel', \r\n'cinematic', 'universe', 'star', 'war', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', \r\n'little', 'house', 'get', 'john', 'well', 'well', 'well', 'jam', 'lot', 'reason', 'one hundred and one', \r\n'reason', 'one million', 'reason', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'store', \r\n'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', \r\n'inside', 'double', 'curly', 'brace', 'stuff', 'single', 'curly', 'brace']\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>And there you have a walkthrough of a simple text data preprocessing process using Python on a sample piece of text. I would encourage you to perform these tasks on some additional texts to verify the results. We will use this same process to clean the text data for our next task, in which we will undertake some actual NLP task, as opposed to spending time preparing our data for such an actual task.</p>\n<p>\u00a0<br/>\n<b>Related</b>:</p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/12/general-approach-preprocessing-text-data.html\">A General Approach to Preprocessing Text Data</a>\n<li><a href=\"/2017/11/framework-approaching-textual-data-tasks.html\">A Framework for Approaching Textual Data Science Tasks</a>\n<li><a href=\"/2017/02/natural-language-processing-key-terms-explained.html\">Natural Language Processing Key Terms, Explained</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/03/new-poll-where-applied-analytics-data-science.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/03/deep-learning-frameworks.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/index.html\">Mar</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/tutorials.html\">Tutorials, Overviews</a> \u00bb Text Data Preprocessing: A Walkthrough in Python (\u00a0<a href=\"/2018/n13.html\">18:n13</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556324214\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.722 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 20:16:54 -->\n<!-- Compression = gzip -->", "content_tokenized": ["matthew", "mayo", "kdnugget", "comment", "pair", "previous", "post", "first", "discuss", "framework", "for", "approach", "textual", "data", "scienc", "task", "and", "follow", "that", "with", "discuss", "general", "approach", "preprocess", "text", "data", "this", "post", "will", "serv", "practic", "walkthrough", "text", "data", "preprocess", "task", "use", "some", "common", "python", "tool", "preprocess", "the", "context", "the", "textual", "data", "scienc", "framework", "our", "goal", "from", "what", "will", "describ", "chunk", "text", "not", "confus", "with", "text", "chunk", "lengthi", "unprocess", "singl", "string", "and", "end", "with", "list", "sever", "list", "clean", "token", "that", "would", "use", "for", "further", "text", "mine", "andor", "natur", "languag", "process", "task", "first", "start", "with", "our", "import", "beyond", "the", "standard", "python", "librari", "are", "also", "use", "the", "follow", "the", "natur", "languag", "toolkit", "one", "the", "bestknown", "and", "mostus", "librari", "the", "python", "ecosystem", "use", "for", "all", "sort", "task", "from", "token", "stem", "part", "speech", "tag", "and", "beyond", "beautifulsoup", "beautifulsoup", "use", "librari", "for", "extract", "data", "from", "and", "document", "inflect", "this", "simpl", "librari", "for", "accomplish", "the", "natur", "languag", "relat", "task", "generat", "plural", "singular", "noun", "ordin", "and", "indefinit", "articl", "and", "most", "interest", "convert", "number", "word", "contract", "anoth", "simpl", "librari", "sole", "for", "expand", "contract", "have", "instal", "yet", "requir", "the", "download", "ani", "addit", "data", "see", "here", "need", "some", "sampl", "text", "start", "with", "someth", "veri", "small", "and", "artifici", "order", "easili", "see", "the", "result", "what", "are", "step", "step", "toy", "dataset", "inde", "but", "make", "mistak", "the", "step", "are", "take", "here", "preprocess", "this", "data", "are", "fulli", "transfer", "the", "text", "data", "preprocess", "framework", "nois", "remov", "let", "loos", "defin", "nois", "remov", "textspecif", "normal", "task", "which", "often", "take", "place", "prior", "token", "would", "argu", "that", "while", "the", "other", "num", "major", "step", "the", "preprocess", "framework", "token", "and", "normal", "are", "basic", "taskindepend", "nois", "remov", "much", "more", "taskspecif", "sampl", "nois", "remov", "task", "could", "includ", "remov", "text", "file", "header", "footer", "remov", "etc", "markup", "and", "metadata", "extract", "valuabl", "data", "from", "other", "format", "such", "can", "imagin", "the", "boundari", "between", "nois", "remov", "and", "data", "collect", "and", "assembl", "the", "one", "hand", "fuzzi", "one", "while", "the", "line", "between", "nois", "remov", "and", "normal", "blur", "the", "other", "given", "close", "relationship", "with", "specif", "text", "and", "their", "collect", "and", "assembl", "mani", "denois", "task", "such", "pars", "structur", "would", "obvious", "need", "implement", "prior", "token", "our", "data", "preprocess", "pipelin", "will", "strip", "away", "markup", "with", "the", "help", "the", "beautifulsoup", "librari", "and", "use", "regular", "express", "remov", "open", "and", "close", "doubl", "bracket", "and", "anyth", "between", "them", "assum", "this", "necessari", "base", "our", "sampl", "text", "while", "not", "mandatori", "this", "stage", "prior", "token", "find", "that", "this", "statement", "the", "norm", "for", "the", "relat", "flexibl", "order", "text", "data", "preprocess", "task", "replac", "contract", "with", "their", "expans", "can", "benefici", "this", "point", "sinc", "our", "word", "token", "will", "split", "word", "like", "into", "and", "not", "imposs", "remedi", "this", "token", "later", "stage", "but", "prior", "make", "easier", "and", "more", "straightforward", "and", "here", "the", "result", "denois", "our", "sampl", "text", "titl", "goe", "here", "bold", "text", "italic", "text", "but", "this", "will", "still", "here", "run", "ran", "she", "run", "will", "they", "stop", "run", "talk", "she", "talk", "they", "talk", "them", "about", "run", "who", "ran", "the", "talk", "runner", "\u00a1sebasti\u00e1n", "nicol\u00e1", "alejandro", "and", "j\u00e9ronimo", "are", "the", "store", "tomorrow", "morn", "someth", "with", "this", "sentenc", "can", "not", "this", "anymor", "not", "know", "them", "whi", "could", "not", "have", "dinner", "the", "restaur", "favorit", "movi", "franchis", "order", "indiana", "jone", "marvel", "cinemat", "univers", "star", "war", "back", "the", "futur", "harri", "potter", "not", "just", "not", "billi", "know", "what", "are", "this", "great", "littl", "hous", "have", "got", "here", "john", "well", "well", "well", "jame", "there", "there", "there", "there", "there", "are", "lot", "reason", "not", "this", "there", "are", "num", "reason", "not", "num", "reason", "actual", "have", "get", "num", "tutus", "from", "num", "differ", "store", "too", "num", "num", "num", "num", "here", "some", "stuff", "insid", "doubl", "cur", "brace", "here", "more", "stuff", "singl", "cur", "brace", "token", "token", "step", "which", "split", "longer", "string", "text", "into", "smaller", "piec", "token", "larger", "chunk", "text", "can", "token", "into", "sentenc", "sentenc", "can", "token", "into", "word", "etc", "further", "process", "general", "perform", "after", "piec", "text", "has", "been", "appropri", "token", "token", "also", "refer", "text", "segment", "lexic", "analysi", "sometim", "segment", "use", "refer", "the", "breakdown", "larg", "chunk", "text", "into", "piec", "larger", "than", "word", "paragraph", "sentenc", "while", "token", "reserv", "for", "the", "breakdown", "process", "which", "result", "exclus", "word", "for", "our", "task", "will", "token", "our", "sampl", "text", "into", "list", "word", "this", "done", "use", "function", "and", "here", "are", "our", "word", "token", "titl", "goe", "here", "bold", "text", "italic", "text", "but", "this", "will", "still", "here", "run", "ran", "she", "run", "will", "they", "stop", "run", "talk", "she", "talk", "they", "talk", "them", "about", "run", "who", "ran", "the", "talk", "runner", "\u00a1sebasti\u00e1n", "nicol\u00e1", "alejandro", "and", "j\u00e9ronimo", "are", "tot", "store", "tomorrow", "morn", "someth", "wrong", "with", "this", "sentenc", "can", "not", "this", "anymor", "not", "know", "them", "whi", "could", "not", "have", "dinner", "the", "restaur", "favorit", "movi", "franchis", "order", "indiana", "jone", "star", "war", "marvel", "cinemat", "univers", "back", "the", "futur", "harri", "potter", "not", "just", "not", "billi", "know", "what", "are", "this", "great", "littl", "hous", "have", "got", "here", "john", "well", "well", "well", "jame", "there", "there", "there", "there", "there", "are", "lot", "reason", "not", "this", "there", "are", "num", "reason", "not", "num", "reason", "actual", "have", "get", "num", "tutus", "from", "num", "differ", "store", "too", "num", "num", "num", "num", "here", "some", "stuff", "insid", "doubl", "cur", "brace", "here", "more", "stuff", "singl", "cur", "brace", "normal", "normal", "general", "refer", "seri", "relat", "task", "meant", "put", "all", "text", "level", "play", "field", "convert", "all", "text", "the", "same", "case", "upper", "lower", "remov", "punctuat", "convert", "number", "their", "word", "equival", "and", "normal", "put", "all", "word", "equal", "foot", "and", "allow", "process", "proceed", "uniform", "normal", "text", "can", "mean", "perform", "number", "task", "but", "for", "our", "framework", "will", "approach", "normal", "num", "distinct", "step", "num", "stem", "num", "lemmat", "and", "num", "everyth", "els", "for", "specif", "what", "these", "distinct", "step", "may", "see", "this", "post", "rememb", "after", "token", "are", "longer", "work", "text", "level", "but", "now", "word", "level", "our", "normal", "function", "shown", "below", "reflect", "this", "function", "name", "and", "comment", "should", "provid", "the", "necessari", "insight", "into", "what", "each", "doe", "after", "call", "the", "normal", "function", "titl", "goe", "bold", "text", "italic", "text", "still", "run", "ran", "run", "stop", "run", "talk", "talk", "talk", "run", "ran", "talk", "runner", "sebastian", "nicola", "alejandro", "jeronimo", "store", "tomorrow", "morn", "someth", "wrong", "sentenc", "anymor", "know", "could", "dinner", "restaur", "favorit", "movi", "franchis", "order", "indiana", "jone", "marvel", "cinemat", "univers", "star", "war", "back", "futur", "harri", "potter", "billi", "know", "great", "littl", "hous", "got", "john", "well", "well", "well", "jame", "lot", "reason", "one", "hundr", "and", "one", "reason", "one", "million", "reason", "actual", "get", "two", "tutus", "two", "differ", "store", "twentytwo", "fortyf", "one", "thousand", "and", "sixtyseven", "four", "hundr", "and", "fortyf", "stuff", "insid", "doubl", "cur", "brace", "stuff", "singl", "cur", "brace", "call", "the", "stem", "and", "lem", "function", "are", "done", "below", "this", "result", "return", "num", "new", "list", "one", "stem", "token", "and", "anoth", "lemmat", "token", "with", "respect", "verb", "depend", "upcom", "task", "prefer", "one", "these", "may", "more", "appropri", "than", "the", "other", "see", "here", "for", "discuss", "lemmat", "stem", "stem", "titl", "goe", "bold", "text", "text", "stil", "run", "ran", "run", "stop", "run", "talk", "talk", "talk", "run", "ran", "talk", "run", "sebast", "nicola", "alejandro", "jeronimo", "stor", "tomorrow", "morn", "someth", "wrong", "sent", "anym", "know", "could", "din", "resta", "favorit", "movi", "franch", "ord", "indian", "jon", "marvel", "cinem", "univ", "star", "war", "back", "fut", "harri", "pot", "bil", "know", "gre", "littl", "hous", "got", "john", "wel", "wel", "wel", "jam", "lot", "reason", "one", "hundr", "and", "reason", "one", "million", "reason", "act", "get", "two", "tut", "two", "diff", "stor", "twentytwo", "fortyf", "one", "thousand", "and", "sixtyseven", "four", "hundr", "and", "fortyf", "stuff", "insid", "doubl", "cur", "brac", "stuff", "singl", "cur", "brac", "lemmat", "titl", "bold", "text", "italic", "text", "still", "run", "run", "run", "stop", "run", "talk", "talk", "talk", "run", "run", "talk", "runner", "sebastian", "nicola", "alejandro", "jeronimo", "store", "tomorrow", "morn", "someth", "wrong", "sentenc", "anymor", "know", "could", "dinner", "restaur", "favorit", "movi", "franchis", "order", "indiana", "jone", "marvel", "cinemat", "univers", "star", "war", "back", "futur", "harri", "potter", "billi", "know", "great", "littl", "hous", "get", "john", "well", "well", "well", "jam", "lot", "reason", "one", "hundr", "and", "one", "reason", "one", "million", "reason", "actual", "get", "two", "tutus", "two", "differ", "store", "twentytwo", "fortyf", "one", "thousand", "and", "sixtyseven", "four", "hundr", "and", "fortyf", "stuff", "insid", "doubl", "cur", "brace", "stuff", "singl", "cur", "brace", "and", "there", "have", "walkthrough", "simpl", "text", "data", "preprocess", "process", "use", "python", "sampl", "piec", "text", "would", "encourag", "perform", "these", "task", "some", "addit", "text", "verifi", "the", "result", "will", "use", "this", "same", "process", "clean", "the", "text", "data", "for", "our", "next", "task", "which", "will", "undertak", "some", "actual", "task", "oppos", "spend", "time", "prepar", "our", "data", "for", "such", "actual", "task", "relat", "general", "approach", "preprocess", "text", "data", "framework", "for", "approach", "textual", "data", "scienc", "task", "natur", "languag", "process", "key", "term", "explain"], "timestamp_scraper": 1556363986.187549, "title": "Text Data Preprocessing: A Walkthrough in Python", "read_time": 534.3, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/matt-mayo\" rel=\"author\" title=\"Posts by Matthew Mayo\">Matthew Mayo</a>, KDnuggets.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p>In a pair of previous posts, we first discussed a <a href=\"/2017/11/framework-approaching-textual-data-tasks.html\" rel=\"noopener\" target=\"_blank\">framework for approaching textual data science tasks</a>, and followed that up with a discussion on a <a href=\"/2017/12/general-approach-preprocessing-text-data.html\" rel=\"noopener\" target=\"_blank\">general approach to preprocessing text data</a>. This post will serve as a practical walkthrough of a text data preprocessing task using some common Python tools.</p>\n<p><center><img alt=\"Data preprocessing\" src=\"/wp-content/uploads/text-data-task-framework-preprocessing.png\" width=\"99%\"><br>\n<font size=\"-1\">Preprocessing, in the context of the textual data science framework.</font></br></img></center></p>\n<p>Our goal is to go from what we will describe as a chunk of text (not to be confused with text chunking), a lengthy, unprocessed single string, and end up with a list (or several lists) of cleaned tokens that would be useful for further text mining and/or natural language processing tasks.</p>\n<p>First we start with our imports.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/4c42d7ad13de9c2bdc340750a75db8c1.js\"></script></p>\n<p>Beyond the standard Python libraries, we are also using the following:</p>\n<ul>\n<li><a href=\"http://www.nltk.org/\" rel=\"noopener\" target=\"_blank\">NLTK</a> - The Natural Language ToolKit is one of the best-known and most-used NLP libraries in the Python ecosystem, useful for all sorts of tasks from tokenization, to stemming, to part of speech tagging, and beyond\n<li><a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" rel=\"noopener\" target=\"_blank\">BeautifulSoup</a> - BeautifulSoup is a useful library for extracting data from HTML and XML documents\n<li><a href=\"https://pypi.python.org/pypi/inflect\" rel=\"noopener\" target=\"_blank\">Inflect</a> - This is a simple library for accomplishing the natural language related tasks of generating plurals, singular nouns, ordinals, and indefinite articles, and (of most interest to us) converting numbers to words\n<li><a href=\"https://github.com/kootenpv/contractions\" rel=\"noopener\" target=\"_blank\">Contractions</a> - Another simple library, solely for expanding contractions\n</li></li></li></li></ul>\n<p>If you have NLTK installed, yet require the download of its any additional data, <a href=\"https://www.nltk.org/data.html\" rel=\"noopener\" target=\"_blank\">see here</a>.</p>\n<p>We need some sample text. We'll start with something very small and artificial in order to easily see the results of what we are doing step by step.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/ad2abd5a3527f75540c443f31b46c39d.js\"></script></p>\n<p>A toy dataset indeed, but make no mistake; the steps we are taking here to preprocessing this data are fully transferable.</p>\n<p><center><img alt=\"Preprocessing framework\" src=\"/wp-content/uploads/text-preprocessing-framework-2.png\" width=\"60%\"/><br>\n<font size=\"-1\">The text data preprocessing framework.</font></br></center></p>\n<p>\u00a0</p>\n<h3>Noise Removal</h3>\n<p>\u00a0<br>\nLet's loosely define <b>noise removal</b> as text-specific normalization tasks which often take place prior to tokenization. I would argue that, while the other 2 major steps of the preprocessing framework (tokenization and normalization) are basically task-independent, noise removal is much more task-specific.</br></p>\n<p>Sample noise removal tasks could include:</p>\n<ul>\n<li>removing text file headers, footers\n<li>removing HTML, XML, etc. markup and metadata\n<li>extracting valuable data from other formats, such as JSON\n</li></li></li></ul>\n<p>As you can imagine, the boundary between noise removal and data collection and assembly, on the one hand, is a fuzzy one, while the line between noise removal and normalization is blurred on the other. Given its close relationship with specific texts and their collection and assembly, many denoising tasks, such as parsing a JSON structure, would obviously need to be implemented prior to tokenization.</p>\n<p>In our data preprocessing pipeline, we will strip away HTML markup with the help of the BeautifulSoup library, and use regular expressions to remove open and close double brackets and anything in between them (we assume this is necessary based on our sample text).</p>\n<p><script src=\"https://gist.github.com/mmmayo13/a14fad3d4a3571bf91552146ea8cea6d.js\"></script></p>\n<p>While not mandatory to do at this stage prior to tokenization (you'll find that this statement is the norm for the relatively flexible ordering of text data preprocessing tasks), replacing contractions with their expansions can be beneficial at this point, since our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\" It's not impossible to remedy this tokenization at a later stage, but doing so prior makes it easier and more straightforward.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/d7458bfab7a2c10bec9a5199f6711d1a.js\"></script></p>\n<p>And here is the result of de-noising on our sample text.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Title Goes Here\r\nBolded Text\r\nItalicized Text\r\n\r\nBut this will still be here!\r\n\r\nI run. He ran. She is running. Will they stop running?\r\n\r\nI talked. She was talking. They talked to them about running. Who ran to the talking runner?\r\n\r\n\r\n\r\n\u00a1Sebasti\u00e1n, Nicol\u00e1s, Alejandro and J\u00e9ronimo are going to the store tomorrow morning!\r\n\r\nsomething... is! wrong() with.,; this :: sentence.\r\n\r\nI cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?\r\n\r\nMy favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\r\n\r\ndo not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\r\n\r\n\r\n\r\nJohn: \"Well, well, well.\"\r\nJames: \"There, there. There, there.\"\r\n\r\n  \r\n\r\nThere are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\r\nI have to go get 2 tutus from 2 different stores, too.\r\n\r\n22    45   1067   445\r\n\r\n{{Here is some stuff inside of double curly braces.}}\r\n{Here is more stuff in single curly braces.}</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>\u00a0</p>\n<h3>Tokenization</h3>\n<p>\u00a0<br/>\nTokenization is a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized. Tokenization is also referred to as text segmentation or lexical analysis. Sometimes segmentation is used to refer to the breakdown of a large chunk of text into pieces larger than words (e.g. paragraphs or sentences), while tokenization is reserved for the breakdown process which results exclusively in words.</p>\n<p>For our task, we will tokenize our sample text into a list of words. This is done using NTLK's <code>word_tokenize()</code> function.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/8fe5d0f3f314e22a0364d754f2257678.js\"></script></p>\n<p>And here are our word tokens:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'But', 'this', 'will', 'still',\r\n'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', \r\n'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', \r\n'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', '\u00a1Sebasti\u00e1n', ',', \r\n'Nicol\u00e1s', ',', 'Alejandro', 'and', 'J\u00e9ronimo', 'are', 'going', 'tot', 'he', 'store', 'tomorrow', \r\n'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', \r\n'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', \r\n'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', \r\n'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Star', 'Wars', ';', 'Marvel', \r\n'Cinematic', 'Universe', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'do', 'not', \r\n'do', 'it', '...', '.', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', \r\n'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', \r\n':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', \r\n'There', ',', 'there', '.', \"''\", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', \r\n'.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', \r\n'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', \r\n'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', \r\n'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', \r\n'.', '}']</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>\u00a0</p>\n<h3>Normalization</h3>\n<p>\u00a0<br/>\nNormalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.</p>\n<p>Normalizing text can mean performing a number of tasks, but for our framework we will approach normalization in 3 distinct steps: (1) stemming, (2) lemmatization, and (3) everything else. For specifics on what these distinct steps may be, <a href=\"/2017/12/general-approach-preprocessing-text-data.html\" rel=\"noopener\" target=\"_blank\">see this post</a>.</p>\n<p>Remember, after tokenization, we are no longer working at a text level, but now at a word level. Our normalization functions, shown below, reflect this. Function names and comments should provide the necessary insight into what each does.</p>\n<p><script src=\"https://gist.github.com/mmmayo13/07252b4eb27e5495b6032888b38e5333.js\"></script></p>\n<p>After calling the normalization function:</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>['title', 'goes', 'bolded', 'text', 'italicized', 'text', 'still', 'run', 'ran', 'running', 'stop', \r\n'running', 'talked', 'talking', 'talked', 'running', 'ran', 'talking', 'runner', 'sebastian', 'nicolas', \r\n'alejandro', 'jeronimo', 'going', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', \r\n'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchises', 'order', \r\n'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'wars', 'back', 'future', 'harry', \r\n'potter', 'billy', 'know', 'great', 'little', 'house', 'got', 'john', 'well', 'well', 'well', 'james', \r\n'lot', 'reasons', 'one hundred and one', 'reasons', 'one million', 'reasons', 'actually', 'go', 'get', \r\n'two', 'tutus', 'two', 'different', 'stores', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', \r\n'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'braces', 'stuff', 'single', \r\n'curly', 'braces']</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Calling the stemming and lemming functions are done as below:</p>\n<p><script src=\"https://gist.github.com/mmmayo13/aa19c8fd5f4e85cdbb06ddde15de70e7.js\"></script></p>\n<p>This results in a return of 2 new lists: one of stemmed tokens, and another of lemmatized tokens with respect to verbs. Depending on your upcoming NLP task or preference, one of these may be more appropriate than the other. See here for a <a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" rel=\"noopener\" target=\"_blank\">discussion on lemmatization vs. stemming</a>.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Stemmed:\r\n ['titl', 'goe', 'bold', 'text', 'it', 'text', 'stil', 'run', 'ran', 'run', 'stop', 'run', 'talk', \r\n'talk', 'talk', 'run', 'ran', 'talk', 'run', 'sebast', 'nicola', 'alejandro', 'jeronimo', 'going', \r\n'stor', 'tomorrow', 'morn', 'someth', 'wrong', 'sent', 'anym', 'know', 'could', 'din', 'resta', \r\n'favorit', 'movy', 'franch', 'ord', 'indian', 'jon', 'marvel', 'cinem', 'univers', 'star', 'war', 'back', \r\n'fut', 'harry', 'pot', 'bil', 'know', 'gre', 'littl', 'hous', 'got', 'john', 'wel', 'wel', 'wel', 'jam', \r\n'lot', 'reason', 'one hundred and on', 'reason', 'one million', 'reason', 'act', 'go', 'get', 'two', \r\n'tut', 'two', 'diff', 'stor', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred \r\nand forty-five', 'stuff', 'insid', 'doubl', 'cur', 'brac', 'stuff', 'singl', 'cur', 'brac']\r\n\r\nLemmatized:\r\n ['title', 'go', 'bolded', 'text', 'italicize', 'text', 'still', 'run', 'run', 'run', 'stop', 'run', \r\n'talk', 'talk', 'talk', 'run', 'run', 'talk', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', \r\n'go', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', \r\n'dinner', 'restaurant', 'favorite', 'movie', 'franchise', 'order', 'indiana', 'jones', 'marvel', \r\n'cinematic', 'universe', 'star', 'war', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', \r\n'little', 'house', 'get', 'john', 'well', 'well', 'well', 'jam', 'lot', 'reason', 'one hundred and one', \r\n'reason', 'one million', 'reason', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'store', \r\n'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', \r\n'inside', 'double', 'curly', 'brace', 'stuff', 'single', 'curly', 'brace']\r\n</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>And there you have a walkthrough of a simple text data preprocessing process using Python on a sample piece of text. I would encourage you to perform these tasks on some additional texts to verify the results. We will use this same process to clean the text data for our next task, in which we will undertake some actual NLP task, as opposed to spending time preparing our data for such an actual task.</p>\n<p>\u00a0<br/>\n<b>Related</b>:</p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/12/general-approach-preprocessing-text-data.html\">A General Approach to Preprocessing Text Data</a>\n<li><a href=\"/2017/11/framework-approaching-textual-data-tasks.html\">A Framework for Approaching Textual Data Science Tasks</a>\n<li><a href=\"/2017/02/natural-language-processing-key-terms-explained.html\">Natural Language Processing Key Terms, Explained</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}