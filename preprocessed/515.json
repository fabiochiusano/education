{"content": "By Brandon Rohrer , iRobot. comments Check out the full course content for How Optimization Works , including video, slides, and code. Optimization is a fancy word for \"finding the best way.\" We can see how it works if we take a closer look at drinking tea. There is a best temperature for tea. If your tea is too hot, it will scald your tongue and you won't be able to taste anything for three days. If it\u2019s lukewarm, it\u2019s entirely unsatisfying. There is a sweet spot in the middle where it is comfortably hot, warming you from the inside out all the way down your throat and radiating through your belly. This is the ideal temperature for tea. This happy medium is what we try to find in optimization. That\u2019s what Goldilocks was looking for when she tried Papa Bear's bed and found it too hard, tried Mama Bear's bed and found it too soft, then tried Baby Bear's bed and found it to be just right. Finding how to get things just right turns out to be a very common problem. Mathematicians and computer scientist love it because it\u2019s very specific and well formulated. You know when you\u2019ve got it right, and you can compare your solution against others to see who got it right faster. When a computer scientist tries to find the right temperature for tea, the first thing they do is flip the problem upside down. Instead of trying to maximize tea drinking enjoyment, they try to minimize suffering while drinking tea. The result is the same, and the math works out in the same way. It's not that all computer scientists are pessimists, it's just that most optimization problems are naturally described in terms of costs - money, time, resources - rather than benefits. In math it's convenient to make all your problems look the same before you work out a solution, so that you can just solve it the one time. In machine learning, this cost is often called an error function, because error is the undesirable thing, the suffering, that is being minimized. It can also be called a cost function, a loss function, or an energy function. They all mean pretty much the same thing. \u00a0 Exhaustive search \u00a0 There are a handful of ways to go about finding the best temperature for serving tea. The most obvious is just to look at the curve and pick the lowest point. Unfortunately, we don't actually know what the curve is when we start out. That is implicit in the optimization problem. But we can make use of our original idea and just measure the curve. We can prepare a cup of tea at a given temperature, serve it, and ask our unwitting test subject how they enjoyed it. Then we can repeat this process for every temperature across the whole range we care about. By the time we're done with this, we do know what the whole curve looks like, and then we can just pick temperature for which our tea drinker reported the most enjoyment, that is, the least suffering. This way of finding the best tea temperature is called exhaustive search. It is straightforward and effective, but may take a while. If our time is limited, it's worth it to check out a few other methods. \u00a0 Gradient descent \u00a0 If you imagine that our tea-suffering curve is actually a physical bowl, then we could easily find the bottom by dropping a marble in and letting it roll until it stops. This is the intuition behind gradient descent - literally \"going downhill\". To use gradient descent we start at an arbitrary temperature. Before beginning, we don't know anything about our curve, so we make a random guess. We brew a cup of tea at that temperature and see how well our tea drinker likes it. From there, the next trick is to figure out which direction is downhill and which is up. To figure this out, we choose a direction, and choose a new temperature a very small distance away. Let's say we choose a temperature to the left. Then we brew up another cup of tea at this slightly cooler temperature and see whether or not it is better than the first. We discover that it is actually inferior. Now we know that \"downhill\" is to the right - that we need to make our next cup warmer to make it better. We take a larger step in the direction of warmer tea, brew up a new cup, and start the process over again. We repeat this until we get to the very best temperature for tea. The steeper the slope, the larger the step we can take. The shallower the slope, the smaller the step. We will know we are all done when we take a small step away and get the exact same level of enjoyment from our tea drinker. This can only happen at the bottom of the bowl, where it is flat and there is no downhill. There are lots of gradient descent methods. Most of them are clever ways to measure the slope as efficiently as possible and to get to the bottom of the bowl in as few steps as possible - to brew as few cups of tea as we can get away with. They use different tricks to avoid completely calculating the slope or to choose a step size that is as large as can be gotten away with, but the underlying intuition is the same. \u00a0 Including curvature \u00a0 One of the tricks to find the bottom in the bowl in fewer steps is to use not just slope, but also curvature, when deciding how big of a step to take. As the marble starts to roll down the side of the bowl, is the slope getting steeper? If so, then the bottom is probably still far away. Take a big step. Or is the slope getting shallower and starting to bottom out? If so, the bottom is probably getting closer. Take smaller steps now. Curvature, this slope-of-the-slope or Hessian, to give it its rightful name, can be very helpful if you are trying to take as few steps as possible, however it can also be much more expensive to compute. This is a trade-off that comes up a lot in optimization. We end up choosing between the number of steps we have to take and how hard it is to compute where the next step should be. \u00a0 How gradient descent can break \u00a0 Like a lot of math problems, the more assumptions you\u2019re able to make, the better the solution you can come up with. Unfortunately, when working with real data, those assumptions don\u2019t always apply. There are a lot of ways that this drop-a-marble approach can fail. If there is more than one valley for a marble to roll into, we might miss the deepest one. Each of these little bowls is called a local minimum. We are interested in finding the global minimum, the deepest of all the bowls. Imagine that we are testing our tea temperatures on a hot day. It may be that once tea becomes cold enough, it makes a great iced tea, which is even more popular. We would never find that out by gradient descent alone. If the error function is not smooth, there are lots of places a marble could get stuck. This could happen if our tea drinkers' enjoyment was heavily impacted by passing trains. The periodic occurrence of trains could introduce a wiggle into our data. If the error function you are trying to optimize makes discrete jumps, that presents a challenge. Marbles don't roll down stairs well. This could happen if our tea drinkers have to rate their enjoyment on a 10-point scale. If the error function is mostly a plateau, but has a bottom that is narrow and deep, then the marble is unlikely to find it. Perhaps our tea drinkers absolutely despise all tea that is anything but perfect. All of these occur in real machine learning optimization problems. \u00a0 Robust methods \u00a0 If we suspect that our tea satisfaction curve has any of these tricky characteristics, we can always fall back to exhaustive search. Unfortunately, exhaustive search takes an extremely long time for a lot of problems. Luckily for us, there is a middle ground. There is a set of methods that is tougher than gradient descent. They go by names like genetic algorithms, evolutionary algorithms, and simulated annealing. They take longer to compute than gradient descent, and they take more steps, but they don't break nearly so easily. Each has its own quirks, but one characteristic that most of them share is a randomness to their steps and jumps. This helps them discover the deepest valleys of the error function, even when they are difficult to find. Optimization algorithms that rely gradient descent are like Formula One race cars. They are extremely fast and efficient, but require a very well behaved track (error function) to work well. A poorly-placed speed bump could wreck it. The more robust methods are like four-wheel-drive pickup trucks. They don't go nearly as fast, but they can handle a lot more variability in the terrain. And exhaustive search is like traveling on foot. You can get absolutely anywhere, but it may take you really long time. They are each invaluable in different situations. \u00a0 Original . Reposted with permission. Related: Optimization 101 for Data Scientists Optimization Using R An Intuitive Introduction to Gradient Descent", "title_html": "<h1 id=\"title\">How Optimization Works</h1> ", "url": "https://www.kdnuggets.com/2019/04/how-optimization-works.html", "tfidf": {"tfidf": {"hand": 1.6152202665600002, "real": 4.56206896552, "anywher": 10.1638924456, "anneal": 260.262295082, "onc": 1.4974533106999999, "too": 5.44755804645, "occur": 1.7453825857499998, "formul": 9.86086956522, "new": 2.0357761108, "bowl": 63.431506849339996, "would": 1.0828729281799998, "number": 1.10142916609, "cup": 40.20937104264, "specif": 1.8719490626099997, "function": 22.458975165, "happi": 6.125, "temperatur": 81.83505154635, "descent": 84.94382022469999, "well": 5.3278743539999995, "behav": 15.6413793103, "done": 4.6605019814999995, "never": 1.55769230769, "complet": 1.24021560816, "approach": 2.07556543339, "name": 2.20423464074, "know": 15.559621038900001, "their": 2.0309581681, "drinker": 606.726114648, "scale": 3.7469907953699995, "gotten": 45.4899713467, "instead": 1.59461631177, "least": 1.6165359943000002, "how": 12.82002624408, "mama": 51.713355048900006, "repost": 933.882352941, "present": 1.25551601423, "end": 1.10680423871, "found": 3.34161229215, "given": 1.35426085473, "word": 1.7965372864099998, "fall": 1.6945244956799999, "deep": 3.6279707495399998, "car": 3.53743315508, "will": 2.44962197192, "full": 1.66729678639, "warmer": 55.8031634446, "next": 4.485168094920001, "under": 1.0781663837, "near": 2.57539135372, "but": 11.17956596889, "need": 1.4372623574099999, "our": 35.36382536385, "longer": 2.02319357716, "cost": 6.95807158509, "where": 3.20145190563, "point": 1.25990000794, "closer": 11.133239831700001, "bear": 10.16823228009, "level": 1.6544393497299998, "has": 3.1309492505999996, "enough": 2.2319696330700003, "stop": 2.1783754116400003, "exhaust": 41.279251170049996, "brandon": 39.8894472362, "pass": 1.61818367139, "use": 5.148193786899999, "resourc": 2.9487369985100003, "out": 11.661836394009999, "enjoy": 19.961441743500004, "tongu": 16.0688259109, "suffer": 6.483528450870001, "alway": 4.13491340018, "tea": 416.52472250220006, "introduc": 1.7258397651900002, "cours": 2.15092805853, "reli": 4.16146788991, "absolut": 10.694509936000001, "optim": 126.9156976747, "global": 3.30612244898, "loss": 2.42529789184, "not": 4.06269592476, "far": 1.71022298826, "arbitrari": 17.8181818182, "bump": 44.22284122560001, "tri": 16.69010629599, "whole": 4.58976582828, "minim": 12.21700654098, "happen": 8.89079708793, "test": 5.31414225942, "handl": 3.9229058561900003, "soft": 8.776119402989998, "then": 7.60605023612, "they": 14.42242554018, "anoth": 1.13643521832, "jump": 16.14234875444, "effect": 1.3963060686000002, "choos": 20.894972361150003, "minimum": 12.05924800608, "larger": 4.481580804519999, "away": 9.25714285715, "liter": 5.468825353080001, "stuck": 18.945107398599998, "algorithm": 83.8521126762, "until": 2.29704116328, "got": 7.23939808482, "suspect": 6.363126252509999, "terrain": 13.2964824121, "those": 1.19548192771, "clever": 30.5307692308, "with": 6.007189253939998, "math": 66.2420027817, "three": 1.06621893889, "lot": 30.86142738126, "slopeoftheslop": 992.25, "small": 2.7189587258, "drop": 2.4594887684, "result": 1.14611608432, "turn": 1.3838912133899999, "content": 3.5421686747, "miss": 3.53664513255, "set": 1.18707940781, "ani": 1.13383802314, "and": 32.00201574816, "from": 3.00170164491, "limit": 1.5186531471200002, "lukewarm": 90.72, "num": 1.00031504001, "teasuff": 992.25, "idea": 2.0930784443, "left": 1.4398693996, "curv": 77.7690692793, "tricki": 115.883211679, "for": 15.004725600150001, "difficult": 2.48957189901, "slide": 15.1056137012, "bottom": 50.18095614384, "roll": 17.10315109076, "unlik": 2.42529789184, "plateau": 22.266479663400002, "characterist": 7.344899375439999, "are": 17.50840090826, "solut": 14.183442525299998, "better": 6.01971688575, "distanc": 3.4754816112099998, "rather": 1.55692850838, "look": 9.543159413299998, "popular": 1.50769230769, "conveni": 9.85474860335, "video": 3.29719626168, "stair": 39.2, "veri": 7.55280685062, "work": 6.69120539478, "imagin": 13.197007481300002, "ice": 5.768895348840001, "again": 1.50883862384, "compar": 1.8662278123900002, "medium": 7.00617828773, "wiggl": 278.526315789, "even": 2.32922535212, "littl": 1.5499365420299998, "give": 1.3653250774, "side": 1.5989525632, "place": 1.1004366812200002, "great": 1.26592775696, "pickup": 56.2978723404, "avoid": 2.45986984816, "assumpt": 18.43902439024, "step": 42.41895261839999, "genet": 10.2558139535, "error": 42.287671232870004, "extrem": 4.73204172876, "big": 5.480151881259999, "behind": 2.0845588235299997, "race": 2.93023255814, "period": 1.3430335843, "probabl": 5.29111814698, "scientist": 18.77705499704, "machin": 8.04866920152, "into": 2.03004922958, "unsatisfi": 105.139072848, "steeper": 314.376237624, "through": 1.07074930869, "requir": 1.52844902282, "tougher": 87.7127071823, "figur": 4.0686827268, "what": 5.01373756512, "implicit": 21.1962616822, "let": 6.97233201582, "guess": 25.0410094637, "against": 1.2902072328299998, "tast": 9.305978898010002, "help": 2.79925945518, "measur": 4.82186788154, "larg": 1.18574949585, "hessian": 172.56521739099998, "pessimist": 79.7788944724, "cold": 3.91903233769, "own": 1.17844418052, "natur": 1.5392670157100001, "who": 1.06279287723, "decid": 1.9257641921400002, "hot": 13.853403141359998, "permiss": 6.280063291139999, "ideal": 4.65571847507, "report": 1.3634489866, "foot": 5.65384615385, "speed": 3.8703071672400005, "describ": 1.47027227264, "long": 2.5314518057799997, "much": 2.3884459154599997, "care": 2.49426551453, "invalu": 66.42677824270001, "about": 3.19458045477, "problem": 14.13398620072, "which": 4.02076738, "thing": 9.626193724439998, "have": 2.0297896822799997, "just": 10.68641144296, "entir": 1.59365589239, "bed": 23.075581395359997, "she": 2.16, "random": 14.3804347826, "check": 13.0131147541, "expens": 3.5453327378300004, "narrow": 5.53363541304, "across": 1.7318642958400001, "train": 3.8731397901999998, "flip": 31.3136094675, "start": 6.3336790872, "warm": 6.7614991482099995, "intuit": 83.1204188481, "love": 2.97303370787, "them": 3.29628347982, "ask": 2.1744966443, "energi": 3.66566612792, "dropamarbl": 992.25, "anyth": 13.76531791908, "shallow": 32.0727272728, "curvatur": 252.0, "track": 3.1276595744700004, "abl": 3.6417020300400003, "appli": 2.2972073506, "over": 1.02525024217, "wreck": 21.3674293405, "perhap": 3.14812611541, "numpoint": 992.25, "calcul": 6.12972972973, "fourwheeldr": 992.25, "unfortun": 29.898305084759997, "same": 6.71147748888, "valley": 8.43346613546, "exact": 3.46864758575, "there": 11.450039339089999, "brew": 140.8070953436, "possibl": 4.2521203464, "formula": 8.64235166032, "upsid": 65.0655737705, "say": 1.7544480053, "best": 7.914257228300001, "origin": 2.27449856734, "right": 9.838172804529998, "slight": 3.25327868852, "take": 15.95463355108, "now": 2.321561746, "robust": 39.8894472362, "discov": 5.04320203304, "code": 3.8807137619199996, "hard": 5.46506024096, "interest": 1.60331246213, "see": 5.08968502044, "than": 5.16393442625, "realli": 4.7476076555, "break": 4.85727397888, "also": 3.04429530201, "trick": 44.1818181819, "money": 2.62283165373, "mean": 1.44906900329, "irobot": 992.25, "fanci": 30.5895953757, "babi": 7.517045454550001, "quirk": 128.032258065, "travel": 1.9655812801800001, "mathematician": 16.1014198783, "the": 85.0, "may": 3.15605327679, "becom": 1.12492028626, "lowest": 6.549504950499999, "sweet": 10.2359767892, "occurr": 13.805217391300001, "challeng": 2.55816951337, "ground": 1.97610156833, "papa": 47.5329341317, "simul": 11.4793926247, "evolutionari": 21.168000000000003, "howev": 1.0945191313299998, "share": 1.8566249561500001, "variabl": 8.747107438019999, "actual": 5.62446858762, "could": 7.226217569399999, "maxim": 12.928338762200001, "poorlyplac": 992.25, "cooler": 27.1384615385, "term": 1.39520168732, "other": 2.01984732824, "introduct": 2.7808723068799996, "whether": 2.20683903253, "one": 6.03764974332, "impact": 2.97526236882, "begin": 1.3305397251100002, "slope": 89.3344051446, "goldilock": 933.882352941, "becaus": 2.2990369994999997, "get": 17.8562591385, "still": 1.1866357724799999, "like": 8.04429967425, "radiat": 11.3643521832, "rohrer": 992.25, "should": 1.6643254009900001, "alon": 2.99716820842, "process": 3.39049652964, "despis": 40.8123393316, "that": 25.09960159375, "undesir": 31.3754940711, "includ": 2.0381282495599997, "repeat": 5.754258789419999, "scald": 496.125, "straightforward": 27.7552447552, "pick": 9.879278158060002, "this": 17.06449165407, "pretti": 15.75, "rang": 1.7848229342299997, "time": 6.06764762088, "discret": 15.0056710775, "back": 1.26070038911, "relat": 1.23750876919, "flat": 5.67811158798, "smaller": 5.18738768176, "differ": 2.4730898045, "most": 6.12578778138, "between": 1.03453668708, "belli": 30.5307692308, "heavili": 3.24132298898, "obvious": 6.44841592201, "fail": 1.9281029876099998, "all": 8.09174311928, "local": 1.51720183486, "inferior": 14.6187845304, "comfort": 7.82068965517, "situat": 2.06611140031, "physic": 2.39132399458, "gradient": 418.89182058000006, "solv": 7.26923076923, "middl": 4.08490930142, "downhil": 327.34020618560004, "day": 2.36743215032, "subject": 1.8715077213299998, "tradeoff": 208.89473684200001, "call": 4.2706119704, "rate": 2.14048806795, "more": 7.1201947719, "spot": 4.52952924394, "unwit": 80.1818181818, "smooth": 11.086592178800002, "these": 3.22246278756, "prepar": 2.43012398592, "benefit": 3.06841901817, "faster": 7.61438848921, "befor": 2.20072082062, "comment": 3.05954904606, "serv": 2.9337521944, "can": 23.5252278284, "comput": 23.566551212280004, "make": 8.610128126880001, "truck": 10.3090909091, "way": 8.5335176227, "search": 16.26972740315, "onli": 1.0256476516600002, "each": 3.56924460432, "size": 2.49387370405, "few": 5.26916694324, "find": 20.752941176519997, "throat": 24.9230769231, "common": 1.4025974025999999, "drink": 18.05458680819, "data": 10.12930667802, "method": 12.857142857150002, "perfect": 4.48601299802, "fast": 9.7458563536, "come": 2.65662650602, "might": 2.1561863370900003, "fewer": 5.94829524166, "worth": 5.210370856580001, "insid": 2.7396031061299997, "marbl": 128.89851150180002, "direct": 3.66679498038, "first": 2.01523229246, "everi": 1.47917637194, "luckili": 191.277108434, "satisfact": 23.4159292035, "while": 2.0883977900599997, "easili": 7.387622149839999, "often": 1.29452054795, "deepest": 135.30681818189998, "down": 5.43559017376, "learn": 4.6455010973, "when": 8.1661415804, "effici": 10.18671799808}, "logtfidf": {"hand": 0.479471335336, "real": 1.649258121148, "anywher": 2.3188414835, "anneal": 5.561689949730001, "onc": 0.403765872355, "too": 1.7896654641659997, "occur": 0.556973778473, "formul": 2.2885743559200002, "new": 0.0354598937022, "bowl": 15.42835379687, "would": 0.0796176279647, "number": 0.0966085784186, "cup": 11.41404365844, "specif": 0.626980167541, "function": 8.230191674345999, "happi": 1.81237875643, "temperatur": 25.449831806099997, "descent": 21.3940500645, "well": 0.317572191578, "behav": 2.7499199224299997, "done": 1.691951966258, "never": 0.443205436091, "complet": 0.215285242047, "approach": 0.7302336145810001, "name": 0.19446633276860004, "know": 5.7175181663879995, "their": 0.030721010245400002, "drinker": 27.697908051300004, "scale": 1.32095306328, "gotten": 3.8174918917, "instead": 0.46663315041500003, "least": 0.480285584745, "how": 3.7725356554400005, "mama": 3.9457160663199997, "repost": 6.83935046985, "present": 0.227546654799, "end": 0.101476798618, "found": 0.323523372144, "given": 0.303255810831, "word": 0.585861082385, "fall": 0.527402167952, "deep": 1.2886734698, "car": 1.2634013667, "will": 0.40557306983, "full": 0.511203624148, "warmer": 6.6573667595599995, "next": 1.206491056497, "under": 0.07526180538319999, "near": 0.505708648068, "but": 0.1781160927909, "need": 0.362740163442, "our": 12.86458821273, "longer": 0.7046772417749999, "cost": 2.52387022854, "where": 0.19497641623710002, "point": 0.23103235903299998, "closer": 3.4335760647400004, "bear": 3.66196826757, "level": 0.503462189943, "has": 0.1281718345644, "enough": 0.802884439169, "stop": 0.778579374963, "exhaust": 10.55461034155, "brandon": 3.6861118086199998, "pass": 0.48130432974, "use": 0.146040098658, "resourc": 1.08137694258, "out": 0.6426903001123, "enjoy": 7.2122581841399995, "tongu": 2.7768811161599998, "suffer": 2.3119577625689995, "alway": 1.452638409144, "tea": 72.12008126882, "introduc": 0.5457137524260001, "cours": 0.765899404133, "reli": 1.42586787018, "absolut": 3.35316667828, "optim": 26.901905749509996, "global": 1.1957760371200001, "loss": 0.885954358842, "not": 0.06220965203, "far": 0.536623764503, "arbitrari": 2.88021938643, "bump": 3.78924142541, "tri": 5.558323762440001, "whole": 1.6613636488119998, "minim": 3.61936355852, "happen": 3.2592132540600005, "test": 1.954448874206, "handl": 1.36683266903, "soft": 2.1720343285099997, "then": 0.5812370566163, "they": 0.41617792674640003, "anoth": 0.127896361652, "jump": 4.17659799102, "effect": 0.333830227158, "choos": 7.1503533036, "minimum": 3.59336930882, "larger": 1.613657323556, "away": 3.079787709345, "liter": 1.6990638498800001, "stuck": 2.94154571342, "algorithm": 9.99132718554, "until": 0.276949326878, "got": 2.5727817698599997, "suspect": 1.85051980572, "terrain": 2.58749951995, "those": 0.17854939087299998, "clever": 3.4187350023299996, "with": 0.00718495028034, "math": 9.284107368539999, "three": 0.06411868822490001, "lot": 10.385178651750001, "slopeoftheslop": 6.89997509166, "small": 0.614203610118, "drop": 0.8999535106219999, "result": 0.136378908381, "turn": 0.324899251064, "content": 1.26473915954, "miss": 1.2631785751200002, "set": 0.171496011289, "ani": 0.125608358366, "and": 0.0020156845460352, "from": 0.0017011625065979999, "limit": 0.41782385463, "lukewarm": 4.50777783998, "num": 0.00031499039539700004, "teasuff": 6.89997509166, "idea": 0.73863592212, "left": 0.364552414753, "curv": 16.85483545179, "tricki": 4.75258288807, "for": 0.004724855930955001, "difficult": 0.912110767588, "slide": 2.7150664430299996, "bottom": 14.689552426879999, "roll": 5.811873438919999, "unlik": 0.885954358842, "plateau": 3.1030823934900003, "characterist": 2.60171785848, "are": 0.5009470509059, "solut": 4.660388928810001, "better": 2.0892838218, "distanc": 1.24573306257, "rather": 0.442714975539, "look": 3.2319334680000003, "popular": 0.41058020877499996, "conveni": 2.28795343073, "video": 1.19307248967, "stair": 3.6686767468, "veri": 1.3809587594280002, "work": 0.654207403638, "imagin": 3.77368583474, "ice": 1.75248061485, "again": 0.411340231612, "compar": 0.6239191809269999, "medium": 1.94679237232, "wiggl": 5.62951254607, "even": 0.304777129668, "littl": 0.438213989466, "give": 0.311392552224, "side": 0.46934876686899996, "place": 0.0957070839572, "great": 0.235805258079, "pickup": 4.03065674296, "avoid": 0.900108441291, "assumpt": 4.4426442578400005, "step": 15.5931758547, "genet": 2.32784475975, "error": 12.5900980401, "extrem": 1.7224191678740002, "big": 2.01597127114, "behind": 0.7345572374320001, "race": 1.07508179126, "period": 0.294930924153, "probabl": 1.945764825826, "scientist": 6.18536513776, "machin": 2.78471916124, "into": 0.0298257264574, "unsatisfi": 4.65528397709, "steeper": 10.11488659412, "through": 0.0683586918849, "requir": 0.424253510675, "tougher": 4.47406678264, "figur": 1.4203442243200002, "what": 0.903549187308, "implicit": 3.0538248303900004, "let": 2.4976051345599997, "guess": 3.22051485947, "against": 0.254802851078, "tast": 2.23065708585, "help": 0.672415442688, "measur": 1.760028399452, "larg": 0.17037506060600002, "hessian": 5.15077523685, "pessimist": 4.37925898918, "cold": 1.3658447706999999, "own": 0.164195077421, "natur": 0.431306339292, "who": 0.0609002329859, "decid": 0.655322871893, "hot": 4.5897558838800006, "permiss": 1.8373800586400002, "ideal": 1.53809624363, "report": 0.31001750903700004, "foot": 1.73233604876, "speed": 1.3533338752700002, "describ": 0.385447603125, "long": 0.471291587756, "much": 0.35499145860200004, "care": 0.9139943029109999, "invalu": 4.19610026197, "about": 0.18853043242380002, "problem": 4.553125794184, "which": 0.02071365538172, "thing": 3.5127741387199998, "have": 0.0295700046824, "just": 2.316251472872, "entir": 0.46603068026999994, "bed": 6.1204880619, "she": 0.7701082216959999, "random": 3.9454428130199997, "check": 3.74562099124, "expens": 1.26563201674, "narrow": 1.71084499792, "across": 0.549198455941, "train": 1.321836625678, "flip": 3.4440528103099997, "start": 1.182216846455, "warm": 1.91124463295, "intuit": 9.965034291570001, "love": 1.08958288195, "them": 0.2825499807279, "ask": 0.776797209847, "energi": 1.29901007269, "dropamarbl": 6.89997509166, "anyth": 4.57061983755, "shallow": 5.54971774154, "curvatur": 13.292450396520001, "track": 1.14028498507, "abl": 1.19860796495, "appli": 0.8316941898119999, "over": 0.0249367214957, "wreck": 3.0618677691900005, "perhap": 1.14680739183, "numpoint": 6.89997509166, "calcul": 1.8131506592099997, "fourwheeldr": 6.89997509166, "unfortun": 6.89756851197, "same": 0.672357897624, "valley": 2.8781213473400005, "exact": 1.2437647732500001, "there": 0.4410768221805, "brew": 14.244385897599999, "possibl": 1.0464164246730001, "formula": 2.15667472869, "upsid": 4.17539558861, "say": 0.562154280552, "best": 2.296139664735, "origin": 0.257224875174, "right": 2.38251897919, "slight": 1.17966331506, "take": 1.829687470758, "now": 0.298185890042, "robust": 5.9859292561199995, "discov": 1.849788047612, "code": 1.35601909597, "hard": 2.01045592812, "interest": 0.47207177798199995, "see": 0.963686341968, "than": 0.16130431109100002, "realli": 1.5576408397, "break": 1.77466038058, "also": 0.0439714734, "trick": 8.069103187289999, "money": 0.964254518011, "mean": 0.37092128352, "irobot": 6.89997509166, "fanci": 3.42065993074, "babi": 2.01717316908, "quirk": 4.8522822483, "travel": 0.675788018461, "mathematician": 2.7789074593, "the": 0.0, "may": 0.1521299858532, "becom": 0.11771217648900001, "lowest": 1.8793894667099997, "sweet": 2.3259086507299997, "occurr": 2.62504659255, "challeng": 0.9392919688950001, "ground": 0.681125998984, "papa": 3.8614228209300006, "simul": 2.44055348224, "evolutionari": 3.0524906073699998, "howev": 0.0903151173475, "share": 0.618760299747, "variabl": 2.1687230672, "actual": 1.885542544944, "could": 1.1157376337400002, "maxim": 2.5594217052, "poorlyplac": 6.89997509166, "cooler": 3.30095196667, "term": 0.33303898354600003, "other": 0.01974949583952, "introduct": 1.02276465794, "whether": 0.791561189647, "one": 0.037532109873, "impact": 1.09033222631, "begin": 0.285584668268, "slope": 17.8253357842, "goldilock": 6.83935046985, "becaus": 0.27868631765, "get": 5.79769005782, "still": 0.17112222142900002, "like": 0.973375035815, "radiat": 2.43048145465, "rohrer": 6.89997509166, "should": 0.509419876758, "alon": 1.09766791236, "process": 1.05565839805, "despis": 3.708984470280001, "that": 0.099403709491, "undesir": 3.4460271446199995, "includ": 0.037769362781, "repeat": 2.1135861182599998, "scald": 6.2068279111, "straightforward": 3.3234248225200003, "pick": 3.19458453522, "this": 0.0643696338925, "pretti": 2.75684036527, "rang": 0.579319213803, "time": 0.0672691131756, "discret": 2.70842820148, "back": 0.23166743089699998, "relat": 0.21310030165399999, "flat": 1.7366187105500002, "smaller": 1.9061661061039998, "differ": 0.424642242624, "most": 0.12448737777359999, "between": 0.033953681165299995, "belli": 3.4187350023299996, "heavili": 1.17598157639, "obvious": 1.86383450716, "fail": 0.656536611573, "all": 0.09122105678239999, "local": 0.416867740206, "inferior": 2.68230731341, "comfort": 2.05677274187, "situat": 0.725668290015, "physic": 0.871847185184, "gradient": 37.3502760882, "solv": 1.9836504770400003, "middl": 1.4283046893459999, "downhil": 17.61882261936, "day": 0.33731741263400006, "subject": 0.6267443740950001, "tradeoff": 5.34183047362, "call": 0.2618510977952, "rate": 0.761033872166, "more": 0.11917452119999998, "spot": 1.5106180144299999, "unwit": 4.38429678321, "smooth": 2.4057364663799996, "these": 0.2146008582024, "prepar": 0.8879422790620001, "benefit": 1.12116245116, "faster": 2.03003967967, "befor": 0.191275543759, "comment": 1.11826753454, "serv": 0.766270071216, "can": 3.2468219278799997, "comput": 8.208413495639999, "make": 0.5879812625831999, "truck": 2.3330261185, "way": 1.3866405695450001, "search": 5.899341270449999, "onli": 0.025324268329099998, "each": 0.521225067912, "size": 0.9138372060609999, "few": 1.102311654612, "find": 6.573375963456, "throat": 3.21579415833, "common": 0.338325805271, "drink": 5.38436244189, "data": 3.6504617544, "method": 4.7223080442050005, "perfect": 1.50096433356, "fast": 3.1673900494800002, "come": 0.5678198130600001, "might": 0.7683410765340001, "fewer": 1.7831046645, "worth": 1.65065103492, "insid": 1.00781305813, "marbl": 18.4035953577, "direct": 0.6021170684880001, "first": 0.015174579624319999, "everi": 0.391485427421, "luckili": 5.25372320611, "satisfact": 3.1534165259599996, "while": 0.08649996758760002, "easili": 2.6133174734, "often": 0.258140393351, "deepest": 11.4267979149, "down": 1.226694964744, "learn": 1.68550412949, "when": 0.1644399108672, "effici": 3.25587506828}, "logidf": {"hand": 0.479471335336, "real": 0.824629060574, "anywher": 2.3188414835, "anneal": 5.561689949730001, "onc": 0.403765872355, "too": 0.5965551547219999, "occur": 0.556973778473, "formul": 2.2885743559200002, "new": 0.0177299468511, "bowl": 2.20405054241, "would": 0.0796176279647, "number": 0.0966085784186, "cup": 1.90234060974, "specif": 0.626980167541, "function": 0.914465741594, "happi": 1.81237875643, "temperatur": 1.6966554537399998, "descent": 2.13940500645, "well": 0.0635144383156, "behav": 2.7499199224299997, "done": 0.845975983129, "never": 0.443205436091, "complet": 0.215285242047, "approach": 0.7302336145810001, "name": 0.09723316638430002, "know": 0.952919694398, "their": 0.015360505122700001, "drinker": 4.61631800855, "scale": 1.32095306328, "gotten": 3.8174918917, "instead": 0.46663315041500003, "least": 0.480285584745, "how": 0.47156695693000006, "mama": 3.9457160663199997, "repost": 6.83935046985, "present": 0.227546654799, "end": 0.101476798618, "found": 0.107841124048, "given": 0.303255810831, "word": 0.585861082385, "fall": 0.527402167952, "deep": 1.2886734698, "car": 1.2634013667, "will": 0.202786534915, "full": 0.511203624148, "warmer": 3.3286833797799997, "next": 0.402163685499, "under": 0.07526180538319999, "near": 0.252854324034, "but": 0.0161923720719, "need": 0.362740163442, "our": 0.8576392141820001, "longer": 0.7046772417749999, "cost": 0.84129007618, "where": 0.0649921387457, "point": 0.23103235903299998, "closer": 1.7167880323700002, "bear": 1.22065608919, "level": 0.503462189943, "has": 0.0427239448548, "enough": 0.802884439169, "stop": 0.778579374963, "exhaust": 2.11092206831, "brandon": 3.6861118086199998, "pass": 0.48130432974, "use": 0.0292080197316, "resourc": 1.08137694258, "out": 0.0584263909193, "enjoy": 1.2020430306899998, "tongu": 2.7768811161599998, "suffer": 0.7706525875229999, "alway": 0.726319204572, "tea": 2.7738492795700003, "introduc": 0.5457137524260001, "cours": 0.765899404133, "reli": 1.42586787018, "absolut": 1.67658333914, "optim": 2.4456277954099996, "global": 1.1957760371200001, "loss": 0.885954358842, "not": 0.0155524130075, "far": 0.536623764503, "arbitrari": 2.88021938643, "bump": 3.78924142541, "tri": 0.61759152916, "whole": 0.8306818244059999, "minim": 1.80968177926, "happen": 1.08640441802, "test": 0.977224437103, "handl": 1.36683266903, "soft": 2.1720343285099997, "then": 0.08303386523089999, "they": 0.0297269947676, "anoth": 0.127896361652, "jump": 2.08829899551, "effect": 0.333830227158, "choos": 1.43007066072, "minimum": 1.79668465441, "larger": 0.806828661778, "away": 0.615957541869, "liter": 1.6990638498800001, "stuck": 2.94154571342, "algorithm": 3.33044239518, "until": 0.138474663439, "got": 1.2863908849299999, "suspect": 1.85051980572, "terrain": 2.58749951995, "those": 0.17854939087299998, "clever": 3.4187350023299996, "with": 0.00119749171339, "math": 3.09470245618, "three": 0.06411868822490001, "lot": 1.4835969502500002, "slopeoftheslop": 6.89997509166, "small": 0.307101805059, "drop": 0.8999535106219999, "result": 0.136378908381, "turn": 0.324899251064, "content": 1.26473915954, "miss": 1.2631785751200002, "set": 0.171496011289, "ani": 0.125608358366, "and": 6.29901420636e-05, "from": 0.000567054168866, "limit": 0.41782385463, "lukewarm": 4.50777783998, "num": 0.00031499039539700004, "teasuff": 6.89997509166, "idea": 0.73863592212, "left": 0.364552414753, "curv": 2.40783363597, "tricki": 4.75258288807, "for": 0.00031499039539700004, "difficult": 0.912110767588, "slide": 2.7150664430299996, "bottom": 1.8361940533599999, "roll": 1.4529683597299998, "unlik": 0.885954358842, "plateau": 3.1030823934900003, "characterist": 1.30085892924, "are": 0.0294674735827, "solut": 1.55346297627, "better": 0.6964279406, "distanc": 1.24573306257, "rather": 0.442714975539, "look": 0.6463866936, "popular": 0.41058020877499996, "conveni": 2.28795343073, "video": 1.19307248967, "stair": 3.6686767468, "veri": 0.230159793238, "work": 0.109034567273, "imagin": 1.88684291737, "ice": 1.75248061485, "again": 0.411340231612, "compar": 0.6239191809269999, "medium": 1.94679237232, "wiggl": 5.62951254607, "even": 0.152388564834, "littl": 0.438213989466, "give": 0.311392552224, "side": 0.46934876686899996, "place": 0.0957070839572, "great": 0.235805258079, "pickup": 4.03065674296, "avoid": 0.900108441291, "assumpt": 2.2213221289200002, "step": 1.03954505698, "genet": 2.32784475975, "error": 1.7985854343, "extrem": 0.8612095839370001, "big": 1.00798563557, "behind": 0.7345572374320001, "race": 1.07508179126, "period": 0.294930924153, "probabl": 0.972882412913, "scientist": 1.54634128444, "machin": 1.39235958062, "into": 0.0149128632287, "unsatisfi": 4.65528397709, "steeper": 5.05744329706, "through": 0.0683586918849, "requir": 0.424253510675, "tougher": 4.47406678264, "figur": 0.7101721121600001, "what": 0.225887296827, "implicit": 3.0538248303900004, "let": 1.2488025672799998, "guess": 3.22051485947, "against": 0.254802851078, "tast": 2.23065708585, "help": 0.336207721344, "measur": 0.880014199726, "larg": 0.17037506060600002, "hessian": 5.15077523685, "pessimist": 4.37925898918, "cold": 1.3658447706999999, "own": 0.164195077421, "natur": 0.431306339292, "who": 0.0609002329859, "decid": 0.655322871893, "hot": 1.52991862796, "permiss": 1.8373800586400002, "ideal": 1.53809624363, "report": 0.31001750903700004, "foot": 1.73233604876, "speed": 1.3533338752700002, "describ": 0.385447603125, "long": 0.235645793878, "much": 0.17749572930100002, "care": 0.9139943029109999, "invalu": 4.19610026197, "about": 0.0628434774746, "problem": 0.569140724273, "which": 0.00517841384543, "thing": 0.8781935346799999, "have": 0.0147850023412, "just": 0.289531434109, "entir": 0.46603068026999994, "bed": 2.0401626873, "she": 0.7701082216959999, "random": 1.9727214065099998, "check": 1.87281049562, "expens": 1.26563201674, "narrow": 1.71084499792, "across": 0.549198455941, "train": 0.660918312839, "flip": 3.4440528103099997, "start": 0.236443369291, "warm": 1.91124463295, "intuit": 3.3216780971900004, "love": 1.08958288195, "them": 0.0941833269093, "ask": 0.776797209847, "energi": 1.29901007269, "dropamarbl": 6.89997509166, "anyth": 1.52353994585, "shallow": 2.77485887077, "curvatur": 4.4308167988400005, "track": 1.14028498507, "abl": 0.599303982475, "appli": 0.8316941898119999, "over": 0.0249367214957, "wreck": 3.0618677691900005, "perhap": 1.14680739183, "numpoint": 6.89997509166, "calcul": 1.8131506592099997, "fourwheeldr": 6.89997509166, "unfortun": 2.29918950399, "same": 0.112059649604, "valley": 1.4390606736700002, "exact": 1.2437647732500001, "there": 0.0400978929255, "brew": 3.5610964743999998, "possibl": 0.348805474891, "formula": 2.15667472869, "upsid": 4.17539558861, "say": 0.562154280552, "best": 0.459227932947, "origin": 0.128612437587, "right": 0.34035985417, "slight": 1.17966331506, "take": 0.130691962197, "now": 0.149092945021, "robust": 2.9929646280599997, "discov": 0.924894023806, "code": 1.35601909597, "hard": 1.00522796406, "interest": 0.47207177798199995, "see": 0.240921585492, "than": 0.0322608622182, "realli": 1.5576408397, "break": 0.88733019029, "also": 0.0146571578, "trick": 2.6897010624299997, "money": 0.964254518011, "mean": 0.37092128352, "irobot": 6.89997509166, "fanci": 3.42065993074, "babi": 2.01717316908, "quirk": 4.8522822483, "travel": 0.675788018461, "mathematician": 2.7789074593, "the": 0.0, "may": 0.050709995284400004, "becom": 0.11771217648900001, "lowest": 1.8793894667099997, "sweet": 2.3259086507299997, "occurr": 2.62504659255, "challeng": 0.9392919688950001, "ground": 0.681125998984, "papa": 3.8614228209300006, "simul": 2.44055348224, "evolutionari": 3.0524906073699998, "howev": 0.0903151173475, "share": 0.618760299747, "variabl": 2.1687230672, "actual": 0.628514181648, "could": 0.18595627229000003, "maxim": 2.5594217052, "poorlyplac": 6.89997509166, "cooler": 3.30095196667, "term": 0.33303898354600003, "other": 0.00987474791976, "introduct": 1.02276465794, "whether": 0.791561189647, "one": 0.0062553516455, "impact": 1.09033222631, "begin": 0.285584668268, "slope": 2.5464765406, "goldilock": 6.83935046985, "becaus": 0.139343158825, "get": 0.579769005782, "still": 0.17112222142900002, "like": 0.139053576545, "radiat": 2.43048145465, "rohrer": 6.89997509166, "should": 0.509419876758, "alon": 1.09766791236, "process": 0.527829199025, "despis": 3.708984470280001, "that": 0.00397614837964, "undesir": 3.4460271446199995, "includ": 0.0188846813905, "repeat": 1.0567930591299999, "scald": 6.2068279111, "straightforward": 3.3234248225200003, "pick": 1.59729226761, "this": 0.0037864490525, "pretti": 2.75684036527, "rang": 0.579319213803, "time": 0.0112115188626, "discret": 2.70842820148, "back": 0.23166743089699998, "relat": 0.21310030165399999, "flat": 1.7366187105500002, "smaller": 0.9530830530519999, "differ": 0.212321121312, "most": 0.020747896295599998, "between": 0.033953681165299995, "belli": 3.4187350023299996, "heavili": 1.17598157639, "obvious": 1.86383450716, "fail": 0.656536611573, "all": 0.011402632097799998, "local": 0.416867740206, "inferior": 2.68230731341, "comfort": 2.05677274187, "situat": 0.725668290015, "physic": 0.871847185184, "gradient": 3.73502760882, "solv": 1.9836504770400003, "middl": 0.7141523446729999, "downhil": 4.40470565484, "day": 0.16865870631700003, "subject": 0.6267443740950001, "tradeoff": 5.34183047362, "call": 0.0654627744488, "rate": 0.761033872166, "more": 0.017024931599999998, "spot": 1.5106180144299999, "unwit": 4.38429678321, "smooth": 2.4057364663799996, "these": 0.0715336194008, "prepar": 0.8879422790620001, "benefit": 1.12116245116, "faster": 2.03003967967, "befor": 0.0956377718795, "comment": 1.11826753454, "serv": 0.383135035608, "can": 0.162341096394, "comput": 1.36806891594, "make": 0.07349765782289999, "truck": 2.3330261185, "way": 0.19809150993500002, "search": 1.1798682540899998, "onli": 0.025324268329099998, "each": 0.173741689304, "size": 0.9138372060609999, "few": 0.275577913653, "find": 0.547781330288, "throat": 3.21579415833, "common": 0.338325805271, "drink": 1.79478748063, "data": 1.2168205848, "method": 0.944461608841, "perfect": 1.50096433356, "fast": 1.5836950247400001, "come": 0.28390990653000003, "might": 0.7683410765340001, "fewer": 1.7831046645, "worth": 1.65065103492, "insid": 1.00781305813, "marbl": 3.06726589295, "direct": 0.200705689496, "first": 0.0075872898121599995, "everi": 0.391485427421, "luckili": 5.25372320611, "satisfact": 3.1534165259599996, "while": 0.04324998379380001, "easili": 1.3066587367, "often": 0.258140393351, "deepest": 3.8089326383, "down": 0.306673741186, "learn": 0.842752064745, "when": 0.0205549888584, "effici": 1.62793753414}, "freq": {"hand": 1, "real": 2, "anywher": 1, "anneal": 1, "onc": 1, "too": 3, "occur": 1, "formul": 1, "new": 2, "bowl": 7, "would": 1, "number": 1, "cup": 6, "specif": 1, "function": 9, "happi": 1, "temperatur": 15, "descent": 10, "well": 5, "behav": 1, "done": 2, "never": 1, "complet": 1, "approach": 1, "name": 2, "know": 6, "their": 2, "drinker": 6, "scale": 1, "gotten": 1, "instead": 1, "least": 1, "how": 8, "mama": 1, "repost": 1, "present": 1, "end": 1, "found": 3, "given": 1, "word": 1, "fall": 1, "deep": 1, "car": 1, "will": 2, "full": 1, "warmer": 2, "next": 3, "under": 1, "near": 2, "but": 11, "need": 1, "our": 15, "longer": 1, "cost": 3, "where": 3, "point": 1, "closer": 2, "bear": 3, "level": 1, "has": 3, "enough": 1, "stop": 1, "exhaust": 5, "brandon": 1, "pass": 1, "use": 5, "resourc": 1, "out": 11, "enjoy": 6, "tongu": 1, "suffer": 3, "alway": 2, "tea": 26, "introduc": 1, "cours": 1, "reli": 1, "absolut": 2, "optim": 11, "global": 1, "loss": 1, "not": 4, "far": 1, "arbitrari": 1, "bump": 1, "tri": 9, "whole": 2, "minim": 2, "happen": 3, "test": 2, "handl": 1, "soft": 1, "then": 7, "they": 14, "anoth": 1, "jump": 2, "effect": 1, "choos": 5, "minimum": 2, "larger": 2, "away": 5, "liter": 1, "stuck": 1, "algorithm": 3, "until": 2, "got": 2, "suspect": 1, "terrain": 1, "those": 1, "clever": 1, "with": 6, "math": 3, "three": 1, "lot": 7, "slopeoftheslop": 1, "small": 2, "drop": 1, "result": 1, "turn": 1, "content": 1, "miss": 1, "set": 1, "ani": 1, "and": 32, "from": 3, "limit": 1, "lukewarm": 1, "num": 1, "teasuff": 1, "idea": 1, "left": 1, "curv": 7, "tricki": 1, "for": 15, "difficult": 1, "slide": 1, "bottom": 8, "roll": 4, "unlik": 1, "plateau": 1, "characterist": 2, "are": 17, "solut": 3, "better": 3, "distanc": 1, "rather": 1, "look": 5, "popular": 1, "conveni": 1, "video": 1, "stair": 1, "veri": 6, "work": 6, "imagin": 2, "ice": 1, "again": 1, "compar": 1, "medium": 1, "wiggl": 1, "even": 2, "littl": 1, "give": 1, "side": 1, "place": 1, "great": 1, "pickup": 1, "avoid": 1, "assumpt": 2, "step": 15, "genet": 1, "error": 7, "extrem": 2, "big": 2, "behind": 1, "race": 1, "period": 1, "probabl": 2, "scientist": 4, "machin": 2, "into": 2, "unsatisfi": 1, "steeper": 2, "through": 1, "requir": 1, "tougher": 1, "figur": 2, "what": 4, "implicit": 1, "let": 2, "guess": 1, "against": 1, "tast": 1, "help": 2, "measur": 2, "larg": 1, "hessian": 1, "pessimist": 1, "cold": 1, "own": 1, "natur": 1, "who": 1, "decid": 1, "hot": 3, "permiss": 1, "ideal": 1, "report": 1, "foot": 1, "speed": 1, "describ": 1, "long": 2, "much": 2, "care": 1, "invalu": 1, "about": 3, "problem": 8, "which": 4, "thing": 4, "have": 2, "just": 8, "entir": 1, "bed": 3, "she": 1, "random": 2, "check": 2, "expens": 1, "narrow": 1, "across": 1, "train": 2, "flip": 1, "start": 5, "warm": 1, "intuit": 3, "love": 1, "them": 3, "ask": 1, "energi": 1, "dropamarbl": 1, "anyth": 3, "shallow": 2, "curvatur": 3, "track": 1, "abl": 2, "appli": 1, "over": 1, "wreck": 1, "perhap": 1, "numpoint": 1, "calcul": 1, "fourwheeldr": 1, "unfortun": 3, "same": 6, "valley": 2, "exact": 1, "there": 11, "brew": 4, "possibl": 3, "formula": 1, "upsid": 1, "say": 1, "best": 5, "origin": 2, "right": 7, "slight": 1, "take": 14, "now": 2, "robust": 2, "discov": 2, "code": 1, "hard": 2, "interest": 1, "see": 4, "than": 5, "realli": 1, "break": 2, "also": 3, "trick": 3, "money": 1, "mean": 1, "irobot": 1, "fanci": 1, "babi": 1, "quirk": 1, "travel": 1, "mathematician": 1, "the": 85, "may": 3, "becom": 1, "lowest": 1, "sweet": 1, "occurr": 1, "challeng": 1, "ground": 1, "papa": 1, "simul": 1, "evolutionari": 1, "howev": 1, "share": 1, "variabl": 1, "actual": 3, "could": 6, "maxim": 1, "poorlyplac": 1, "cooler": 1, "term": 1, "other": 2, "introduct": 1, "whether": 1, "one": 6, "impact": 1, "begin": 1, "slope": 7, "goldilock": 1, "becaus": 2, "get": 10, "still": 1, "like": 7, "radiat": 1, "rohrer": 1, "should": 1, "alon": 1, "process": 2, "despis": 1, "that": 25, "undesir": 1, "includ": 2, "repeat": 2, "scald": 1, "straightforward": 1, "pick": 2, "this": 17, "pretti": 1, "rang": 1, "time": 6, "discret": 1, "back": 1, "relat": 1, "flat": 1, "smaller": 2, "differ": 2, "most": 6, "between": 1, "belli": 1, "heavili": 1, "obvious": 1, "fail": 1, "all": 8, "local": 1, "inferior": 1, "comfort": 1, "situat": 1, "physic": 1, "gradient": 10, "solv": 1, "middl": 2, "downhil": 4, "day": 2, "subject": 1, "tradeoff": 1, "call": 4, "rate": 1, "more": 7, "spot": 1, "unwit": 1, "smooth": 1, "these": 3, "prepar": 1, "benefit": 1, "faster": 1, "befor": 2, "comment": 1, "serv": 2, "can": 20, "comput": 6, "make": 8, "truck": 1, "way": 7, "search": 5, "onli": 1, "each": 3, "size": 1, "few": 4, "find": 12, "throat": 1, "common": 1, "drink": 3, "data": 3, "method": 5, "perfect": 1, "fast": 2, "come": 2, "might": 1, "fewer": 1, "worth": 1, "insid": 1, "marbl": 6, "direct": 3, "first": 2, "everi": 1, "luckili": 1, "satisfact": 1, "while": 2, "easili": 2, "often": 1, "deepest": 3, "down": 4, "learn": 2, "when": 8, "effici": 2}, "idf": {"hand": 1.6152202665600002, "real": 2.28103448276, "anywher": 10.1638924456, "anneal": 260.262295082, "onc": 1.4974533106999999, "too": 1.81585268215, "occur": 1.7453825857499998, "formul": 9.86086956522, "new": 1.0178880554, "bowl": 9.06164383562, "would": 1.0828729281799998, "number": 1.10142916609, "cup": 6.70156184044, "specif": 1.8719490626099997, "function": 2.495441685, "happi": 6.125, "temperatur": 5.45567010309, "descent": 8.494382022469999, "well": 1.0655748708, "behav": 15.6413793103, "done": 2.3302509907499998, "never": 1.55769230769, "complet": 1.24021560816, "approach": 2.07556543339, "name": 1.10211732037, "know": 2.59327017315, "their": 1.01547908405, "drinker": 101.121019108, "scale": 3.7469907953699995, "gotten": 45.4899713467, "instead": 1.59461631177, "least": 1.6165359943000002, "how": 1.60250328051, "mama": 51.713355048900006, "repost": 933.882352941, "present": 1.25551601423, "end": 1.10680423871, "found": 1.11387076405, "given": 1.35426085473, "word": 1.7965372864099998, "fall": 1.6945244956799999, "deep": 3.6279707495399998, "car": 3.53743315508, "will": 1.22481098596, "full": 1.66729678639, "warmer": 27.9015817223, "next": 1.4950560316400001, "under": 1.0781663837, "near": 1.28769567686, "but": 1.01632417899, "need": 1.4372623574099999, "our": 2.35758835759, "longer": 2.02319357716, "cost": 2.31935719503, "where": 1.06715063521, "point": 1.25990000794, "closer": 5.5666199158500005, "bear": 3.38941076003, "level": 1.6544393497299998, "has": 1.0436497502, "enough": 2.2319696330700003, "stop": 2.1783754116400003, "exhaust": 8.25585023401, "brandon": 39.8894472362, "pass": 1.61818367139, "use": 1.0296387573799999, "resourc": 2.9487369985100003, "out": 1.06016694491, "enjoy": 3.3269069572500003, "tongu": 16.0688259109, "suffer": 2.16117615029, "alway": 2.06745670009, "tea": 16.020181634700002, "introduc": 1.7258397651900002, "cours": 2.15092805853, "reli": 4.16146788991, "absolut": 5.3472549680000006, "optim": 11.5377906977, "global": 3.30612244898, "loss": 2.42529789184, "not": 1.01567398119, "far": 1.71022298826, "arbitrari": 17.8181818182, "bump": 44.22284122560001, "tri": 1.8544562551099997, "whole": 2.29488291414, "minim": 6.10850327049, "happen": 2.96359902931, "test": 2.65707112971, "handl": 3.9229058561900003, "soft": 8.776119402989998, "then": 1.08657860516, "they": 1.03017325287, "anoth": 1.13643521832, "jump": 8.07117437722, "effect": 1.3963060686000002, "choos": 4.17899447223, "minimum": 6.02962400304, "larger": 2.2407904022599996, "away": 1.85142857143, "liter": 5.468825353080001, "stuck": 18.945107398599998, "algorithm": 27.9507042254, "until": 1.14852058164, "got": 3.61969904241, "suspect": 6.363126252509999, "terrain": 13.2964824121, "those": 1.19548192771, "clever": 30.5307692308, "with": 1.0011982089899998, "math": 22.0806675939, "three": 1.06621893889, "lot": 4.40877534018, "slopeoftheslop": 992.25, "small": 1.3594793629, "drop": 2.4594887684, "result": 1.14611608432, "turn": 1.3838912133899999, "content": 3.5421686747, "miss": 3.53664513255, "set": 1.18707940781, "ani": 1.13383802314, "and": 1.00006299213, "from": 1.00056721497, "limit": 1.5186531471200002, "lukewarm": 90.72, "num": 1.00031504001, "teasuff": 992.25, "idea": 2.0930784443, "left": 1.4398693996, "curv": 11.1098670399, "tricki": 115.883211679, "for": 1.00031504001, "difficult": 2.48957189901, "slide": 15.1056137012, "bottom": 6.27261951798, "roll": 4.27578777269, "unlik": 2.42529789184, "plateau": 22.266479663400002, "characterist": 3.6724496877199995, "are": 1.02990593578, "solut": 4.7278141751, "better": 2.0065722952500002, "distanc": 3.4754816112099998, "rather": 1.55692850838, "look": 1.9086318826599997, "popular": 1.50769230769, "conveni": 9.85474860335, "video": 3.29719626168, "stair": 39.2, "veri": 1.25880114177, "work": 1.11520089913, "imagin": 6.598503740650001, "ice": 5.768895348840001, "again": 1.50883862384, "compar": 1.8662278123900002, "medium": 7.00617828773, "wiggl": 278.526315789, "even": 1.16461267606, "littl": 1.5499365420299998, "give": 1.3653250774, "side": 1.5989525632, "place": 1.1004366812200002, "great": 1.26592775696, "pickup": 56.2978723404, "avoid": 2.45986984816, "assumpt": 9.21951219512, "step": 2.8279301745599996, "genet": 10.2558139535, "error": 6.04109589041, "extrem": 2.36602086438, "big": 2.7400759406299997, "behind": 2.0845588235299997, "race": 2.93023255814, "period": 1.3430335843, "probabl": 2.64555907349, "scientist": 4.69426374926, "machin": 4.02433460076, "into": 1.01502461479, "unsatisfi": 105.139072848, "steeper": 157.188118812, "through": 1.07074930869, "requir": 1.52844902282, "tougher": 87.7127071823, "figur": 2.0343413634, "what": 1.25343439128, "implicit": 21.1962616822, "let": 3.48616600791, "guess": 25.0410094637, "against": 1.2902072328299998, "tast": 9.305978898010002, "help": 1.39962972759, "measur": 2.41093394077, "larg": 1.18574949585, "hessian": 172.56521739099998, "pessimist": 79.7788944724, "cold": 3.91903233769, "own": 1.17844418052, "natur": 1.5392670157100001, "who": 1.06279287723, "decid": 1.9257641921400002, "hot": 4.6178010471199995, "permiss": 6.280063291139999, "ideal": 4.65571847507, "report": 1.3634489866, "foot": 5.65384615385, "speed": 3.8703071672400005, "describ": 1.47027227264, "long": 1.2657259028899999, "much": 1.1942229577299999, "care": 2.49426551453, "invalu": 66.42677824270001, "about": 1.06486015159, "problem": 1.76674827509, "which": 1.005191845, "thing": 2.4065484311099996, "have": 1.0148948411399998, "just": 1.33580143037, "entir": 1.59365589239, "bed": 7.6918604651199995, "she": 2.16, "random": 7.1902173913, "check": 6.50655737705, "expens": 3.5453327378300004, "narrow": 5.53363541304, "across": 1.7318642958400001, "train": 1.9365698950999999, "flip": 31.3136094675, "start": 1.26673581744, "warm": 6.7614991482099995, "intuit": 27.7068062827, "love": 2.97303370787, "them": 1.09876115994, "ask": 2.1744966443, "energi": 3.66566612792, "dropamarbl": 992.25, "anyth": 4.58843930636, "shallow": 16.0363636364, "curvatur": 84.0, "track": 3.1276595744700004, "abl": 1.8208510150200001, "appli": 2.2972073506, "over": 1.02525024217, "wreck": 21.3674293405, "perhap": 3.14812611541, "numpoint": 992.25, "calcul": 6.12972972973, "fourwheeldr": 992.25, "unfortun": 9.966101694919999, "same": 1.11857958148, "valley": 4.21673306773, "exact": 3.46864758575, "there": 1.04091266719, "brew": 35.2017738359, "possibl": 1.4173734488, "formula": 8.64235166032, "upsid": 65.0655737705, "say": 1.7544480053, "best": 1.5828514456600002, "origin": 1.13724928367, "right": 1.4054532577899999, "slight": 3.25327868852, "take": 1.13961668222, "now": 1.160780873, "robust": 19.9447236181, "discov": 2.52160101652, "code": 3.8807137619199996, "hard": 2.73253012048, "interest": 1.60331246213, "see": 1.27242125511, "than": 1.03278688525, "realli": 4.7476076555, "break": 2.42863698944, "also": 1.01476510067, "trick": 14.7272727273, "money": 2.62283165373, "mean": 1.44906900329, "irobot": 992.25, "fanci": 30.5895953757, "babi": 7.517045454550001, "quirk": 128.032258065, "travel": 1.9655812801800001, "mathematician": 16.1014198783, "the": 1.0, "may": 1.05201775893, "becom": 1.12492028626, "lowest": 6.549504950499999, "sweet": 10.2359767892, "occurr": 13.805217391300001, "challeng": 2.55816951337, "ground": 1.97610156833, "papa": 47.5329341317, "simul": 11.4793926247, "evolutionari": 21.168000000000003, "howev": 1.0945191313299998, "share": 1.8566249561500001, "variabl": 8.747107438019999, "actual": 1.87482286254, "could": 1.2043695949, "maxim": 12.928338762200001, "poorlyplac": 992.25, "cooler": 27.1384615385, "term": 1.39520168732, "other": 1.00992366412, "introduct": 2.7808723068799996, "whether": 2.20683903253, "one": 1.00627495722, "impact": 2.97526236882, "begin": 1.3305397251100002, "slope": 12.7620578778, "goldilock": 933.882352941, "becaus": 1.1495184997499999, "get": 1.78562591385, "still": 1.1866357724799999, "like": 1.14918566775, "radiat": 11.3643521832, "rohrer": 992.25, "should": 1.6643254009900001, "alon": 2.99716820842, "process": 1.69524826482, "despis": 40.8123393316, "that": 1.00398406375, "undesir": 31.3754940711, "includ": 1.0190641247799999, "repeat": 2.8771293947099994, "scald": 496.125, "straightforward": 27.7552447552, "pick": 4.939639079030001, "this": 1.00379362671, "pretti": 15.75, "rang": 1.7848229342299997, "time": 1.01127460348, "discret": 15.0056710775, "back": 1.26070038911, "relat": 1.23750876919, "flat": 5.67811158798, "smaller": 2.59369384088, "differ": 1.23654490225, "most": 1.02096463023, "between": 1.03453668708, "belli": 30.5307692308, "heavili": 3.24132298898, "obvious": 6.44841592201, "fail": 1.9281029876099998, "all": 1.01146788991, "local": 1.51720183486, "inferior": 14.6187845304, "comfort": 7.82068965517, "situat": 2.06611140031, "physic": 2.39132399458, "gradient": 41.889182058, "solv": 7.26923076923, "middl": 2.04245465071, "downhil": 81.83505154640001, "day": 1.18371607516, "subject": 1.8715077213299998, "tradeoff": 208.89473684200001, "call": 1.0676529926, "rate": 2.14048806795, "more": 1.0171706817, "spot": 4.52952924394, "unwit": 80.1818181818, "smooth": 11.086592178800002, "these": 1.07415426252, "prepar": 2.43012398592, "benefit": 3.06841901817, "faster": 7.61438848921, "befor": 1.10036041031, "comment": 3.05954904606, "serv": 1.4668760972, "can": 1.17626139142, "comput": 3.9277585353800006, "make": 1.0762660158600001, "truck": 10.3090909091, "way": 1.2190739461, "search": 3.2539454806299997, "onli": 1.0256476516600002, "each": 1.18974820144, "size": 2.49387370405, "few": 1.31729173581, "find": 1.7294117647099998, "throat": 24.9230769231, "common": 1.4025974025999999, "drink": 6.01819560273, "data": 3.37643555934, "method": 2.5714285714300003, "perfect": 4.48601299802, "fast": 4.8729281768, "come": 1.32831325301, "might": 2.1561863370900003, "fewer": 5.94829524166, "worth": 5.210370856580001, "insid": 2.7396031061299997, "marbl": 21.483085250300004, "direct": 1.22226499346, "first": 1.00761614623, "everi": 1.47917637194, "luckili": 191.277108434, "satisfact": 23.4159292035, "while": 1.0441988950299999, "easili": 3.6938110749199997, "often": 1.29452054795, "deepest": 45.1022727273, "down": 1.35889754344, "learn": 2.32275054865, "when": 1.02076769755, "effici": 5.09335899904}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  How Optimization Works</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/how-optimization-works.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb How Optimization Works Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr10-apr16.html\" rel=\"prev\" title=\"Top KDnuggets tweets, Apr 10\u201316: Math for Programmers teaches you the #math you need to know; The Third Wave Data Scientist \u2013 what skills are required?\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/distributed-artificial-intelligence-multi-agent-systems-agent-based-modeling-swarm-intelligence.html\" rel=\"next\" title=\"Distributed Artificial Intelligence: A primer on Multi-Agent Systems, Agent-Based Modeling, and Swarm Intelligence\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2019/04/how-optimization-works.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=92914\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/how-optimization-works.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-92914 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 18-Apr, 2019  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/index.html\">Apr</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/tutorials.html\">Tutorials, Overviews</a> \u00bb How Optimization Works (\u00a0<a href=\"/2019/n16.html\">19:n16</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">How Optimization Works</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr10-apr16.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/04/distributed-artificial-intelligence-multi-agent-systems-agent-based-modeling-swarm-intelligence.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/data-science\" rel=\"tag\">Data Science</a>, <a href=\"https://www.kdnuggets.com/tag/data-scientist\" rel=\"tag\">Data Scientist</a>, <a href=\"https://www.kdnuggets.com/tag/gradient-descent\" rel=\"tag\">Gradient Descent</a>, <a href=\"https://www.kdnuggets.com/tag/optimization\" rel=\"tag\">Optimization</a>, <a href=\"https://www.kdnuggets.com/tag/prescriptive-analytics\" rel=\"tag\">Prescriptive Analytics</a></div>\n<br/>\n<p class=\"excerpt\">\n     Optimization problems are naturally described in terms of costs - money, time, resources - rather than benefits. In math it's convenient to make all your problems look the same before you work out a solution, so that you can just solve it the one time. \n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/brandon-rohrer\" rel=\"author\" title=\"Posts by Brandon Rohrer\">Brandon Rohrer</a>, iRobot.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><a href=\"https://end-to-end-machine-learning.teachable.com/p/building-blocks-how-optimization-works/\" rel=\"noopener noreferrer\" target=\"_blank\">Check out the full course content for How Optimization Works</a>, including video, slides, and code.</p>\n<p>Optimization is a fancy word for \"finding the best way.\" We can see how it works if we take a closer look at drinking tea.</p>\n<p>There is a best temperature for tea. If your tea is too hot, it will scald your tongue and you won't be able to taste anything for three days. If it\u2019s lukewarm, it\u2019s entirely unsatisfying. There is a sweet spot in the middle where it is comfortably hot, warming you from the inside out all the way down your throat and radiating through your belly. This is the ideal temperature for tea.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/ideal_temperature_for_tea.png\" width=\"75%\"/></div>\n<p>This happy medium is what we try to find in optimization. That\u2019s what Goldilocks was looking for when she tried Papa Bear's bed and found it too hard, tried Mama Bear's bed and found it too soft, then tried Baby Bear's bed and found it to be just right.</p>\n<p>Finding how to get things just right turns out to be a very common problem. Mathematicians and computer scientist love it because it\u2019s very specific and well formulated. You know when you\u2019ve got it right, and you can compare your solution against others to see who got it right faster.</p>\n<p>When a computer scientist tries to find the right temperature for tea, the first thing they do is flip the problem upside down. Instead of trying to maximize tea drinking enjoyment, they try to minimize suffering while drinking tea. The result is the same, and the math works out in the same way. It's not that all computer scientists are pessimists, it's just that most optimization problems are naturally described in terms of costs - money, time, resources - rather than benefits. In math it's convenient to make all your problems look the same before you work out a solution, so that you can just solve it the one time.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/ideal_temperature_for_tea2.png\" width=\"75%\"/></div>\n<p>In machine learning, this cost is often called an error function, because error is the undesirable thing, the suffering, that is being minimized. It can also be called a cost function, a loss function, or an energy function. They all mean pretty much the same thing.</p>\n<p>\u00a0</p>\n<h3>Exhaustive search</h3>\n<p>\u00a0<br>\nThere are a handful of ways to go about finding the best temperature for serving tea. The most obvious is just to look at the curve and pick the lowest point. Unfortunately, we don't actually know what the curve is when we start out. That is implicit in the optimization problem.</br></p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/exhaustive-search.png\" width=\"75%\"/></div>\n<p>But we can make use of our original idea and just measure the curve. We can prepare a cup of tea at a given temperature, serve it, and ask our unwitting test subject how they enjoyed it. Then we can repeat this process for every temperature across the whole range we care about. By the time we're done with this, we do know what the whole curve looks like, and then we can just pick temperature for which our tea drinker reported the most enjoyment, that is, the least suffering.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://www.kdnuggets.com/wp-content/uploads/exhaustive-search2.png\" width=\"75%\"/></div>\n<p>This way of finding the best tea temperature is called exhaustive search. It is straightforward and effective, but may take a while. If our time is limited, it's worth it to check out a few other methods.</p>\n<p>\u00a0</p>\n<h3>Gradient descent</h3>\n<p>\u00a0<br>\nIf you imagine that our tea-suffering curve is actually a physical bowl, then we could easily find the bottom by dropping a marble in and letting it roll until it stops. This is the intuition behind gradient descent - literally \"going downhill\".</br></p>\n<p>To use gradient descent we start at an arbitrary temperature. Before beginning, we don't know anything about our curve, so we make a random guess. We brew a cup of tea at that temperature and see how well our tea drinker likes it.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent\" src=\"/wp-content/uploads/gradient-descent.png\" width=\"75%\"/></div>\n<p>From there, the next trick is to figure out which direction is downhill and which is up. To figure this out, we choose a direction, and choose a new temperature a very small distance away. Let's say we choose a temperature to the left. Then we brew up another cup of tea at this slightly cooler temperature and see whether or not it is better than the first. We discover that it is actually inferior. Now we know that \"downhill\" is to the right - that we need to make our next cup warmer to make it better.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent2\" src=\"https://www.kdnuggets.com/wp-content/uploads/gradient-descent2.png\" width=\"75%\"/></div>\n<p>We take a larger step in the direction of warmer tea, brew up a new cup, and start the process over again.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent\" src=\"/wp-content/uploads/gradient-descent3.png\" width=\"75%\"/></div>\n<p>We repeat this until we get to the very best temperature for tea.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent\" src=\"https://www.kdnuggets.com/wp-content/uploads/gradient-descent4.png\" width=\"75%\"/></div>\n<p>The steeper the slope, the larger the step we can take. The shallower the slope, the smaller the step.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent\" src=\"/wp-content/uploads/gradient-descent5.png\" width=\"75%\"/></div>\n<p>We will know we are all done when we take a small step away and get the exact same level of enjoyment from our tea drinker. This can only happen at the bottom of the bowl, where it is flat and there is no downhill.</p>\n<p>There are lots of gradient descent methods. Most of them are clever ways to measure the slope as efficiently as possible and to get to the bottom of the bowl in as few steps as possible - to brew as few cups of tea as we can get away with. They use different tricks to avoid completely calculating the slope or to choose a step size that is as large as can be gotten away with, but the underlying intuition is the same.</p>\n<p>\u00a0</p>\n<h3>Including curvature</h3>\n<p>\u00a0<br>\nOne of the tricks to find the bottom in the bowl in fewer steps is to use not just slope, but also curvature, when deciding how big of a step to take. As the marble starts to roll down the side of the bowl, is the slope getting steeper?</br></p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent-curvature\" src=\"/wp-content/uploads/gradient-descent-curvature.png\" width=\"75%\"/></div>\n<p>If so, then the bottom is probably still far away. Take a big step.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent-curvature\" src=\"/wp-content/uploads/gradient-descent-curvature1.png\" width=\"75%\"/></div>\n<p>Or is the slope getting shallower and starting to bottom out?</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent-curvature\" src=\"/wp-content/uploads/gradient-descent-curvature2.png\" width=\"75%\"/></div>\n<p>If so, the bottom is probably getting closer. Take smaller steps now.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent-curvature\" src=\"/wp-content/uploads/gradient-descent-curvature3.png\" width=\"75%\"/></div>\n<p>Curvature, this slope-of-the-slope or Hessian, to give it its rightful name, can be very helpful if you are trying to take as few steps as possible, however it can also be much more expensive to compute. This is a trade-off that comes up a lot in optimization. We end up choosing between the number of steps we have to take and how hard it is to compute where the next step should be.</p>\n<p>\u00a0</p>\n<h3>How gradient descent can break</h3>\n<p>\u00a0<br>\nLike a lot of math problems, the more assumptions you\u2019re able to make, the better the solution you can come up with. Unfortunately, when working with real data, those assumptions don\u2019t always apply.</br></p>\n<p>There are a lot of ways that this drop-a-marble approach can fail. If there is more than one valley for a marble to roll into, we might miss the deepest one. Each of these little bowls is called a local minimum. We are interested in finding the global minimum, the deepest of all the bowls. Imagine that we are testing our tea temperatures on a hot day. It may be that once tea becomes cold enough, it makes a great iced tea, which is even more popular. We would never find that out by gradient descent alone.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://www.kdnuggets.com/wp-content/uploads/tea-drinking-temperature.png\" width=\"75%\"/></div>\n<p>If the error function is not smooth, there are lots of places a marble could get stuck. This could happen if our tea drinkers' enjoyment was heavily impacted by passing trains. The periodic occurrence of trains could introduce a wiggle into our data.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://www.kdnuggets.com/wp-content/uploads/tea-drinking-temperature1.png\" width=\"75%\"/></div>\n<p>If the error function you are trying to optimize makes discrete jumps, that presents a challenge. Marbles don't roll down stairs well. This could happen if our tea drinkers have to rate their enjoyment on a 10-point scale.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/tea-drinking-temperature2.png\" width=\"75%\"/></div>\n<p>If the error function is mostly a plateau, but has a bottom that is narrow and deep, then the marble is unlikely to find it. Perhaps our tea drinkers absolutely despise all tea that is anything but perfect.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/tea-drinking-temperature3.png\" width=\"75%\"/></div>\n<p>All of these occur in real machine learning optimization problems.</p>\n<p>\u00a0</p>\n<h3>Robust methods</h3>\n<p>\u00a0<br/>\nIf we suspect that our tea satisfaction curve has any of these tricky characteristics, we can always fall back to exhaustive search. Unfortunately, exhaustive search takes an extremely long time for a lot of problems. Luckily for us, there is a middle ground. There is a set of methods that is tougher than gradient descent. They go by names like genetic algorithms, evolutionary algorithms, and simulated annealing. They take longer to compute than gradient descent, and they take more steps, but they don't break nearly so easily. Each has its own quirks, but one characteristic that most of them share is a randomness to their steps and jumps. This helps them discover the deepest valleys of the error function, even when they are difficult to find.</p>\n<p>Optimization algorithms that rely gradient descent are like Formula One race cars. They are extremely fast and efficient, but require a very well behaved track (error function) to work well. A poorly-placed speed bump could wreck it. The more robust methods are like four-wheel-drive pickup trucks. They don't go nearly as fast, but they can handle a lot more variability in the terrain. And exhaustive search is like traveling on foot. You can get absolutely anywhere, but it may take you <em>really</em> long time. They are each invaluable in different situations.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://www.kdnuggets.com/wp-content/uploads/assumptions-performance.png\" width=\"99%\"/></div>\n<p>\u00a0<br/>\n<a href=\"https://brohrer.github.io/how_optimization_works_1.html\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/08/optimization-101-data-scientists.html\">Optimization 101 for Data Scientists</a>\n<li><a href=\"/2018/05/optimization-using-r.html\">Optimization Using R</a>\n<li><a href=\"/2018/06/intuitive-introduction-gradient-descent.html\">An Intuitive Introduction to Gradient Descent</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr10-apr16.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/04/distributed-artificial-intelligence-multi-agent-systems-agent-based-modeling-swarm-intelligence.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/index.html\">Apr</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/tutorials.html\">Tutorials, Overviews</a> \u00bb How Optimization Works (\u00a0<a href=\"/2019/n16.html\">19:n16</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556324533\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.761 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 20:22:13 -->\n<!-- Compression = gzip -->", "content_tokenized": ["brandon", "rohrer", "irobot", "comment", "check", "out", "the", "full", "cours", "content", "for", "how", "optim", "work", "includ", "video", "slide", "and", "code", "optim", "fanci", "word", "for", "find", "the", "best", "way", "can", "see", "how", "work", "take", "closer", "look", "drink", "tea", "there", "best", "temperatur", "for", "tea", "tea", "too", "hot", "will", "scald", "tongu", "and", "abl", "tast", "anyth", "for", "three", "day", "lukewarm", "entir", "unsatisfi", "there", "sweet", "spot", "the", "middl", "where", "comfort", "hot", "warm", "from", "the", "insid", "out", "all", "the", "way", "down", "throat", "and", "radiat", "through", "belli", "this", "the", "ideal", "temperatur", "for", "tea", "this", "happi", "medium", "what", "tri", "find", "optim", "that", "what", "goldilock", "look", "for", "when", "she", "tri", "papa", "bear", "bed", "and", "found", "too", "hard", "tri", "mama", "bear", "bed", "and", "found", "too", "soft", "then", "tri", "babi", "bear", "bed", "and", "found", "just", "right", "find", "how", "get", "thing", "just", "right", "turn", "out", "veri", "common", "problem", "mathematician", "and", "comput", "scientist", "love", "becaus", "veri", "specif", "and", "well", "formul", "know", "when", "got", "right", "and", "can", "compar", "solut", "against", "other", "see", "who", "got", "right", "faster", "when", "comput", "scientist", "tri", "find", "the", "right", "temperatur", "for", "tea", "the", "first", "thing", "they", "flip", "the", "problem", "upsid", "down", "instead", "tri", "maxim", "tea", "drink", "enjoy", "they", "tri", "minim", "suffer", "while", "drink", "tea", "the", "result", "the", "same", "and", "the", "math", "work", "out", "the", "same", "way", "not", "that", "all", "comput", "scientist", "are", "pessimist", "just", "that", "most", "optim", "problem", "are", "natur", "describ", "term", "cost", "money", "time", "resourc", "rather", "than", "benefit", "math", "conveni", "make", "all", "problem", "look", "the", "same", "befor", "work", "out", "solut", "that", "can", "just", "solv", "the", "one", "time", "machin", "learn", "this", "cost", "often", "call", "error", "function", "becaus", "error", "the", "undesir", "thing", "the", "suffer", "that", "minim", "can", "also", "call", "cost", "function", "loss", "function", "energi", "function", "they", "all", "mean", "pretti", "much", "the", "same", "thing", "exhaust", "search", "there", "are", "hand", "way", "about", "find", "the", "best", "temperatur", "for", "serv", "tea", "the", "most", "obvious", "just", "look", "the", "curv", "and", "pick", "the", "lowest", "point", "unfortun", "actual", "know", "what", "the", "curv", "when", "start", "out", "that", "implicit", "the", "optim", "problem", "but", "can", "make", "use", "our", "origin", "idea", "and", "just", "measur", "the", "curv", "can", "prepar", "cup", "tea", "given", "temperatur", "serv", "and", "ask", "our", "unwit", "test", "subject", "how", "they", "enjoy", "then", "can", "repeat", "this", "process", "for", "everi", "temperatur", "across", "the", "whole", "rang", "care", "about", "the", "time", "done", "with", "this", "know", "what", "the", "whole", "curv", "look", "like", "and", "then", "can", "just", "pick", "temperatur", "for", "which", "our", "tea", "drinker", "report", "the", "most", "enjoy", "that", "the", "least", "suffer", "this", "way", "find", "the", "best", "tea", "temperatur", "call", "exhaust", "search", "straightforward", "and", "effect", "but", "may", "take", "while", "our", "time", "limit", "worth", "check", "out", "few", "other", "method", "gradient", "descent", "imagin", "that", "our", "teasuff", "curv", "actual", "physic", "bowl", "then", "could", "easili", "find", "the", "bottom", "drop", "marbl", "and", "let", "roll", "until", "stop", "this", "the", "intuit", "behind", "gradient", "descent", "liter", "downhil", "use", "gradient", "descent", "start", "arbitrari", "temperatur", "befor", "begin", "know", "anyth", "about", "our", "curv", "make", "random", "guess", "brew", "cup", "tea", "that", "temperatur", "and", "see", "how", "well", "our", "tea", "drinker", "like", "from", "there", "the", "next", "trick", "figur", "out", "which", "direct", "downhil", "and", "which", "figur", "this", "out", "choos", "direct", "and", "choos", "new", "temperatur", "veri", "small", "distanc", "away", "let", "say", "choos", "temperatur", "the", "left", "then", "brew", "anoth", "cup", "tea", "this", "slight", "cooler", "temperatur", "and", "see", "whether", "not", "better", "than", "the", "first", "discov", "that", "actual", "inferior", "now", "know", "that", "downhil", "the", "right", "that", "need", "make", "our", "next", "cup", "warmer", "make", "better", "take", "larger", "step", "the", "direct", "warmer", "tea", "brew", "new", "cup", "and", "start", "the", "process", "over", "again", "repeat", "this", "until", "get", "the", "veri", "best", "temperatur", "for", "tea", "the", "steeper", "the", "slope", "the", "larger", "the", "step", "can", "take", "the", "shallow", "the", "slope", "the", "smaller", "the", "step", "will", "know", "are", "all", "done", "when", "take", "small", "step", "away", "and", "get", "the", "exact", "same", "level", "enjoy", "from", "our", "tea", "drinker", "this", "can", "onli", "happen", "the", "bottom", "the", "bowl", "where", "flat", "and", "there", "downhil", "there", "are", "lot", "gradient", "descent", "method", "most", "them", "are", "clever", "way", "measur", "the", "slope", "effici", "possibl", "and", "get", "the", "bottom", "the", "bowl", "few", "step", "possibl", "brew", "few", "cup", "tea", "can", "get", "away", "with", "they", "use", "differ", "trick", "avoid", "complet", "calcul", "the", "slope", "choos", "step", "size", "that", "larg", "can", "gotten", "away", "with", "but", "the", "under", "intuit", "the", "same", "includ", "curvatur", "one", "the", "trick", "find", "the", "bottom", "the", "bowl", "fewer", "step", "use", "not", "just", "slope", "but", "also", "curvatur", "when", "decid", "how", "big", "step", "take", "the", "marbl", "start", "roll", "down", "the", "side", "the", "bowl", "the", "slope", "get", "steeper", "then", "the", "bottom", "probabl", "still", "far", "away", "take", "big", "step", "the", "slope", "get", "shallow", "and", "start", "bottom", "out", "the", "bottom", "probabl", "get", "closer", "take", "smaller", "step", "now", "curvatur", "this", "slopeoftheslop", "hessian", "give", "right", "name", "can", "veri", "help", "are", "tri", "take", "few", "step", "possibl", "howev", "can", "also", "much", "more", "expens", "comput", "this", "tradeoff", "that", "come", "lot", "optim", "end", "choos", "between", "the", "number", "step", "have", "take", "and", "how", "hard", "comput", "where", "the", "next", "step", "should", "how", "gradient", "descent", "can", "break", "like", "lot", "math", "problem", "the", "more", "assumpt", "abl", "make", "the", "better", "the", "solut", "can", "come", "with", "unfortun", "when", "work", "with", "real", "data", "those", "assumpt", "alway", "appli", "there", "are", "lot", "way", "that", "this", "dropamarbl", "approach", "can", "fail", "there", "more", "than", "one", "valley", "for", "marbl", "roll", "into", "might", "miss", "the", "deepest", "one", "each", "these", "littl", "bowl", "call", "local", "minimum", "are", "interest", "find", "the", "global", "minimum", "the", "deepest", "all", "the", "bowl", "imagin", "that", "are", "test", "our", "tea", "temperatur", "hot", "day", "may", "that", "onc", "tea", "becom", "cold", "enough", "make", "great", "ice", "tea", "which", "even", "more", "popular", "would", "never", "find", "that", "out", "gradient", "descent", "alon", "the", "error", "function", "not", "smooth", "there", "are", "lot", "place", "marbl", "could", "get", "stuck", "this", "could", "happen", "our", "tea", "drinker", "enjoy", "heavili", "impact", "pass", "train", "the", "period", "occurr", "train", "could", "introduc", "wiggl", "into", "our", "data", "the", "error", "function", "are", "tri", "optim", "make", "discret", "jump", "that", "present", "challeng", "marbl", "roll", "down", "stair", "well", "this", "could", "happen", "our", "tea", "drinker", "have", "rate", "their", "enjoy", "numpoint", "scale", "the", "error", "function", "most", "plateau", "but", "has", "bottom", "that", "narrow", "and", "deep", "then", "the", "marbl", "unlik", "find", "perhap", "our", "tea", "drinker", "absolut", "despis", "all", "tea", "that", "anyth", "but", "perfect", "all", "these", "occur", "real", "machin", "learn", "optim", "problem", "robust", "method", "suspect", "that", "our", "tea", "satisfact", "curv", "has", "ani", "these", "tricki", "characterist", "can", "alway", "fall", "back", "exhaust", "search", "unfortun", "exhaust", "search", "take", "extrem", "long", "time", "for", "lot", "problem", "luckili", "for", "there", "middl", "ground", "there", "set", "method", "that", "tougher", "than", "gradient", "descent", "they", "name", "like", "genet", "algorithm", "evolutionari", "algorithm", "and", "simul", "anneal", "they", "take", "longer", "comput", "than", "gradient", "descent", "and", "they", "take", "more", "step", "but", "they", "break", "near", "easili", "each", "has", "own", "quirk", "but", "one", "characterist", "that", "most", "them", "share", "random", "their", "step", "and", "jump", "this", "help", "them", "discov", "the", "deepest", "valley", "the", "error", "function", "even", "when", "they", "are", "difficult", "find", "optim", "algorithm", "that", "reli", "gradient", "descent", "are", "like", "formula", "one", "race", "car", "they", "are", "extrem", "fast", "and", "effici", "but", "requir", "veri", "well", "behav", "track", "error", "function", "work", "well", "poorlyplac", "speed", "bump", "could", "wreck", "the", "more", "robust", "method", "are", "like", "fourwheeldr", "pickup", "truck", "they", "near", "fast", "but", "they", "can", "handl", "lot", "more", "variabl", "the", "terrain", "and", "exhaust", "search", "like", "travel", "foot", "can", "get", "absolut", "anywher", "but", "may", "take", "realli", "long", "time", "they", "are", "each", "invalu", "differ", "situat", "origin", "repost", "with", "permiss", "relat", "optim", "num", "for", "data", "scientist", "optim", "use", "intuit", "introduct", "gradient", "descent"], "timestamp_scraper": 1556362666.774206, "title": "How Optimization Works", "read_time": 478.79999999999995, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/brandon-rohrer\" rel=\"author\" title=\"Posts by Brandon Rohrer\">Brandon Rohrer</a>, iRobot.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><a href=\"https://end-to-end-machine-learning.teachable.com/p/building-blocks-how-optimization-works/\" rel=\"noopener noreferrer\" target=\"_blank\">Check out the full course content for How Optimization Works</a>, including video, slides, and code.</p>\n<p>Optimization is a fancy word for \"finding the best way.\" We can see how it works if we take a closer look at drinking tea.</p>\n<p>There is a best temperature for tea. If your tea is too hot, it will scald your tongue and you won't be able to taste anything for three days. If it\u2019s lukewarm, it\u2019s entirely unsatisfying. There is a sweet spot in the middle where it is comfortably hot, warming you from the inside out all the way down your throat and radiating through your belly. This is the ideal temperature for tea.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/ideal_temperature_for_tea.png\" width=\"75%\"/></div>\n<p>This happy medium is what we try to find in optimization. That\u2019s what Goldilocks was looking for when she tried Papa Bear's bed and found it too hard, tried Mama Bear's bed and found it too soft, then tried Baby Bear's bed and found it to be just right.</p>\n<p>Finding how to get things just right turns out to be a very common problem. Mathematicians and computer scientist love it because it\u2019s very specific and well formulated. You know when you\u2019ve got it right, and you can compare your solution against others to see who got it right faster.</p>\n<p>When a computer scientist tries to find the right temperature for tea, the first thing they do is flip the problem upside down. Instead of trying to maximize tea drinking enjoyment, they try to minimize suffering while drinking tea. The result is the same, and the math works out in the same way. It's not that all computer scientists are pessimists, it's just that most optimization problems are naturally described in terms of costs - money, time, resources - rather than benefits. In math it's convenient to make all your problems look the same before you work out a solution, so that you can just solve it the one time.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/ideal_temperature_for_tea2.png\" width=\"75%\"/></div>\n<p>In machine learning, this cost is often called an error function, because error is the undesirable thing, the suffering, that is being minimized. It can also be called a cost function, a loss function, or an energy function. They all mean pretty much the same thing.</p>\n<p>\u00a0</p>\n<h3>Exhaustive search</h3>\n<p>\u00a0<br>\nThere are a handful of ways to go about finding the best temperature for serving tea. The most obvious is just to look at the curve and pick the lowest point. Unfortunately, we don't actually know what the curve is when we start out. That is implicit in the optimization problem.</br></p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/exhaustive-search.png\" width=\"75%\"/></div>\n<p>But we can make use of our original idea and just measure the curve. We can prepare a cup of tea at a given temperature, serve it, and ask our unwitting test subject how they enjoyed it. Then we can repeat this process for every temperature across the whole range we care about. By the time we're done with this, we do know what the whole curve looks like, and then we can just pick temperature for which our tea drinker reported the most enjoyment, that is, the least suffering.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://www.kdnuggets.com/wp-content/uploads/exhaustive-search2.png\" width=\"75%\"/></div>\n<p>This way of finding the best tea temperature is called exhaustive search. It is straightforward and effective, but may take a while. If our time is limited, it's worth it to check out a few other methods.</p>\n<p>\u00a0</p>\n<h3>Gradient descent</h3>\n<p>\u00a0<br>\nIf you imagine that our tea-suffering curve is actually a physical bowl, then we could easily find the bottom by dropping a marble in and letting it roll until it stops. This is the intuition behind gradient descent - literally \"going downhill\".</br></p>\n<p>To use gradient descent we start at an arbitrary temperature. Before beginning, we don't know anything about our curve, so we make a random guess. We brew a cup of tea at that temperature and see how well our tea drinker likes it.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent\" src=\"/wp-content/uploads/gradient-descent.png\" width=\"75%\"/></div>\n<p>From there, the next trick is to figure out which direction is downhill and which is up. To figure this out, we choose a direction, and choose a new temperature a very small distance away. Let's say we choose a temperature to the left. Then we brew up another cup of tea at this slightly cooler temperature and see whether or not it is better than the first. We discover that it is actually inferior. Now we know that \"downhill\" is to the right - that we need to make our next cup warmer to make it better.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent2\" src=\"https://www.kdnuggets.com/wp-content/uploads/gradient-descent2.png\" width=\"75%\"/></div>\n<p>We take a larger step in the direction of warmer tea, brew up a new cup, and start the process over again.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent\" src=\"/wp-content/uploads/gradient-descent3.png\" width=\"75%\"/></div>\n<p>We repeat this until we get to the very best temperature for tea.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent\" src=\"https://www.kdnuggets.com/wp-content/uploads/gradient-descent4.png\" width=\"75%\"/></div>\n<p>The steeper the slope, the larger the step we can take. The shallower the slope, the smaller the step.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent\" src=\"/wp-content/uploads/gradient-descent5.png\" width=\"75%\"/></div>\n<p>We will know we are all done when we take a small step away and get the exact same level of enjoyment from our tea drinker. This can only happen at the bottom of the bowl, where it is flat and there is no downhill.</p>\n<p>There are lots of gradient descent methods. Most of them are clever ways to measure the slope as efficiently as possible and to get to the bottom of the bowl in as few steps as possible - to brew as few cups of tea as we can get away with. They use different tricks to avoid completely calculating the slope or to choose a step size that is as large as can be gotten away with, but the underlying intuition is the same.</p>\n<p>\u00a0</p>\n<h3>Including curvature</h3>\n<p>\u00a0<br>\nOne of the tricks to find the bottom in the bowl in fewer steps is to use not just slope, but also curvature, when deciding how big of a step to take. As the marble starts to roll down the side of the bowl, is the slope getting steeper?</br></p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent-curvature\" src=\"/wp-content/uploads/gradient-descent-curvature.png\" width=\"75%\"/></div>\n<p>If so, then the bottom is probably still far away. Take a big step.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent-curvature\" src=\"/wp-content/uploads/gradient-descent-curvature1.png\" width=\"75%\"/></div>\n<p>Or is the slope getting shallower and starting to bottom out?</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent-curvature\" src=\"/wp-content/uploads/gradient-descent-curvature2.png\" width=\"75%\"/></div>\n<p>If so, the bottom is probably getting closer. Take smaller steps now.</p>\n<div style=\"text-align:center\"><img alt=\"gradient-descent-curvature\" src=\"/wp-content/uploads/gradient-descent-curvature3.png\" width=\"75%\"/></div>\n<p>Curvature, this slope-of-the-slope or Hessian, to give it its rightful name, can be very helpful if you are trying to take as few steps as possible, however it can also be much more expensive to compute. This is a trade-off that comes up a lot in optimization. We end up choosing between the number of steps we have to take and how hard it is to compute where the next step should be.</p>\n<p>\u00a0</p>\n<h3>How gradient descent can break</h3>\n<p>\u00a0<br>\nLike a lot of math problems, the more assumptions you\u2019re able to make, the better the solution you can come up with. Unfortunately, when working with real data, those assumptions don\u2019t always apply.</br></p>\n<p>There are a lot of ways that this drop-a-marble approach can fail. If there is more than one valley for a marble to roll into, we might miss the deepest one. Each of these little bowls is called a local minimum. We are interested in finding the global minimum, the deepest of all the bowls. Imagine that we are testing our tea temperatures on a hot day. It may be that once tea becomes cold enough, it makes a great iced tea, which is even more popular. We would never find that out by gradient descent alone.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://www.kdnuggets.com/wp-content/uploads/tea-drinking-temperature.png\" width=\"75%\"/></div>\n<p>If the error function is not smooth, there are lots of places a marble could get stuck. This could happen if our tea drinkers' enjoyment was heavily impacted by passing trains. The periodic occurrence of trains could introduce a wiggle into our data.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://www.kdnuggets.com/wp-content/uploads/tea-drinking-temperature1.png\" width=\"75%\"/></div>\n<p>If the error function you are trying to optimize makes discrete jumps, that presents a challenge. Marbles don't roll down stairs well. This could happen if our tea drinkers have to rate their enjoyment on a 10-point scale.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/tea-drinking-temperature2.png\" width=\"75%\"/></div>\n<p>If the error function is mostly a plateau, but has a bottom that is narrow and deep, then the marble is unlikely to find it. Perhaps our tea drinkers absolutely despise all tea that is anything but perfect.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"/wp-content/uploads/tea-drinking-temperature3.png\" width=\"75%\"/></div>\n<p>All of these occur in real machine learning optimization problems.</p>\n<p>\u00a0</p>\n<h3>Robust methods</h3>\n<p>\u00a0<br/>\nIf we suspect that our tea satisfaction curve has any of these tricky characteristics, we can always fall back to exhaustive search. Unfortunately, exhaustive search takes an extremely long time for a lot of problems. Luckily for us, there is a middle ground. There is a set of methods that is tougher than gradient descent. They go by names like genetic algorithms, evolutionary algorithms, and simulated annealing. They take longer to compute than gradient descent, and they take more steps, but they don't break nearly so easily. Each has its own quirks, but one characteristic that most of them share is a randomness to their steps and jumps. This helps them discover the deepest valleys of the error function, even when they are difficult to find.</p>\n<p>Optimization algorithms that rely gradient descent are like Formula One race cars. They are extremely fast and efficient, but require a very well behaved track (error function) to work well. A poorly-placed speed bump could wreck it. The more robust methods are like four-wheel-drive pickup trucks. They don't go nearly as fast, but they can handle a lot more variability in the terrain. And exhaustive search is like traveling on foot. You can get absolutely anywhere, but it may take you <em>really</em> long time. They are each invaluable in different situations.</p>\n<div style=\"text-align:center\"><img alt=\"figure-name\" src=\"https://www.kdnuggets.com/wp-content/uploads/assumptions-performance.png\" width=\"99%\"/></div>\n<p>\u00a0<br/>\n<a href=\"https://brohrer.github.io/how_optimization_works_1.html\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/08/optimization-101-data-scientists.html\">Optimization 101 for Data Scientists</a>\n<li><a href=\"/2018/05/optimization-using-r.html\">Optimization Using R</a>\n<li><a href=\"/2018/06/intuitive-introduction-gradient-descent.html\">An Intuitive Introduction to Gradient Descent</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}