{"content": "By Shweta Bhatt , Youplus. comments Reinforcement Learning is one of the hottest research topics currently and its popularity is only growing day by day. Let\u2019s look at 5 useful things to know about RL. What is reinforcement learning? How does it relate with other ML techniques? Reinforcement  is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences. Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning where feedback provided to the agent is correct set of actions for performing a task, reinforcement learning uses rewards and punishment as signals for positive and negative behavior. As compared to unsupervised learning, reinforcement learning is different in terms of goals. While the goal in unsupervised learning is to find similarities and differences between data points, in reinforcement learning the goal is to find a suitable action model that would maximize the total cumulative reward of the agent. The figure below represents the basic idea and elements involved in a reinforcement learning model. \u00a0 Figure 1 \u00a0 How to formulate a basic reinforcement Learning problem? \u00a0 Some key terms that describe the elements of a RL problem are: \u00a0 Environment : Physical world in which the agent operates State : Current situation of the agent Reward : Feedback from the environment Policy : Method to map agent\u2019s state to actions Value : Future reward that an agent would receive by taking an action in a particular state A Reinforcement Learning problem can be best explained through games. Let\u2019s take the game of PacMan where the goal of the agent (PacMan) is to eat the food in the grid while avoiding the ghosts on its way. The grid world is the interactive environment for the agent. PacMan receives a reward for eating food and punishment if it gets killed by the ghost (loses the game). The states are the location of PacMan in the grid world and the total cumulative reward is PacMan winning the game. In order to build an optimal policy, the agent faces the dilemma of exploring new states while maximizing its reward at the same time. This is called Exploration vs Exploitation trade-off . Markov Decision Processes (MDPs) are mathematical frameworks to describe an environment in reinforcement learning and almost all RL problems can be formalized using MDPs. \u00a0An MDP consists of a set of finite environment states S, a set of possible actions  in each state, a real valued reward function  and a transition model . However, real world environments are more likely to lack any prior knowledge of environment dynamics. Model-free RL methods come handy in such cases. Q-learning is a commonly used model free approach which can be used for building a self-playing PacMan agent. It revolves around the notion of updating Q values which denotes value of doing action a in state s . The value update rule is the core of the Q-learning algorithm. \u00a0 Figure 2: Reinforcement Learning Update Rule \u00a0 Figure 3: PacMan \u00a0 Here\u2019s a video of a Deep reinforcement learning PacMan agent \u00a0 What are some most used Reinforcement Learning algorithms? \u00a0 Q-learning and SARSA (State-Action-Reward-State-Action) are two commonly used model-free RL algorithms. They differ in terms of their exploration strategies while their exploitation strategies are similar. While Q-learning is an off-policy method in which the agent learns the value based on action a* derived from the another policy, SARSA is an on-policy method where it learns the value based on its current action a derived from its current policy. These two methods are simple to implement but lack generality as they do not have the ability to estimate values for unseen states. This can be overcome by more advanced algorithms such as Deep Q-Networks which use Neural Networks to estimate Q-values. But DQNs can only handle discrete, low-dimensional action spaces.  is a model-free, off-policy, actor-critic algorithm that tackles this problem by learning policies in high dimensional, continuous action spaces. \u00a0 Figure 4: actor-critic architecture for Reinforcement Learning \u00a0 What are the practical applications of Reinforcement Learning? Since, RL requires a lot of data, therefore it is most applicable in domains where simulated data is readily available like gameplay, robotics. RL is quite widely used in building AI for playing computer games. AlphaGo Zero is the first computer program to defeat a world champion in the ancient Chinese game of Go. Others include ATARI games, Backgammon, etc In robotics and industrial automation,RL is used to enable the robot to create an efficient adaptive control system for itself which learns from its own experience and behavior. DeepMind\u2019s work on Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Policy updates is a good example of the same. Watch this interesting demonstration video . Other applications of RL include text summarization engines, dialog agents (text, speech) which can learn from user interactions and improve with time, learning optimal treatment policies in healthcare and RL based agents for online stock trading. \u00a0 How can I get started with Reinforcement Learning? For understanding the basic concepts of RL, Reinforcement Learning-An Introduction , a book by the father of Reinforcement Learning- Richard Sutton and his doctoral advisor Andrew Barto . An online draft of the book is available here /book/the-book-2nd.html Teaching material from David Silver including video lectures is a great introductory course on RL Here\u2019s another technical tutorial on RL by Pieter Abbeel and John Schulman (Open AI/ Berkeley AI Research Lab). For getting started with building and testing RL agents, This blog on how to train a Neural Network ATARI Pong agent with Policy Gradients from raw pixels by Andrej Karpathy will help you get your first Deep Reinforcement Learning agent up and running in just 130 lines of Python code. DeepMind Lab is an open source 3D game-like platform created for agent-based AI research with rich simulated environments. Project Malmo is another AI experimentation platform for supporting fundamental research in AI. OpenAI gym is a toolkit for building and comparing reinforcement learning algorithms. Bio: Shweta Bhatt is AI researcher with experience in private and public sector, passionate about the impact and applications of deriving knowledge from data to solve challenging problems. She likes telling stories with data and is based in London. Related: Resurgence of AI During 1983-2010 Exclusive: Interview with Rich Sutton, the Father of Reinforcement Learning When reinforcement learning should not be used? Making Machine Learning Simple", "title_html": "<h1 id=\"title\">5 Things You Need to Know about Reinforcement Learning</h1> ", "url": "https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html", "tfidf": {"tfidf": {"real": 4.56206896552, "food": 5.922775601560001, "behavior": 11.05956112852, "hottest": 44.1, "unlik": 2.42529789184, "watch": 3.92581602374, "actorcrit": 3175.2, "signal": 5.12459651388, "wide": 1.5598349381, "new": 1.0178880554, "pieter": 105.139072848, "space": 4.79637462236, "grow": 2.27287043665, "would": 2.1657458563599996, "berkeley": 11.6735294118, "etc": 4.2066772655, "unseen": 40.8123393316, "function": 2.495441685, "david": 1.84970290108, "python": 56.2978723404, "transit": 3.36355932203, "unsupervis": 690.2608695660001, "resurg": 21.4251012146, "teach": 3.98594024605, "approach": 2.07556543339, "technic": 3.1400316455699997, "know": 2.59327017315, "their": 2.0309581681, "but": 2.03264835798, "particular": 1.3814827706200001, "abbeel": 1587.6, "creat": 2.4985835694, "summar": 15.1056137012, "thing": 2.4065484311099996, "pong": 226.8, "quit": 2.8849718335500003, "interest": 1.60331246213, "deep": 14.511882998159999, "will": 1.22481098596, "exploit": 11.58832116788, "concept": 2.65707112971, "open": 2.49113447356, "updat": 22.25867507888, "below": 2.25607503197, "perform": 1.5313977042500002, "healthcar": 18.7659574468, "demonstr": 2.64997496244, "feedback": 73.95652173900001, "all": 1.01146788991, "point": 1.25990000794, "applic": 13.70688538744, "lab": 28.8654545454, "robot": 80.0807061792, "have": 1.0148948411399998, "agentbas": 1587.6, "formul": 9.86086956522, "champion": 5.03680203046, "experi": 5.61187698834, "qvalu": 1587.6, "defeat": 2.87869446963, "model": 8.3623913616, "andrej": 369.209302326, "good": 1.51981619759, "toolkit": 189.0, "lowdimension": 1587.6, "cours": 2.15092805853, "optim": 23.0755813954, "malmo": 1587.6, "ghost": 22.5191489362, "involv": 1.4498630137000001, "youplus": 1587.6, "not": 2.03134796238, "stori": 2.02396736359, "deepmind": 3175.2, "advisor": 12.18419033, "gameplay": 97.3987730061, "handl": 3.9229058561900003, "almost": 1.53584212054, "though": 1.36076112111, "they": 2.06034650574, "anoth": 3.40930565496, "core": 4.623179965059999, "let": 6.97233201582, "world": 5.5670103093000005, "pixel": 86.28260869569999, "blog": 14.1876675603, "onpolici": 1587.6, "passion": 8.14571575167, "selfplay": 1587.6, "lack": 3.8543335761199997, "simpl": 6.7962328767199995, "that": 5.01992031875, "itself": 1.74557449148, "provid": 1.21552714187, "valu": 18.222094691520002, "lectur": 6.58481957694, "such": 2.12302754748, "explor": 10.18780748664, "with": 10.011982089899998, "network": 5.18738768176, "gym": 42.223404255300004, "deriv": 8.35139400315, "lot": 4.40877534018, "suitabl": 6.23811394892, "sourc": 1.69760479042, "should": 1.6643254009900001, "both": 1.05215720061, "locat": 1.59766529134, "punish": 12.283172147, "supervis": 15.48122866894, "receiv": 2.6109694926400002, "set": 3.56123822343, "futur": 1.8577112099200002, "openai": 1587.6, "two": 2.0275862069, "and": 26.00163779538, "from": 9.00510493473, "privat": 2.02396736359, "work": 1.11520089913, "num": 7.00220528007, "idea": 2.0930784443, "free": 1.71818181818, "environ": 30.92057996103, "for": 16.00504064016, "manipul": 9.145161290319999, "output": 7.676982591880001, "book": 2.86829268292, "handi": 102.425806452, "are": 9.26915342202, "current": 6.1303214596, "pacman": 12700.8, "look": 1.9086318826599997, "popular": 1.50769230769, "video": 9.89158878504, "modelfre": 4762.799999999999, "formal": 2.44622496148, "sutton": 66.2881002088, "asynchron": 165.375, "rich": 7.7652237711000005, "negat": 3.75852272727, "public": 1.22424429365, "posit": 1.37252528746, "karpathi": 1587.6, "eat": 15.01276595744, "general": 1.1218202374200001, "compar": 3.7324556247800005, "tackl": 19.8698372966, "revolv": 13.099009900999999, "markov": 174.46153846200002, "doe": 1.70581282905, "john": 1.35553278689, "bookthebooknumndhtml": 1587.6, "train": 1.9365698950999999, "simul": 22.9587852494, "avoid": 2.45986984816, "goal": 13.12608515916, "total": 3.0920245398799997, "framework": 8.200413223139998, "his": 1.0943682360200002, "continu": 1.13928955867, "type": 2.0281042411900003, "overcom": 8.38668779715, "reinforc": 154.88780487816, "machin": 8.04866920152, "prior": 2.17807655371, "through": 1.07074930869, "requir": 1.52844902282, "doctor": 3.9394540942900003, "face": 1.80327124035, "what": 3.7603031738399997, "tell": 3.36142282448, "map": 8.145715751660001, "help": 1.39962972759, "denot": 10.1965317919, "dialog": 71.8371040724, "line": 1.4182597820299998, "own": 2.35688836104, "base": 4.5851263538, "relat": 2.47501753838, "can": 8.23382973994, "here": 7.26923076924, "stock": 5.01611374408, "dqns": 1587.6, "comput": 7.855517070760001, "onlin": 5.210370856580001, "fundament": 5.32930513595, "about": 2.12972030318, "problem": 10.60048965054, "neural": 118.9213483146, "silver": 4.89697717458, "how": 6.41001312204, "just": 1.33580143037, "test": 2.65707112971, "adapt": 3.32272917539, "barto": 1587.6, "stateactionrewardstateact": 1587.6, "correct": 3.6631287494199998, "she": 2.16, "topic": 5.457545548300001, "dure": 1.0503473370799998, "dynam": 6.52527743527, "where": 4.26860254084, "notion": 7.356811862839999, "practic": 1.70434782609, "start": 2.53347163488, "run": 1.55692850838, "enabl": 7.0843373494, "explain": 2.60049140049, "order": 1.24625166811, "lose": 3.0851146521599997, "autom": 19.8202247191, "raw": 10.6478873239, "support": 1.2685577307200002, "play": 1.46390041494, "father": 4.5379448335, "the": 50.0, "build": 8.170869789000001, "kill": 2.09916699722, "algorithm": 167.7042253524, "learningan": 1587.6, "error": 6.04109589041, "consist": 1.4901445466499998, "same": 2.23715916296, "possibl": 1.4173734488, "industri": 2.02319357716, "best": 1.5828514456600002, "exampl": 1.50483412322, "trade": 2.37522441652, "interview": 3.3981164383599998, "introductori": 30.297709923699998, "knowledg": 6.7962328767199995, "take": 2.27923336444, "strategi": 8.88416340236, "code": 3.8807137619199996, "interact": 13.2557751183, "high": 1.14777327935, "basic": 8.190541702500001, "abil": 2.70875277256, "tutori": 59.4606741573, "qnetwork": 1587.6, "around": 1.21394708671, "treatment": 3.87125091441, "draft": 5.103182256509999, "materi": 2.13014893332, "task": 3.88641370869, "research": 9.710091743100001, "therefor": 2.33401940606, "polici": 20.23709369024, "zero": 8.75192943771, "challeng": 2.55816951337, "qlearn": 6350.4, "sector": 5.493425605540001, "schulman": 378.0, "system": 1.38739840951, "great": 1.26592775696, "howev": 1.0945191313299998, "trial": 4.04175152749, "win": 2.75290445639, "maxim": 25.856677524400002, "which": 7.036342915, "term": 4.18560506196, "estim": 4.6998223801, "other": 3.02977099236, "introduct": 2.7808723068799996, "techniqu": 7.458773784360001, "one": 1.00627495722, "impact": 2.97526236882, "some": 2.08073394496, "mathemat": 7.391061452510001, "backgammon": 453.6, "get": 7.1425036554, "like": 3.44755700325, "implement": 3.57648118946, "ancient": 3.38796414853, "chines": 4.270037654649999, "control": 1.46959178006, "dilemma": 27.9507042254, "agent": 76.65450643782, "process": 1.69524826482, "element": 4.72008324662, "includ": 3.0571923743399996, "rule": 3.4831066257199996, "grid": 54.369863013599996, "this": 5.0189681335500005, "figur": 10.171706816999999, "time": 2.02254920696, "finit": 28.1989342806, "engin": 2.47135740971, "similar": 2.75028150714, "project": 1.7534791252500002, "differ": 3.7096347067499997, "reward": 70.36454293632, "decis": 2.16, "most": 2.04192926046, "between": 2.06907337416, "readili": 12.5402843602, "richard": 2.20041580042, "domain": 9.39408284024, "gamelik": 1587.6, "shweta": 3175.2, "offpolici": 3175.2, "program": 2.02139037433, "physic": 2.39132399458, "gradient": 41.889182058, "solv": 7.26923076923, "case": 1.48498737256, "day": 2.36743215032, "architectur": 5.12790697674, "tradeoff": 208.89473684200001, "experiment": 6.07112810707, "call": 1.0676529926, "more": 2.0343413634, "improv": 2.04376930999, "london": 1.97782484116, "these": 1.07415426252, "bhatt": 756.0, "cumul": 70.4035476718, "action": 20.00412371133, "comment": 3.05954904606, "state": 9.429419916809998, "mdps": 3175.2, "platform": 12.466431095399999, "avail": 3.4576935642, "describ": 2.94054454528, "make": 1.0762660158600001, "way": 1.2190739461, "onli": 2.0512953033200003, "each": 1.18974820144, "andrew": 3.82462057336, "situat": 2.06611140031, "key": 2.28005170185, "ani": 1.13383802314, "speech": 3.8227787141800005, "find": 3.4588235294199996, "advanc": 1.9997480791, "common": 2.8051948051999998, "data": 16.8821777967, "oper": 1.55479384977, "method": 12.857142857150002, "sinc": 1.08368600683, "bio": 42.336000000000006, "come": 1.32831325301, "repres": 1.46972782818, "text": 6.25655172414, "dimension": 54.1843003413, "understand": 2.96858638743, "discret": 15.0056710775, "alphago": 1587.6, "first": 2.01523229246, "game": 18.05849853752, "input": 12.2029208301, "use": 13.385303845939998, "while": 5.2209944751499995, "exclus": 3.40906162766, "effici": 5.09335899904, "learn": 78.9735186541, "when": 1.02076769755, "user": 7.71053909665}, "logtfidf": {"real": 1.649258121148, "food": 2.17131602016, "behavior": 3.42029626756, "hottest": 3.78645978245, "unlik": 0.885954358842, "watch": 1.36757423376, "actorcrit": 14.739957441820001, "signal": 1.6340517929299998, "wide": 0.44458000675399995, "new": 0.0177299468511, "pieter": 4.65528397709, "space": 1.749426329944, "grow": 0.821043542212, "would": 0.1592352559294, "berkeley": 2.4573238351700004, "etc": 1.4366730879700003, "unseen": 3.708984470280001, "function": 0.914465741594, "david": 0.615025032185, "python": 4.03065674296, "transit": 1.2129997353200002, "unsupervis": 11.687844834819998, "resurg": 3.06456318861, "teach": 1.38277323072, "approach": 0.7302336145810001, "technic": 1.14423287808, "know": 0.952919694398, "their": 0.030721010245400002, "but": 0.0323847441438, "particular": 0.323157393804, "abbeel": 7.369978720910001, "creat": 0.445153637028, "summar": 2.7150664430299996, "thing": 0.8781935346799999, "pong": 5.4240685718499995, "quit": 1.05951513684, "interest": 0.47207177798199995, "deep": 5.1546938792, "will": 0.202786534915, "exploit": 3.5137012290400005, "concept": 0.977224437103, "open": 0.439182076058, "updat": 6.865749850759999, "below": 0.813626591936, "perform": 0.42618085058, "healthcar": 2.9320444543, "demonstr": 0.9745501918189999, "feedback": 9.614595263399998, "all": 0.011402632097799998, "point": 0.23103235903299998, "applic": 4.92641571396, "lab": 5.33899671024, "robot": 11.98696236908, "have": 0.0147850023412, "agentbas": 7.369978720910001, "formul": 2.2885743559200002, "champion": 1.6167713629299998, "experi": 1.878818861799, "qvalu": 7.369978720910001, "defeat": 1.05733688222, "model": 2.9498002924440003, "andrej": 5.91136369821, "good": 0.418589404907, "toolkit": 5.24174701506, "lowdimension": 7.369978720910001, "cours": 0.765899404133, "optim": 4.891255590819999, "malmo": 7.369978720910001, "ghost": 4.84243766106, "involv": 0.371469078658, "youplus": 7.369978720910001, "not": 0.031104826015, "stori": 0.705059626587, "deepmind": 14.739957441820001, "advisor": 2.50013923678, "gameplay": 4.5788136131, "handl": 1.36683266903, "almost": 0.42907884333400004, "though": 0.308044191079, "they": 0.0594539895352, "anoth": 0.38368908495599996, "core": 1.53108277245, "let": 2.4976051345599997, "world": 0.537101243105, "pixel": 4.45762805629, "blog": 2.65237310559, "onpolici": 7.369978720910001, "passion": 2.0974921144, "selfplay": 7.369978720910001, "lack": 1.312101877814, "simpl": 2.4464425787799997, "that": 0.019880741898199997, "itself": 0.5570837229510001, "provid": 0.19517784432500002, "valu": 6.585546481184, "lectur": 1.8847669357199999, "such": 0.119391955612, "explor": 3.66773811654, "with": 0.0119749171339, "network": 1.9061661061039998, "gym": 3.74297467051, "deriv": 3.0714485482500002, "lot": 1.4835969502500002, "suitabl": 1.83067788492, "sourc": 0.529218310751, "should": 0.509419876758, "both": 0.050842533389300004, "locat": 0.46854337067199997, "punish": 3.6301660539199996, "supervis": 4.09296211166, "receiv": 0.533148849844, "set": 0.5144880338669999, "futur": 0.619345197699, "openai": 7.369978720910001, "two": 0.0273976887164, "and": 0.0016377436936536, "from": 0.005103487519794, "privat": 0.705059626587, "work": 0.109034567273, "num": 0.0022049327677790003, "idea": 0.73863592212, "free": 0.5412666492670001, "environ": 11.107776627269999, "for": 0.005039846326352001, "manipul": 2.21322491868, "output": 2.03822657827, "book": 0.7211395764, "handi": 4.62913869698, "are": 0.2652072622443, "current": 1.7078113113800002, "pacman": 58.959829767280006, "look": 0.6463866936, "popular": 0.41058020877499996, "video": 3.57921746901, "modelfre": 22.10993616273, "formal": 0.894546004205, "sutton": 7.00172643298, "asynchron": 5.10821562244, "rich": 2.71301618708, "negat": 1.32402598852, "public": 0.20232375048700002, "posit": 0.316652318608, "karpathi": 7.369978720910001, "eat": 4.03150744484, "general": 0.114952578063, "compar": 1.2478383618539999, "tackl": 2.98920286814, "revolv": 2.57253664727, "markov": 5.16170430739, "doe": 0.5340417297169999, "john": 0.304194577702, "bookthebooknumndhtml": 7.369978720910001, "train": 0.660918312839, "simul": 4.88110696448, "avoid": 0.900108441291, "goal": 4.75322849092, "total": 0.8713577734100001, "framework": 2.10418454607, "his": 0.0901772433641, "continu": 0.13040487398700001, "type": 0.707101485387, "overcom": 2.12664566269, "reinforc": 44.75153243952, "machin": 2.78471916124, "prior": 0.778442172521, "through": 0.0683586918849, "requir": 0.424253510675, "doctor": 1.37104215896, "face": 0.589602371257, "what": 0.677661890481, "tell": 1.21236434401, "map": 2.80868986768, "help": 0.336207721344, "denot": 2.3220476420700003, "dialog": 4.2744011123900005, "line": 0.349430614452, "own": 0.328390154842, "base": 0.5460932091480001, "relat": 0.42620060330799997, "can": 1.136387674758, "here": 2.6551145651100003, "stock": 1.61265547932, "dqns": 7.369978720910001, "comput": 2.73613783188, "onlin": 1.915007708714, "fundament": 1.67322086119, "about": 0.1256869549492, "problem": 3.414844345638, "neural": 8.170630311, "silver": 1.5886181116100002, "how": 1.8862678277200002, "just": 0.289531434109, "test": 0.977224437103, "adapt": 1.2007864860200002, "barto": 7.369978720910001, "stateactionrewardstateact": 7.369978720910001, "correct": 1.29831763181, "she": 0.7701082216959999, "topic": 1.6969991554100001, "dure": 0.0491209066894, "dynam": 1.8756834711200001, "where": 0.2599685549828, "notion": 1.9956266680799999, "practic": 0.533182530867, "start": 0.472886738582, "run": 0.442714975539, "enabl": 2.52947831908, "explain": 0.955700427358, "order": 0.22014038079300002, "lose": 1.1265888210600001, "autom": 2.9867028668299995, "raw": 2.36536149914, "support": 0.237880610037, "play": 0.38110439064199997, "father": 1.638654098314, "the": 0.0, "build": 2.455687260455, "kill": 0.741540598047, "algorithm": 19.98265437108, "learningan": 7.369978720910001, "error": 1.7985854343, "consist": 0.398873126426, "same": 0.224119299208, "possibl": 0.348805474891, "industri": 0.7046772417749999, "best": 0.459227932947, "exampl": 0.40868267499899996, "trade": 0.865091924188, "interview": 1.2232212893899999, "introductori": 3.41107212958, "knowledg": 2.4464425787799997, "take": 0.261383924394, "strategi": 2.98224623636, "code": 1.35601909597, "interact": 4.457463080369999, "high": 0.13782378654000002, "basic": 3.01310324685, "abil": 0.996488297427, "tutori": 4.0853151555, "qnetwork": 7.369978720910001, "around": 0.19387710578200001, "treatment": 1.3535776885100002, "draft": 1.62986431701, "materi": 0.7561918990209999, "task": 1.35748680661, "research": 3.3186390906899996, "therefor": 0.847591848336, "polici": 7.4246056004, "zero": 2.1692741832299998, "challeng": 0.9392919688950001, "qlearn": 29.479914883640003, "sector": 1.7035520328, "schulman": 5.934894195619999, "system": 0.327430345585, "great": 0.235805258079, "howev": 0.0903151173475, "trial": 1.3966781444299998, "win": 1.01265652029, "maxim": 5.1188434104, "which": 0.036248896918010004, "term": 0.9991169506380001, "estim": 1.70875507195, "other": 0.02962424375928, "introduct": 1.02276465794, "techniqu": 2.63248769614, "one": 0.0062553516455, "impact": 1.09033222631, "some": 0.079147018129, "mathemat": 2.00027135827, "backgammon": 6.117215752409999, "get": 2.319076023128, "like": 0.417160729635, "implement": 1.27437940907, "ancient": 1.22022919484, "chines": 1.45162264562, "control": 0.38498466158600003, "dilemma": 3.33044239518, "agent": 26.080859485799998, "process": 0.527829199025, "element": 1.7173585117539998, "includ": 0.0566540441715, "rule": 1.109554847074, "grid": 8.69159316891, "this": 0.0189322452625, "figur": 3.5508605608000003, "time": 0.0224230377252, "finit": 3.33928418576, "engin": 0.904767558276, "similar": 0.637112184228, "project": 0.561601885907, "differ": 0.6369633639360001, "reward": 17.39398354512, "decis": 0.7701082216959999, "most": 0.041495792591199995, "between": 0.06790736233059999, "readili": 2.5289462112, "richard": 0.788646342695, "domain": 2.24008000599, "gamelik": 7.369978720910001, "shweta": 14.739957441820001, "offpolici": 14.739957441820001, "program": 0.7037855787649999, "physic": 0.871847185184, "gradient": 3.73502760882, "solv": 1.9836504770400003, "case": 0.395406268889, "day": 0.33731741263400006, "architectur": 1.63469757919, "tradeoff": 5.34183047362, "experiment": 1.8035444374, "call": 0.0654627744488, "more": 0.034049863199999995, "improv": 0.7147958039319999, "london": 0.6819976757709999, "these": 0.0715336194008, "bhatt": 11.869788391239998, "cumul": 7.1221929487999995, "action": 6.578474815759, "comment": 1.11826753454, "state": 0.4194900249012, "mdps": 14.739957441820001, "platform": 3.6597846778400003, "avail": 1.094909172578, "describ": 0.77089520625, "make": 0.07349765782289999, "way": 0.19809150993500002, "onli": 0.050648536658199995, "each": 0.173741689304, "andrew": 1.34145926585, "situat": 0.725668290015, "key": 0.82419811896, "ani": 0.125608358366, "speech": 1.3409775702700002, "find": 1.095562660576, "advanc": 0.6930212121780001, "common": 0.676651610542, "data": 6.084102924, "oper": 0.441342964347, "method": 4.7223080442050005, "sinc": 0.0803681994577, "bio": 3.7456377879300002, "come": 0.28390990653000003, "repres": 0.38507723275, "text": 2.28096401998, "dimension": 3.99239120489, "understand": 1.0880858756799998, "discret": 2.70842820148, "alphago": 7.369978720910001, "first": 0.015174579624319999, "game": 6.633943806147, "input": 2.50167533539, "use": 0.37970425651080003, "while": 0.21624991896900006, "exclus": 1.22643707092, "effici": 1.62793753414, "learn": 28.653570201329998, "when": 0.0205549888584, "user": 2.04258810688}, "logidf": {"real": 0.824629060574, "food": 1.08565801008, "behavior": 1.71014813378, "hottest": 3.78645978245, "unlik": 0.885954358842, "watch": 1.36757423376, "actorcrit": 7.369978720910001, "signal": 1.6340517929299998, "wide": 0.44458000675399995, "new": 0.0177299468511, "pieter": 4.65528397709, "space": 0.874713164972, "grow": 0.821043542212, "would": 0.0796176279647, "berkeley": 2.4573238351700004, "etc": 1.4366730879700003, "unseen": 3.708984470280001, "function": 0.914465741594, "david": 0.615025032185, "python": 4.03065674296, "transit": 1.2129997353200002, "unsupervis": 5.843922417409999, "resurg": 3.06456318861, "teach": 1.38277323072, "approach": 0.7302336145810001, "technic": 1.14423287808, "know": 0.952919694398, "their": 0.015360505122700001, "but": 0.0161923720719, "particular": 0.323157393804, "abbeel": 7.369978720910001, "creat": 0.222576818514, "summar": 2.7150664430299996, "thing": 0.8781935346799999, "pong": 5.4240685718499995, "quit": 1.05951513684, "interest": 0.47207177798199995, "deep": 1.2886734698, "will": 0.202786534915, "exploit": 1.7568506145200002, "concept": 0.977224437103, "open": 0.219591038029, "updat": 1.7164374626899999, "below": 0.813626591936, "perform": 0.42618085058, "healthcar": 2.9320444543, "demonstr": 0.9745501918189999, "feedback": 3.2048650877999996, "all": 0.011402632097799998, "point": 0.23103235903299998, "applic": 1.23160392849, "lab": 2.66949835512, "robot": 2.99674059227, "have": 0.0147850023412, "agentbas": 7.369978720910001, "formul": 2.2885743559200002, "champion": 1.6167713629299998, "experi": 0.626272953933, "qvalu": 7.369978720910001, "defeat": 1.05733688222, "model": 0.7374500731110001, "andrej": 5.91136369821, "good": 0.418589404907, "toolkit": 5.24174701506, "lowdimension": 7.369978720910001, "cours": 0.765899404133, "optim": 2.4456277954099996, "malmo": 7.369978720910001, "ghost": 2.42121883053, "involv": 0.371469078658, "youplus": 7.369978720910001, "not": 0.0155524130075, "stori": 0.705059626587, "deepmind": 7.369978720910001, "advisor": 2.50013923678, "gameplay": 4.5788136131, "handl": 1.36683266903, "almost": 0.42907884333400004, "though": 0.308044191079, "they": 0.0297269947676, "anoth": 0.127896361652, "core": 1.53108277245, "let": 1.2488025672799998, "world": 0.107420248621, "pixel": 4.45762805629, "blog": 2.65237310559, "onpolici": 7.369978720910001, "passion": 2.0974921144, "selfplay": 7.369978720910001, "lack": 0.656050938907, "simpl": 1.2232212893899999, "that": 0.00397614837964, "itself": 0.5570837229510001, "provid": 0.19517784432500002, "valu": 0.823193310148, "lectur": 1.8847669357199999, "such": 0.059695977806, "explor": 1.22257937218, "with": 0.00119749171339, "network": 0.9530830530519999, "gym": 3.74297467051, "deriv": 1.02381618275, "lot": 1.4835969502500002, "suitabl": 1.83067788492, "sourc": 0.529218310751, "should": 0.509419876758, "both": 0.050842533389300004, "locat": 0.46854337067199997, "punish": 1.8150830269599998, "supervis": 2.04648105583, "receiv": 0.266574424922, "set": 0.171496011289, "futur": 0.619345197699, "openai": 7.369978720910001, "two": 0.0136988443582, "and": 6.29901420636e-05, "from": 0.000567054168866, "privat": 0.705059626587, "work": 0.109034567273, "num": 0.00031499039539700004, "idea": 0.73863592212, "free": 0.5412666492670001, "environ": 1.2341974030299998, "for": 0.00031499039539700004, "manipul": 2.21322491868, "output": 2.03822657827, "book": 0.3605697882, "handi": 4.62913869698, "are": 0.0294674735827, "current": 0.42695282784500005, "pacman": 7.369978720910001, "look": 0.6463866936, "popular": 0.41058020877499996, "video": 1.19307248967, "modelfre": 7.369978720910001, "formal": 0.894546004205, "sutton": 3.50086321649, "asynchron": 5.10821562244, "rich": 1.35650809354, "negat": 1.32402598852, "public": 0.20232375048700002, "posit": 0.316652318608, "karpathi": 7.369978720910001, "eat": 2.01575372242, "general": 0.114952578063, "compar": 0.6239191809269999, "tackl": 2.98920286814, "revolv": 2.57253664727, "markov": 5.16170430739, "doe": 0.5340417297169999, "john": 0.304194577702, "bookthebooknumndhtml": 7.369978720910001, "train": 0.660918312839, "simul": 2.44055348224, "avoid": 0.900108441291, "goal": 1.18830712273, "total": 0.43567888670500005, "framework": 2.10418454607, "his": 0.0901772433641, "continu": 0.13040487398700001, "type": 0.707101485387, "overcom": 2.12664566269, "reinforc": 1.86464718498, "machin": 1.39235958062, "prior": 0.778442172521, "through": 0.0683586918849, "requir": 0.424253510675, "doctor": 1.37104215896, "face": 0.589602371257, "what": 0.225887296827, "tell": 1.21236434401, "map": 1.40434493384, "help": 0.336207721344, "denot": 2.3220476420700003, "dialog": 4.2744011123900005, "line": 0.349430614452, "own": 0.164195077421, "base": 0.13652330228700002, "relat": 0.21310030165399999, "can": 0.162341096394, "here": 0.8850381883700001, "stock": 1.61265547932, "dqns": 7.369978720910001, "comput": 1.36806891594, "onlin": 0.957503854357, "fundament": 1.67322086119, "about": 0.0628434774746, "problem": 0.569140724273, "neural": 4.0853151555, "silver": 1.5886181116100002, "how": 0.47156695693000006, "just": 0.289531434109, "test": 0.977224437103, "adapt": 1.2007864860200002, "barto": 7.369978720910001, "stateactionrewardstateact": 7.369978720910001, "correct": 1.29831763181, "she": 0.7701082216959999, "topic": 1.6969991554100001, "dure": 0.0491209066894, "dynam": 1.8756834711200001, "where": 0.0649921387457, "notion": 1.9956266680799999, "practic": 0.533182530867, "start": 0.236443369291, "run": 0.442714975539, "enabl": 1.26473915954, "explain": 0.955700427358, "order": 0.22014038079300002, "lose": 1.1265888210600001, "autom": 2.9867028668299995, "raw": 2.36536149914, "support": 0.237880610037, "play": 0.38110439064199997, "father": 0.819327049157, "the": 0.0, "build": 0.491137452091, "kill": 0.741540598047, "algorithm": 3.33044239518, "learningan": 7.369978720910001, "error": 1.7985854343, "consist": 0.398873126426, "same": 0.112059649604, "possibl": 0.348805474891, "industri": 0.7046772417749999, "best": 0.459227932947, "exampl": 0.40868267499899996, "trade": 0.865091924188, "interview": 1.2232212893899999, "introductori": 3.41107212958, "knowledg": 1.2232212893899999, "take": 0.130691962197, "strategi": 1.49112311818, "code": 1.35601909597, "interact": 1.4858210267899998, "high": 0.13782378654000002, "basic": 1.00436774895, "abil": 0.996488297427, "tutori": 4.0853151555, "qnetwork": 7.369978720910001, "around": 0.19387710578200001, "treatment": 1.3535776885100002, "draft": 1.62986431701, "materi": 0.7561918990209999, "task": 1.35748680661, "research": 0.663727818138, "therefor": 0.847591848336, "polici": 0.92807570005, "zero": 2.1692741832299998, "challeng": 0.9392919688950001, "qlearn": 7.369978720910001, "sector": 1.7035520328, "schulman": 5.934894195619999, "system": 0.327430345585, "great": 0.235805258079, "howev": 0.0903151173475, "trial": 1.3966781444299998, "win": 1.01265652029, "maxim": 2.5594217052, "which": 0.00517841384543, "term": 0.33303898354600003, "estim": 0.854377535975, "other": 0.00987474791976, "introduct": 1.02276465794, "techniqu": 1.31624384807, "one": 0.0062553516455, "impact": 1.09033222631, "some": 0.0395735090645, "mathemat": 2.00027135827, "backgammon": 6.117215752409999, "get": 0.579769005782, "like": 0.139053576545, "implement": 1.27437940907, "ancient": 1.22022919484, "chines": 1.45162264562, "control": 0.38498466158600003, "dilemma": 3.33044239518, "agent": 1.4489366381, "process": 0.527829199025, "element": 0.8586792558769999, "includ": 0.0188846813905, "rule": 0.554777423537, "grid": 2.89719772297, "this": 0.0037864490525, "figur": 0.7101721121600001, "time": 0.0112115188626, "finit": 3.33928418576, "engin": 0.904767558276, "similar": 0.318556092114, "project": 0.561601885907, "differ": 0.212321121312, "reward": 2.17424794314, "decis": 0.7701082216959999, "most": 0.020747896295599998, "between": 0.033953681165299995, "readili": 2.5289462112, "richard": 0.788646342695, "domain": 2.24008000599, "gamelik": 7.369978720910001, "shweta": 7.369978720910001, "offpolici": 7.369978720910001, "program": 0.7037855787649999, "physic": 0.871847185184, "gradient": 3.73502760882, "solv": 1.9836504770400003, "case": 0.395406268889, "day": 0.16865870631700003, "architectur": 1.63469757919, "tradeoff": 5.34183047362, "experiment": 1.8035444374, "call": 0.0654627744488, "more": 0.017024931599999998, "improv": 0.7147958039319999, "london": 0.6819976757709999, "these": 0.0715336194008, "bhatt": 5.934894195619999, "cumul": 3.5610964743999998, "action": 0.598043165069, "comment": 1.11826753454, "state": 0.0466100027668, "mdps": 7.369978720910001, "platform": 1.8298923389200001, "avail": 0.547454586289, "describ": 0.385447603125, "make": 0.07349765782289999, "way": 0.19809150993500002, "onli": 0.025324268329099998, "each": 0.173741689304, "andrew": 1.34145926585, "situat": 0.725668290015, "key": 0.82419811896, "ani": 0.125608358366, "speech": 1.3409775702700002, "find": 0.547781330288, "advanc": 0.6930212121780001, "common": 0.338325805271, "data": 1.2168205848, "oper": 0.441342964347, "method": 0.944461608841, "sinc": 0.0803681994577, "bio": 3.7456377879300002, "come": 0.28390990653000003, "repres": 0.38507723275, "text": 1.14048200999, "dimension": 3.99239120489, "understand": 1.0880858756799998, "discret": 2.70842820148, "alphago": 7.369978720910001, "first": 0.0075872898121599995, "game": 0.9477062580210001, "input": 2.50167533539, "use": 0.0292080197316, "while": 0.04324998379380001, "exclus": 1.22643707092, "effici": 1.62793753414, "learn": 0.842752064745, "when": 0.0205549888584, "user": 2.04258810688}, "freq": {"real": 2, "food": 2, "behavior": 2, "hottest": 1, "unlik": 1, "watch": 1, "actorcrit": 2, "signal": 1, "wide": 1, "new": 1, "pieter": 1, "space": 2, "grow": 1, "would": 2, "berkeley": 1, "etc": 1, "unseen": 1, "function": 1, "david": 1, "python": 1, "transit": 1, "unsupervis": 2, "resurg": 1, "teach": 1, "approach": 1, "technic": 1, "know": 1, "their": 2, "but": 2, "particular": 1, "abbeel": 1, "creat": 2, "summar": 1, "thing": 1, "pong": 1, "quit": 1, "interest": 1, "deep": 4, "will": 1, "exploit": 2, "concept": 1, "open": 2, "updat": 4, "below": 1, "perform": 1, "healthcar": 1, "demonstr": 1, "feedback": 3, "all": 1, "point": 1, "applic": 4, "lab": 2, "robot": 4, "have": 1, "agentbas": 1, "formul": 1, "champion": 1, "experi": 3, "qvalu": 1, "defeat": 1, "model": 4, "andrej": 1, "good": 1, "toolkit": 1, "lowdimension": 1, "cours": 1, "optim": 2, "malmo": 1, "ghost": 2, "involv": 1, "youplus": 1, "not": 2, "stori": 1, "deepmind": 2, "advisor": 1, "gameplay": 1, "handl": 1, "almost": 1, "though": 1, "they": 2, "anoth": 3, "core": 1, "let": 2, "world": 5, "pixel": 1, "blog": 1, "onpolici": 1, "passion": 1, "selfplay": 1, "lack": 2, "simpl": 2, "that": 5, "itself": 1, "provid": 1, "valu": 8, "lectur": 1, "such": 2, "explor": 3, "with": 10, "network": 2, "gym": 1, "deriv": 3, "lot": 1, "suitabl": 1, "sourc": 1, "should": 1, "both": 1, "locat": 1, "punish": 2, "supervis": 2, "receiv": 2, "set": 3, "futur": 1, "openai": 1, "two": 2, "and": 26, "from": 9, "privat": 1, "work": 1, "num": 7, "idea": 1, "free": 1, "environ": 9, "for": 16, "manipul": 1, "output": 1, "book": 2, "handi": 1, "are": 9, "current": 4, "pacman": 8, "look": 1, "popular": 1, "video": 3, "modelfre": 3, "formal": 1, "sutton": 2, "asynchron": 1, "rich": 2, "negat": 1, "public": 1, "posit": 1, "karpathi": 1, "eat": 2, "general": 1, "compar": 2, "tackl": 1, "revolv": 1, "markov": 1, "doe": 1, "john": 1, "bookthebooknumndhtml": 1, "train": 1, "simul": 2, "avoid": 1, "goal": 4, "total": 2, "framework": 1, "his": 1, "continu": 1, "type": 1, "overcom": 1, "reinforc": 24, "machin": 2, "prior": 1, "through": 1, "requir": 1, "doctor": 1, "face": 1, "what": 3, "tell": 1, "map": 2, "help": 1, "denot": 1, "dialog": 1, "line": 1, "own": 2, "base": 4, "relat": 2, "can": 7, "here": 3, "stock": 1, "dqns": 1, "comput": 2, "onlin": 2, "fundament": 1, "about": 2, "problem": 6, "neural": 2, "silver": 1, "how": 4, "just": 1, "test": 1, "adapt": 1, "barto": 1, "stateactionrewardstateact": 1, "correct": 1, "she": 1, "topic": 1, "dure": 1, "dynam": 1, "where": 4, "notion": 1, "practic": 1, "start": 2, "run": 1, "enabl": 2, "explain": 1, "order": 1, "lose": 1, "autom": 1, "raw": 1, "support": 1, "play": 1, "father": 2, "the": 50, "build": 5, "kill": 1, "algorithm": 6, "learningan": 1, "error": 1, "consist": 1, "same": 2, "possibl": 1, "industri": 1, "best": 1, "exampl": 1, "trade": 1, "interview": 1, "introductori": 1, "knowledg": 2, "take": 2, "strategi": 2, "code": 1, "interact": 3, "high": 1, "basic": 3, "abil": 1, "tutori": 1, "qnetwork": 1, "around": 1, "treatment": 1, "draft": 1, "materi": 1, "task": 1, "research": 5, "therefor": 1, "polici": 8, "zero": 1, "challeng": 1, "qlearn": 4, "sector": 1, "schulman": 1, "system": 1, "great": 1, "howev": 1, "trial": 1, "win": 1, "maxim": 2, "which": 7, "term": 3, "estim": 2, "other": 3, "introduct": 1, "techniqu": 2, "one": 1, "impact": 1, "some": 2, "mathemat": 1, "backgammon": 1, "get": 4, "like": 3, "implement": 1, "ancient": 1, "chines": 1, "control": 1, "dilemma": 1, "agent": 18, "process": 1, "element": 2, "includ": 3, "rule": 2, "grid": 3, "this": 5, "figur": 5, "time": 2, "finit": 1, "engin": 1, "similar": 2, "project": 1, "differ": 3, "reward": 8, "decis": 1, "most": 2, "between": 2, "readili": 1, "richard": 1, "domain": 1, "gamelik": 1, "shweta": 2, "offpolici": 2, "program": 1, "physic": 1, "gradient": 1, "solv": 1, "case": 1, "day": 2, "architectur": 1, "tradeoff": 1, "experiment": 1, "call": 1, "more": 2, "improv": 1, "london": 1, "these": 1, "bhatt": 2, "cumul": 2, "action": 11, "comment": 1, "state": 9, "mdps": 2, "platform": 2, "avail": 2, "describ": 2, "make": 1, "way": 1, "onli": 2, "each": 1, "andrew": 1, "situat": 1, "key": 1, "ani": 1, "speech": 1, "find": 2, "advanc": 1, "common": 2, "data": 5, "oper": 1, "method": 5, "sinc": 1, "bio": 1, "come": 1, "repres": 1, "text": 2, "dimension": 1, "understand": 1, "discret": 1, "alphago": 1, "first": 2, "game": 7, "input": 1, "use": 13, "while": 5, "exclus": 1, "effici": 1, "learn": 34, "when": 1, "user": 1}, "idf": {"real": 2.28103448276, "food": 2.9613878007800003, "behavior": 5.52978056426, "hottest": 44.1, "unlik": 2.42529789184, "watch": 3.92581602374, "actorcrit": 1587.6, "signal": 5.12459651388, "wide": 1.5598349381, "new": 1.0178880554, "pieter": 105.139072848, "space": 2.39818731118, "grow": 2.27287043665, "would": 1.0828729281799998, "berkeley": 11.6735294118, "etc": 4.2066772655, "unseen": 40.8123393316, "function": 2.495441685, "david": 1.84970290108, "python": 56.2978723404, "transit": 3.36355932203, "unsupervis": 345.13043478300006, "resurg": 21.4251012146, "teach": 3.98594024605, "approach": 2.07556543339, "technic": 3.1400316455699997, "know": 2.59327017315, "their": 1.01547908405, "but": 1.01632417899, "particular": 1.3814827706200001, "abbeel": 1587.6, "creat": 1.2492917847, "summar": 15.1056137012, "thing": 2.4065484311099996, "pong": 226.8, "quit": 2.8849718335500003, "interest": 1.60331246213, "deep": 3.6279707495399998, "will": 1.22481098596, "exploit": 5.79416058394, "concept": 2.65707112971, "open": 1.24556723678, "updat": 5.56466876972, "below": 2.25607503197, "perform": 1.5313977042500002, "healthcar": 18.7659574468, "demonstr": 2.64997496244, "feedback": 24.652173913000002, "all": 1.01146788991, "point": 1.25990000794, "applic": 3.42672134686, "lab": 14.4327272727, "robot": 20.0201765448, "have": 1.0148948411399998, "agentbas": 1587.6, "formul": 9.86086956522, "champion": 5.03680203046, "experi": 1.87062566278, "qvalu": 1587.6, "defeat": 2.87869446963, "model": 2.0905978404, "andrej": 369.209302326, "good": 1.51981619759, "toolkit": 189.0, "lowdimension": 1587.6, "cours": 2.15092805853, "optim": 11.5377906977, "malmo": 1587.6, "ghost": 11.2595744681, "involv": 1.4498630137000001, "youplus": 1587.6, "not": 1.01567398119, "stori": 2.02396736359, "deepmind": 1587.6, "advisor": 12.18419033, "gameplay": 97.3987730061, "handl": 3.9229058561900003, "almost": 1.53584212054, "though": 1.36076112111, "they": 1.03017325287, "anoth": 1.13643521832, "core": 4.623179965059999, "let": 3.48616600791, "world": 1.11340206186, "pixel": 86.28260869569999, "blog": 14.1876675603, "onpolici": 1587.6, "passion": 8.14571575167, "selfplay": 1587.6, "lack": 1.9271667880599999, "simpl": 3.3981164383599998, "that": 1.00398406375, "itself": 1.74557449148, "provid": 1.21552714187, "valu": 2.2777618364400003, "lectur": 6.58481957694, "such": 1.06151377374, "explor": 3.39593582888, "with": 1.0011982089899998, "network": 2.59369384088, "gym": 42.223404255300004, "deriv": 2.78379800105, "lot": 4.40877534018, "suitabl": 6.23811394892, "sourc": 1.69760479042, "should": 1.6643254009900001, "both": 1.05215720061, "locat": 1.59766529134, "punish": 6.1415860735, "supervis": 7.74061433447, "receiv": 1.3054847463200001, "set": 1.18707940781, "futur": 1.8577112099200002, "openai": 1587.6, "two": 1.01379310345, "and": 1.00006299213, "from": 1.00056721497, "privat": 2.02396736359, "work": 1.11520089913, "num": 1.00031504001, "idea": 2.0930784443, "free": 1.71818181818, "environ": 3.43561999567, "for": 1.00031504001, "manipul": 9.145161290319999, "output": 7.676982591880001, "book": 1.43414634146, "handi": 102.425806452, "are": 1.02990593578, "current": 1.5325803649, "pacman": 1587.6, "look": 1.9086318826599997, "popular": 1.50769230769, "video": 3.29719626168, "modelfre": 1587.6, "formal": 2.44622496148, "sutton": 33.1440501044, "asynchron": 165.375, "rich": 3.8826118855500003, "negat": 3.75852272727, "public": 1.22424429365, "posit": 1.37252528746, "karpathi": 1587.6, "eat": 7.50638297872, "general": 1.1218202374200001, "compar": 1.8662278123900002, "tackl": 19.8698372966, "revolv": 13.099009900999999, "markov": 174.46153846200002, "doe": 1.70581282905, "john": 1.35553278689, "bookthebooknumndhtml": 1587.6, "train": 1.9365698950999999, "simul": 11.4793926247, "avoid": 2.45986984816, "goal": 3.28152128979, "total": 1.5460122699399999, "framework": 8.200413223139998, "his": 1.0943682360200002, "continu": 1.13928955867, "type": 2.0281042411900003, "overcom": 8.38668779715, "reinforc": 6.453658536590001, "machin": 4.02433460076, "prior": 2.17807655371, "through": 1.07074930869, "requir": 1.52844902282, "doctor": 3.9394540942900003, "face": 1.80327124035, "what": 1.25343439128, "tell": 3.36142282448, "map": 4.0728578758300005, "help": 1.39962972759, "denot": 10.1965317919, "dialog": 71.8371040724, "line": 1.4182597820299998, "own": 1.17844418052, "base": 1.14628158845, "relat": 1.23750876919, "can": 1.17626139142, "here": 2.42307692308, "stock": 5.01611374408, "dqns": 1587.6, "comput": 3.9277585353800006, "onlin": 2.6051854282900004, "fundament": 5.32930513595, "about": 1.06486015159, "problem": 1.76674827509, "neural": 59.4606741573, "silver": 4.89697717458, "how": 1.60250328051, "just": 1.33580143037, "test": 2.65707112971, "adapt": 3.32272917539, "barto": 1587.6, "stateactionrewardstateact": 1587.6, "correct": 3.6631287494199998, "she": 2.16, "topic": 5.457545548300001, "dure": 1.0503473370799998, "dynam": 6.52527743527, "where": 1.06715063521, "notion": 7.356811862839999, "practic": 1.70434782609, "start": 1.26673581744, "run": 1.55692850838, "enabl": 3.5421686747, "explain": 2.60049140049, "order": 1.24625166811, "lose": 3.0851146521599997, "autom": 19.8202247191, "raw": 10.6478873239, "support": 1.2685577307200002, "play": 1.46390041494, "father": 2.26897241675, "the": 1.0, "build": 1.6341739578, "kill": 2.09916699722, "algorithm": 27.9507042254, "learningan": 1587.6, "error": 6.04109589041, "consist": 1.4901445466499998, "same": 1.11857958148, "possibl": 1.4173734488, "industri": 2.02319357716, "best": 1.5828514456600002, "exampl": 1.50483412322, "trade": 2.37522441652, "interview": 3.3981164383599998, "introductori": 30.297709923699998, "knowledg": 3.3981164383599998, "take": 1.13961668222, "strategi": 4.44208170118, "code": 3.8807137619199996, "interact": 4.4185917061, "high": 1.14777327935, "basic": 2.7301805675, "abil": 2.70875277256, "tutori": 59.4606741573, "qnetwork": 1587.6, "around": 1.21394708671, "treatment": 3.87125091441, "draft": 5.103182256509999, "materi": 2.13014893332, "task": 3.88641370869, "research": 1.9420183486200002, "therefor": 2.33401940606, "polici": 2.52963671128, "zero": 8.75192943771, "challeng": 2.55816951337, "qlearn": 1587.6, "sector": 5.493425605540001, "schulman": 378.0, "system": 1.38739840951, "great": 1.26592775696, "howev": 1.0945191313299998, "trial": 4.04175152749, "win": 2.75290445639, "maxim": 12.928338762200001, "which": 1.005191845, "term": 1.39520168732, "estim": 2.34991119005, "other": 1.00992366412, "introduct": 2.7808723068799996, "techniqu": 3.7293868921800004, "one": 1.00627495722, "impact": 2.97526236882, "some": 1.04036697248, "mathemat": 7.391061452510001, "backgammon": 453.6, "get": 1.78562591385, "like": 1.14918566775, "implement": 3.57648118946, "ancient": 3.38796414853, "chines": 4.270037654649999, "control": 1.46959178006, "dilemma": 27.9507042254, "agent": 4.25858369099, "process": 1.69524826482, "element": 2.36004162331, "includ": 1.0190641247799999, "rule": 1.7415533128599998, "grid": 18.1232876712, "this": 1.00379362671, "figur": 2.0343413634, "time": 1.01127460348, "finit": 28.1989342806, "engin": 2.47135740971, "similar": 1.37514075357, "project": 1.7534791252500002, "differ": 1.23654490225, "reward": 8.79556786704, "decis": 2.16, "most": 1.02096463023, "between": 1.03453668708, "readili": 12.5402843602, "richard": 2.20041580042, "domain": 9.39408284024, "gamelik": 1587.6, "shweta": 1587.6, "offpolici": 1587.6, "program": 2.02139037433, "physic": 2.39132399458, "gradient": 41.889182058, "solv": 7.26923076923, "case": 1.48498737256, "day": 1.18371607516, "architectur": 5.12790697674, "tradeoff": 208.89473684200001, "experiment": 6.07112810707, "call": 1.0676529926, "more": 1.0171706817, "improv": 2.04376930999, "london": 1.97782484116, "these": 1.07415426252, "bhatt": 378.0, "cumul": 35.2017738359, "action": 1.81855670103, "comment": 3.05954904606, "state": 1.0477133240899998, "mdps": 1587.6, "platform": 6.2332155476999995, "avail": 1.7288467821, "describ": 1.47027227264, "make": 1.0762660158600001, "way": 1.2190739461, "onli": 1.0256476516600002, "each": 1.18974820144, "andrew": 3.82462057336, "situat": 2.06611140031, "key": 2.28005170185, "ani": 1.13383802314, "speech": 3.8227787141800005, "find": 1.7294117647099998, "advanc": 1.9997480791, "common": 1.4025974025999999, "data": 3.37643555934, "oper": 1.55479384977, "method": 2.5714285714300003, "sinc": 1.08368600683, "bio": 42.336000000000006, "come": 1.32831325301, "repres": 1.46972782818, "text": 3.12827586207, "dimension": 54.1843003413, "understand": 2.96858638743, "discret": 15.0056710775, "alphago": 1587.6, "first": 1.00761614623, "game": 2.57978550536, "input": 12.2029208301, "use": 1.0296387573799999, "while": 1.0441988950299999, "exclus": 3.40906162766, "effici": 5.09335899904, "learn": 2.32275054865, "when": 1.02076769755, "user": 7.71053909665}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  5 Things You Need to Know about Reinforcement Learning</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb 5 Things You Need to Know about Reinforcement Learning Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/understanding-feature-engineering-deep-learning-methods-text-data.html\" rel=\"prev\" title=\"Understanding Feature Engineering: Deep Learning Methods for Text Data\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/wharton-successful-applications-customer-analytics.html\" rel=\"next\" title=\"Wharton: Successful Applications of Customer Analytics, May 9-10, Philadelphia\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=79345\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-79345 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 28-Mar, 2018  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/index.html\">Mar</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/tutorials.html\">Tutorials, Overviews</a> \u00bb 5 Things You Need to Know about Reinforcement Learning (\u00a0<a href=\"/2018/n14.html\">18:n14</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">5 Things You Need to Know about Reinforcement Learning</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/03/understanding-feature-engineering-deep-learning-methods-text-data.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/03/wharton-successful-applications-customer-analytics.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/machine-learning\" rel=\"tag\">Machine Learning</a>, <a href=\"https://www.kdnuggets.com/tag/markov-chains\" rel=\"tag\">Markov Chains</a>, <a href=\"https://www.kdnuggets.com/tag/reinforcement-learning\" rel=\"tag\">Reinforcement Learning</a>, <a href=\"https://www.kdnuggets.com/tag/rich-sutton\" rel=\"tag\">Rich Sutton</a></div>\n<br/>\n<p class=\"excerpt\">\n     With the popularity of Reinforcement Learning continuing to grow, we take a look at five things you need to know about RL.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/shweta-bhatt\" rel=\"author\" title=\"Posts by Shweta Bhatt\">Shweta Bhatt</a>, Youplus.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p>Reinforcement Learning is one of the hottest research topics currently and its popularity is only growing day by day. Let\u2019s look at 5 useful things to know about RL.</p>\n<ol>\n<li><strong>What is reinforcement learning? How does it relate with other ML techniques?</strong></li>\n</ol>\n<p>Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.</p>\n<p>Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning where feedback provided to the agent is correct set of actions for performing a task, reinforcement learning uses rewards and punishment as signals for positive and negative behavior.</p>\n<p>As compared to unsupervised learning, reinforcement learning is different in terms of goals. While the goal in unsupervised learning is to find similarities and differences between data points, in reinforcement learning the goal is to find a suitable action model that would maximize the total cumulative reward of the agent. The figure below represents the basic idea and elements involved in a reinforcement learning model.</p>\n<p>\u00a0</p>\n<p><img alt=\"Reinforcement Learning Fig. 1\" src=\"/images/reinforcement-learning-fig1-700.jpg\" width=\"100%\"/></p>\n<p>Figure 1</p>\n<p>\u00a0</p>\n<ol start=\"2\">\n<li><strong> How to formulate a basic reinforcement Learning problem?</strong></li>\n</ol>\n<p>\u00a0</p>\n<p>Some key terms that describe the elements of a RL problem are:</p>\n<p>\u00a0</p>\n<p><strong>Environment</strong>: Physical world in which the agent operates</p>\n<p><strong>State</strong>: Current situation of the agent</p>\n<p><strong>Reward</strong>: Feedback from the environment</p>\n<p><strong>Policy</strong>: Method to map agent\u2019s state to actions</p>\n<p><strong>Value</strong>: Future reward that an agent would receive by taking an action in a particular state</p>\n<p>A Reinforcement Learning problem can be best explained through games. Let\u2019s take the game of PacMan where the goal of the agent (PacMan) is to eat the food in the grid while avoiding the ghosts on its way. The grid world is the interactive environment for the agent. PacMan receives a reward for eating food and punishment if it gets killed by the ghost (loses the game). The states are the location of PacMan in the grid world and the total cumulative reward is PacMan winning the game.</p>\n<p>In order to build an optimal policy, the agent faces the dilemma of exploring new states while maximizing its reward at the same time. This is called <strong>Exploration vs Exploitation trade-off</strong>.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Markov_decision_process\">Markov Decision Processes (MDPs)</a> are mathematical frameworks to describe an environment in reinforcement learning and almost all RL problems can be formalized using MDPs. \u00a0An MDP consists of a set of finite environment states S, a set of possible actions A(s) in each state, a real valued reward function R(s) and a transition model P(s\u2019, s | a). However, real world environments are more likely to lack any prior knowledge of environment dynamics. Model-free RL methods come handy in such cases.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Q-learning\"><strong>Q-learning</strong></a> is a commonly used model free approach which can be used for building a self-playing PacMan agent. It revolves around the notion of updating Q values which denotes value of doing action <em>a</em> in state <em>s</em>. The value update rule is the core of the Q-learning algorithm.</p>\n<p>\u00a0</p>\n<p><img alt=\"Reinforcement Learning Fig2: Update Rule\" src=\"/images/reinforcement-learning-fig2-666.jpg\" width=\"100%\"/></p>\n<p><strong>Figure 2: Reinforcement Learning Update Rule</strong></p>\n<p>\u00a0</p>\n<p><img alt=\"Reinforcement Learning Fig3 Pacman\" src=\"/images/reinforcement-learning-fig3-pacman.gif\" width=\"637\"/></p>\n<p><strong>Figure 3: PacMan</strong></p>\n<p>\u00a0</p>\n<p>Here\u2019s a <a href=\"https://www.youtube.com/watch?v=QilHGSYbjDQ\">video</a> of a Deep reinforcement learning PacMan agent</p>\n<p>\u00a0</p>\n<ol start=\"3\">\n<li><strong>What are some most used Reinforcement Learning algorithms?</strong></li>\n</ol>\n<p>\u00a0</p>\n<p>Q-learning and SARSA (State-Action-Reward-State-Action) are two commonly used model-free RL algorithms. They differ in terms of their exploration strategies while their exploitation strategies are similar. While Q-learning is an off-policy method in which the agent learns the value based on action a* derived from the another policy, SARSA is an on-policy method where it learns the value based on its current action <em>a</em>derived from its current policy. These two methods are simple to implement but lack generality as they do not have the ability to estimate values for unseen states.</p>\n<p>This can be overcome by more advanced algorithms such as <a href=\"https://deepmind.com/research/dqn/\">Deep Q-Networks</a> which use Neural Networks to estimate Q-values. But DQNs can only handle discrete, low-dimensional action spaces. <a href=\"https://arxiv.org/abs/1509.02971\">DDPG(Deep Deterministic Policy Gradient)</a>is a model-free, off-policy, actor-critic algorithm that tackles this problem by learning policies in high dimensional, continuous action spaces.</p>\n<p>\u00a0</p>\n<p><img alt=\"Reinforcement Learning Fig4: actor-critic architecture\" src=\"/images/reinforcement-learning-fig4-552.jpg\" width=\"100%\"/></p>\n<p><strong>Figure 4: actor-critic architecture for Reinforcement Learning</strong></p>\n<p>\u00a0</p>\n<ol start=\"4\">\n<li><strong> What are the practical applications of Reinforcement Learning? </strong></li>\n</ol>\n<p>Since, RL requires a lot of data, therefore it is most applicable in domains where simulated data is readily available like gameplay, robotics.</p>\n<ul>\n<li>RL is quite widely used in building AI for playing computer games. <a href=\"https://deepmind.com/blog/alphago-zero-learning-scratch/\">AlphaGo Zero</a> is the first computer program to defeat a world champion in the ancient Chinese game of Go. Others include ATARI games, Backgammon, etc</li>\n<li>In robotics and industrial automation,RL is used to enable the robot to create an efficient adaptive control system for itself which learns from its own experience and behavior.<a href=\"https://deepmind.com/research/publications/deep-reinforcement-learning-robotic-manipulation/\">DeepMind\u2019s work</a> on Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Policy updates is a good example of the same.</li>\n<li>Watch this interesting demonstration <a href=\"https://www.youtube.com/watch?v=ZhsEKTo7V04&amp;t=48s\">video</a>.</li>\n<li>Other applications of RL include text summarization engines, dialog agents (text, speech) which can learn from user interactions and improve with time, learning optimal treatment policies in healthcare and RL based agents for online stock trading.</li>\n</ul>\n<p><strong>\u00a0</strong></p>\n<ol start=\"5\">\n<li><strong> How can I get started with Reinforcement Learning?</strong></li>\n</ol>\n<p>For understanding the basic concepts of RL,</p>\n<ul>\n<li><strong>Reinforcement Learning-An Introduction</strong>, a book by the father of Reinforcement Learning- <a href=\"https://en.wikipedia.org/wiki/Richard_S._Sutton\">Richard Sutton</a> and his doctoral advisor <a href=\"https://en.wikipedia.org/wiki/Andrew_Barto\">Andrew Barto</a>. An online draft of the book is available here <a href=\"http://incompleteideas.net/book/the-book-2nd.html\">http://incompleteideas.net/book/the-book-2nd.html</a></li>\n<li><a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\"><strong>Teaching material</strong></a>from <strong>David Silver</strong> including video lectures is a great introductory course on RL</li>\n<li>Here\u2019s another <a href=\"http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf\">technical tutorial</a> on RL by <strong>Pieter Abbeel</strong> and <strong>John Schulman</strong> (Open AI/ Berkeley AI Research Lab).</li>\n<li>For getting started with building and testing RL agents,</li>\n<li><a href=\"http://karpathy.github.io/2016/05/31/rl/\">This blog</a> on how to train a Neural Network ATARI Pong agent with Policy Gradients from raw pixels by<strong> Andrej Karpathy</strong> will help you get your first Deep Reinforcement Learning agent up and running in just 130 lines of Python code.</li>\n<li><a href=\"https://deepmind.com/blog/open-sourcing-deepmind-lab/\">DeepMind Lab</a> is an open source 3D game-like platform created for agent-based AI research with rich simulated environments.</li>\n<li><a href=\"https://www.microsoft.com/en-us/research/project/project-malmo/\">Project Malmo</a> is another AI experimentation platform for supporting fundamental research in AI.</li>\n<li><a href=\"https://gym.openai.com/\">OpenAI gym</a> is a toolkit for building and comparing reinforcement learning algorithms.</li>\n</ul>\n<p><b>Bio: <a href=\"https://www.linkedin.com/in/shweta-bhatt-1a930b12/\">Shweta Bhatt</a></b> is AI researcher with experience in private and public sector, passionate about the impact and applications of deriving knowledge from data to solve challenging problems. She likes telling stories with data and is based in London.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"https://www.kdnuggets.com/2018/02/resurgence-ai-1983-2010.html\">Resurgence of AI During 1983-2010</a></li>\n<li><a href=\"https://www.kdnuggets.com/2017/12/interview-rich-sutton-reinforcement-learning.html\"><b>Exclusive: Interview with Rich Sutton, the Father of Reinforcement Learning</b></a>\n<li><a href=\"https://www.kdnuggets.com/2017/12/when-reinforcement-learning-not-used.html\">When reinforcement learning should not be used?</a></li>\n<li><a href=\"https://www.kdnuggets.com/2018/03/databricks-ebook-making-machine-learning-simple.html\">Making Machine Learning Simple</a></li>\n</li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets'; \n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/03/understanding-feature-engineering-deep-learning-methods-text-data.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/03/wharton-successful-applications-customer-analytics.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/index.html\">Mar</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/03/tutorials.html\">Tutorials, Overviews</a> \u00bb 5 Things You Need to Know about Reinforcement Learning (\u00a0<a href=\"/2018/n14.html\">18:n14</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556324115\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></body></html>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n\n\n<!-- Dynamic page generated in 0.644 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 20:15:15 -->\n<!-- Compression = gzip -->", "content_tokenized": ["shweta", "bhatt", "youplus", "comment", "reinforc", "learn", "one", "the", "hottest", "research", "topic", "current", "and", "popular", "onli", "grow", "day", "day", "let", "look", "num", "use", "thing", "know", "about", "what", "reinforc", "learn", "how", "doe", "relat", "with", "other", "techniqu", "reinforc", "type", "machin", "learn", "techniqu", "that", "enabl", "agent", "learn", "interact", "environ", "trial", "and", "error", "use", "feedback", "from", "own", "action", "and", "experi", "though", "both", "supervis", "and", "reinforc", "learn", "use", "map", "between", "input", "and", "output", "unlik", "supervis", "learn", "where", "feedback", "provid", "the", "agent", "correct", "set", "action", "for", "perform", "task", "reinforc", "learn", "use", "reward", "and", "punish", "signal", "for", "posit", "and", "negat", "behavior", "compar", "unsupervis", "learn", "reinforc", "learn", "differ", "term", "goal", "while", "the", "goal", "unsupervis", "learn", "find", "similar", "and", "differ", "between", "data", "point", "reinforc", "learn", "the", "goal", "find", "suitabl", "action", "model", "that", "would", "maxim", "the", "total", "cumul", "reward", "the", "agent", "the", "figur", "below", "repres", "the", "basic", "idea", "and", "element", "involv", "reinforc", "learn", "model", "figur", "num", "how", "formul", "basic", "reinforc", "learn", "problem", "some", "key", "term", "that", "describ", "the", "element", "problem", "are", "environ", "physic", "world", "which", "the", "agent", "oper", "state", "current", "situat", "the", "agent", "reward", "feedback", "from", "the", "environ", "polici", "method", "map", "agent", "state", "action", "valu", "futur", "reward", "that", "agent", "would", "receiv", "take", "action", "particular", "state", "reinforc", "learn", "problem", "can", "best", "explain", "through", "game", "let", "take", "the", "game", "pacman", "where", "the", "goal", "the", "agent", "pacman", "eat", "the", "food", "the", "grid", "while", "avoid", "the", "ghost", "way", "the", "grid", "world", "the", "interact", "environ", "for", "the", "agent", "pacman", "receiv", "reward", "for", "eat", "food", "and", "punish", "get", "kill", "the", "ghost", "lose", "the", "game", "the", "state", "are", "the", "locat", "pacman", "the", "grid", "world", "and", "the", "total", "cumul", "reward", "pacman", "win", "the", "game", "order", "build", "optim", "polici", "the", "agent", "face", "the", "dilemma", "explor", "new", "state", "while", "maxim", "reward", "the", "same", "time", "this", "call", "explor", "exploit", "tradeoff", "markov", "decis", "process", "mdps", "are", "mathemat", "framework", "describ", "environ", "reinforc", "learn", "and", "almost", "all", "problem", "can", "formal", "use", "mdps", "consist", "set", "finit", "environ", "state", "set", "possibl", "action", "each", "state", "real", "valu", "reward", "function", "and", "transit", "model", "howev", "real", "world", "environ", "are", "more", "like", "lack", "ani", "prior", "knowledg", "environ", "dynam", "modelfre", "method", "come", "handi", "such", "case", "qlearn", "common", "use", "model", "free", "approach", "which", "can", "use", "for", "build", "selfplay", "pacman", "agent", "revolv", "around", "the", "notion", "updat", "valu", "which", "denot", "valu", "action", "state", "the", "valu", "updat", "rule", "the", "core", "the", "qlearn", "algorithm", "figur", "num", "reinforc", "learn", "updat", "rule", "figur", "num", "pacman", "here", "video", "deep", "reinforc", "learn", "pacman", "agent", "what", "are", "some", "most", "use", "reinforc", "learn", "algorithm", "qlearn", "and", "stateactionrewardstateact", "are", "two", "common", "use", "modelfre", "algorithm", "they", "differ", "term", "their", "explor", "strategi", "while", "their", "exploit", "strategi", "are", "similar", "while", "qlearn", "offpolici", "method", "which", "the", "agent", "learn", "the", "valu", "base", "action", "deriv", "from", "the", "anoth", "polici", "onpolici", "method", "where", "learn", "the", "valu", "base", "current", "action", "deriv", "from", "current", "polici", "these", "two", "method", "are", "simpl", "implement", "but", "lack", "general", "they", "not", "have", "the", "abil", "estim", "valu", "for", "unseen", "state", "this", "can", "overcom", "more", "advanc", "algorithm", "such", "deep", "qnetwork", "which", "use", "neural", "network", "estim", "qvalu", "but", "dqns", "can", "onli", "handl", "discret", "lowdimension", "action", "space", "modelfre", "offpolici", "actorcrit", "algorithm", "that", "tackl", "this", "problem", "learn", "polici", "high", "dimension", "continu", "action", "space", "figur", "num", "actorcrit", "architectur", "for", "reinforc", "learn", "what", "are", "the", "practic", "applic", "reinforc", "learn", "sinc", "requir", "lot", "data", "therefor", "most", "applic", "domain", "where", "simul", "data", "readili", "avail", "like", "gameplay", "robot", "quit", "wide", "use", "build", "for", "play", "comput", "game", "alphago", "zero", "the", "first", "comput", "program", "defeat", "world", "champion", "the", "ancient", "chines", "game", "other", "includ", "game", "backgammon", "etc", "robot", "and", "industri", "autom", "use", "enabl", "the", "robot", "creat", "effici", "adapt", "control", "system", "for", "itself", "which", "learn", "from", "own", "experi", "and", "behavior", "deepmind", "work", "deep", "reinforc", "learn", "for", "robot", "manipul", "with", "asynchron", "polici", "updat", "good", "exampl", "the", "same", "watch", "this", "interest", "demonstr", "video", "other", "applic", "includ", "text", "summar", "engin", "dialog", "agent", "text", "speech", "which", "can", "learn", "from", "user", "interact", "and", "improv", "with", "time", "learn", "optim", "treatment", "polici", "healthcar", "and", "base", "agent", "for", "onlin", "stock", "trade", "how", "can", "get", "start", "with", "reinforc", "learn", "for", "understand", "the", "basic", "concept", "reinforc", "learningan", "introduct", "book", "the", "father", "reinforc", "learn", "richard", "sutton", "and", "his", "doctor", "advisor", "andrew", "barto", "onlin", "draft", "the", "book", "avail", "here", "bookthebooknumndhtml", "teach", "materi", "from", "david", "silver", "includ", "video", "lectur", "great", "introductori", "cours", "here", "anoth", "technic", "tutori", "pieter", "abbeel", "and", "john", "schulman", "open", "berkeley", "research", "lab", "for", "get", "start", "with", "build", "and", "test", "agent", "this", "blog", "how", "train", "neural", "network", "pong", "agent", "with", "polici", "gradient", "from", "raw", "pixel", "andrej", "karpathi", "will", "help", "get", "first", "deep", "reinforc", "learn", "agent", "and", "run", "just", "num", "line", "python", "code", "deepmind", "lab", "open", "sourc", "gamelik", "platform", "creat", "for", "agentbas", "research", "with", "rich", "simul", "environ", "project", "malmo", "anoth", "experiment", "platform", "for", "support", "fundament", "research", "openai", "gym", "toolkit", "for", "build", "and", "compar", "reinforc", "learn", "algorithm", "bio", "shweta", "bhatt", "research", "with", "experi", "privat", "and", "public", "sector", "passion", "about", "the", "impact", "and", "applic", "deriv", "knowledg", "from", "data", "solv", "challeng", "problem", "she", "like", "tell", "stori", "with", "data", "and", "base", "london", "relat", "resurg", "dure", "num", "exclus", "interview", "with", "rich", "sutton", "the", "father", "reinforc", "learn", "when", "reinforc", "learn", "should", "not", "use", "make", "machin", "learn", "simpl"], "timestamp_scraper": 1556367597.241304, "title": "5 Things You Need to Know about Reinforcement Learning", "read_time": 325.2, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/shweta-bhatt\" rel=\"author\" title=\"Posts by Shweta Bhatt\">Shweta Bhatt</a>, Youplus.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p>Reinforcement Learning is one of the hottest research topics currently and its popularity is only growing day by day. Let\u2019s look at 5 useful things to know about RL.</p>\n<ol>\n<li><strong>What is reinforcement learning? How does it relate with other ML techniques?</strong></li>\n</ol>\n<p>Reinforcement Learning(RL) is a type of machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.</p>\n<p>Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning where feedback provided to the agent is correct set of actions for performing a task, reinforcement learning uses rewards and punishment as signals for positive and negative behavior.</p>\n<p>As compared to unsupervised learning, reinforcement learning is different in terms of goals. While the goal in unsupervised learning is to find similarities and differences between data points, in reinforcement learning the goal is to find a suitable action model that would maximize the total cumulative reward of the agent. The figure below represents the basic idea and elements involved in a reinforcement learning model.</p>\n<p>\u00a0</p>\n<p><img alt=\"Reinforcement Learning Fig. 1\" src=\"/images/reinforcement-learning-fig1-700.jpg\" width=\"100%\"/></p>\n<p>Figure 1</p>\n<p>\u00a0</p>\n<ol start=\"2\">\n<li><strong> How to formulate a basic reinforcement Learning problem?</strong></li>\n</ol>\n<p>\u00a0</p>\n<p>Some key terms that describe the elements of a RL problem are:</p>\n<p>\u00a0</p>\n<p><strong>Environment</strong>: Physical world in which the agent operates</p>\n<p><strong>State</strong>: Current situation of the agent</p>\n<p><strong>Reward</strong>: Feedback from the environment</p>\n<p><strong>Policy</strong>: Method to map agent\u2019s state to actions</p>\n<p><strong>Value</strong>: Future reward that an agent would receive by taking an action in a particular state</p>\n<p>A Reinforcement Learning problem can be best explained through games. Let\u2019s take the game of PacMan where the goal of the agent (PacMan) is to eat the food in the grid while avoiding the ghosts on its way. The grid world is the interactive environment for the agent. PacMan receives a reward for eating food and punishment if it gets killed by the ghost (loses the game). The states are the location of PacMan in the grid world and the total cumulative reward is PacMan winning the game.</p>\n<p>In order to build an optimal policy, the agent faces the dilemma of exploring new states while maximizing its reward at the same time. This is called <strong>Exploration vs Exploitation trade-off</strong>.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Markov_decision_process\">Markov Decision Processes (MDPs)</a> are mathematical frameworks to describe an environment in reinforcement learning and almost all RL problems can be formalized using MDPs. \u00a0An MDP consists of a set of finite environment states S, a set of possible actions A(s) in each state, a real valued reward function R(s) and a transition model P(s\u2019, s | a). However, real world environments are more likely to lack any prior knowledge of environment dynamics. Model-free RL methods come handy in such cases.</p>\n<p><a href=\"https://en.wikipedia.org/wiki/Q-learning\"><strong>Q-learning</strong></a> is a commonly used model free approach which can be used for building a self-playing PacMan agent. It revolves around the notion of updating Q values which denotes value of doing action <em>a</em> in state <em>s</em>. The value update rule is the core of the Q-learning algorithm.</p>\n<p>\u00a0</p>\n<p><img alt=\"Reinforcement Learning Fig2: Update Rule\" src=\"/images/reinforcement-learning-fig2-666.jpg\" width=\"100%\"/></p>\n<p><strong>Figure 2: Reinforcement Learning Update Rule</strong></p>\n<p>\u00a0</p>\n<p><img alt=\"Reinforcement Learning Fig3 Pacman\" src=\"/images/reinforcement-learning-fig3-pacman.gif\" width=\"637\"/></p>\n<p><strong>Figure 3: PacMan</strong></p>\n<p>\u00a0</p>\n<p>Here\u2019s a <a href=\"https://www.youtube.com/watch?v=QilHGSYbjDQ\">video</a> of a Deep reinforcement learning PacMan agent</p>\n<p>\u00a0</p>\n<ol start=\"3\">\n<li><strong>What are some most used Reinforcement Learning algorithms?</strong></li>\n</ol>\n<p>\u00a0</p>\n<p>Q-learning and SARSA (State-Action-Reward-State-Action) are two commonly used model-free RL algorithms. They differ in terms of their exploration strategies while their exploitation strategies are similar. While Q-learning is an off-policy method in which the agent learns the value based on action a* derived from the another policy, SARSA is an on-policy method where it learns the value based on its current action <em>a</em>derived from its current policy. These two methods are simple to implement but lack generality as they do not have the ability to estimate values for unseen states.</p>\n<p>This can be overcome by more advanced algorithms such as <a href=\"https://deepmind.com/research/dqn/\">Deep Q-Networks</a> which use Neural Networks to estimate Q-values. But DQNs can only handle discrete, low-dimensional action spaces. <a href=\"https://arxiv.org/abs/1509.02971\">DDPG(Deep Deterministic Policy Gradient)</a>is a model-free, off-policy, actor-critic algorithm that tackles this problem by learning policies in high dimensional, continuous action spaces.</p>\n<p>\u00a0</p>\n<p><img alt=\"Reinforcement Learning Fig4: actor-critic architecture\" src=\"/images/reinforcement-learning-fig4-552.jpg\" width=\"100%\"/></p>\n<p><strong>Figure 4: actor-critic architecture for Reinforcement Learning</strong></p>\n<p>\u00a0</p>\n<ol start=\"4\">\n<li><strong> What are the practical applications of Reinforcement Learning? </strong></li>\n</ol>\n<p>Since, RL requires a lot of data, therefore it is most applicable in domains where simulated data is readily available like gameplay, robotics.</p>\n<ul>\n<li>RL is quite widely used in building AI for playing computer games. <a href=\"https://deepmind.com/blog/alphago-zero-learning-scratch/\">AlphaGo Zero</a> is the first computer program to defeat a world champion in the ancient Chinese game of Go. Others include ATARI games, Backgammon, etc</li>\n<li>In robotics and industrial automation,RL is used to enable the robot to create an efficient adaptive control system for itself which learns from its own experience and behavior.<a href=\"https://deepmind.com/research/publications/deep-reinforcement-learning-robotic-manipulation/\">DeepMind\u2019s work</a> on Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Policy updates is a good example of the same.</li>\n<li>Watch this interesting demonstration <a href=\"https://www.youtube.com/watch?v=ZhsEKTo7V04&amp;t=48s\">video</a>.</li>\n<li>Other applications of RL include text summarization engines, dialog agents (text, speech) which can learn from user interactions and improve with time, learning optimal treatment policies in healthcare and RL based agents for online stock trading.</li>\n</ul>\n<p><strong>\u00a0</strong></p>\n<ol start=\"5\">\n<li><strong> How can I get started with Reinforcement Learning?</strong></li>\n</ol>\n<p>For understanding the basic concepts of RL,</p>\n<ul>\n<li><strong>Reinforcement Learning-An Introduction</strong>, a book by the father of Reinforcement Learning- <a href=\"https://en.wikipedia.org/wiki/Richard_S._Sutton\">Richard Sutton</a> and his doctoral advisor <a href=\"https://en.wikipedia.org/wiki/Andrew_Barto\">Andrew Barto</a>. An online draft of the book is available here <a href=\"http://incompleteideas.net/book/the-book-2nd.html\">http://incompleteideas.net/book/the-book-2nd.html</a></li>\n<li><a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\"><strong>Teaching material</strong></a>from <strong>David Silver</strong> including video lectures is a great introductory course on RL</li>\n<li>Here\u2019s another <a href=\"http://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf\">technical tutorial</a> on RL by <strong>Pieter Abbeel</strong> and <strong>John Schulman</strong> (Open AI/ Berkeley AI Research Lab).</li>\n<li>For getting started with building and testing RL agents,</li>\n<li><a href=\"http://karpathy.github.io/2016/05/31/rl/\">This blog</a> on how to train a Neural Network ATARI Pong agent with Policy Gradients from raw pixels by<strong> Andrej Karpathy</strong> will help you get your first Deep Reinforcement Learning agent up and running in just 130 lines of Python code.</li>\n<li><a href=\"https://deepmind.com/blog/open-sourcing-deepmind-lab/\">DeepMind Lab</a> is an open source 3D game-like platform created for agent-based AI research with rich simulated environments.</li>\n<li><a href=\"https://www.microsoft.com/en-us/research/project/project-malmo/\">Project Malmo</a> is another AI experimentation platform for supporting fundamental research in AI.</li>\n<li><a href=\"https://gym.openai.com/\">OpenAI gym</a> is a toolkit for building and comparing reinforcement learning algorithms.</li>\n</ul>\n<p><b>Bio: <a href=\"https://www.linkedin.com/in/shweta-bhatt-1a930b12/\">Shweta Bhatt</a></b> is AI researcher with experience in private and public sector, passionate about the impact and applications of deriving knowledge from data to solve challenging problems. She likes telling stories with data and is based in London.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"https://www.kdnuggets.com/2018/02/resurgence-ai-1983-2010.html\">Resurgence of AI During 1983-2010</a></li>\n<li><a href=\"https://www.kdnuggets.com/2017/12/interview-rich-sutton-reinforcement-learning.html\"><b>Exclusive: Interview with Rich Sutton, the Father of Reinforcement Learning</b></a>\n<li><a href=\"https://www.kdnuggets.com/2017/12/when-reinforcement-learning-not-used.html\">When reinforcement learning should not be used?</a></li>\n<li><a href=\"https://www.kdnuggets.com/2018/03/databricks-ebook-making-machine-learning-simple.html\">Making Machine Learning Simple</a></li>\n</li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets'; \n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}