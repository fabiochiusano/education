{"content": "By Andres Milioto, University of Bonn. Autonomous ground robot spraying FLOURISH using the selective spraying tool, as seen from UAV. To produce high-quality food and feed a growing world population with the given amount of arable land in a sustainable manner, we must develop new methods of sustainable farming that increase yield while minimizing chemical inputs such as fertilizers, herbicides, and pesticides. I and my colleagues are working on a robotics-centered approaches to address this grand challenge. My name is Andres Milioto, and I am a research assistant and Ph.D. student in robotics at the Photogrammetry and Robotics Lab (  ) at the University of Bonn, Germany. Together with Philipp Lottes, Nived Chebrolu, and our supervisor Prof. Dr. Cyrill Stachniss we are developing an adaptable ground and aerial robots for smart farming in the context of the EC-funded project \u201cFlourish\u201d ( / ), where we collaborate with several other Universities and industry partners across Europe. The Flourish consortium is committed to develop new robotic methods for sustainable farming that aim at minimizing chemical inputs such as fertilizers, herbicides, and pesticides in order to reduce the side-effects on our environment. Our precision agriculture techniques seek to address this challenge by monitoring key indicators of crop health and targeting treatment only to plants or infested areas that need it. The development of these novel methods is a very active and ongoing area of research, and the main goal of the project is to bridge the gap between the current and desired capabilities of agricultural robots by developing an adaptable robotic solution for precision farming. While conventional weed control systems treat the whole field uniformly with the same dose of herbicide, more novel perception-controlled weeding systems offer the potential to perform a treatment on a per-plant level, for example by selective spraying or mechanical weeding. Automatically treating plants at an individual level requires a plant classification system, which can analyze the sensor data perceived by the the robots in the field in real time, detects individual plants, and thoroughly distinguishes the crops and weeds. Our team is responsible for the perception aspect of the approach. From left to Right: Full pipeline from raw RGB+NIR images to classification output Left: Keypoint extraction and area around keypoint where features are calculated. Right: Classification of the Keypoints and results. We focus on a detection on a per-plant basis to estimate the amount of crops as well as various weed species as a part of an autonomous robotic perception system. We are working on vision-based classification system for identifying crops and weeds in both, RGB-only as well as RGB combined with near infra-red (NIR) imagery. At the beginning of the project, we approached the implementation of the crop vs. weed detector by using a random forest classifier over 500 statistical, shape, and geometrical features around different image keypoints containing vegetation, and later applying a Markov Random Field to smooth the results taking neighbor information into consideration. This approach works extraordinarily well, reaching more than 95% precision and recall for both crops and weeds. We further improved this approach for detecting crops and weeds at a very early growth stage, i.e at a leaf size smaller than 0.5 cm^2. A big challenge with purely vision-based approaches is the transfer of a learned classification model to unseen field environments, where the soil conditions, growth stages of the plants and weed types may have changed. We solved this generalization problem by exploiting the spatial structure of the crop plants, which are sowed in rows with a certain lattice distance in between. We developed a geometry-based classification system, which interacts with the vision-based system in a semi-supervised manner in order to exploit the time invariant geometry information to retrain the vision classifier online while the robot discovers new field environments. Furthermore, we introduced a method for initializing the whole classification system with \u00a0a labeling effort of only 1 minute and still achieve state-of-the-art performance (see publications by Lottes et al., 2016, 2017). The experience we have made in the last two years during the research and development of the classification system led to the following conclusions. First, the design of the hand-crafted features is a very time consuming task that requires a very high level of specialization. Second, the implementation of the feature extraction in GPU code (using CUDA for NVIDIA GPUs) in order to make the system run in real time requires significant effort and expertise, and it took several months. Third, even when computing all features efficiently in parallel, the execution time was not close to the frame rate of a camera, and our approach was still very reliant on the availability of near infrared information, which comes at a high cost. The advances in image classification, object detection, and semantic segmentation using deep Convolutional Neural Networks, which spawned the availability of open source tools such as Caffe and TensorFlow (to name a couple) to easily manipulate neural network graphs, and to quickly prototype, train, and deploy using off the shelf GPUs made a very strong case in favor of CNNs for our classifier. Another thing that made a deep learning approach possible was that during the last 2 years of project we have been able to gather a dataset containing a large amount of data (in the order of 10^5 labeled images). A big part of these dataset has been made publicly available to allow other institutions to benefit from it (see publications by Nived Chebrolu et. al), and also to compare algorithmic results. Our first attempt was in favor of an object detection pipeline, using an approach similar to R-CNN. The key difference in our approach is that, given our specific problem, there is a possibility to pre-segment the vegetation from the soil, using well known vegetation indexes such as the NDVI (when NIR information is present) and/or Excess green (for RGB-only). Therefore, instead of using selective search for our region proposals, we were able to use the connected components in this vegetation mask, which speeds up the process significantly. This new approach had similar results to the random forest based one, but also had two big advantages: the main one is that it runs at over 5Hz in an NVIDIA Jetson TX2; the second one is that it took less than 2 months to implement, including the time to learn the tools. At the time, we didn\u2019t even have dedicated hardware to train the networks, and we were working off of a notebook GPU, and the lightweight network worked very well after 3 days of training. (See publications by Andres Milioto et. al, 2017) From left to right: RGB, NIR images. Vegetation Segmentation. Blob extraction. Classification of blobs. Overlay of classified masks to original image This motivated us to further pursue this direction, and we therefore acquired a dedicated GPU computer with 4 NVIDIA GTX1080Ti\u2019s. In order to quantify how much deep learning is affecting the lab, after roughly 3 months of receiving the first cluster, we have already ordered a second identical twin to the one we already have, because we are already having trouble scheduling GPU time. Full semantic segmentation pipeline. Coming back to the approach, we have currently dropped the object detection pipeline in favor of a full semantic segmentation one. This allows us to eliminate some of the limitations of the old approach, such as an impossibility to handle overlapping plants, and reliance on the pre-segmentation, which needed hyperparameter tuning when transferring to a new field. This new approach allows us, in addition, to get rid of the costly NIR information that we needed for an accurate pre-segmentation, and to work in constant time complexity. In order to help the architecture generalize well to different fields, and to obtain an accurate segmentation, we add to the RGB inputs several remappings and representations of the latter, which had already proven useful when designing features for the random classifier, such as vegetation indexes, different color spaces, gradients, edges, etc. We have then been able to significantly increase the performance of the semantic segmentation without hyperparameter re-tuning, and since the architecture is designed with real-time performance in mind, we are able to run at near frame rate of a commercial camera. The perception solution as a part of our project goals has already reached a maturity state and yields a high performance, and we have been working hard to bridge the last gaps towards an applicable system in real world. What this last gap entails is to get the algorithms running with zero re-labeling effort when applying it to different fields and crop species. We are exploring generative models, and several unsupervised learning approaches in order to achieve this, all of which would use the autonomous capabilities of the robots to gather new data in the new field and allow it to intelligently auto-retrain itself for the new task in hand. We are also always working on getting the models to run faster and faster, and using less resources and power, for example by using newly available hardware accelerators for neural networks. It has been a very exciting path to walk, and we are very eager to see what the future holds! Check our website for updates on this exciting journey! Resources: Photogrammetry and Robotics Lab Philipp Lottes Nived Chebrolu Cyrill Stachniss Andres Milioto Flourish Project \u00a0 Bio: Andres Milioto is a PhD Candidate in Robotics and AI at The University of Bonn. His drive is to help people lead better lives through technology by creating algorithms for cool robots that can perceive and interact with the world around us. Related: Medical Image Analysis with Deep Learning\u200a A Beginner\u2019s Guide To Understanding Convolutional Neural Networks Part 1 An Intuitive Explanation of Convolutional Neural Networks", "title_html": "<h1 id=\"title\">Real World Deep Learning: Neural Networks for Smart Crops</h1> ", "url": "https://www.kdnuggets.com/2017/11/real-world-deep-learning-neural-networks-smart-crops.html", "tfidf": {"tfidf": {"after": 2.04140414042, "hand": 1.6152202665600002, "real": 6.84310344828, "food": 2.9613878007800003, "arabl": 63.0, "roboticscent": 1134.0, "tensorflow": 1134.0, "quantifi": 41.669291338600004, "soil": 15.773472429220002, "stage": 4.1663823645199995, "basi": 2.42122922068, "troubl": 4.99088337001, "geometrybas": 1134.0, "chemic": 12.510638297880002, "agricultur": 8.91409320606, "space": 2.39818731118, "addit": 1.24634950542, "lott": 360.81818181899996, "would": 1.0828729281799998, "etc": 4.2066772655, "dataset": 387.219512196, "fertil": 20.1089297024, "assist": 2.17300848618, "crop": 87.39082568808, "manner": 7.8632986627, "well": 6.393449224799999, "unsupervis": 345.13043478300006, "work": 8.92160719304, "yield": 12.93887530562, "approach": 31.13348150085, "path": 4.6421052631599995, "geometri": 25.4423076923, "name": 2.20423464074, "motiv": 5.01611374408, "automat": 6.787516032490001, "mask": 23.554896142399997, "left": 4.3196081988, "instead": 1.59461631177, "creat": 1.2492917847, "how": 1.60250328051, "land": 1.7209756097599997, "philipp": 39.297029703, "segment": 45.33841028082, "perceiv": 9.84558139534, "dose": 32.0080645161, "had": 3.1427251732199997, "increas": 2.6404989605, "special": 1.4881889763799998, "initi": 1.35, "frame": 12.560126582279999, "given": 2.70852170946, "invari": 22.4237288136, "deep": 14.511882998159999, "beginn": 53.4545454545, "exploit": 11.58832116788, "open": 1.24556723678, "see": 5.08968502044, "autoretrain": 1134.0, "second": 3.3392694383999997, "updat": 5.56466876972, "new": 9.1609924986, "near": 3.86308703058, "but": 1.01632417899, "certain": 1.8077886586200003, "need": 4.31178707223, "our": 28.29106029108, "classif": 80.6707317073, "commit": 2.8860207235, "cost": 4.63871439006, "object": 7.04660452731, "remap": 793.8, "entail": 21.2530120482, "caff": 992.25, "applic": 3.42672134686, "walk": 3.56363636364, "lab": 43.2981818181, "level": 4.96331804919, "robot": 280.28247162720004, "has": 3.1309492505999996, "generat": 2.05275407292, "collabor": 4.45454545455, "region": 1.7647843486, "advantag": 3.32412060302, "use": 13.385303845939998, "experi": 1.87062566278, "resourc": 5.8974739970200005, "green": 2.63065451533, "bonn": 164.2344827586, "model": 6.2717935212, "treat": 7.18046132972, "hardwar": 37.6208530806, "alway": 2.06745670009, "excit": 19.636363636360002, "preseg": 1134.0, "introduc": 1.7258397651900002, "germani": 2.61118421053, "hyperparamet": 2268.0, "gtxnumti": 1134.0, "field": 16.0112057373, "visionbas": 3402.0, "not": 1.01567398119, "newli": 3.1847542627900003, "ecfund": 1134.0, "compon": 4.09491875161, "growth": 6.30125024806, "are": 10.299059357800001, "herbicid": 602.886075948, "minim": 12.21700654098, "less": 2.93809567872, "aim": 2.8960233491400005, "specif": 1.8719490626099997, "quick": 2.205, "whole": 4.58976582828, "handl": 3.9229058561900003, "better": 2.0065722952500002, "than": 3.0983606557499996, "then": 1.08657860516, "index": 13.993829881, "design": 4.37475888675, "spawn": 16.9615384615, "imageri": 19.7955112219, "reach": 2.99603698812, "last": 4.846893604040001, "allow": 5.086423708440001, "world": 3.34020618558, "task": 7.77282741738, "current": 3.0651607298, "featur": 9.16275490572, "rid": 23.837837837800002, "feed": 7.77853993141, "respons": 1.5066907089300001, "alreadi": 9.775862068950001, "that": 11.04382470125, "itself": 1.74557449148, "algorithm": 83.8521126762, "offer": 1.53896859248, "consortium": 22.3605633803, "pure": 4.716577540109999, "develop": 8.369003690040001, "such": 6.36908264244, "inform": 7.876562810100001, "sever": 4.28965144556, "explor": 3.39593582888, "therefor": 4.66803881212, "network": 18.15585688616, "aspect": 3.0893169877399997, "andor": 690.260869565, "phd": 44.7211267606, "control": 1.46959178006, "add": 4.61243463103, "may": 1.05201775893, "result": 4.58446433728, "unseen": 40.8123393316, "produc": 1.36932896326, "rate": 4.2809761359, "without": 1.29547123623, "bridg": 7.413495213639999, "matur": 9.31690140845, "futur": 1.8577112099200002, "twin": 6.954007884360001, "and": 55.00346456715, "from": 6.00340328982, "limit": 1.5186531471200002, "num": 14.00441056014, "excess": 5.30792377131, "technolog": 2.6034765496900003, "keypoint": 4536.0, "environ": 10.30685998701, "acquir": 3.10563380282, "for": 20.006300800200002, "color": 3.8255421686699997, "manipul": 9.145161290319999, "output": 7.676982591880001, "relianc": 21.2245989305, "seen": 1.61079545455, "smooth": 11.086592178800002, "with": 14.016774925859997, "zero": 8.75192943771, "presegment": 2268.0, "these": 2.14830852504, "handcraft": 233.470588235, "solut": 9.4556283502, "explan": 6.50922509225, "semant": 156.4137931036, "effort": 5.67743473596, "distanc": 3.4754816112099998, "month": 4.5239361702, "combin": 1.69760479042, "furthermor": 5.50294627383, "reduc": 1.98698372966, "veri": 11.32921027593, "extract": 23.109170305680003, "toward": 1.6303142329, "milioto": 5670.0, "weed": 518.823529412, "forest": 9.79093432008, "later": 1.08650424309, "focus": 2.01012914662, "train": 5.8097096853, "compar": 1.8662278123900002, "perceptioncontrol": 1134.0, "markov": 174.46153846200002, "spatial": 24.4246153846, "even": 2.32922535212, "ongo": 6.04569687738, "retrain": 180.409090909, "chebrolu": 3402.0, "search": 3.2539454806299997, "goal": 6.56304257958, "his": 1.0943682360200002, "big": 8.22022782189, "cmnum": 417.78947368400003, "numhz": 1134.0, "anoth": 1.13643521832, "infest": 75.961722488, "row": 5.549108703250001, "analyz": 9.68639414277, "notebook": 40.1924050633, "into": 1.01502461479, "medic": 3.27542809986, "through": 1.07074930869, "constant": 3.6589075823900004, "requir": 4.58534706846, "where": 3.20145190563, "what": 2.50686878256, "live": 1.30591428806, "techniqu": 3.7293868921800004, "popul": 2.17807655371, "help": 2.79925945518, "main": 2.50607734806, "type": 2.0281042411900003, "larg": 1.18574949585, "overlap": 12.0913937548, "potenti": 2.52080025405, "consider": 2.29920347574, "conclus": 4.84615384615, "base": 1.14628158845, "relat": 1.23750876919, "label": 8.95431472082, "grand": 3.06486486486, "geometr": 24.4622496148, "univers": 4.99559471364, "hold": 1.6551292744, "photogrammetri": 2268.0, "old": 1.52844902282, "camera": 17.979614949040002, "speed": 3.8703071672400005, "pursu": 4.15384615385, "jetson": 588.0, "onlin": 2.6051854282900004, "much": 1.1942229577299999, "togeth": 1.58095996813, "contain": 3.19629555064, "parallel": 4.57917507932, "problem": 3.53349655018, "deploy": 7.41869158879, "neural": 297.3033707865, "mind": 3.5918552036199998, "sow": 61.7743190661, "context": 4.25972632144, "when": 5.10383848775, "relabel": 512.129032258, "adapt": 6.64545835078, "understand": 2.96858638743, "speci": 13.296482412060001, "treatment": 7.74250182882, "dure": 2.1006946741599997, "supervisor": 25.898858075, "represent": 5.928304705, "classifi": 26.468822941, "gpus": 2116.8, "check": 6.50655737705, "present": 1.25551601423, "led": 1.33782758911, "percept": 25.04100946371, "across": 1.7318642958400001, "andr": 138.5340314135, "complex": 2.34021226415, "tool": 14.991501416430001, "latter": 2.34159292035, "overlay": 96.2181818182, "took": 2.8019767031400002, "intuit": 27.7068062827, "guid": 2.49113447356, "execut": 2.2363713199, "order": 9.97001334488, "raw": 10.6478873239, "health": 2.71570304482, "abl": 7.2834040600800005, "tune": 10.4173228346, "target": 3.2189781021900004, "pesticid": 132.3, "over": 2.05050048434, "colleagu": 8.23443983402, "analysi": 3.47852760736, "monitor": 6.05723006486, "calcul": 6.12972972973, "select": 6.07035432066, "consum": 4.93043478261, "earli": 1.12468121281, "same": 1.11857958148, "research": 5.8260550458600004, "thorough": 10.956521739100001, "there": 1.04091266719, "connect": 1.8843916913900003, "perplant": 2268.0, "europ": 2.0172808132099997, "eager": 15.5799803729, "shelf": 47.391044776099996, "possibl": 2.8347468976, "industri": 2.02319357716, "plant": 23.371608832809997, "semisupervis": 1134.0, "blob": 447.211267606, "exampl": 3.00966824644, "right": 4.21635977337, "recal": 5.30614973262, "take": 1.13961668222, "journey": 5.41843003413, "follow": 1.04640126549, "farm": 17.708867819279998, "further": 2.723623263, "discov": 2.52160101652, "code": 3.8807137619199996, "hard": 2.73253012048, "similar": 2.75028150714, "interact": 8.8371834122, "desir": 3.00170164492, "high": 3.44331983805, "power": 1.3396337861799998, "indic": 2.0826446281, "edg": 4.45704660303, "also": 3.04429530201, "coupl": 3.2572835453400004, "third": 1.4195278969999998, "around": 3.6418412601299996, "seek": 2.83753351206, "reliant": 57.7309090909, "acceler": 8.15408320493, "shape": 3.20338983051, "two": 2.0275862069, "aerial": 13.6391752577, "signific": 4.35874439463, "extraordinarili": 54.5567010309, "the": 99.0, "area": 4.16437877067, "run": 7.7846425419, "candid": 4.51279135873, "public": 4.8969771746, "peopl": 1.21320495186, "nive": 3402.0, "commerci": 2.4036336109, "challeng": 7.674508540110001, "veget": 62.340314136, "ground": 3.95220313666, "sideeffect": 1134.0, "system": 15.261382504610001, "individu": 3.6008165116800006, "rgbon": 2268.0, "partner": 4.173501577290001, "various": 1.3323262839899999, "elimin": 3.67670217693, "flourish": 29.69326683291, "minut": 3.11233091551, "uniform": 5.7231434751300005, "structur": 2.0580762250499998, "were": 2.04917715392, "which": 9.046726605, "random": 28.7608695652, "thing": 2.4065484311099996, "activ": 1.46403541129, "ident": 2.80792359392, "estim": 2.34991119005, "mechan": 3.41492794149, "other": 2.01984732824, "appli": 4.5944147012, "convent": 2.64159733777, "one": 5.031374786100001, "graph": 37.7102137767, "prof": 252.0, "known": 1.0859097127200001, "begin": 1.3305397251100002, "full": 5.00189035917, "some": 1.04036697248, "becaus": 1.1495184997499999, "imag": 18.909647779470003, "sourc": 1.69760479042, "get": 5.35687774155, "still": 2.3732715449599997, "intellig": 4.19334389857, "implement": 10.729443568379999, "sensor": 28.8654545455, "drop": 2.4594887684, "autonom": 33.259776536400004, "detect": 32.47732696896, "part": 4.17322731156, "both": 2.10431440122, "attempt": 1.4721810088999998, "includ": 1.0190641247799999, "precis": 15.966476701320001, "cool": 6.8578833693300005, "receiv": 1.3054847463200001, "team": 2.2748244734200003, "gather": 7.57262103506, "stateoftheart": 1134.0, "this": 14.05311077394, "schedul": 3.6648199445999996, "time": 9.10147143132, "process": 1.69524826482, "back": 1.26070038911, "chang": 1.1808985421, "perform": 7.6569885212500015, "smaller": 2.59369384088, "project": 10.520874751500001, "differ": 6.182724511249999, "leaf": 20.379974326099997, "been": 5.119638826199999, "institut": 1.7792222346700002, "statist": 4.24265098878, "between": 2.06907337416, "drive": 2.93510815308, "obtain": 2.68629441624, "all": 2.02293577982, "grow": 2.27287043665, "expertis": 20.0201765448, "stachniss": 2268.0, "strong": 1.6439888163999998, "gradient": 41.889182058, "condit": 1.92483026188, "solv": 7.26923076923, "case": 1.48498737256, "smart": 16.746835443, "day": 1.18371607516, "architectur": 10.25581395348, "websit": 2.52160101652, "cluster": 12.5007874016, "off": 3.0242880274400004, "more": 2.0343413634, "improv": 2.04376930999, "infrar": 78.9850746268, "dedic": 6.41066020594, "novel": 8.13111395646, "achiev": 3.74433962264, "rough": 3.29582727839, "benefit": 3.06841901817, "capabl": 7.316129032260001, "amount": 6.8108108108100005, "general": 2.2436404748400003, "made": 4.28155339804, "faster": 15.22877697842, "realtim": 429.081081081, "lead": 1.2664326739, "state": 1.0477133240899998, "close": 1.2848818387799998, "affect": 2.4794627518400003, "detector": 45.6206896552, "sustain": 13.081021697340002, "can": 2.35252278284, "avail": 6.9153871284, "comput": 7.855517070760001, "make": 1.0762660158600001, "favor": 9.399644760210002, "onli": 2.0512953033200003, "gap": 21.90800367984, "size": 2.49387370405, "distinguish": 3.36926994907, "highqual": 1134.0, "key": 4.5601034037, "student": 2.47174217655, "accur": 11.537790697680002, "neighbor": 5.781500364169999, "transfer": 5.45098712446, "advanc": 1.9997480791, "imposs": 4.96125, "data": 10.12930667802, "lightweight": 32.4, "cnns": 1134.0, "address": 5.72314347512, "cyril": 63.631262525, "method": 10.285714285720001, "spray": 88.3636363635, "sinc": 1.08368600683, "convolut": 303.363057324, "bio": 42.336000000000006, "retun": 690.260869565, "come": 2.65662650602, "must": 1.9220338983099996, "origin": 1.13724928367, "lattic": 72.16363636359999, "identifi": 2.30187037843, "direct": 1.22226499346, "first": 3.0228484386899996, "vision": 4.88041807562, "input": 36.6087624903, "propos": 1.9902218879299998, "have": 10.1489484114, "while": 3.1325966850899993, "proven": 9.818181818180001, "year": 2.0970873786400004, "easili": 3.6938110749199997, "prototyp": 11.7426035503, "learn": 13.936503291900001, "pipelin": 128.5506072876, "effici": 5.09335899904}, "logtfidf": {"after": 0.040981389296199995, "hand": 0.479471335336, "real": 2.473887181722, "food": 1.08565801008, "arabl": 4.14313472639, "roboticscent": 7.033506484289999, "tensorflow": 7.033506484289999, "quantifi": 3.72976443878, "soil": 4.13036477652, "stage": 1.467801880474, "basi": 0.884275353639, "troubl": 1.60761292215, "geometrybas": 7.033506484289999, "chemic": 3.6668643312599998, "agricultur": 2.9889727000999997, "space": 0.874713164972, "addit": 0.220218882972, "lott": 14.369285673959999, "would": 0.0796176279647, "etc": 1.4366730879700003, "dataset": 10.53168913328, "fertil": 4.6160335993, "assist": 0.776112606548, "crop": 20.45849157513, "manner": 2.73811803006, "well": 0.3810866298936, "unsupervis": 5.843922417409999, "work": 0.872276538184, "yield": 3.73417837726, "approach": 10.953504218715002, "path": 1.5351679838499999, "geometri": 3.2364134455299998, "name": 0.19446633276860004, "motiv": 1.61265547932, "automat": 1.9150850473199998, "mask": 4.93237304486, "left": 1.093657244259, "instead": 0.46663315041500003, "creat": 0.222576818514, "how": 0.47156695693000006, "land": 0.5428913449939999, "philipp": 5.95600351076, "segment": 12.13437067836, "perceiv": 3.1877511693400002, "dose": 3.4659878871800007, "had": 0.1394340732333, "increas": 0.555641437858, "special": 0.39755992860100003, "initi": 0.30010459245, "frame": 3.6747601172800004, "given": 0.606511621662, "invari": 3.1101197202099997, "deep": 5.1546938792, "beginn": 3.9788316751, "exploit": 3.5137012290400005, "open": 0.219591038029, "see": 0.963686341968, "autoretrain": 7.033506484289999, "second": 0.32141929014, "updat": 1.7164374626899999, "new": 0.1595695216599, "near": 0.758562972102, "but": 0.0161923720719, "certain": 0.592104362781, "need": 1.088220490326, "our": 10.291670570184001, "classif": 20.8779073629, "commit": 1.0598786410299998, "cost": 1.68258015236, "object": 2.561800754409, "remap": 6.676831540349999, "entail": 3.0564986287700004, "caff": 6.89997509166, "applic": 1.23160392849, "walk": 1.270781474, "lab": 8.00849506536, "level": 1.510386569829, "robot": 41.95436829178, "has": 0.1281718345644, "generat": 0.719182341736, "collabor": 1.4939250253100003, "region": 0.568028500824, "advantag": 1.20120515883, "use": 0.37970425651080003, "experi": 0.626272953933, "resourc": 2.16275388516, "green": 0.9672326803710001, "bonn": 12.008048672759998, "model": 2.2123502193330005, "treat": 2.55643290498, "hardwar": 5.86882263862, "alway": 0.726319204572, "excit": 4.56847190866, "preseg": 7.033506484289999, "introduc": 0.5457137524260001, "germani": 0.959803838943, "hyperparamet": 14.067012968579998, "gtxnumti": 7.033506484289999, "field": 5.184578325159, "visionbas": 21.100519452869996, "not": 0.0155524130075, "newli": 1.1583751315100002, "ecfund": 7.033506484289999, "compon": 1.40974687623, "growth": 2.2952017704400003, "are": 0.294674735827, "herbicid": 15.909347884319999, "minim": 3.61936355852, "less": 0.7692289252, "aim": 1.06333853704, "specif": 0.626980167541, "quick": 0.790727508899, "whole": 1.6613636488119998, "handl": 1.36683266903, "better": 0.6964279406, "than": 0.0967825866546, "then": 0.08303386523089999, "index": 3.89093865824, "design": 1.131717354066, "spawn": 2.8309483374299997, "imageri": 2.98545520604, "reach": 0.8082864617000001, "last": 0.7681745784440001, "allow": 0.9611224447560001, "world": 0.322260745863, "task": 2.71497361322, "current": 0.8539056556900001, "featur": 2.5403245088519997, "rid": 3.17127414336, "feed": 2.05136865109, "respons": 0.40991566230300003, "alreadi": 3.3523919037349996, "that": 0.043737632176039994, "itself": 0.5570837229510001, "algorithm": 9.99132718554, "offer": 0.431112446902, "consortium": 3.10729884387, "pure": 1.55108343915, "develop": 1.2503728643910001, "such": 0.358175866836, "inform": 2.27226852331, "sever": 0.27964448158759997, "explor": 1.22257937218, "therefor": 1.695183696672, "network": 6.671581371364, "aspect": 1.12795002691, "andor": 6.5370695979699995, "phd": 6.21459768774, "control": 0.38498466158600003, "add": 1.52875583713, "may": 0.050709995284400004, "result": 0.545515633524, "unseen": 3.708984470280001, "produc": 0.314320812003, "rate": 1.522067744332, "without": 0.258874517941, "bridg": 2.62030967258, "matur": 2.23183010651, "futur": 0.619345197699, "twin": 1.9393181673700002, "and": 0.0034644578134979996, "from": 0.0034023250131959997, "limit": 0.41782385463, "num": 0.0044098655355580005, "excess": 1.6692007552700001, "technolog": 0.956847686355, "keypoint": 28.134025937159997, "environ": 3.7025922090899996, "acquir": 1.1332178178499999, "for": 0.006299807907940001, "color": 1.3417002006799998, "manipul": 2.21322491868, "output": 2.03822657827, "relianc": 3.0551608359299998, "seen": 0.47672812813, "smooth": 2.4057364663799996, "with": 0.01676488398746, "zero": 2.1692741832299998, "presegment": 14.067012968579998, "these": 0.1430672388016, "handcraft": 5.45305610873, "solut": 3.10692595254, "explan": 1.87322041569, "semant": 14.6648426172, "effort": 1.913661633171, "distanc": 1.24573306257, "month": 1.232310481014, "combin": 0.529218310751, "furthermor": 1.70528363496, "reduc": 0.686617775143, "veri": 2.0714381391420003, "extract": 6.124851699030001, "toward": 0.48877277716000006, "milioto": 35.16753242145, "weed": 39.489787119499994, "forest": 3.1766194152, "later": 0.0829654259878, "focus": 0.6981989720559999, "train": 1.982754938517, "compar": 0.6239191809269999, "perceptioncontrol": 7.033506484289999, "markov": 5.16170430739, "spatial": 3.1955914510100003, "even": 0.304777129668, "ongo": 1.79934675904, "retrain": 5.19522699942, "chebrolu": 21.100519452869996, "search": 1.1798682540899998, "goal": 2.37661424546, "his": 0.0901772433641, "big": 3.0239569067100005, "cmnum": 6.0349776541799995, "numhz": 7.033506484289999, "anoth": 0.127896361652, "infest": 4.33022956194, "row": 1.71363732085, "analyz": 2.2707222351599996, "notebook": 3.693678049, "into": 0.0149128632287, "medic": 1.18644857806, "through": 0.0683586918849, "constant": 1.2971646281, "requir": 1.272760532025, "where": 0.19497641623710002, "what": 0.451774593654, "live": 0.266903399347, "techniqu": 1.31624384807, "popul": 0.778442172521, "help": 0.672415442688, "main": 0.451143081176, "type": 0.707101485387, "larg": 0.17037506060600002, "overlap": 2.4924939396, "potenti": 0.9245764122419999, "consider": 0.8325627480600001, "conclus": 1.57818536893, "base": 0.13652330228700002, "relat": 0.21310030165399999, "label": 2.99797665454, "grand": 1.12000347865, "geometr": 3.1971310972, "univers": 0.889048422744, "hold": 0.503879117196, "photogrammetri": 14.067012968579998, "old": 0.424253510675, "camera": 4.392182865480001, "speed": 1.3533338752700002, "pursu": 1.4240346891, "jetson": 6.3767269479, "onlin": 0.957503854357, "much": 0.17749572930100002, "togeth": 0.458032237308, "contain": 0.937690636472, "parallel": 1.52151886822, "problem": 1.138281448546, "deploy": 2.00400270589, "neural": 20.4265757775, "mind": 1.2786688388299998, "sow": 4.12348772901, "context": 1.44920491442, "when": 0.102774944292, "relabel": 6.238576609419999, "adapt": 2.4015729720400003, "understand": 1.0880858756799998, "speci": 3.7887046787800003, "treatment": 2.7071553770200003, "dure": 0.0982418133788, "supervisor": 3.25419887797, "represent": 1.7797382876499999, "classifi": 8.332648175749998, "gpus": 13.929027225599999, "check": 1.87281049562, "present": 0.227546654799, "led": 0.29104709623799996, "percept": 6.365707712400001, "across": 0.549198455941, "andr": 16.608390485950004, "complex": 0.8502416364309999, "tool": 4.826613538889999, "latter": 0.850831432969, "overlay": 4.56661834, "took": 0.674355905906, "intuit": 3.3216780971900004, "guid": 0.912738218589, "execut": 0.804854605864, "order": 1.7611230463440002, "raw": 2.36536149914, "health": 0.9990508682320001, "abl": 2.3972159299, "tune": 2.3434700776599997, "target": 1.1690639496200002, "pesticid": 8.38384978112, "over": 0.0498734429914, "colleagu": 2.10832533873, "analysi": 1.2466091029200002, "monitor": 1.80125261058, "calcul": 1.8131506592099997, "select": 2.114414061399, "consum": 1.5954271753600002, "earli": 0.117499629108, "same": 0.112059649604, "research": 1.991183454414, "thorough": 2.39393487158, "there": 0.0400978929255, "connect": 0.633605058682, "perplant": 14.067012968579998, "europ": 0.7017504724920001, "eager": 2.74598678068, "shelf": 3.85843328208, "possibl": 0.697610949782, "industri": 0.7046772417749999, "plant": 8.439282872440002, "semisupervis": 7.033506484289999, "blob": 10.81976787372, "exampl": 0.8173653499979999, "right": 1.02107956251, "recal": 1.6688664748100002, "take": 0.130691962197, "journey": 1.68980611189, "follow": 0.045356911094199995, "farm": 5.95108463896, "further": 0.617631790594, "discov": 0.924894023806, "code": 1.35601909597, "hard": 1.00522796406, "similar": 0.637112184228, "interact": 2.9716420535799997, "desir": 1.0991793428399999, "high": 0.41347135962000003, "power": 0.292396282715, "indic": 0.7336385419149999, "edg": 1.4944863500499999, "also": 0.0439714734, "coupl": 1.18089357972, "third": 0.35032434942900004, "around": 0.581631317346, "seek": 1.04293519316, "reliant": 4.05579271624, "acceler": 2.0985188085299997, "shape": 1.16420957115, "two": 0.0273976887164, "aerial": 2.61294618561, "signific": 1.120715232996, "extraordinarili": 3.9992405467300003, "the": 0.0, "area": 0.9838644633659999, "run": 2.213574877695, "candid": 1.50691588861, "public": 0.8092950019480001, "peopl": 0.193265578473, "nive": 21.100519452869996, "commerci": 0.8769815969470001, "challeng": 2.8178759066850003, "veget": 14.04509306508, "ground": 1.362251997968, "sideeffect": 7.033506484289999, "system": 3.601733801435, "individu": 1.17602689597, "rgbon": 14.067012968579998, "partner": 1.4287553902399999, "various": 0.28692650007, "elimin": 1.30201620283, "flourish": 6.876924076439999, "minut": 1.1353719359799999, "uniform": 1.74451821303, "structur": 0.7217716751350001, "were": 0.048582287362199994, "which": 0.04660572460887, "random": 7.890885626039999, "thing": 0.8781935346799999, "activ": 0.381196603284, "ident": 1.03244527565, "estim": 0.854377535975, "mechan": 1.22815639221, "other": 0.01974949583952, "appli": 1.6633883796239999, "convent": 0.971383786374, "one": 0.0312767582275, "graph": 3.6299309802199997, "prof": 5.52942908751, "known": 0.0824180805992, "begin": 0.285584668268, "full": 1.533610872444, "some": 0.0395735090645, "becaus": 0.139343158825, "imag": 6.95633475103, "sourc": 0.529218310751, "get": 1.739307017346, "still": 0.34224444285800004, "intellig": 1.43349848213, "implement": 3.8231382272100003, "sensor": 3.36264553568, "drop": 0.8999535106219999, "autonom": 7.217209399139999, "detect": 10.132696469579999, "part": 0.16958124393120003, "both": 0.10168506677860001, "attempt": 0.38674498075099994, "includ": 0.0188846813905, "precis": 5.01563708817, "cool": 1.9253988473800001, "receiv": 0.266574424922, "team": 0.821902894886, "gather": 2.6627841334599998, "stateoftheart": 7.033506484289999, "this": 0.053010286734999995, "schedul": 1.2987792057799998, "time": 0.1009036697634, "process": 0.527829199025, "back": 0.23166743089699998, "chang": 0.166275625058, "perform": 2.1309042528999997, "smaller": 0.9530830530519999, "project": 3.369611315442, "differ": 1.0616056065600001, "leaf": 3.0145527680299997, "been": 0.11822991184200002, "institut": 0.576176322003, "statist": 1.4451883070700002, "between": 0.06790736233059999, "drive": 1.07674430203, "obtain": 0.988162703503, "all": 0.022805264195599997, "grow": 0.821043542212, "expertis": 2.99674059227, "stachniss": 14.067012968579998, "strong": 0.49712549393600003, "gradient": 3.73502760882, "condit": 0.654837788206, "solv": 1.9836504770400003, "case": 0.395406268889, "smart": 2.81820931165, "day": 0.16865870631700003, "architectur": 3.26939515838, "websit": 0.924894023806, "cluster": 2.52579163445, "off": 0.8270570407760001, "more": 0.034049863199999995, "improv": 0.7147958039319999, "infrar": 7.352223450560001, "dedic": 2.32963016262, "novel": 2.80510150326, "achiev": 1.2541961702339999, "rough": 1.1926572072700001, "benefit": 1.12116245116, "capabl": 2.5938683736200003, "amount": 2.459696658597, "general": 0.229905156126, "made": 0.2720861043892, "faster": 4.06007935934, "realtim": 6.0616459012599995, "lead": 0.23620402986699998, "state": 0.0466100027668, "close": 0.250666759864, "affect": 0.908041904384, "detector": 3.8203613341300007, "sustain": 4.4176504971899995, "can": 0.324682192788, "avail": 2.189818345156, "comput": 2.73613783188, "make": 0.07349765782289999, "favor": 3.4261788252899996, "onli": 0.050648536658199995, "gap": 5.964719238660001, "size": 0.9138372060609999, "distinguish": 1.21469608857, "highqual": 7.033506484289999, "key": 1.64839623792, "student": 0.904923236645, "accur": 3.5049612297, "neighbor": 1.7546632275799998, "transfer": 2.00529907094, "advanc": 0.6930212121780001, "imposs": 1.60165772512, "data": 3.6504617544, "lightweight": 3.4781584227999995, "cnns": 7.033506484289999, "address": 2.10274206494, "cyril": 6.9199154363, "method": 3.777846435364, "spray": 10.14854472897, "sinc": 0.0803681994577, "convolut": 13.848954025650002, "bio": 3.7456377879300002, "retun": 6.5370695979699995, "come": 0.5678198130600001, "must": 0.653383947388, "origin": 0.128612437587, "lattic": 4.27893626755, "identifi": 0.833722000472, "direct": 0.200705689496, "first": 0.02276186943648, "vision": 1.58523088743, "input": 7.50502600617, "propos": 0.6882461339920001, "have": 0.14785002341200001, "while": 0.12974995138140002, "proven": 2.28423595433, "year": 0.09480447778920001, "easili": 1.3066587367, "prototyp": 2.4632235573, "learn": 5.05651238847, "pipelin": 13.88011318688, "effici": 1.62793753414}, "logidf": {"after": 0.020490694648099998, "hand": 0.479471335336, "real": 0.824629060574, "food": 1.08565801008, "arabl": 4.14313472639, "roboticscent": 7.033506484289999, "tensorflow": 7.033506484289999, "quantifi": 3.72976443878, "soil": 2.06518238826, "stage": 0.733900940237, "basi": 0.884275353639, "troubl": 1.60761292215, "geometrybas": 7.033506484289999, "chemic": 1.8334321656299999, "agricultur": 1.4944863500499999, "space": 0.874713164972, "addit": 0.220218882972, "lott": 4.7897618913199995, "would": 0.0796176279647, "etc": 1.4366730879700003, "dataset": 5.26584456664, "fertil": 2.30801679965, "assist": 0.776112606548, "crop": 2.27316573057, "manner": 1.36905901503, "well": 0.0635144383156, "unsupervis": 5.843922417409999, "work": 0.109034567273, "yield": 1.86708918863, "approach": 0.7302336145810001, "path": 1.5351679838499999, "geometri": 3.2364134455299998, "name": 0.09723316638430002, "motiv": 1.61265547932, "automat": 1.9150850473199998, "mask": 2.46618652243, "left": 0.364552414753, "instead": 0.46663315041500003, "creat": 0.222576818514, "how": 0.47156695693000006, "land": 0.5428913449939999, "philipp": 2.97800175538, "segment": 2.02239511306, "perceiv": 1.5938755846700001, "dose": 3.4659878871800007, "had": 0.0464780244111, "increas": 0.277820718929, "special": 0.39755992860100003, "initi": 0.30010459245, "frame": 1.8373800586400002, "given": 0.303255810831, "invari": 3.1101197202099997, "deep": 1.2886734698, "beginn": 3.9788316751, "exploit": 1.7568506145200002, "open": 0.219591038029, "see": 0.240921585492, "autoretrain": 7.033506484289999, "second": 0.10713976337999999, "updat": 1.7164374626899999, "new": 0.0177299468511, "near": 0.252854324034, "but": 0.0161923720719, "certain": 0.592104362781, "need": 0.362740163442, "our": 0.8576392141820001, "classif": 2.08779073629, "commit": 1.0598786410299998, "cost": 0.84129007618, "object": 0.853933584803, "remap": 6.676831540349999, "entail": 3.0564986287700004, "caff": 6.89997509166, "applic": 1.23160392849, "walk": 1.270781474, "lab": 2.66949835512, "level": 0.503462189943, "robot": 2.99674059227, "has": 0.0427239448548, "generat": 0.719182341736, "collabor": 1.4939250253100003, "region": 0.568028500824, "advantag": 1.20120515883, "use": 0.0292080197316, "experi": 0.626272953933, "resourc": 1.08137694258, "green": 0.9672326803710001, "bonn": 4.002682890919999, "model": 0.7374500731110001, "treat": 1.27821645249, "hardwar": 2.93441131931, "alway": 0.726319204572, "excit": 2.28423595433, "preseg": 7.033506484289999, "introduc": 0.5457137524260001, "germani": 0.959803838943, "hyperparamet": 7.033506484289999, "gtxnumti": 7.033506484289999, "field": 0.5760642583510001, "visionbas": 7.033506484289999, "not": 0.0155524130075, "newli": 1.1583751315100002, "ecfund": 7.033506484289999, "compon": 1.40974687623, "growth": 1.1476008852200001, "are": 0.0294674735827, "herbicid": 5.30311596144, "minim": 1.80968177926, "less": 0.3846144626, "aim": 1.06333853704, "specif": 0.626980167541, "quick": 0.790727508899, "whole": 0.8306818244059999, "handl": 1.36683266903, "better": 0.6964279406, "than": 0.0322608622182, "then": 0.08303386523089999, "index": 1.94546932912, "design": 0.377239118022, "spawn": 2.8309483374299997, "imageri": 2.98545520604, "reach": 0.40414323085000003, "last": 0.19204364461100001, "allow": 0.24028061118900002, "world": 0.107420248621, "task": 1.35748680661, "current": 0.42695282784500005, "featur": 0.423387418142, "rid": 3.17127414336, "feed": 2.05136865109, "respons": 0.40991566230300003, "alreadi": 0.670478380747, "that": 0.00397614837964, "itself": 0.5570837229510001, "algorithm": 3.33044239518, "offer": 0.431112446902, "consortium": 3.10729884387, "pure": 1.55108343915, "develop": 0.178624694913, "such": 0.059695977806, "inform": 0.454453704662, "sever": 0.06991112039689999, "explor": 1.22257937218, "therefor": 0.847591848336, "network": 0.9530830530519999, "aspect": 1.12795002691, "andor": 6.5370695979699995, "phd": 3.10729884387, "control": 0.38498466158600003, "add": 1.52875583713, "may": 0.050709995284400004, "result": 0.136378908381, "unseen": 3.708984470280001, "produc": 0.314320812003, "rate": 0.761033872166, "without": 0.258874517941, "bridg": 1.31015483629, "matur": 2.23183010651, "futur": 0.619345197699, "twin": 1.9393181673700002, "and": 6.29901420636e-05, "from": 0.000567054168866, "limit": 0.41782385463, "num": 0.00031499039539700004, "excess": 1.6692007552700001, "technolog": 0.956847686355, "keypoint": 7.033506484289999, "environ": 1.2341974030299998, "acquir": 1.1332178178499999, "for": 0.00031499039539700004, "color": 1.3417002006799998, "manipul": 2.21322491868, "output": 2.03822657827, "relianc": 3.0551608359299998, "seen": 0.47672812813, "smooth": 2.4057364663799996, "with": 0.00119749171339, "zero": 2.1692741832299998, "presegment": 7.033506484289999, "these": 0.0715336194008, "handcraft": 5.45305610873, "solut": 1.55346297627, "explan": 1.87322041569, "semant": 3.6662106543, "effort": 0.637887211057, "distanc": 1.24573306257, "month": 0.410770160338, "combin": 0.529218310751, "furthermor": 1.70528363496, "reduc": 0.686617775143, "veri": 0.230159793238, "extract": 2.04161723301, "toward": 0.48877277716000006, "milioto": 7.033506484289999, "weed": 3.9489787119499997, "forest": 1.5883097076, "later": 0.0829654259878, "focus": 0.6981989720559999, "train": 0.660918312839, "compar": 0.6239191809269999, "perceptioncontrol": 7.033506484289999, "markov": 5.16170430739, "spatial": 3.1955914510100003, "even": 0.152388564834, "ongo": 1.79934675904, "retrain": 5.19522699942, "chebrolu": 7.033506484289999, "search": 1.1798682540899998, "goal": 1.18830712273, "his": 0.0901772433641, "big": 1.00798563557, "cmnum": 6.0349776541799995, "numhz": 7.033506484289999, "anoth": 0.127896361652, "infest": 4.33022956194, "row": 1.71363732085, "analyz": 2.2707222351599996, "notebook": 3.693678049, "into": 0.0149128632287, "medic": 1.18644857806, "through": 0.0683586918849, "constant": 1.2971646281, "requir": 0.424253510675, "where": 0.0649921387457, "what": 0.225887296827, "live": 0.266903399347, "techniqu": 1.31624384807, "popul": 0.778442172521, "help": 0.336207721344, "main": 0.225571540588, "type": 0.707101485387, "larg": 0.17037506060600002, "overlap": 2.4924939396, "potenti": 0.9245764122419999, "consider": 0.8325627480600001, "conclus": 1.57818536893, "base": 0.13652330228700002, "relat": 0.21310030165399999, "label": 1.49898832727, "grand": 1.12000347865, "geometr": 3.1971310972, "univers": 0.222262105686, "hold": 0.503879117196, "photogrammetri": 7.033506484289999, "old": 0.424253510675, "camera": 2.1960914327400003, "speed": 1.3533338752700002, "pursu": 1.4240346891, "jetson": 6.3767269479, "onlin": 0.957503854357, "much": 0.17749572930100002, "togeth": 0.458032237308, "contain": 0.468845318236, "parallel": 1.52151886822, "problem": 0.569140724273, "deploy": 2.00400270589, "neural": 4.0853151555, "mind": 1.2786688388299998, "sow": 4.12348772901, "context": 1.44920491442, "when": 0.0205549888584, "relabel": 6.238576609419999, "adapt": 1.2007864860200002, "understand": 1.0880858756799998, "speci": 1.8943523393900001, "treatment": 1.3535776885100002, "dure": 0.0491209066894, "supervisor": 3.25419887797, "represent": 1.7797382876499999, "classifi": 1.6665296351499999, "gpus": 6.964513612799999, "check": 1.87281049562, "present": 0.227546654799, "led": 0.29104709623799996, "percept": 2.1219025708, "across": 0.549198455941, "andr": 3.3216780971900004, "complex": 0.8502416364309999, "tool": 1.60887117963, "latter": 0.850831432969, "overlay": 4.56661834, "took": 0.337177952953, "intuit": 3.3216780971900004, "guid": 0.912738218589, "execut": 0.804854605864, "order": 0.22014038079300002, "raw": 2.36536149914, "health": 0.9990508682320001, "abl": 0.599303982475, "tune": 2.3434700776599997, "target": 1.1690639496200002, "pesticid": 4.19192489056, "over": 0.0249367214957, "colleagu": 2.10832533873, "analysi": 1.2466091029200002, "monitor": 1.80125261058, "calcul": 1.8131506592099997, "select": 0.704804687133, "consum": 1.5954271753600002, "earli": 0.117499629108, "same": 0.112059649604, "research": 0.663727818138, "thorough": 2.39393487158, "there": 0.0400978929255, "connect": 0.633605058682, "perplant": 7.033506484289999, "europ": 0.7017504724920001, "eager": 2.74598678068, "shelf": 3.85843328208, "possibl": 0.348805474891, "industri": 0.7046772417749999, "plant": 1.2056118389200001, "semisupervis": 7.033506484289999, "blob": 5.40988393686, "exampl": 0.40868267499899996, "right": 0.34035985417, "recal": 1.6688664748100002, "take": 0.130691962197, "journey": 1.68980611189, "follow": 0.045356911094199995, "farm": 1.48777115974, "further": 0.308815895297, "discov": 0.924894023806, "code": 1.35601909597, "hard": 1.00522796406, "similar": 0.318556092114, "interact": 1.4858210267899998, "desir": 1.0991793428399999, "high": 0.13782378654000002, "power": 0.292396282715, "indic": 0.7336385419149999, "edg": 1.4944863500499999, "also": 0.0146571578, "coupl": 1.18089357972, "third": 0.35032434942900004, "around": 0.19387710578200001, "seek": 1.04293519316, "reliant": 4.05579271624, "acceler": 2.0985188085299997, "shape": 1.16420957115, "two": 0.0136988443582, "aerial": 2.61294618561, "signific": 0.373571744332, "extraordinarili": 3.9992405467300003, "the": 0.0, "area": 0.327954821122, "run": 0.442714975539, "candid": 1.50691588861, "public": 0.20232375048700002, "peopl": 0.193265578473, "nive": 7.033506484289999, "commerci": 0.8769815969470001, "challeng": 0.9392919688950001, "veget": 2.34084884418, "ground": 0.681125998984, "sideeffect": 7.033506484289999, "system": 0.327430345585, "individu": 0.588013447985, "rgbon": 7.033506484289999, "partner": 1.4287553902399999, "various": 0.28692650007, "elimin": 1.30201620283, "flourish": 2.2923080254799997, "minut": 1.1353719359799999, "uniform": 1.74451821303, "structur": 0.7217716751350001, "were": 0.024291143681099997, "which": 0.00517841384543, "random": 1.9727214065099998, "thing": 0.8781935346799999, "activ": 0.381196603284, "ident": 1.03244527565, "estim": 0.854377535975, "mechan": 1.22815639221, "other": 0.00987474791976, "appli": 0.8316941898119999, "convent": 0.971383786374, "one": 0.0062553516455, "graph": 3.6299309802199997, "prof": 5.52942908751, "known": 0.0824180805992, "begin": 0.285584668268, "full": 0.511203624148, "some": 0.0395735090645, "becaus": 0.139343158825, "imag": 0.99376210729, "sourc": 0.529218310751, "get": 0.579769005782, "still": 0.17112222142900002, "intellig": 1.43349848213, "implement": 1.27437940907, "sensor": 3.36264553568, "drop": 0.8999535106219999, "autonom": 2.4057364663799996, "detect": 1.68878274493, "part": 0.04239531098280001, "both": 0.050842533389300004, "attempt": 0.38674498075099994, "includ": 0.0188846813905, "precis": 1.67187902939, "cool": 1.9253988473800001, "receiv": 0.266574424922, "team": 0.821902894886, "gather": 1.3313920667299999, "stateoftheart": 7.033506484289999, "this": 0.0037864490525, "schedul": 1.2987792057799998, "time": 0.0112115188626, "process": 0.527829199025, "back": 0.23166743089699998, "chang": 0.166275625058, "perform": 0.42618085058, "smaller": 0.9530830530519999, "project": 0.561601885907, "differ": 0.212321121312, "leaf": 3.0145527680299997, "been": 0.023645982368400004, "institut": 0.576176322003, "statist": 1.4451883070700002, "between": 0.033953681165299995, "drive": 1.07674430203, "obtain": 0.988162703503, "all": 0.011402632097799998, "grow": 0.821043542212, "expertis": 2.99674059227, "stachniss": 7.033506484289999, "strong": 0.49712549393600003, "gradient": 3.73502760882, "condit": 0.654837788206, "solv": 1.9836504770400003, "case": 0.395406268889, "smart": 2.81820931165, "day": 0.16865870631700003, "architectur": 1.63469757919, "websit": 0.924894023806, "cluster": 2.52579163445, "off": 0.41352852038800003, "more": 0.017024931599999998, "improv": 0.7147958039319999, "infrar": 3.6761117252800006, "dedic": 1.16481508131, "novel": 1.40255075163, "achiev": 0.6270980851169999, "rough": 1.1926572072700001, "benefit": 1.12116245116, "capabl": 1.2969341868100002, "amount": 0.819898886199, "general": 0.114952578063, "made": 0.0680215260973, "faster": 2.03003967967, "realtim": 6.0616459012599995, "lead": 0.23620402986699998, "state": 0.0466100027668, "close": 0.250666759864, "affect": 0.908041904384, "detector": 3.8203613341300007, "sustain": 1.4725501657299997, "can": 0.162341096394, "avail": 0.547454586289, "comput": 1.36806891594, "make": 0.07349765782289999, "favor": 1.1420596084299999, "onli": 0.025324268329099998, "gap": 1.98823974622, "size": 0.9138372060609999, "distinguish": 1.21469608857, "highqual": 7.033506484289999, "key": 0.82419811896, "student": 0.904923236645, "accur": 1.75248061485, "neighbor": 1.7546632275799998, "transfer": 1.00264953547, "advanc": 0.6930212121780001, "imposs": 1.60165772512, "data": 1.2168205848, "lightweight": 3.4781584227999995, "cnns": 7.033506484289999, "address": 1.05137103247, "cyril": 3.45995771815, "method": 0.944461608841, "spray": 3.38284824299, "sinc": 0.0803681994577, "convolut": 4.61631800855, "bio": 3.7456377879300002, "retun": 6.5370695979699995, "come": 0.28390990653000003, "must": 0.653383947388, "origin": 0.128612437587, "lattic": 4.27893626755, "identifi": 0.833722000472, "direct": 0.200705689496, "first": 0.0075872898121599995, "vision": 1.58523088743, "input": 2.50167533539, "propos": 0.6882461339920001, "have": 0.0147850023412, "while": 0.04324998379380001, "proven": 2.28423595433, "year": 0.047402238894600005, "easili": 1.3066587367, "prototyp": 2.4632235573, "learn": 0.842752064745, "pipelin": 3.47002829672, "effici": 1.62793753414}, "freq": {"after": 2, "hand": 1, "real": 3, "food": 1, "arabl": 1, "roboticscent": 1, "tensorflow": 1, "quantifi": 1, "soil": 2, "stage": 2, "basi": 1, "troubl": 1, "geometrybas": 1, "chemic": 2, "agricultur": 2, "space": 1, "addit": 1, "lott": 3, "would": 1, "etc": 1, "dataset": 2, "fertil": 2, "assist": 1, "crop": 9, "manner": 2, "well": 6, "unsupervis": 1, "work": 8, "yield": 2, "approach": 15, "path": 1, "geometri": 1, "name": 2, "motiv": 1, "automat": 1, "mask": 2, "left": 3, "instead": 1, "creat": 1, "how": 1, "land": 1, "philipp": 2, "segment": 6, "perceiv": 2, "dose": 1, "had": 3, "increas": 2, "special": 1, "initi": 1, "frame": 2, "given": 2, "invari": 1, "deep": 4, "beginn": 1, "exploit": 2, "open": 1, "see": 4, "autoretrain": 1, "second": 3, "updat": 1, "new": 9, "near": 3, "but": 1, "certain": 1, "need": 3, "our": 12, "classif": 10, "commit": 1, "cost": 2, "object": 3, "remap": 1, "entail": 1, "caff": 1, "applic": 1, "walk": 1, "lab": 3, "level": 3, "robot": 14, "has": 3, "generat": 1, "collabor": 1, "region": 1, "advantag": 1, "use": 13, "experi": 1, "resourc": 2, "green": 1, "bonn": 3, "model": 3, "treat": 2, "hardwar": 2, "alway": 1, "excit": 2, "preseg": 1, "introduc": 1, "germani": 1, "hyperparamet": 2, "gtxnumti": 1, "field": 9, "visionbas": 3, "not": 1, "newli": 1, "ecfund": 1, "compon": 1, "growth": 2, "are": 10, "herbicid": 3, "minim": 2, "less": 2, "aim": 1, "specif": 1, "quick": 1, "whole": 2, "handl": 1, "better": 1, "than": 3, "then": 1, "index": 2, "design": 3, "spawn": 1, "imageri": 1, "reach": 2, "last": 4, "allow": 4, "world": 3, "task": 2, "current": 2, "featur": 6, "rid": 1, "feed": 1, "respons": 1, "alreadi": 5, "that": 11, "itself": 1, "algorithm": 3, "offer": 1, "consortium": 1, "pure": 1, "develop": 7, "such": 6, "inform": 5, "sever": 4, "explor": 1, "therefor": 2, "network": 7, "aspect": 1, "andor": 1, "phd": 2, "control": 1, "add": 1, "may": 1, "result": 4, "unseen": 1, "produc": 1, "rate": 2, "without": 1, "bridg": 2, "matur": 1, "futur": 1, "twin": 1, "and": 55, "from": 6, "limit": 1, "num": 14, "excess": 1, "technolog": 1, "keypoint": 4, "environ": 3, "acquir": 1, "for": 20, "color": 1, "manipul": 1, "output": 1, "relianc": 1, "seen": 1, "smooth": 1, "with": 14, "zero": 1, "presegment": 2, "these": 2, "handcraft": 1, "solut": 2, "explan": 1, "semant": 4, "effort": 3, "distanc": 1, "month": 3, "combin": 1, "furthermor": 1, "reduc": 1, "veri": 9, "extract": 3, "toward": 1, "milioto": 5, "weed": 10, "forest": 2, "later": 1, "focus": 1, "train": 3, "compar": 1, "perceptioncontrol": 1, "markov": 1, "spatial": 1, "even": 2, "ongo": 1, "retrain": 1, "chebrolu": 3, "search": 1, "goal": 2, "his": 1, "big": 3, "cmnum": 1, "numhz": 1, "anoth": 1, "infest": 1, "row": 1, "analyz": 1, "notebook": 1, "into": 1, "medic": 1, "through": 1, "constant": 1, "requir": 3, "where": 3, "what": 2, "live": 1, "techniqu": 1, "popul": 1, "help": 2, "main": 2, "type": 1, "larg": 1, "overlap": 1, "potenti": 1, "consider": 1, "conclus": 1, "base": 1, "relat": 1, "label": 2, "grand": 1, "geometr": 1, "univers": 4, "hold": 1, "photogrammetri": 2, "old": 1, "camera": 2, "speed": 1, "pursu": 1, "jetson": 1, "onlin": 1, "much": 1, "togeth": 1, "contain": 2, "parallel": 1, "problem": 2, "deploy": 1, "neural": 5, "mind": 1, "sow": 1, "context": 1, "when": 5, "relabel": 1, "adapt": 2, "understand": 1, "speci": 2, "treatment": 2, "dure": 2, "supervisor": 1, "represent": 1, "classifi": 5, "gpus": 2, "check": 1, "present": 1, "led": 1, "percept": 3, "across": 1, "andr": 5, "complex": 1, "tool": 3, "latter": 1, "overlay": 1, "took": 2, "intuit": 1, "guid": 1, "execut": 1, "order": 8, "raw": 1, "health": 1, "abl": 4, "tune": 1, "target": 1, "pesticid": 2, "over": 2, "colleagu": 1, "analysi": 1, "monitor": 1, "calcul": 1, "select": 3, "consum": 1, "earli": 1, "same": 1, "research": 3, "thorough": 1, "there": 1, "connect": 1, "perplant": 2, "europ": 1, "eager": 1, "shelf": 1, "possibl": 2, "industri": 1, "plant": 7, "semisupervis": 1, "blob": 2, "exampl": 2, "right": 3, "recal": 1, "take": 1, "journey": 1, "follow": 1, "farm": 4, "further": 2, "discov": 1, "code": 1, "hard": 1, "similar": 2, "interact": 2, "desir": 1, "high": 3, "power": 1, "indic": 1, "edg": 1, "also": 3, "coupl": 1, "third": 1, "around": 3, "seek": 1, "reliant": 1, "acceler": 1, "shape": 1, "two": 2, "aerial": 1, "signific": 3, "extraordinarili": 1, "the": 99, "area": 3, "run": 5, "candid": 1, "public": 4, "peopl": 1, "nive": 3, "commerci": 1, "challeng": 3, "veget": 6, "ground": 2, "sideeffect": 1, "system": 11, "individu": 2, "rgbon": 2, "partner": 1, "various": 1, "elimin": 1, "flourish": 3, "minut": 1, "uniform": 1, "structur": 1, "were": 2, "which": 9, "random": 4, "thing": 1, "activ": 1, "ident": 1, "estim": 1, "mechan": 1, "other": 2, "appli": 2, "convent": 1, "one": 5, "graph": 1, "prof": 1, "known": 1, "begin": 1, "full": 3, "some": 1, "becaus": 1, "imag": 7, "sourc": 1, "get": 3, "still": 2, "intellig": 1, "implement": 3, "sensor": 1, "drop": 1, "autonom": 3, "detect": 6, "part": 4, "both": 2, "attempt": 1, "includ": 1, "precis": 3, "cool": 1, "receiv": 1, "team": 1, "gather": 2, "stateoftheart": 1, "this": 14, "schedul": 1, "time": 9, "process": 1, "back": 1, "chang": 1, "perform": 5, "smaller": 1, "project": 6, "differ": 5, "leaf": 1, "been": 5, "institut": 1, "statist": 1, "between": 2, "drive": 1, "obtain": 1, "all": 2, "grow": 1, "expertis": 1, "stachniss": 2, "strong": 1, "gradient": 1, "condit": 1, "solv": 1, "case": 1, "smart": 1, "day": 1, "architectur": 2, "websit": 1, "cluster": 1, "off": 2, "more": 2, "improv": 1, "infrar": 2, "dedic": 2, "novel": 2, "achiev": 2, "rough": 1, "benefit": 1, "capabl": 2, "amount": 3, "general": 2, "made": 4, "faster": 2, "realtim": 1, "lead": 1, "state": 1, "close": 1, "affect": 1, "detector": 1, "sustain": 3, "can": 2, "avail": 4, "comput": 2, "make": 1, "favor": 3, "onli": 2, "gap": 3, "size": 1, "distinguish": 1, "highqual": 1, "key": 2, "student": 1, "accur": 2, "neighbor": 1, "transfer": 2, "advanc": 1, "imposs": 1, "data": 3, "lightweight": 1, "cnns": 1, "address": 2, "cyril": 2, "method": 4, "spray": 3, "sinc": 1, "convolut": 3, "bio": 1, "retun": 1, "come": 2, "must": 1, "origin": 1, "lattic": 1, "identifi": 1, "direct": 1, "first": 3, "vision": 1, "input": 3, "propos": 1, "have": 10, "while": 3, "proven": 1, "year": 2, "easili": 1, "prototyp": 1, "learn": 6, "pipelin": 4, "effici": 1}, "idf": {"after": 1.02070207021, "hand": 1.6152202665600002, "real": 2.28103448276, "food": 2.9613878007800003, "arabl": 63.0, "roboticscent": 1134.0, "tensorflow": 1134.0, "quantifi": 41.669291338600004, "soil": 7.886736214610001, "stage": 2.0831911822599998, "basi": 2.42122922068, "troubl": 4.99088337001, "geometrybas": 1134.0, "chemic": 6.255319148940001, "agricultur": 4.45704660303, "space": 2.39818731118, "addit": 1.24634950542, "lott": 120.27272727299999, "would": 1.0828729281799998, "etc": 4.2066772655, "dataset": 193.609756098, "fertil": 10.0544648512, "assist": 2.17300848618, "crop": 9.71009174312, "manner": 3.93164933135, "well": 1.0655748708, "unsupervis": 345.13043478300006, "work": 1.11520089913, "yield": 6.46943765281, "approach": 2.07556543339, "path": 4.6421052631599995, "geometri": 25.4423076923, "name": 1.10211732037, "motiv": 5.01611374408, "automat": 6.787516032490001, "mask": 11.777448071199998, "left": 1.4398693996, "instead": 1.59461631177, "creat": 1.2492917847, "how": 1.60250328051, "land": 1.7209756097599997, "philipp": 19.6485148515, "segment": 7.55640171347, "perceiv": 4.92279069767, "dose": 32.0080645161, "had": 1.0475750577399998, "increas": 1.32024948025, "special": 1.4881889763799998, "initi": 1.35, "frame": 6.280063291139999, "given": 1.35426085473, "invari": 22.4237288136, "deep": 3.6279707495399998, "beginn": 53.4545454545, "exploit": 5.79416058394, "open": 1.24556723678, "see": 1.27242125511, "autoretrain": 1134.0, "second": 1.1130898128, "updat": 5.56466876972, "new": 1.0178880554, "near": 1.28769567686, "but": 1.01632417899, "certain": 1.8077886586200003, "need": 1.4372623574099999, "our": 2.35758835759, "classif": 8.067073170730001, "commit": 2.8860207235, "cost": 2.31935719503, "object": 2.3488681757700003, "remap": 793.8, "entail": 21.2530120482, "caff": 992.25, "applic": 3.42672134686, "walk": 3.56363636364, "lab": 14.4327272727, "level": 1.6544393497299998, "robot": 20.0201765448, "has": 1.0436497502, "generat": 2.05275407292, "collabor": 4.45454545455, "region": 1.7647843486, "advantag": 3.32412060302, "use": 1.0296387573799999, "experi": 1.87062566278, "resourc": 2.9487369985100003, "green": 2.63065451533, "bonn": 54.744827586199996, "model": 2.0905978404, "treat": 3.59023066486, "hardwar": 18.8104265403, "alway": 2.06745670009, "excit": 9.818181818180001, "preseg": 1134.0, "introduc": 1.7258397651900002, "germani": 2.61118421053, "hyperparamet": 1134.0, "gtxnumti": 1134.0, "field": 1.7790228597, "visionbas": 1134.0, "not": 1.01567398119, "newli": 3.1847542627900003, "ecfund": 1134.0, "compon": 4.09491875161, "growth": 3.15062512403, "are": 1.02990593578, "herbicid": 200.962025316, "minim": 6.10850327049, "less": 1.46904783936, "aim": 2.8960233491400005, "specif": 1.8719490626099997, "quick": 2.205, "whole": 2.29488291414, "handl": 3.9229058561900003, "better": 2.0065722952500002, "than": 1.03278688525, "then": 1.08657860516, "index": 6.9969149405, "design": 1.45825296225, "spawn": 16.9615384615, "imageri": 19.7955112219, "reach": 1.49801849406, "last": 1.2117234010100002, "allow": 1.2716059271100002, "world": 1.11340206186, "task": 3.88641370869, "current": 1.5325803649, "featur": 1.52712581762, "rid": 23.837837837800002, "feed": 7.77853993141, "respons": 1.5066907089300001, "alreadi": 1.9551724137900002, "that": 1.00398406375, "itself": 1.74557449148, "algorithm": 27.9507042254, "offer": 1.53896859248, "consortium": 22.3605633803, "pure": 4.716577540109999, "develop": 1.1955719557200002, "such": 1.06151377374, "inform": 1.5753125620200001, "sever": 1.07241286139, "explor": 3.39593582888, "therefor": 2.33401940606, "network": 2.59369384088, "aspect": 3.0893169877399997, "andor": 690.260869565, "phd": 22.3605633803, "control": 1.46959178006, "add": 4.61243463103, "may": 1.05201775893, "result": 1.14611608432, "unseen": 40.8123393316, "produc": 1.36932896326, "rate": 2.14048806795, "without": 1.29547123623, "bridg": 3.7067476068199996, "matur": 9.31690140845, "futur": 1.8577112099200002, "twin": 6.954007884360001, "and": 1.00006299213, "from": 1.00056721497, "limit": 1.5186531471200002, "num": 1.00031504001, "excess": 5.30792377131, "technolog": 2.6034765496900003, "keypoint": 1134.0, "environ": 3.43561999567, "acquir": 3.10563380282, "for": 1.00031504001, "color": 3.8255421686699997, "manipul": 9.145161290319999, "output": 7.676982591880001, "relianc": 21.2245989305, "seen": 1.61079545455, "smooth": 11.086592178800002, "with": 1.0011982089899998, "zero": 8.75192943771, "presegment": 1134.0, "these": 1.07415426252, "handcraft": 233.470588235, "solut": 4.7278141751, "explan": 6.50922509225, "semant": 39.1034482759, "effort": 1.89247824532, "distanc": 3.4754816112099998, "month": 1.5079787234, "combin": 1.69760479042, "furthermor": 5.50294627383, "reduc": 1.98698372966, "veri": 1.25880114177, "extract": 7.703056768560001, "toward": 1.6303142329, "milioto": 1134.0, "weed": 51.8823529412, "forest": 4.89546716004, "later": 1.08650424309, "focus": 2.01012914662, "train": 1.9365698950999999, "compar": 1.8662278123900002, "perceptioncontrol": 1134.0, "markov": 174.46153846200002, "spatial": 24.4246153846, "even": 1.16461267606, "ongo": 6.04569687738, "retrain": 180.409090909, "chebrolu": 1134.0, "search": 3.2539454806299997, "goal": 3.28152128979, "his": 1.0943682360200002, "big": 2.7400759406299997, "cmnum": 417.78947368400003, "numhz": 1134.0, "anoth": 1.13643521832, "infest": 75.961722488, "row": 5.549108703250001, "analyz": 9.68639414277, "notebook": 40.1924050633, "into": 1.01502461479, "medic": 3.27542809986, "through": 1.07074930869, "constant": 3.6589075823900004, "requir": 1.52844902282, "where": 1.06715063521, "what": 1.25343439128, "live": 1.30591428806, "techniqu": 3.7293868921800004, "popul": 2.17807655371, "help": 1.39962972759, "main": 1.25303867403, "type": 2.0281042411900003, "larg": 1.18574949585, "overlap": 12.0913937548, "potenti": 2.52080025405, "consider": 2.29920347574, "conclus": 4.84615384615, "base": 1.14628158845, "relat": 1.23750876919, "label": 4.47715736041, "grand": 3.06486486486, "geometr": 24.4622496148, "univers": 1.24889867841, "hold": 1.6551292744, "photogrammetri": 1134.0, "old": 1.52844902282, "camera": 8.989807474520001, "speed": 3.8703071672400005, "pursu": 4.15384615385, "jetson": 588.0, "onlin": 2.6051854282900004, "much": 1.1942229577299999, "togeth": 1.58095996813, "contain": 1.59814777532, "parallel": 4.57917507932, "problem": 1.76674827509, "deploy": 7.41869158879, "neural": 59.4606741573, "mind": 3.5918552036199998, "sow": 61.7743190661, "context": 4.25972632144, "when": 1.02076769755, "relabel": 512.129032258, "adapt": 3.32272917539, "understand": 2.96858638743, "speci": 6.648241206030001, "treatment": 3.87125091441, "dure": 1.0503473370799998, "supervisor": 25.898858075, "represent": 5.928304705, "classifi": 5.2937645882, "gpus": 1058.4, "check": 6.50655737705, "present": 1.25551601423, "led": 1.33782758911, "percept": 8.34700315457, "across": 1.7318642958400001, "andr": 27.7068062827, "complex": 2.34021226415, "tool": 4.99716713881, "latter": 2.34159292035, "overlay": 96.2181818182, "took": 1.4009883515700001, "intuit": 27.7068062827, "guid": 2.49113447356, "execut": 2.2363713199, "order": 1.24625166811, "raw": 10.6478873239, "health": 2.71570304482, "abl": 1.8208510150200001, "tune": 10.4173228346, "target": 3.2189781021900004, "pesticid": 66.15, "over": 1.02525024217, "colleagu": 8.23443983402, "analysi": 3.47852760736, "monitor": 6.05723006486, "calcul": 6.12972972973, "select": 2.02345144022, "consum": 4.93043478261, "earli": 1.12468121281, "same": 1.11857958148, "research": 1.9420183486200002, "thorough": 10.956521739100001, "there": 1.04091266719, "connect": 1.8843916913900003, "perplant": 1134.0, "europ": 2.0172808132099997, "eager": 15.5799803729, "shelf": 47.391044776099996, "possibl": 1.4173734488, "industri": 2.02319357716, "plant": 3.3388012618299996, "semisupervis": 1134.0, "blob": 223.605633803, "exampl": 1.50483412322, "right": 1.4054532577899999, "recal": 5.30614973262, "take": 1.13961668222, "journey": 5.41843003413, "follow": 1.04640126549, "farm": 4.4272169548199996, "further": 1.3618116315, "discov": 2.52160101652, "code": 3.8807137619199996, "hard": 2.73253012048, "similar": 1.37514075357, "interact": 4.4185917061, "desir": 3.00170164492, "high": 1.14777327935, "power": 1.3396337861799998, "indic": 2.0826446281, "edg": 4.45704660303, "also": 1.01476510067, "coupl": 3.2572835453400004, "third": 1.4195278969999998, "around": 1.21394708671, "seek": 2.83753351206, "reliant": 57.7309090909, "acceler": 8.15408320493, "shape": 3.20338983051, "two": 1.01379310345, "aerial": 13.6391752577, "signific": 1.4529147982100001, "extraordinarili": 54.5567010309, "the": 1.0, "area": 1.3881262568900001, "run": 1.55692850838, "candid": 4.51279135873, "public": 1.22424429365, "peopl": 1.21320495186, "nive": 1134.0, "commerci": 2.4036336109, "challeng": 2.55816951337, "veget": 10.390052356, "ground": 1.97610156833, "sideeffect": 1134.0, "system": 1.38739840951, "individu": 1.8004082558400003, "rgbon": 1134.0, "partner": 4.173501577290001, "various": 1.3323262839899999, "elimin": 3.67670217693, "flourish": 9.89775561097, "minut": 3.11233091551, "uniform": 5.7231434751300005, "structur": 2.0580762250499998, "were": 1.02458857696, "which": 1.005191845, "random": 7.1902173913, "thing": 2.4065484311099996, "activ": 1.46403541129, "ident": 2.80792359392, "estim": 2.34991119005, "mechan": 3.41492794149, "other": 1.00992366412, "appli": 2.2972073506, "convent": 2.64159733777, "one": 1.00627495722, "graph": 37.7102137767, "prof": 252.0, "known": 1.0859097127200001, "begin": 1.3305397251100002, "full": 1.66729678639, "some": 1.04036697248, "becaus": 1.1495184997499999, "imag": 2.70137825421, "sourc": 1.69760479042, "get": 1.78562591385, "still": 1.1866357724799999, "intellig": 4.19334389857, "implement": 3.57648118946, "sensor": 28.8654545455, "drop": 2.4594887684, "autonom": 11.086592178800002, "detect": 5.41288782816, "part": 1.04330682789, "both": 1.05215720061, "attempt": 1.4721810088999998, "includ": 1.0190641247799999, "precis": 5.322158900440001, "cool": 6.8578833693300005, "receiv": 1.3054847463200001, "team": 2.2748244734200003, "gather": 3.78631051753, "stateoftheart": 1134.0, "this": 1.00379362671, "schedul": 3.6648199445999996, "time": 1.01127460348, "process": 1.69524826482, "back": 1.26070038911, "chang": 1.1808985421, "perform": 1.5313977042500002, "smaller": 2.59369384088, "project": 1.7534791252500002, "differ": 1.23654490225, "leaf": 20.379974326099997, "been": 1.0239277652399998, "institut": 1.7792222346700002, "statist": 4.24265098878, "between": 1.03453668708, "drive": 2.93510815308, "obtain": 2.68629441624, "all": 1.01146788991, "grow": 2.27287043665, "expertis": 20.0201765448, "stachniss": 1134.0, "strong": 1.6439888163999998, "gradient": 41.889182058, "condit": 1.92483026188, "solv": 7.26923076923, "case": 1.48498737256, "smart": 16.746835443, "day": 1.18371607516, "architectur": 5.12790697674, "websit": 2.52160101652, "cluster": 12.5007874016, "off": 1.5121440137200002, "more": 1.0171706817, "improv": 2.04376930999, "infrar": 39.4925373134, "dedic": 3.20533010297, "novel": 4.06555697823, "achiev": 1.87216981132, "rough": 3.29582727839, "benefit": 3.06841901817, "capabl": 3.6580645161300005, "amount": 2.27027027027, "general": 1.1218202374200001, "made": 1.07038834951, "faster": 7.61438848921, "realtim": 429.081081081, "lead": 1.2664326739, "state": 1.0477133240899998, "close": 1.2848818387799998, "affect": 2.4794627518400003, "detector": 45.6206896552, "sustain": 4.3603405657800005, "can": 1.17626139142, "avail": 1.7288467821, "comput": 3.9277585353800006, "make": 1.0762660158600001, "favor": 3.1332149200700004, "onli": 1.0256476516600002, "gap": 7.302667893280001, "size": 2.49387370405, "distinguish": 3.36926994907, "highqual": 1134.0, "key": 2.28005170185, "student": 2.47174217655, "accur": 5.768895348840001, "neighbor": 5.781500364169999, "transfer": 2.72549356223, "advanc": 1.9997480791, "imposs": 4.96125, "data": 3.37643555934, "lightweight": 32.4, "cnns": 1134.0, "address": 2.86157173756, "cyril": 31.8156312625, "method": 2.5714285714300003, "spray": 29.4545454545, "sinc": 1.08368600683, "convolut": 101.121019108, "bio": 42.336000000000006, "retun": 690.260869565, "come": 1.32831325301, "must": 1.9220338983099996, "origin": 1.13724928367, "lattic": 72.16363636359999, "identifi": 2.30187037843, "direct": 1.22226499346, "first": 1.00761614623, "vision": 4.88041807562, "input": 12.2029208301, "propos": 1.9902218879299998, "have": 1.0148948411399998, "while": 1.0441988950299999, "proven": 9.818181818180001, "year": 1.0485436893200002, "easili": 3.6938110749199997, "prototyp": 11.7426035503, "learn": 2.32275054865, "pipelin": 32.1376518219, "effici": 5.09335899904}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Real World Deep Learning: Neural Networks for Smart Crops</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/11/real-world-deep-learning-neural-networks-smart-crops.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Real World Deep Learning: Neural Networks for Smart Crops Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/jobs/17/11-07-citrix-machine-learning-ai-architect.html\" rel=\"prev\" title=\"Citrix: Machine Learning / AI Architect \u2013 Research &amp; Development\"/>\n<link href=\"https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html\" rel=\"next\" title=\"Interpreting Machine Learning Models: An Overview\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2017/11/real-world-deep-learning-neural-networks-smart-crops.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=74158\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2017/11/real-world-deep-learning-neural-networks-smart-crops.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-74158 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 7-Nov, 2017  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/11/index.html\">Nov</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/11/tutorials.html\">Tutorials, Overviews</a> \u00bb Real World Deep Learning: Neural Networks for Smart Crops (\u00a0<a href=\"/2017/n43.html\">17:n43</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Real World Deep Learning: Neural Networks for Smart Crops</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/jobs/17/11-07-citrix-machine-learning-ai-architect.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/computer-vision\" rel=\"tag\">Computer Vision</a>, <a href=\"https://www.kdnuggets.com/tag/convolutional-neural-networks\" rel=\"tag\">Convolutional Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/image-recognition\" rel=\"tag\">Image Recognition</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a></div>\n<br/>\n<p class=\"excerpt\">\n     The advances in image classification, object detection, and semantic segmentation using deep Convolutional Neural Networks, which spawned the availability of open source tools such as Caffe and TensorFlow (to name a couple) to easily manipulate neural network graphs... made a very strong case in favor of CNNs for our classifier. \n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><b>By Andres Milioto, University of Bonn.</b></p>\n<p><center><img alt=\"Header image\" src=\"/wp-content/uploads/milioto-1.jpg\" width=\"90%\"/><br>\n<font size=\"-1\">Autonomous ground robot spraying FLOURISH using the selective spraying tool, as seen from UAV.</font></br></center></p>\n<p>To produce high-quality food and feed a growing world population with the given amount of arable land in a sustainable manner, we must develop new methods of sustainable farming that increase yield while minimizing chemical inputs such as fertilizers, herbicides, and pesticides. I and my colleagues are working on a robotics-centered approaches to address this grand challenge. My name is Andres Milioto, and I am a research assistant and Ph.D. student in robotics at the Photogrammetry and Robotics Lab (<a href=\"http://www.ipb.uni-bonn.de\" target=\"_blank\">http://www.ipb.uni-bonn.de</a>) at the University of Bonn, Germany. Together with Philipp Lottes, Nived Chebrolu, and our supervisor Prof. Dr. Cyrill Stachniss we are developing an adaptable ground and aerial robots for smart farming in the context of the EC-funded project \u201cFlourish\u201d (<a href=\"http://flourish-project.eu/\" target=\"_blank\">http://flourish-project.eu/</a>), where we collaborate with several other Universities and industry partners across Europe.</p>\n<p>The Flourish consortium is committed to develop new robotic methods for sustainable farming that aim at minimizing chemical inputs such as fertilizers, herbicides, and pesticides in order to reduce the side-effects on our environment. Our precision agriculture techniques seek to address this challenge by monitoring key indicators of crop health and targeting treatment only to plants or infested areas that need it. The development of these novel methods is a very active and ongoing area of research, and the main goal of the project is to bridge the gap between the current and desired capabilities of agricultural robots by developing an adaptable robotic solution for precision farming. While conventional weed control systems treat the whole field uniformly with the same dose of herbicide, more novel perception-controlled weeding systems offer the potential to perform a treatment on a per-plant level, for example by selective spraying or mechanical weeding. </p>\n<p>Automatically treating plants at an individual level requires a plant classification system, which can analyze the sensor data perceived by the the robots in the field in real time, detects individual plants, and thoroughly distinguishes the crops and weeds. Our team is responsible for the perception aspect of the approach.</p>\n<p><center><img alt=\"Milioto image\" src=\"/wp-content/uploads/milioto-2.jpg\" width=\"99%\"/><br>\n<font size=\"-1\">From left to Right: Full pipeline from raw RGB+NIR images to classification output</font></br></center></p>\n<p><center><img alt=\"Milioto image\" src=\"/wp-content/uploads/milioto-3.jpg\" width=\"75%\"/><br>\n<font size=\"-1\">Left: Keypoint extraction and area around keypoint where features are calculated.<br>\nRight: Classification of the Keypoints and results.</br></font></br></center></p>\n<p>We focus on a detection on a per-plant basis to estimate the amount of crops as well as various weed species as a part of an autonomous robotic perception system. We are working on vision-based classification system for identifying crops and weeds in both, RGB-only as well as RGB combined with near infra-red (NIR) imagery. </p>\n<p>At the beginning of the project, we approached the implementation of the crop vs. weed detector by using a random forest classifier over 500 statistical, shape, and geometrical features around different image keypoints containing vegetation, and later applying a Markov Random Field to smooth the results taking neighbor information into consideration. This approach works extraordinarily well, reaching more than 95% precision and recall for both crops and weeds. We further improved this approach for detecting crops and weeds at a very early growth stage, i.e at a leaf size smaller than 0.5 cm^2. A big challenge with purely vision-based approaches is the transfer of a learned classification model to unseen field environments, where the soil conditions, growth stages of the plants and weed types may have changed. We solved this generalization problem by exploiting the spatial structure of the crop plants, which are sowed in rows with a certain lattice distance in between. We developed a geometry-based classification system, which interacts with the vision-based system in a semi-supervised manner in order to exploit the time invariant geometry information to retrain the vision classifier online while the robot discovers new field environments. Furthermore, we introduced a method for initializing the whole classification system with \u00a0a labeling effort of only 1 minute and still achieve state-of-the-art performance (see publications by Lottes et al., 2016, 2017).</p>\n<p>The experience we have made in the last two years during the research and development of the classification system led to the following conclusions. First, the design of the hand-crafted features is a very time consuming task that requires a very high level of specialization. Second, the implementation of the feature extraction in GPU code (using CUDA for NVIDIA GPUs) in order to make the system run in real time requires significant effort and expertise, and it took several months. Third, even when computing all features efficiently in parallel, the execution time was not close to the frame rate of a camera, and our approach was still very reliant on the availability of near infrared information, which comes at a high cost. </p>\n<p>The advances in image classification, object detection, and semantic segmentation using deep Convolutional Neural Networks, which spawned the availability of open source tools such as Caffe and TensorFlow (to name a couple) to easily manipulate neural network graphs, and to quickly prototype, train, and deploy using off the shelf GPUs made a very strong case in favor of CNNs for our classifier. Another thing that made a deep learning approach possible was that during the last 2 years of project we have been able to gather a dataset containing a large amount of data (in the order of 10^5 labeled images). A big part of these dataset has been made publicly available to allow other institutions to benefit from it (see publications by Nived Chebrolu et. al), and also to compare algorithmic results.</p>\n<p>Our first attempt was in favor of an object detection pipeline, using an approach similar to R-CNN. The key difference in our approach is that, given our specific problem, there is a possibility to pre-segment the vegetation from the soil, using well known vegetation indexes such as the NDVI (when NIR information is present) and/or Excess green (for RGB-only). Therefore, instead of using selective search for our region proposals, we were able to use the connected components in this vegetation mask, which speeds up the process significantly. This new approach had similar results to the random forest based one, but also had two big advantages: the main one is that it runs at over 5Hz in an NVIDIA Jetson TX2; the second one is that it took less than 2 months to implement, including the time to learn the tools. At the time, we didn\u2019t even have dedicated hardware to train the networks, and we were working off of a notebook GPU, and the lightweight network worked very well after 3 days of training. (See publications by Andres Milioto et. al, 2017)</p>\n<p><center><img alt=\"\" src=\"/wp-content/uploads/milioto-4.jpg\" width=\"90%\"/><br/>\n<font size=\"-1\">From left to right: RGB, NIR images. Vegetation Segmentation. Blob extraction. Classification of blobs. Overlay of classified masks to original image</font></center></p>\n<p>This motivated us to further pursue this direction, and we therefore acquired a dedicated GPU computer with 4 NVIDIA GTX1080Ti\u2019s. In order to quantify how much deep learning is affecting the lab, after roughly 3 months of receiving the first cluster, we have already ordered a second identical twin to the one we already have, because we are already having trouble scheduling GPU time.</p>\n<p><center><img alt=\"Milioto image\" src=\"/wp-content/uploads/milioto-5.jpg\" width=\"99%\"/><br/>\n<font size=\"-1\">Full semantic segmentation pipeline.</font></center></p>\n<p>Coming back to the approach, we have currently dropped the object detection pipeline in favor of a full semantic segmentation one. This allows us to eliminate some of the limitations of the old approach, such as an impossibility to handle overlapping plants, and reliance on the pre-segmentation, which needed hyperparameter tuning when transferring to a new field. This new approach allows us, in addition, to get rid of the costly NIR information that we needed for an accurate pre-segmentation, and to work in constant time complexity. In order to help the architecture generalize well to different fields, and to obtain an accurate segmentation, we add to the RGB inputs several remappings and representations of the latter, which had already proven useful when designing features for the random classifier, such as vegetation indexes, different color spaces, gradients, edges, etc. We have then been able to significantly increase the performance of the semantic segmentation without hyperparameter re-tuning, and since the architecture is designed with real-time performance in mind, we are able to run at near frame rate of a commercial camera.</p>\n<p>The perception solution as a part of our project goals has already reached a maturity state and yields a high performance, and we have been working hard to bridge the last gaps towards an applicable system in real world. What this last gap entails is to get the algorithms running with zero re-labeling effort when applying it to different fields and crop species. We are exploring generative models, and several unsupervised learning approaches in order to achieve this, all of which would use the autonomous capabilities of the robots to gather new data in the new field and allow it to intelligently auto-retrain itself for the new task in hand. We are also always working on getting the models to run faster and faster, and using less resources and power, for example by using newly available hardware accelerators for neural networks.</p>\n<p>It has been a very exciting path to walk, and we are very eager to see what the future holds! Check our website for updates on this exciting journey!</p>\n<p><b>Resources:</b></p>\n<ul>\n<li><a href=\"http://www.ipb.uni-bonn.de\" rel=\"noopener\" target=\"_blank\">Photogrammetry and Robotics Lab</a>\n<li><a href=\"http://www.ipb.uni-bonn.de/people/philipp-lottes/\" rel=\"noopener\" target=\"_blank\">Philipp Lottes</a>\n<li><a href=\"http://www.ipb.uni-bonn.de/people/nived-chebrolu/\" rel=\"noopener\" target=\"_blank\">Nived Chebrolu</a>\n<li><a href=\"http://www.ipb.uni-bonn.de/people/cyrill-stachniss/\" rel=\"noopener\" target=\"_blank\">Cyrill Stachniss</a>\n<li><a href=\"http://www.ipb.uni-bonn.de/people/andres-milioto/\" rel=\"noopener\" target=\"_blank\">Andres Milioto</a>\n<li><a href=\"http://flourish-project.eu\" rel=\"noopener\" target=\"_blank\">Flourish Project</a>\n</li></li></li></li></li></li></ul>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"https://www.linkedin.com/in/amilioto/\" target=\"_blank\">Andres Milioto</a></b> is a PhD Candidate in Robotics and AI at The University of Bonn. His drive is to help people lead better lives through technology by creating algorithms for cool robots that can perceive and interact with the world around us.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/03/medical-image-analysis-deep-learning.html\">Medical Image Analysis with Deep Learning\u200a</a>\n<li><a href=\"/2016/09/beginners-guide-understanding-convolutional-neural-networks-part-1.html\">A Beginner\u2019s Guide To Understanding Convolutional Neural Networks Part 1</a>\n<li><a href=\"/2016/11/intuitive-explanation-convolutional-neural-networks.html\">An Intuitive Explanation of Convolutional Neural Networks</a>\n</li></li></li></ul>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/jobs/17/11-07-citrix-machine-learning-ai-architect.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/11/index.html\">Nov</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/11/tutorials.html\">Tutorials, Overviews</a> \u00bb Real World Deep Learning: Neural Networks for Smart Crops (\u00a0<a href=\"/2017/n43.html\">17:n43</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556324189\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.735 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 20:16:29 -->\n<!-- Compression = gzip -->", "content_tokenized": ["andr", "milioto", "univers", "bonn", "autonom", "ground", "robot", "spray", "use", "the", "select", "spray", "tool", "seen", "from", "produc", "highqual", "food", "and", "feed", "grow", "world", "popul", "with", "the", "given", "amount", "arabl", "land", "sustain", "manner", "must", "develop", "new", "method", "sustain", "farm", "that", "increas", "yield", "while", "minim", "chemic", "input", "such", "fertil", "herbicid", "and", "pesticid", "and", "colleagu", "are", "work", "roboticscent", "approach", "address", "this", "grand", "challeng", "name", "andr", "milioto", "and", "research", "assist", "and", "phd", "student", "robot", "the", "photogrammetri", "and", "robot", "lab", "the", "univers", "bonn", "germani", "togeth", "with", "philipp", "lott", "nive", "chebrolu", "and", "our", "supervisor", "prof", "cyril", "stachniss", "are", "develop", "adapt", "ground", "and", "aerial", "robot", "for", "smart", "farm", "the", "context", "the", "ecfund", "project", "flourish", "where", "collabor", "with", "sever", "other", "univers", "and", "industri", "partner", "across", "europ", "the", "flourish", "consortium", "commit", "develop", "new", "robot", "method", "for", "sustain", "farm", "that", "aim", "minim", "chemic", "input", "such", "fertil", "herbicid", "and", "pesticid", "order", "reduc", "the", "sideeffect", "our", "environ", "our", "precis", "agricultur", "techniqu", "seek", "address", "this", "challeng", "monitor", "key", "indic", "crop", "health", "and", "target", "treatment", "onli", "plant", "infest", "area", "that", "need", "the", "develop", "these", "novel", "method", "veri", "activ", "and", "ongo", "area", "research", "and", "the", "main", "goal", "the", "project", "bridg", "the", "gap", "between", "the", "current", "and", "desir", "capabl", "agricultur", "robot", "develop", "adapt", "robot", "solut", "for", "precis", "farm", "while", "convent", "weed", "control", "system", "treat", "the", "whole", "field", "uniform", "with", "the", "same", "dose", "herbicid", "more", "novel", "perceptioncontrol", "weed", "system", "offer", "the", "potenti", "perform", "treatment", "perplant", "level", "for", "exampl", "select", "spray", "mechan", "weed", "automat", "treat", "plant", "individu", "level", "requir", "plant", "classif", "system", "which", "can", "analyz", "the", "sensor", "data", "perceiv", "the", "the", "robot", "the", "field", "real", "time", "detect", "individu", "plant", "and", "thorough", "distinguish", "the", "crop", "and", "weed", "our", "team", "respons", "for", "the", "percept", "aspect", "the", "approach", "from", "left", "right", "full", "pipelin", "from", "raw", "imag", "classif", "output", "left", "keypoint", "extract", "and", "area", "around", "keypoint", "where", "featur", "are", "calcul", "right", "classif", "the", "keypoint", "and", "result", "focus", "detect", "perplant", "basi", "estim", "the", "amount", "crop", "well", "various", "weed", "speci", "part", "autonom", "robot", "percept", "system", "are", "work", "visionbas", "classif", "system", "for", "identifi", "crop", "and", "weed", "both", "rgbon", "well", "combin", "with", "near", "infrar", "imageri", "the", "begin", "the", "project", "approach", "the", "implement", "the", "crop", "weed", "detector", "use", "random", "forest", "classifi", "over", "num", "statist", "shape", "and", "geometr", "featur", "around", "differ", "imag", "keypoint", "contain", "veget", "and", "later", "appli", "markov", "random", "field", "smooth", "the", "result", "take", "neighbor", "inform", "into", "consider", "this", "approach", "work", "extraordinarili", "well", "reach", "more", "than", "num", "precis", "and", "recal", "for", "both", "crop", "and", "weed", "further", "improv", "this", "approach", "for", "detect", "crop", "and", "weed", "veri", "earli", "growth", "stage", "leaf", "size", "smaller", "than", "num", "cmnum", "big", "challeng", "with", "pure", "visionbas", "approach", "the", "transfer", "learn", "classif", "model", "unseen", "field", "environ", "where", "the", "soil", "condit", "growth", "stage", "the", "plant", "and", "weed", "type", "may", "have", "chang", "solv", "this", "general", "problem", "exploit", "the", "spatial", "structur", "the", "crop", "plant", "which", "are", "sow", "row", "with", "certain", "lattic", "distanc", "between", "develop", "geometrybas", "classif", "system", "which", "interact", "with", "the", "visionbas", "system", "semisupervis", "manner", "order", "exploit", "the", "time", "invari", "geometri", "inform", "retrain", "the", "vision", "classifi", "onlin", "while", "the", "robot", "discov", "new", "field", "environ", "furthermor", "introduc", "method", "for", "initi", "the", "whole", "classif", "system", "with", "label", "effort", "onli", "num", "minut", "and", "still", "achiev", "stateoftheart", "perform", "see", "public", "lott", "num", "num", "the", "experi", "have", "made", "the", "last", "two", "year", "dure", "the", "research", "and", "develop", "the", "classif", "system", "led", "the", "follow", "conclus", "first", "the", "design", "the", "handcraft", "featur", "veri", "time", "consum", "task", "that", "requir", "veri", "high", "level", "special", "second", "the", "implement", "the", "featur", "extract", "code", "use", "for", "gpus", "order", "make", "the", "system", "run", "real", "time", "requir", "signific", "effort", "and", "expertis", "and", "took", "sever", "month", "third", "even", "when", "comput", "all", "featur", "effici", "parallel", "the", "execut", "time", "not", "close", "the", "frame", "rate", "camera", "and", "our", "approach", "still", "veri", "reliant", "the", "avail", "near", "infrar", "inform", "which", "come", "high", "cost", "the", "advanc", "imag", "classif", "object", "detect", "and", "semant", "segment", "use", "deep", "convolut", "neural", "network", "which", "spawn", "the", "avail", "open", "sourc", "tool", "such", "caff", "and", "tensorflow", "name", "coupl", "easili", "manipul", "neural", "network", "graph", "and", "quick", "prototyp", "train", "and", "deploy", "use", "off", "the", "shelf", "gpus", "made", "veri", "strong", "case", "favor", "cnns", "for", "our", "classifi", "anoth", "thing", "that", "made", "deep", "learn", "approach", "possibl", "that", "dure", "the", "last", "num", "year", "project", "have", "been", "abl", "gather", "dataset", "contain", "larg", "amount", "data", "the", "order", "num", "label", "imag", "big", "part", "these", "dataset", "has", "been", "made", "public", "avail", "allow", "other", "institut", "benefit", "from", "see", "public", "nive", "chebrolu", "and", "also", "compar", "algorithm", "result", "our", "first", "attempt", "favor", "object", "detect", "pipelin", "use", "approach", "similar", "the", "key", "differ", "our", "approach", "that", "given", "our", "specif", "problem", "there", "possibl", "preseg", "the", "veget", "from", "the", "soil", "use", "well", "known", "veget", "index", "such", "the", "when", "inform", "present", "andor", "excess", "green", "for", "rgbon", "therefor", "instead", "use", "select", "search", "for", "our", "region", "propos", "were", "abl", "use", "the", "connect", "compon", "this", "veget", "mask", "which", "speed", "the", "process", "signific", "this", "new", "approach", "had", "similar", "result", "the", "random", "forest", "base", "one", "but", "also", "had", "two", "big", "advantag", "the", "main", "one", "that", "run", "over", "numhz", "jetson", "the", "second", "one", "that", "took", "less", "than", "num", "month", "implement", "includ", "the", "time", "learn", "the", "tool", "the", "time", "even", "have", "dedic", "hardwar", "train", "the", "network", "and", "were", "work", "off", "notebook", "and", "the", "lightweight", "network", "work", "veri", "well", "after", "num", "day", "train", "see", "public", "andr", "milioto", "num", "from", "left", "right", "imag", "veget", "segment", "blob", "extract", "classif", "blob", "overlay", "classifi", "mask", "origin", "imag", "this", "motiv", "further", "pursu", "this", "direct", "and", "therefor", "acquir", "dedic", "comput", "with", "num", "gtxnumti", "order", "quantifi", "how", "much", "deep", "learn", "affect", "the", "lab", "after", "rough", "num", "month", "receiv", "the", "first", "cluster", "have", "alreadi", "order", "second", "ident", "twin", "the", "one", "alreadi", "have", "becaus", "are", "alreadi", "have", "troubl", "schedul", "time", "full", "semant", "segment", "pipelin", "come", "back", "the", "approach", "have", "current", "drop", "the", "object", "detect", "pipelin", "favor", "full", "semant", "segment", "one", "this", "allow", "elimin", "some", "the", "limit", "the", "old", "approach", "such", "imposs", "handl", "overlap", "plant", "and", "relianc", "the", "presegment", "which", "need", "hyperparamet", "tune", "when", "transfer", "new", "field", "this", "new", "approach", "allow", "addit", "get", "rid", "the", "cost", "inform", "that", "need", "for", "accur", "presegment", "and", "work", "constant", "time", "complex", "order", "help", "the", "architectur", "general", "well", "differ", "field", "and", "obtain", "accur", "segment", "add", "the", "input", "sever", "remap", "and", "represent", "the", "latter", "which", "had", "alreadi", "proven", "use", "when", "design", "featur", "for", "the", "random", "classifi", "such", "veget", "index", "differ", "color", "space", "gradient", "edg", "etc", "have", "then", "been", "abl", "signific", "increas", "the", "perform", "the", "semant", "segment", "without", "hyperparamet", "retun", "and", "sinc", "the", "architectur", "design", "with", "realtim", "perform", "mind", "are", "abl", "run", "near", "frame", "rate", "commerci", "camera", "the", "percept", "solut", "part", "our", "project", "goal", "has", "alreadi", "reach", "matur", "state", "and", "yield", "high", "perform", "and", "have", "been", "work", "hard", "bridg", "the", "last", "gap", "toward", "applic", "system", "real", "world", "what", "this", "last", "gap", "entail", "get", "the", "algorithm", "run", "with", "zero", "relabel", "effort", "when", "appli", "differ", "field", "and", "crop", "speci", "are", "explor", "generat", "model", "and", "sever", "unsupervis", "learn", "approach", "order", "achiev", "this", "all", "which", "would", "use", "the", "autonom", "capabl", "the", "robot", "gather", "new", "data", "the", "new", "field", "and", "allow", "intellig", "autoretrain", "itself", "for", "the", "new", "task", "hand", "are", "also", "alway", "work", "get", "the", "model", "run", "faster", "and", "faster", "and", "use", "less", "resourc", "and", "power", "for", "exampl", "use", "newli", "avail", "hardwar", "acceler", "for", "neural", "network", "has", "been", "veri", "excit", "path", "walk", "and", "are", "veri", "eager", "see", "what", "the", "futur", "hold", "check", "our", "websit", "for", "updat", "this", "excit", "journey", "resourc", "photogrammetri", "and", "robot", "lab", "philipp", "lott", "nive", "chebrolu", "cyril", "stachniss", "andr", "milioto", "flourish", "project", "bio", "andr", "milioto", "phd", "candid", "robot", "and", "the", "univers", "bonn", "his", "drive", "help", "peopl", "lead", "better", "live", "through", "technolog", "creat", "algorithm", "for", "cool", "robot", "that", "can", "perceiv", "and", "interact", "with", "the", "world", "around", "relat", "medic", "imag", "analysi", "with", "deep", "learn", "beginn", "guid", "understand", "convolut", "neural", "network", "part", "num", "intuit", "explan", "convolut", "neural", "network"], "timestamp_scraper": 1556379378.598331, "title": "Real World Deep Learning: Neural Networks for Smart Crops", "read_time": 486.0, "content_html": "<div class=\"post\" id=\"post-\">\n<p><b>By Andres Milioto, University of Bonn.</b></p>\n<p><center><img alt=\"Header image\" src=\"/wp-content/uploads/milioto-1.jpg\" width=\"90%\"/><br>\n<font size=\"-1\">Autonomous ground robot spraying FLOURISH using the selective spraying tool, as seen from UAV.</font></br></center></p>\n<p>To produce high-quality food and feed a growing world population with the given amount of arable land in a sustainable manner, we must develop new methods of sustainable farming that increase yield while minimizing chemical inputs such as fertilizers, herbicides, and pesticides. I and my colleagues are working on a robotics-centered approaches to address this grand challenge. My name is Andres Milioto, and I am a research assistant and Ph.D. student in robotics at the Photogrammetry and Robotics Lab (<a href=\"http://www.ipb.uni-bonn.de\" target=\"_blank\">http://www.ipb.uni-bonn.de</a>) at the University of Bonn, Germany. Together with Philipp Lottes, Nived Chebrolu, and our supervisor Prof. Dr. Cyrill Stachniss we are developing an adaptable ground and aerial robots for smart farming in the context of the EC-funded project \u201cFlourish\u201d (<a href=\"http://flourish-project.eu/\" target=\"_blank\">http://flourish-project.eu/</a>), where we collaborate with several other Universities and industry partners across Europe.</p>\n<p>The Flourish consortium is committed to develop new robotic methods for sustainable farming that aim at minimizing chemical inputs such as fertilizers, herbicides, and pesticides in order to reduce the side-effects on our environment. Our precision agriculture techniques seek to address this challenge by monitoring key indicators of crop health and targeting treatment only to plants or infested areas that need it. The development of these novel methods is a very active and ongoing area of research, and the main goal of the project is to bridge the gap between the current and desired capabilities of agricultural robots by developing an adaptable robotic solution for precision farming. While conventional weed control systems treat the whole field uniformly with the same dose of herbicide, more novel perception-controlled weeding systems offer the potential to perform a treatment on a per-plant level, for example by selective spraying or mechanical weeding. </p>\n<p>Automatically treating plants at an individual level requires a plant classification system, which can analyze the sensor data perceived by the the robots in the field in real time, detects individual plants, and thoroughly distinguishes the crops and weeds. Our team is responsible for the perception aspect of the approach.</p>\n<p><center><img alt=\"Milioto image\" src=\"/wp-content/uploads/milioto-2.jpg\" width=\"99%\"/><br>\n<font size=\"-1\">From left to Right: Full pipeline from raw RGB+NIR images to classification output</font></br></center></p>\n<p><center><img alt=\"Milioto image\" src=\"/wp-content/uploads/milioto-3.jpg\" width=\"75%\"/><br>\n<font size=\"-1\">Left: Keypoint extraction and area around keypoint where features are calculated.<br>\nRight: Classification of the Keypoints and results.</br></font></br></center></p>\n<p>We focus on a detection on a per-plant basis to estimate the amount of crops as well as various weed species as a part of an autonomous robotic perception system. We are working on vision-based classification system for identifying crops and weeds in both, RGB-only as well as RGB combined with near infra-red (NIR) imagery. </p>\n<p>At the beginning of the project, we approached the implementation of the crop vs. weed detector by using a random forest classifier over 500 statistical, shape, and geometrical features around different image keypoints containing vegetation, and later applying a Markov Random Field to smooth the results taking neighbor information into consideration. This approach works extraordinarily well, reaching more than 95% precision and recall for both crops and weeds. We further improved this approach for detecting crops and weeds at a very early growth stage, i.e at a leaf size smaller than 0.5 cm^2. A big challenge with purely vision-based approaches is the transfer of a learned classification model to unseen field environments, where the soil conditions, growth stages of the plants and weed types may have changed. We solved this generalization problem by exploiting the spatial structure of the crop plants, which are sowed in rows with a certain lattice distance in between. We developed a geometry-based classification system, which interacts with the vision-based system in a semi-supervised manner in order to exploit the time invariant geometry information to retrain the vision classifier online while the robot discovers new field environments. Furthermore, we introduced a method for initializing the whole classification system with \u00a0a labeling effort of only 1 minute and still achieve state-of-the-art performance (see publications by Lottes et al., 2016, 2017).</p>\n<p>The experience we have made in the last two years during the research and development of the classification system led to the following conclusions. First, the design of the hand-crafted features is a very time consuming task that requires a very high level of specialization. Second, the implementation of the feature extraction in GPU code (using CUDA for NVIDIA GPUs) in order to make the system run in real time requires significant effort and expertise, and it took several months. Third, even when computing all features efficiently in parallel, the execution time was not close to the frame rate of a camera, and our approach was still very reliant on the availability of near infrared information, which comes at a high cost. </p>\n<p>The advances in image classification, object detection, and semantic segmentation using deep Convolutional Neural Networks, which spawned the availability of open source tools such as Caffe and TensorFlow (to name a couple) to easily manipulate neural network graphs, and to quickly prototype, train, and deploy using off the shelf GPUs made a very strong case in favor of CNNs for our classifier. Another thing that made a deep learning approach possible was that during the last 2 years of project we have been able to gather a dataset containing a large amount of data (in the order of 10^5 labeled images). A big part of these dataset has been made publicly available to allow other institutions to benefit from it (see publications by Nived Chebrolu et. al), and also to compare algorithmic results.</p>\n<p>Our first attempt was in favor of an object detection pipeline, using an approach similar to R-CNN. The key difference in our approach is that, given our specific problem, there is a possibility to pre-segment the vegetation from the soil, using well known vegetation indexes such as the NDVI (when NIR information is present) and/or Excess green (for RGB-only). Therefore, instead of using selective search for our region proposals, we were able to use the connected components in this vegetation mask, which speeds up the process significantly. This new approach had similar results to the random forest based one, but also had two big advantages: the main one is that it runs at over 5Hz in an NVIDIA Jetson TX2; the second one is that it took less than 2 months to implement, including the time to learn the tools. At the time, we didn\u2019t even have dedicated hardware to train the networks, and we were working off of a notebook GPU, and the lightweight network worked very well after 3 days of training. (See publications by Andres Milioto et. al, 2017)</p>\n<p><center><img alt=\"\" src=\"/wp-content/uploads/milioto-4.jpg\" width=\"90%\"/><br/>\n<font size=\"-1\">From left to right: RGB, NIR images. Vegetation Segmentation. Blob extraction. Classification of blobs. Overlay of classified masks to original image</font></center></p>\n<p>This motivated us to further pursue this direction, and we therefore acquired a dedicated GPU computer with 4 NVIDIA GTX1080Ti\u2019s. In order to quantify how much deep learning is affecting the lab, after roughly 3 months of receiving the first cluster, we have already ordered a second identical twin to the one we already have, because we are already having trouble scheduling GPU time.</p>\n<p><center><img alt=\"Milioto image\" src=\"/wp-content/uploads/milioto-5.jpg\" width=\"99%\"/><br/>\n<font size=\"-1\">Full semantic segmentation pipeline.</font></center></p>\n<p>Coming back to the approach, we have currently dropped the object detection pipeline in favor of a full semantic segmentation one. This allows us to eliminate some of the limitations of the old approach, such as an impossibility to handle overlapping plants, and reliance on the pre-segmentation, which needed hyperparameter tuning when transferring to a new field. This new approach allows us, in addition, to get rid of the costly NIR information that we needed for an accurate pre-segmentation, and to work in constant time complexity. In order to help the architecture generalize well to different fields, and to obtain an accurate segmentation, we add to the RGB inputs several remappings and representations of the latter, which had already proven useful when designing features for the random classifier, such as vegetation indexes, different color spaces, gradients, edges, etc. We have then been able to significantly increase the performance of the semantic segmentation without hyperparameter re-tuning, and since the architecture is designed with real-time performance in mind, we are able to run at near frame rate of a commercial camera.</p>\n<p>The perception solution as a part of our project goals has already reached a maturity state and yields a high performance, and we have been working hard to bridge the last gaps towards an applicable system in real world. What this last gap entails is to get the algorithms running with zero re-labeling effort when applying it to different fields and crop species. We are exploring generative models, and several unsupervised learning approaches in order to achieve this, all of which would use the autonomous capabilities of the robots to gather new data in the new field and allow it to intelligently auto-retrain itself for the new task in hand. We are also always working on getting the models to run faster and faster, and using less resources and power, for example by using newly available hardware accelerators for neural networks.</p>\n<p>It has been a very exciting path to walk, and we are very eager to see what the future holds! Check our website for updates on this exciting journey!</p>\n<p><b>Resources:</b></p>\n<ul>\n<li><a href=\"http://www.ipb.uni-bonn.de\" rel=\"noopener\" target=\"_blank\">Photogrammetry and Robotics Lab</a>\n<li><a href=\"http://www.ipb.uni-bonn.de/people/philipp-lottes/\" rel=\"noopener\" target=\"_blank\">Philipp Lottes</a>\n<li><a href=\"http://www.ipb.uni-bonn.de/people/nived-chebrolu/\" rel=\"noopener\" target=\"_blank\">Nived Chebrolu</a>\n<li><a href=\"http://www.ipb.uni-bonn.de/people/cyrill-stachniss/\" rel=\"noopener\" target=\"_blank\">Cyrill Stachniss</a>\n<li><a href=\"http://www.ipb.uni-bonn.de/people/andres-milioto/\" rel=\"noopener\" target=\"_blank\">Andres Milioto</a>\n<li><a href=\"http://flourish-project.eu\" rel=\"noopener\" target=\"_blank\">Flourish Project</a>\n</li></li></li></li></li></li></ul>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"https://www.linkedin.com/in/amilioto/\" target=\"_blank\">Andres Milioto</a></b> is a PhD Candidate in Robotics and AI at The University of Bonn. His drive is to help people lead better lives through technology by creating algorithms for cool robots that can perceive and interact with the world around us.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/03/medical-image-analysis-deep-learning.html\">Medical Image Analysis with Deep Learning\u200a</a>\n<li><a href=\"/2016/09/beginners-guide-understanding-convolutional-neural-networks-part-1.html\">A Beginner\u2019s Guide To Understanding Convolutional Neural Networks Part 1</a>\n<li><a href=\"/2016/11/intuitive-explanation-convolutional-neural-networks.html\">An Intuitive Explanation of Convolutional Neural Networks</a>\n</li></li></li></ul>\n</div> ", "website": "kdnuggets"}