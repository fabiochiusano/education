{"title": "What Does GPT-2 Think About the AI Arms Race?", "tfidf": {"tfidf": {"question": 2.20408163265, "sourc": 1.69760479042, "occur": 1.7453825857499998, "but": 9.14691761091, "speed": 3.8703071672400005, "further": 1.3618116315, "fatal": 7.48867924528, "evid": 4.49745042492, "then": 2.17315721032, "from": 4.00226885988, "abl": 1.8208510150200001, "will": 12.2481098596, "contempl": 18.3114186851, "suggest": 1.7571665744299998, "phase": 4.3012733676499995, "pattern": 3.79173632673, "all": 4.04587155964, "into": 1.01502461479, "other": 2.01984732824, "higher": 2.1218925421, "arm": 9.3663716814, "front": 2.32820061593, "this": 8.03034901368, "and": 15.00094488195, "dsq": 1134.0, "mani": 3.13280273631, "merg": 5.28319467554, "longer": 2.02319357716, "life": 1.37051104972, "hurdl": 84.8983957219, "calcul": 6.12972972973, "capabl": 3.6580645161300005, "prime": 3.49845747025, "cours": 2.15092805853, "larg": 1.18574949585, "compet": 3.26465144972, "artifici": 41.58198009430001, "key": 2.28005170185, "around": 1.21394708671, "news": 2.08182533438, "equip": 6.49059689288, "ask": 2.1744966443, "learn": 13.936503291900001, "import": 5.360796893480001, "comment": 3.05954904606, "first": 2.01523229246, "natur": 1.5392670157100001, "produc": 1.36932896326, "art": 3.9989924433199997, "distil": 43.9778393352, "numer": 3.66651270208, "evalu": 6.9509632224199995, "our": 9.43035343036, "wage": 8.15827338129, "want": 3.99396226416, "general": 2.2436404748400003, "goal": 6.56304257958, "embrac": 8.45817794353, "gas": 5.7730909090900004, "actor": 8.48530197756, "oversea": 7.62902450745, "disprov": 62.750988142299995, "heard": 4.45204711161, "frontier": 8.7422907489, "thought": 1.9854927463699998, "predict": 10.3696930111, "divers": 7.94395796848, "resourc": 2.9487369985100003, "short": 1.41295834817, "mistak": 8.71350164654, "have": 4.059579364559999, "realiti": 4.563380281690001, "knowledg": 3.3981164383599998, "made": 1.07038834951, "global": 3.30612244898, "make": 3.2287980475800007, "array": 20.2888178914, "who": 2.12558575446, "impend": 26.241322314, "against": 1.2902072328299998, "move": 1.29125660838, "human": 15.172380838639999, "peopl": 1.21320495186, "data": 23.63504891538, "discuss": 4.39352428394, "structur": 4.1161524500999995, "hundr": 2.4698195395099996, "subject": 1.8715077213299998, "cursori": 288.654545455, "process": 6.78099305928, "entrepreneur": 16.4008264463, "limit": 1.5186531471200002, "energi": 3.66566612792, "dsqsrc": 1134.0, "matthew": 6.908616187989999, "helixlik": 1134.0, "civil": 2.10306000795, "museum": 2.99490662139, "genuin": 11.4793926247, "copious": 190.13173652700002, "anticip": 7.793814432989999, "socal": 2268.0, "time": 3.03382381044, "openai": 2268.0, "overlook": 22.8103448276, "bit": 8.33385826772, "within": 2.4738605376, "select": 2.02345144022, "one": 1.00627495722, "were": 1.02458857696, "repeat": 2.8771293947099994, "machin": 8.04866920152, "interbert": 1134.0, "case": 4.45496211768, "that": 11.04382470125, "pretti": 15.75, "place": 3.3013100436600005, "advanc": 3.9994961582, "safe": 5.02723242559, "fed": 25.0805687204, "core": 4.623179965059999, "determin": 4.331787175980001, "prompt": 8.88913773796, "logic": 8.929133858270001, "dataset": 387.219512196, "can": 4.70504556568, "million": 3.4558119286, "way": 4.8762957844, "while": 2.0883977900599997, "hand": 1.6152202665600002, "fulli": 2.79015817223, "same": 1.11857958148, "dsqtype": 1134.0, "with": 9.010783880909997, "envisag": 37.0934579439, "spend": 4.15928739848, "major": 1.14852058164, "mental": 13.02913418138, "result": 5.7305804216, "textjavascript": 1134.0, "has": 1.0436497502, "tri": 5.563368765329999, "becaus": 1.1495184997499999, "than": 4.131147541, "shkreli": 1134.0, "readi": 5.15789473684, "longterm": 512.129032258, "train": 5.8097096853, "written": 3.9146837627999997, "number": 2.20285833218, "pursu": 4.15384615385, "disqusshortnam": 1134.0, "forc": 1.32399299475, "abil": 2.70875277256, "utilis": 29.345656192199996, "mere": 4.20222339862, "doe": 1.70581282905, "delay": 4.23247134098, "decid": 3.8515283842800003, "line": 2.8365195640599996, "take": 2.27923336444, "think": 2.90715986083, "final": 1.34008609775, "like": 2.2983713355, "theori": 3.02745995423, "robert": 1.8618505922400002, "hope": 5.01769911504, "controversi": 5.2508681991, "best": 3.1657028913200005, "ultim": 12.92623351245, "releas": 5.5131380946899995, "overtak": 50.2405063291, "obvious": 6.44841592201, "neural": 237.8426966292, "better": 4.0131445905000005, "idea": 2.0930784443, "sure": 7.453521126760001, "what": 8.77404073896, "exercis": 4.73627684964, "deal": 4.36693714758, "task": 7.77282741738, "perform": 3.0627954085000004, "world": 2.22680412372, "servic": 1.51300867245, "certain": 1.8077886586200003, "without": 1.29547123623, "martin": 3.07793718496, "how": 3.20500656102, "object": 2.3488681757700003, "would": 1.0828729281799998, "highlight": 5.83033419023, "there": 3.12273800157, "altern": 2.1390460792200003, "success": 1.32002993265, "out": 5.30083472455, "set": 3.56123822343, "most": 2.04192926046, "their": 2.0309581681, "overview": 12.6805111821, "stop": 2.1783754116400003, "overal": 3.0442953020099996, "the": 59.0, "littleunderstood": 1134.0, "demonstr": 2.64997496244, "part": 1.04330682789, "criticis": 9.879278158060002, "recent": 1.54405757635, "whether": 4.41367806506, "wrote": 2.10473286491, "happen": 2.96359902931, "refer": 2.6004914005, "given": 1.35426085473, "nod": 61.0615384615, "topic": 5.457545548300001, "ammount": 1134.0, "bring": 2.03616775683, "studi": 1.53184098804, "also": 1.01476510067, "thus": 1.6463756092500001, "far": 1.71022298826, "paramet": 51.769565217300006, "live": 1.30591428806, "system": 5.54959363804, "provid": 2.43105428374, "yourself": 26.592964824099997, "turn": 1.3838912133899999, "web": 5.17133550489, "race": 11.72093023256, "more": 3.0515120451, "long": 1.2657259028899999, "outlin": 6.38102893891, "techniqu": 7.458773784360001, "gain": 1.84819557625, "retriev": 2.16826003824, "carbon": 12.1469013007, "next": 5.9802241265600005, "work": 2.23040179826, "influenc": 1.77246846042, "promis": 7.006178287739999, "illfat": 1134.0, "cherrypick": 1134.0, "domain": 37.57633136096, "perhap": 3.14812611541, "miss": 3.53664513255, "though": 2.72152224222, "not": 6.09404388714, "might": 4.312372674180001, "are": 6.17943561468, "some": 1.04036697248, "omin": 117.6, "alreadi": 1.9551724137900002, "imagin": 6.598503740650001, "lead": 1.2664326739, "mayo": 49.7680250784, "scaleup": 1134.0, "kdnugget": 2268.0, "term": 1.39520168732, "power": 1.3396337861799998, "usher": 46.4210526316, "for": 7.00220528007, "model": 18.8153805636, "understand": 2.96858638743, "crucial": 7.7443902439, "direct": 3.66679498038, "stage": 2.0831911822599998, "deconstruct": 78.5940594059, "dispar": 22.583214793699998, "record": 1.42334588488, "develop": 1.1955719557200002, "seem": 11.45619858565, "just": 2.67160286074, "network": 10.37477536352, "transformerbas": 1134.0, "succeed": 3.3550295857999997, "accept": 1.7377408056, "attrit": 53.4545454545, "relat": 1.23750876919, "day": 1.18371607516, "cuttingedg": 1134.0, "approach": 4.15113086678, "takeaway": 529.2, "schedul": 3.6648199445999996, "deep": 21.76782449724, "reason": 1.72340425532, "requir": 1.52844902282, "grave": 13.5114893617, "fun": 12.8863636364, "use": 1.0296387573799999, "var": 236.95522388, "advent": 23.837837837800002, "distinct": 2.2836593786000003, "failur": 3.28559602649, "nonstat": 2268.0, "drop": 2.4594887684, "follow": 2.09280253098, "late": 1.31740104556, "languag": 9.17953165656, "onli": 2.0512953033200003, "unlik": 2.42529789184, "spell": 6.75, "small": 1.3594793629, "tie": 3.31786833856, "text": 6.25655172414, "mind": 7.1837104072399995, "new": 2.0357761108, "simpl": 6.7962328767199995, "futur": 3.7154224198400003, "come": 1.32831325301, "may": 6.31210655358, "those": 2.39096385542, "bayesian": 178.38202247200002, "state": 2.0954266481799997, "least": 1.6165359943000002, "run": 4.6707855251399995, "word": 7.186149145639999, "caus": 4.15565831952, "year": 1.0485436893200002, "implement": 3.57648118946, "contain": 1.59814777532, "complet": 1.24021560816, "ourselv": 27.184931506799998, "sens": 5.673039128099999, "seri": 1.46511627907, "parri": 55.7052631579, "find": 3.4588235294199996, "point": 1.25990000794, "few": 1.31729173581, "intellig": 25.160063391420003, "modern": 6.127955225319999, "war": 1.41320989852, "endeavour": 33.2830188679, "code": 3.8807137619199996, "premis": 13.4428450466, "grammar": 14.7, "sampl": 36.1640091116, "prevent": 2.16117615029, "tech": 19.1739130435, "underpin": 44.3463687151, "page": 2.03669018602, "across": 1.7318642958400001, "ton": 10.5278514589, "nation": 2.45264946702, "featur": 1.52712581762, "them": 3.29628347982, "true": 2.55569864778, "notebook": 40.1924050633, "demand": 2.34159292035, "quick": 6.615, "effort": 1.89247824532, "common": 1.4025974025999999, "now": 2.321561746, "maintain": 1.77306231852, "april": 1.45039283757, "generat": 4.10550814584, "everyon": 6.3964544722, "need": 1.4372623574099999, "sequenc": 6.07112810707, "playground": 34.2894168467, "biocatalyt": 1134.0, "belief": 3.9463087248300006, "which": 5.025959224999999, "num": 5.0015752000500004, "financ": 4.255159474669999, "overcom": 8.38668779715, "surround": 2.49858356941, "talent": 6.320063694269999, "full": 3.33459357278, "default": 21.1398135819, "colab": 1134.0, "finit": 28.1989342806, "oil": 4.810909090909999, "everi": 2.95835274388, "search": 6.507890961259999, "februari": 1.5621371642200002, "garbag": 43.9778393352, "help": 6.99814863795, "vast": 12.16862544711, "good": 1.51981619759, "normal": 2.61075481006, "prove": 2.45720476706, "amount": 4.54054054054, "here": 7.26923076924, "been": 1.0239277652399998, "billion": 4.8669527897, "such": 4.24605509496, "these": 3.22246278756, "dsqasync": 1134.0, "instruct": 4.169117647059999, "someth": 6.56304257958, "hype": 80.5888324873, "fool": 19.4797546012, "therapi": 16.6764705882, "previous": 1.42846859816, "coopt": 1134.0, "play": 1.46390041494, "never": 1.55769230769, "veri": 2.51760228354, "read": 4.629921259840001}, "idf": {"question": 2.20408163265, "sourc": 1.69760479042, "occur": 1.7453825857499998, "but": 1.01632417899, "speed": 3.8703071672400005, "further": 1.3618116315, "fatal": 7.48867924528, "evid": 2.24872521246, "then": 1.08657860516, "from": 1.00056721497, "abl": 1.8208510150200001, "will": 1.22481098596, "contempl": 18.3114186851, "suggest": 1.7571665744299998, "phase": 4.3012733676499995, "pattern": 3.79173632673, "all": 1.01146788991, "into": 1.01502461479, "other": 1.00992366412, "higher": 2.1218925421, "arm": 2.34159292035, "front": 2.32820061593, "this": 1.00379362671, "and": 1.00006299213, "dsq": 1134.0, "mani": 1.04426757877, "merg": 5.28319467554, "longer": 2.02319357716, "life": 1.37051104972, "hurdl": 84.8983957219, "calcul": 6.12972972973, "capabl": 3.6580645161300005, "prime": 3.49845747025, "cours": 2.15092805853, "larg": 1.18574949585, "compet": 3.26465144972, "artifici": 8.31639601886, "key": 2.28005170185, "around": 1.21394708671, "news": 2.08182533438, "equip": 3.24529844644, "ask": 2.1744966443, "learn": 2.32275054865, "import": 1.3401992233700002, "comment": 3.05954904606, "first": 1.00761614623, "natur": 1.5392670157100001, "produc": 1.36932896326, "art": 1.9994962216599999, "distil": 43.9778393352, "numer": 1.83325635104, "evalu": 6.9509632224199995, "our": 2.35758835759, "wage": 8.15827338129, "want": 1.99698113208, "general": 1.1218202374200001, "goal": 3.28152128979, "embrac": 8.45817794353, "gas": 5.7730909090900004, "actor": 4.24265098878, "oversea": 7.62902450745, "disprov": 62.750988142299995, "heard": 4.45204711161, "frontier": 8.7422907489, "thought": 1.9854927463699998, "predict": 5.18484650555, "divers": 3.97197898424, "resourc": 2.9487369985100003, "short": 1.41295834817, "mistak": 8.71350164654, "have": 1.0148948411399998, "realiti": 4.563380281690001, "knowledg": 3.3981164383599998, "made": 1.07038834951, "global": 3.30612244898, "make": 1.0762660158600001, "array": 10.1444089457, "who": 1.06279287723, "impend": 26.241322314, "against": 1.2902072328299998, "move": 1.29125660838, "human": 1.8965476048299998, "peopl": 1.21320495186, "data": 3.37643555934, "discuss": 2.19676214197, "structur": 2.0580762250499998, "hundr": 2.4698195395099996, "subject": 1.8715077213299998, "cursori": 288.654545455, "process": 1.69524826482, "entrepreneur": 16.4008264463, "limit": 1.5186531471200002, "energi": 3.66566612792, "dsqsrc": 1134.0, "matthew": 6.908616187989999, "helixlik": 1134.0, "civil": 2.10306000795, "museum": 2.99490662139, "genuin": 11.4793926247, "copious": 95.06586826350001, "anticip": 7.793814432989999, "socal": 1134.0, "time": 1.01127460348, "openai": 1134.0, "overlook": 11.4051724138, "bit": 8.33385826772, "within": 1.2369302688, "select": 2.02345144022, "one": 1.00627495722, "were": 1.02458857696, "repeat": 2.8771293947099994, "machin": 4.02433460076, "interbert": 1134.0, "case": 1.48498737256, "that": 1.00398406375, "pretti": 15.75, "place": 1.1004366812200002, "advanc": 1.9997480791, "safe": 5.02723242559, "fed": 12.5402843602, "core": 4.623179965059999, "determin": 2.1658935879900003, "prompt": 4.44456886898, "logic": 8.929133858270001, "dataset": 193.609756098, "can": 1.17626139142, "million": 1.7279059643, "way": 1.2190739461, "while": 1.0441988950299999, "hand": 1.6152202665600002, "fulli": 2.79015817223, "same": 1.11857958148, "dsqtype": 1134.0, "with": 1.0011982089899998, "envisag": 37.0934579439, "spend": 4.15928739848, "major": 1.14852058164, "mental": 6.51456709069, "result": 1.14611608432, "textjavascript": 1134.0, "has": 1.0436497502, "tri": 1.8544562551099997, "becaus": 1.1495184997499999, "than": 1.03278688525, "shkreli": 1134.0, "readi": 5.15789473684, "longterm": 512.129032258, "train": 1.9365698950999999, "written": 1.9573418813999999, "number": 1.10142916609, "pursu": 4.15384615385, "disqusshortnam": 1134.0, "forc": 1.32399299475, "abil": 2.70875277256, "utilis": 29.345656192199996, "mere": 4.20222339862, "doe": 1.70581282905, "delay": 4.23247134098, "decid": 1.9257641921400002, "line": 1.4182597820299998, "take": 1.13961668222, "think": 2.90715986083, "final": 1.34008609775, "like": 1.14918566775, "theori": 3.02745995423, "robert": 1.8618505922400002, "hope": 2.50884955752, "controversi": 2.62543409955, "best": 1.5828514456600002, "ultim": 2.58524670249, "releas": 1.8377126982299998, "overtak": 50.2405063291, "obvious": 6.44841592201, "neural": 59.4606741573, "better": 2.0065722952500002, "idea": 2.0930784443, "sure": 7.453521126760001, "what": 1.25343439128, "exercis": 4.73627684964, "deal": 2.18346857379, "task": 3.88641370869, "perform": 1.5313977042500002, "world": 1.11340206186, "servic": 1.51300867245, "certain": 1.8077886586200003, "without": 1.29547123623, "martin": 3.07793718496, "how": 1.60250328051, "object": 2.3488681757700003, "would": 1.0828729281799998, "highlight": 5.83033419023, "there": 1.04091266719, "altern": 2.1390460792200003, "success": 1.32002993265, "out": 1.06016694491, "set": 1.18707940781, "most": 1.02096463023, "their": 1.01547908405, "overview": 12.6805111821, "stop": 2.1783754116400003, "overal": 3.0442953020099996, "the": 1.0, "littleunderstood": 1134.0, "demonstr": 2.64997496244, "part": 1.04330682789, "criticis": 9.879278158060002, "recent": 1.54405757635, "whether": 2.20683903253, "wrote": 2.10473286491, "happen": 2.96359902931, "refer": 1.30024570025, "given": 1.35426085473, "nod": 61.0615384615, "topic": 5.457545548300001, "ammount": 1134.0, "bring": 2.03616775683, "studi": 1.53184098804, "also": 1.01476510067, "thus": 1.6463756092500001, "far": 1.71022298826, "paramet": 17.256521739100002, "live": 1.30591428806, "system": 1.38739840951, "provid": 1.21552714187, "yourself": 26.592964824099997, "turn": 1.3838912133899999, "web": 5.17133550489, "race": 2.93023255814, "more": 1.0171706817, "long": 1.2657259028899999, "outlin": 6.38102893891, "techniqu": 3.7293868921800004, "gain": 1.84819557625, "retriev": 2.16826003824, "carbon": 12.1469013007, "next": 1.4950560316400001, "work": 1.11520089913, "influenc": 1.77246846042, "promis": 3.5030891438699996, "illfat": 1134.0, "cherrypick": 1134.0, "domain": 9.39408284024, "perhap": 3.14812611541, "miss": 3.53664513255, "though": 1.36076112111, "not": 1.01567398119, "might": 2.1561863370900003, "are": 1.02990593578, "some": 1.04036697248, "omin": 117.6, "alreadi": 1.9551724137900002, "imagin": 6.598503740650001, "lead": 1.2664326739, "mayo": 49.7680250784, "scaleup": 1134.0, "kdnugget": 1134.0, "term": 1.39520168732, "power": 1.3396337861799998, "usher": 23.2105263158, "for": 1.00031504001, "model": 2.0905978404, "understand": 2.96858638743, "crucial": 7.7443902439, "direct": 1.22226499346, "stage": 2.0831911822599998, "deconstruct": 78.5940594059, "dispar": 22.583214793699998, "record": 1.42334588488, "develop": 1.1955719557200002, "seem": 2.29123971713, "just": 1.33580143037, "network": 2.59369384088, "transformerbas": 1134.0, "succeed": 3.3550295857999997, "accept": 1.7377408056, "attrit": 53.4545454545, "relat": 1.23750876919, "day": 1.18371607516, "cuttingedg": 1134.0, "approach": 2.07556543339, "takeaway": 529.2, "schedul": 3.6648199445999996, "deep": 3.6279707495399998, "reason": 1.72340425532, "requir": 1.52844902282, "grave": 6.75574468085, "fun": 12.8863636364, "use": 1.0296387573799999, "var": 118.47761194, "advent": 11.918918918900001, "distinct": 2.2836593786000003, "failur": 3.28559602649, "nonstat": 1134.0, "drop": 2.4594887684, "follow": 1.04640126549, "late": 1.31740104556, "languag": 2.29488291414, "onli": 1.0256476516600002, "unlik": 2.42529789184, "spell": 6.75, "small": 1.3594793629, "tie": 3.31786833856, "text": 3.12827586207, "mind": 3.5918552036199998, "new": 1.0178880554, "simpl": 3.3981164383599998, "futur": 1.8577112099200002, "come": 1.32831325301, "may": 1.05201775893, "those": 1.19548192771, "bayesian": 178.38202247200002, "state": 1.0477133240899998, "least": 1.6165359943000002, "run": 1.55692850838, "word": 1.7965372864099998, "caus": 1.38521943984, "year": 1.0485436893200002, "implement": 3.57648118946, "contain": 1.59814777532, "complet": 1.24021560816, "ourselv": 27.184931506799998, "sens": 2.8365195640499996, "seri": 1.46511627907, "parri": 55.7052631579, "find": 1.7294117647099998, "point": 1.25990000794, "few": 1.31729173581, "intellig": 4.19334389857, "modern": 1.5319888063299998, "war": 1.41320989852, "endeavour": 33.2830188679, "code": 3.8807137619199996, "premis": 13.4428450466, "grammar": 14.7, "sampl": 7.23280182232, "prevent": 2.16117615029, "tech": 19.1739130435, "underpin": 44.3463687151, "page": 2.03669018602, "across": 1.7318642958400001, "ton": 10.5278514589, "nation": 1.22632473351, "featur": 1.52712581762, "them": 1.09876115994, "true": 2.55569864778, "notebook": 40.1924050633, "demand": 2.34159292035, "quick": 2.205, "effort": 1.89247824532, "common": 1.4025974025999999, "now": 1.160780873, "maintain": 1.77306231852, "april": 1.45039283757, "generat": 2.05275407292, "everyon": 6.3964544722, "need": 1.4372623574099999, "sequenc": 6.07112810707, "playground": 34.2894168467, "biocatalyt": 1134.0, "belief": 3.9463087248300006, "which": 1.005191845, "num": 1.00031504001, "financ": 4.255159474669999, "overcom": 8.38668779715, "surround": 2.49858356941, "talent": 6.320063694269999, "full": 1.66729678639, "default": 21.1398135819, "colab": 1134.0, "finit": 28.1989342806, "oil": 4.810909090909999, "everi": 1.47917637194, "search": 3.2539454806299997, "februari": 1.5621371642200002, "garbag": 43.9778393352, "help": 1.39962972759, "vast": 4.05620848237, "good": 1.51981619759, "normal": 2.61075481006, "prove": 2.45720476706, "amount": 2.27027027027, "here": 2.42307692308, "been": 1.0239277652399998, "billion": 4.8669527897, "such": 1.06151377374, "these": 1.07415426252, "dsqasync": 1134.0, "instruct": 4.169117647059999, "someth": 3.28152128979, "hype": 80.5888324873, "fool": 19.4797546012, "therapi": 16.6764705882, "previous": 1.42846859816, "coopt": 1134.0, "play": 1.46390041494, "never": 1.55769230769, "veri": 1.25880114177, "read": 2.3149606299200003}, "logidf": {"question": 0.790310929014, "sourc": 0.529218310751, "occur": 0.556973778473, "but": 0.0161923720719, "speed": 1.3533338752700002, "further": 0.308815895297, "fatal": 2.01339244624, "evid": 0.8103634834160001, "then": 0.08303386523089999, "from": 0.000567054168866, "abl": 0.599303982475, "will": 0.202786534915, "contempl": 2.90752483712, "suggest": 0.563702610877, "phase": 1.4589111108700001, "pattern": 1.33282404788, "all": 0.011402632097799998, "into": 0.0149128632287, "other": 0.00987474791976, "higher": 0.752308398995, "arm": 0.850831432969, "front": 0.845095701382, "this": 0.0037864490525, "and": 6.29901420636e-05, "dsq": 7.033506484289999, "mani": 0.0433157581221, "merg": 1.66453096693, "longer": 0.7046772417749999, "life": 0.315183699277, "hurdl": 4.44145519705, "calcul": 1.8131506592099997, "capabl": 1.2969341868100002, "prime": 1.25232214856, "cours": 0.765899404133, "larg": 0.17037506060600002, "compet": 1.1831530035, "artifici": 2.11822899018, "key": 0.82419811896, "around": 0.19387710578200001, "news": 0.733245073485, "equip": 1.1772073171, "ask": 0.776797209847, "learn": 0.842752064745, "import": 0.292818277066, "comment": 1.11826753454, "first": 0.0075872898121599995, "natur": 0.431306339292, "produc": 0.314320812003, "art": 0.6928952596619999, "distil": 3.78368585557, "numer": 0.606093812346, "evalu": 1.9388802431299998, "our": 0.8576392141820001, "wage": 2.09903255116, "want": 0.6916366062549999, "general": 0.114952578063, "goal": 1.18830712273, "embrac": 2.13513377732, "gas": 1.75320762324, "actor": 1.4451883070700002, "oversea": 2.03195998751, "disprov": 4.13917432518, "heard": 1.4933640154799999, "frontier": 2.16817225474, "thought": 0.685867118283, "predict": 1.6457402376899999, "divers": 1.37926445519, "resourc": 1.08137694258, "short": 0.345685625679, "mistak": 2.1648737360799997, "have": 0.0147850023412, "realiti": 1.51806363875, "knowledg": 1.2232212893899999, "made": 0.0680215260973, "global": 1.1957760371200001, "make": 0.07349765782289999, "array": 2.31692271093, "who": 0.0609002329859, "impend": 3.2673353558700002, "against": 0.254802851078, "move": 0.255615859253, "human": 0.640035183243, "peopl": 0.193265578473, "data": 1.2168205848, "discuss": 0.78698452262, "structur": 0.7217716751350001, "hundr": 0.904145087046, "subject": 0.6267443740950001, "cursori": 5.66523062867, "process": 0.527829199025, "entrepreneur": 2.79733172663, "limit": 0.41782385463, "energi": 1.29901007269, "dsqsrc": 7.033506484289999, "matthew": 1.9327693554900003, "helixlik": 7.033506484289999, "civil": 0.7433934307629999, "museum": 1.0969130529200002, "genuin": 2.44055348224, "copious": 4.55457000149, "anticip": 2.05333039768, "socal": 7.033506484289999, "time": 0.0112115188626, "openai": 7.033506484289999, "overlook": 2.43406697301, "bit": 2.12032652634, "within": 0.21263272059799998, "select": 0.704804687133, "one": 0.0062553516455, "were": 0.024291143681099997, "repeat": 1.0567930591299999, "machin": 1.39235958062, "interbert": 7.033506484289999, "case": 0.395406268889, "that": 0.00397614837964, "pretti": 2.75684036527, "place": 0.0957070839572, "advanc": 0.6930212121780001, "safe": 1.61486961909, "fed": 2.5289462112, "core": 1.53108277245, "determin": 0.772833019022, "prompt": 1.4916828719100002, "logic": 2.18931939783, "dataset": 5.26584456664, "can": 0.162341096394, "million": 0.5469102500940001, "way": 0.19809150993500002, "while": 0.04324998379380001, "hand": 0.479471335336, "fulli": 1.02609828678, "same": 0.112059649604, "dsqtype": 7.033506484289999, "with": 0.00119749171339, "envisag": 3.6134406183199994, "spend": 1.42534376116, "major": 0.138474663439, "mental": 1.87404076028, "result": 0.136378908381, "textjavascript": 7.033506484289999, "has": 0.0427239448548, "tri": 0.61759152916, "becaus": 0.139343158825, "than": 0.0322608622182, "shkreli": 7.033506484289999, "readi": 1.6405284994999998, "longterm": 6.238576609419999, "train": 0.660918312839, "written": 0.671587369833, "number": 0.0966085784186, "pursu": 1.4240346891, "disqusshortnam": 7.033506484289999, "forc": 0.280652166524, "abil": 0.996488297427, "utilis": 3.37914453506, "mere": 1.43561376584, "doe": 0.5340417297169999, "delay": 1.44278606382, "decid": 0.655322871893, "line": 0.349430614452, "take": 0.130691962197, "think": 1.06717661175, "final": 0.292733863948, "like": 0.139053576545, "theori": 1.10772396902, "robert": 0.6215709351609999, "hope": 0.919824304455, "controversi": 0.9652462536299999, "best": 0.459227932947, "ultim": 0.9498209395739999, "releas": 0.608521699544, "overtak": 3.9168216003199996, "obvious": 1.86383450716, "neural": 4.0853151555, "better": 0.6964279406, "idea": 0.73863592212, "sure": 2.0086865552, "what": 0.225887296827, "exercis": 1.5552513523, "deal": 0.780914701253, "task": 1.35748680661, "perform": 0.42618085058, "world": 0.107420248621, "servic": 0.41410016674500005, "certain": 0.592104362781, "without": 0.258874517941, "martin": 1.12425962746, "how": 0.47156695693000006, "object": 0.853933584803, "would": 0.0796176279647, "highlight": 1.76307432123, "there": 0.0400978929255, "altern": 0.760359972282, "success": 0.27765441259199997, "out": 0.0584263909193, "set": 0.171496011289, "most": 0.020747896295599998, "their": 0.015360505122700001, "overview": 2.54006626224, "stop": 0.778579374963, "overal": 1.1132694464700001, "the": 0.0, "littleunderstood": 7.033506484289999, "demonstr": 0.9745501918189999, "part": 0.04239531098280001, "criticis": 2.29043944817, "recent": 0.434413741288, "whether": 0.791561189647, "wrote": 0.744188554049, "happen": 1.08640441802, "refer": 0.262553246798, "given": 0.303255810831, "nod": 4.1118821828900005, "topic": 1.6969991554100001, "ammount": 7.033506484289999, "bring": 0.7110694905930001, "studi": 0.426470272221, "also": 0.0146571578, "thus": 0.49857627139300004, "far": 0.536623764503, "paramet": 2.8481901438599997, "live": 0.266903399347, "system": 0.327430345585, "provid": 0.19517784432500002, "yourself": 3.28064670051, "turn": 0.324899251064, "web": 1.6431309733200001, "race": 1.07508179126, "more": 0.017024931599999998, "long": 0.235645793878, "outlin": 1.85332936004, "techniqu": 1.31624384807, "gain": 0.6142097989249999, "retriev": 0.773925020223, "carbon": 2.49707410028, "next": 0.402163685499, "work": 0.109034567273, "influenc": 0.572373185428, "promis": 1.25364519176, "illfat": 7.033506484289999, "cherrypick": 7.033506484289999, "domain": 2.24008000599, "perhap": 1.14680739183, "miss": 1.2631785751200002, "though": 0.308044191079, "not": 0.0155524130075, "might": 0.7683410765340001, "are": 0.0294674735827, "some": 0.0395735090645, "omin": 4.76728903546, "alreadi": 0.670478380747, "imagin": 1.88684291737, "lead": 0.23620402986699998, "mayo": 3.90737271112, "scaleup": 7.033506484289999, "kdnugget": 7.033506484289999, "term": 0.33303898354600003, "power": 0.292396282715, "usher": 3.1446058962800003, "for": 0.00031499039539700004, "model": 0.7374500731110001, "understand": 1.0880858756799998, "crucial": 2.04696874177, "direct": 0.200705689496, "stage": 0.733900940237, "deconstruct": 4.364296116499999, "dispar": 3.11720692209, "record": 0.353010356953, "develop": 0.178624694913, "seem": 0.829093032276, "just": 0.289531434109, "network": 0.9530830530519999, "transformerbas": 7.033506484289999, "succeed": 1.2104605888, "accept": 0.552585882007, "attrit": 3.9788316751, "relat": 0.21310030165399999, "day": 0.16865870631700003, "cuttingedg": 7.033506484289999, "approach": 0.7302336145810001, "takeaway": 6.27136643224, "schedul": 1.2987792057799998, "deep": 1.2886734698, "reason": 0.544301552962, "requir": 0.424253510675, "grave": 1.91039320676, "fun": 2.5561696698099996, "use": 0.0292080197316, "var": 4.77472401395, "advent": 2.4781269628, "distinct": 0.825779146958, "failur": 1.18954807429, "nonstat": 7.033506484289999, "drop": 0.8999535106219999, "follow": 0.045356911094199995, "late": 0.275660890876, "languag": 0.8306818244059999, "onli": 0.025324268329099998, "unlik": 0.885954358842, "spell": 1.90954250488, "small": 0.307101805059, "tie": 1.19932251002, "text": 1.14048200999, "mind": 1.2786688388299998, "new": 0.0177299468511, "simpl": 1.2232212893899999, "futur": 0.619345197699, "come": 0.28390990653000003, "may": 0.050709995284400004, "those": 0.17854939087299998, "bayesian": 5.18392744417, "state": 0.0466100027668, "least": 0.480285584745, "run": 0.442714975539, "word": 0.585861082385, "caus": 0.325858567406, "year": 0.047402238894600005, "implement": 1.27437940907, "contain": 0.468845318236, "complet": 0.215285242047, "ourselv": 3.30266283107, "sens": 1.04257779501, "seri": 0.38193461069799994, "parri": 4.02007463363, "find": 0.547781330288, "point": 0.23103235903299998, "few": 0.275577913653, "intellig": 1.43349848213, "modern": 0.426566764719, "war": 0.345863640811, "endeavour": 3.50504732301, "code": 1.35601909597, "premis": 2.59844699771, "grammar": 2.68784749378, "sampl": 1.9786264883900002, "prevent": 0.7706525875229999, "tech": 2.9535506595200003, "underpin": 3.7920308275, "page": 0.711326032411, "across": 0.549198455941, "ton": 2.35402426534, "nation": 0.204021674798, "featur": 0.423387418142, "them": 0.0941833269093, "true": 0.938325629634, "notebook": 3.693678049, "demand": 0.850831432969, "quick": 0.790727508899, "effort": 0.637887211057, "common": 0.338325805271, "now": 0.149092945021, "maintain": 0.572708175102, "april": 0.37183444219899997, "generat": 0.719182341736, "everyon": 1.8557438481400002, "need": 0.362740163442, "sequenc": 1.8035444374, "playground": 3.53483675982, "biocatalyt": 7.033506484289999, "belief": 1.37278064195, "which": 0.00517841384543, "num": 0.00031499039539700004, "financ": 1.44813224068, "overcom": 2.12664566269, "surround": 0.915723999073, "talent": 1.8437292863099999, "full": 0.511203624148, "default": 3.0511581621399997, "colab": 7.033506484289999, "finit": 3.33928418576, "oil": 1.5708860664500002, "everi": 0.391485427421, "search": 1.1798682540899998, "februari": 0.446054860765, "garbag": 3.78368585557, "help": 0.336207721344, "vast": 1.40024866595, "good": 0.418589404907, "normal": 0.959639378783, "prove": 0.899024430345, "amount": 0.819898886199, "here": 0.8850381883700001, "been": 0.023645982368400004, "billion": 1.5824680307199999, "such": 0.059695977806, "these": 0.0715336194008, "dsqasync": 7.033506484289999, "instruct": 1.42770441799, "someth": 1.18830712273, "hype": 4.38936008516, "fool": 2.9693757006599997, "therapi": 2.8139987791100003, "previous": 0.356602960063, "coopt": 7.033506484289999, "play": 0.38110439064199997, "never": 0.443205436091, "veri": 0.230159793238, "read": 0.83939268088}, "freq": {"question": 1, "sourc": 1, "occur": 1, "but": 9, "speed": 1, "further": 1, "fatal": 1, "evid": 2, "then": 2, "from": 4, "abl": 1, "will": 10, "contempl": 1, "suggest": 1, "phase": 1, "pattern": 1, "all": 4, "into": 1, "other": 2, "higher": 1, "arm": 4, "front": 1, "this": 8, "and": 15, "dsq": 1, "mani": 3, "merg": 1, "longer": 1, "life": 1, "hurdl": 1, "calcul": 1, "capabl": 1, "prime": 1, "cours": 1, "larg": 1, "compet": 1, "artifici": 5, "key": 1, "around": 1, "news": 1, "equip": 2, "ask": 1, "learn": 6, "import": 4, "comment": 1, "first": 2, "natur": 1, "produc": 1, "art": 2, "distil": 1, "numer": 2, "evalu": 1, "our": 4, "wage": 1, "want": 2, "general": 2, "goal": 2, "embrac": 1, "gas": 1, "actor": 2, "oversea": 1, "disprov": 1, "heard": 1, "frontier": 1, "thought": 1, "predict": 2, "divers": 2, "resourc": 1, "short": 1, "mistak": 1, "have": 4, "realiti": 1, "knowledg": 1, "made": 1, "global": 1, "make": 3, "array": 2, "who": 2, "impend": 1, "against": 1, "move": 1, "human": 8, "peopl": 1, "data": 7, "discuss": 2, "structur": 2, "hundr": 1, "subject": 1, "cursori": 1, "process": 4, "entrepreneur": 1, "limit": 1, "energi": 1, "dsqsrc": 1, "matthew": 1, "helixlik": 1, "civil": 1, "museum": 1, "genuin": 1, "copious": 2, "anticip": 1, "socal": 2, "time": 3, "openai": 2, "overlook": 2, "bit": 1, "within": 2, "select": 1, "one": 1, "were": 1, "repeat": 1, "machin": 2, "interbert": 1, "case": 3, "that": 11, "pretti": 1, "place": 3, "advanc": 2, "safe": 1, "fed": 2, "core": 1, "determin": 2, "prompt": 2, "logic": 1, "dataset": 2, "can": 4, "million": 2, "way": 4, "while": 2, "hand": 1, "fulli": 1, "same": 1, "dsqtype": 1, "with": 9, "envisag": 1, "spend": 1, "major": 1, "mental": 2, "result": 5, "textjavascript": 1, "has": 1, "tri": 3, "becaus": 1, "than": 4, "shkreli": 1, "readi": 1, "longterm": 1, "train": 3, "written": 2, "number": 2, "pursu": 1, "disqusshortnam": 1, "forc": 1, "abil": 1, "utilis": 1, "mere": 1, "doe": 1, "delay": 1, "decid": 2, "line": 2, "take": 2, "think": 1, "final": 1, "like": 2, "theori": 1, "robert": 1, "hope": 2, "controversi": 2, "best": 2, "ultim": 5, "releas": 3, "overtak": 1, "obvious": 1, "neural": 4, "better": 2, "idea": 1, "sure": 1, "what": 7, "exercis": 1, "deal": 2, "task": 2, "perform": 2, "world": 2, "servic": 1, "certain": 1, "without": 1, "martin": 1, "how": 2, "object": 1, "would": 1, "highlight": 1, "there": 3, "altern": 1, "success": 1, "out": 5, "set": 3, "most": 2, "their": 2, "overview": 1, "stop": 1, "overal": 1, "the": 59, "littleunderstood": 1, "demonstr": 1, "part": 1, "criticis": 1, "recent": 1, "whether": 2, "wrote": 1, "happen": 1, "refer": 2, "given": 1, "nod": 1, "topic": 1, "ammount": 1, "bring": 1, "studi": 1, "also": 1, "thus": 1, "far": 1, "paramet": 3, "live": 1, "system": 4, "provid": 2, "yourself": 1, "turn": 1, "web": 1, "race": 4, "more": 3, "long": 1, "outlin": 1, "techniqu": 2, "gain": 1, "retriev": 1, "carbon": 1, "next": 4, "work": 2, "influenc": 1, "promis": 2, "illfat": 1, "cherrypick": 1, "domain": 4, "perhap": 1, "miss": 1, "though": 2, "not": 6, "might": 2, "are": 6, "some": 1, "omin": 1, "alreadi": 1, "imagin": 1, "lead": 1, "mayo": 1, "scaleup": 1, "kdnugget": 2, "term": 1, "power": 1, "usher": 2, "for": 7, "model": 9, "understand": 1, "crucial": 1, "direct": 3, "stage": 1, "deconstruct": 1, "dispar": 1, "record": 1, "develop": 1, "seem": 5, "just": 2, "network": 4, "transformerbas": 1, "succeed": 1, "accept": 1, "attrit": 1, "relat": 1, "day": 1, "cuttingedg": 1, "approach": 2, "takeaway": 1, "schedul": 1, "deep": 6, "reason": 1, "requir": 1, "grave": 2, "fun": 1, "use": 1, "var": 2, "advent": 2, "distinct": 1, "failur": 1, "nonstat": 2, "drop": 1, "follow": 2, "late": 1, "languag": 4, "onli": 2, "unlik": 1, "spell": 1, "small": 1, "tie": 1, "text": 2, "mind": 2, "new": 2, "simpl": 2, "futur": 2, "come": 1, "may": 6, "those": 2, "bayesian": 1, "state": 2, "least": 1, "run": 3, "word": 4, "caus": 3, "year": 1, "implement": 1, "contain": 1, "complet": 1, "ourselv": 1, "sens": 2, "seri": 1, "parri": 1, "find": 2, "point": 1, "few": 1, "intellig": 6, "modern": 4, "war": 1, "endeavour": 1, "code": 1, "premis": 1, "grammar": 1, "sampl": 5, "prevent": 1, "tech": 1, "underpin": 1, "page": 1, "across": 1, "ton": 1, "nation": 2, "featur": 1, "them": 3, "true": 1, "notebook": 1, "demand": 1, "quick": 3, "effort": 1, "common": 1, "now": 2, "maintain": 1, "april": 1, "generat": 2, "everyon": 1, "need": 1, "sequenc": 1, "playground": 1, "biocatalyt": 1, "belief": 1, "which": 5, "num": 5, "financ": 1, "overcom": 1, "surround": 1, "talent": 1, "full": 2, "default": 1, "colab": 1, "finit": 1, "oil": 1, "everi": 2, "search": 2, "februari": 1, "garbag": 1, "help": 5, "vast": 3, "good": 1, "normal": 1, "prove": 1, "amount": 2, "here": 3, "been": 1, "billion": 1, "such": 4, "these": 3, "dsqasync": 1, "instruct": 1, "someth": 2, "hype": 1, "fool": 1, "therapi": 1, "previous": 1, "coopt": 1, "play": 1, "never": 1, "veri": 2, "read": 2}, "logtfidf": {"question": 0.790310929014, "sourc": 0.529218310751, "occur": 0.556973778473, "but": 0.1457313486471, "speed": 1.3533338752700002, "further": 0.308815895297, "fatal": 2.01339244624, "evid": 1.6207269668320001, "then": 0.16606773046179998, "from": 0.002268216675464, "abl": 0.599303982475, "will": 2.0278653491500003, "contempl": 2.90752483712, "suggest": 0.563702610877, "phase": 1.4589111108700001, "pattern": 1.33282404788, "all": 0.04561052839119999, "into": 0.0149128632287, "other": 0.01974949583952, "higher": 0.752308398995, "arm": 3.403325731876, "front": 0.845095701382, "this": 0.03029159242, "and": 0.000944852130954, "dsq": 7.033506484289999, "mani": 0.1299472743663, "merg": 1.66453096693, "longer": 0.7046772417749999, "life": 0.315183699277, "hurdl": 4.44145519705, "calcul": 1.8131506592099997, "capabl": 1.2969341868100002, "prime": 1.25232214856, "cours": 0.765899404133, "larg": 0.17037506060600002, "compet": 1.1831530035, "artifici": 10.5911449509, "key": 0.82419811896, "around": 0.19387710578200001, "news": 0.733245073485, "equip": 2.3544146342, "ask": 0.776797209847, "learn": 5.05651238847, "import": 1.171273108264, "comment": 1.11826753454, "first": 0.015174579624319999, "natur": 0.431306339292, "produc": 0.314320812003, "art": 1.3857905193239999, "distil": 3.78368585557, "numer": 1.212187624692, "evalu": 1.9388802431299998, "our": 3.4305568567280003, "wage": 2.09903255116, "want": 1.3832732125099998, "general": 0.229905156126, "goal": 2.37661424546, "embrac": 2.13513377732, "gas": 1.75320762324, "actor": 2.8903766141400005, "oversea": 2.03195998751, "disprov": 4.13917432518, "heard": 1.4933640154799999, "frontier": 2.16817225474, "thought": 0.685867118283, "predict": 3.2914804753799998, "divers": 2.75852891038, "resourc": 1.08137694258, "short": 0.345685625679, "mistak": 2.1648737360799997, "have": 0.0591400093648, "realiti": 1.51806363875, "knowledg": 1.2232212893899999, "made": 0.0680215260973, "global": 1.1957760371200001, "make": 0.22049297346869998, "array": 4.63384542186, "who": 0.1218004659718, "impend": 3.2673353558700002, "against": 0.254802851078, "move": 0.255615859253, "human": 5.120281465944, "peopl": 0.193265578473, "data": 8.5177440936, "discuss": 1.57396904524, "structur": 1.4435433502700001, "hundr": 0.904145087046, "subject": 0.6267443740950001, "cursori": 5.66523062867, "process": 2.1113167961, "entrepreneur": 2.79733172663, "limit": 0.41782385463, "energi": 1.29901007269, "dsqsrc": 7.033506484289999, "matthew": 1.9327693554900003, "helixlik": 7.033506484289999, "civil": 0.7433934307629999, "museum": 1.0969130529200002, "genuin": 2.44055348224, "copious": 9.10914000298, "anticip": 2.05333039768, "socal": 14.067012968579998, "time": 0.0336345565878, "openai": 14.067012968579998, "overlook": 4.86813394602, "bit": 2.12032652634, "within": 0.42526544119599996, "select": 0.704804687133, "one": 0.0062553516455, "were": 0.024291143681099997, "repeat": 1.0567930591299999, "machin": 2.78471916124, "interbert": 7.033506484289999, "case": 1.186218806667, "that": 0.043737632176039994, "pretti": 2.75684036527, "place": 0.2871212518716, "advanc": 1.3860424243560001, "safe": 1.61486961909, "fed": 5.0578924224, "core": 1.53108277245, "determin": 1.545666038044, "prompt": 2.9833657438200003, "logic": 2.18931939783, "dataset": 10.53168913328, "can": 0.649364385576, "million": 1.0938205001880001, "way": 0.7923660397400001, "while": 0.08649996758760002, "hand": 0.479471335336, "fulli": 1.02609828678, "same": 0.112059649604, "dsqtype": 7.033506484289999, "with": 0.01077742542051, "envisag": 3.6134406183199994, "spend": 1.42534376116, "major": 0.138474663439, "mental": 3.74808152056, "result": 0.681894541905, "textjavascript": 7.033506484289999, "has": 0.0427239448548, "tri": 1.8527745874800001, "becaus": 0.139343158825, "than": 0.1290434488728, "shkreli": 7.033506484289999, "readi": 1.6405284994999998, "longterm": 6.238576609419999, "train": 1.982754938517, "written": 1.343174739666, "number": 0.1932171568372, "pursu": 1.4240346891, "disqusshortnam": 7.033506484289999, "forc": 0.280652166524, "abil": 0.996488297427, "utilis": 3.37914453506, "mere": 1.43561376584, "doe": 0.5340417297169999, "delay": 1.44278606382, "decid": 1.310645743786, "line": 0.698861228904, "take": 0.261383924394, "think": 1.06717661175, "final": 0.292733863948, "like": 0.27810715309, "theori": 1.10772396902, "robert": 0.6215709351609999, "hope": 1.83964860891, "controversi": 1.9304925072599999, "best": 0.918455865894, "ultim": 4.74910469787, "releas": 1.825565098632, "overtak": 3.9168216003199996, "obvious": 1.86383450716, "neural": 16.341260622, "better": 1.3928558812, "idea": 0.73863592212, "sure": 2.0086865552, "what": 1.581211077789, "exercis": 1.5552513523, "deal": 1.561829402506, "task": 2.71497361322, "perform": 0.85236170116, "world": 0.214840497242, "servic": 0.41410016674500005, "certain": 0.592104362781, "without": 0.258874517941, "martin": 1.12425962746, "how": 0.9431339138600001, "object": 0.853933584803, "would": 0.0796176279647, "highlight": 1.76307432123, "there": 0.12029367877649999, "altern": 0.760359972282, "success": 0.27765441259199997, "out": 0.2921319545965, "set": 0.5144880338669999, "most": 0.041495792591199995, "their": 0.030721010245400002, "overview": 2.54006626224, "stop": 0.778579374963, "overal": 1.1132694464700001, "the": 0.0, "littleunderstood": 7.033506484289999, "demonstr": 0.9745501918189999, "part": 0.04239531098280001, "criticis": 2.29043944817, "recent": 0.434413741288, "whether": 1.583122379294, "wrote": 0.744188554049, "happen": 1.08640441802, "refer": 0.525106493596, "given": 0.303255810831, "nod": 4.1118821828900005, "topic": 1.6969991554100001, "ammount": 7.033506484289999, "bring": 0.7110694905930001, "studi": 0.426470272221, "also": 0.0146571578, "thus": 0.49857627139300004, "far": 0.536623764503, "paramet": 8.544570431579999, "live": 0.266903399347, "system": 1.30972138234, "provid": 0.39035568865000003, "yourself": 3.28064670051, "turn": 0.324899251064, "web": 1.6431309733200001, "race": 4.30032716504, "more": 0.05107479479999999, "long": 0.235645793878, "outlin": 1.85332936004, "techniqu": 2.63248769614, "gain": 0.6142097989249999, "retriev": 0.773925020223, "carbon": 2.49707410028, "next": 1.608654741996, "work": 0.218069134546, "influenc": 0.572373185428, "promis": 2.50729038352, "illfat": 7.033506484289999, "cherrypick": 7.033506484289999, "domain": 8.96032002396, "perhap": 1.14680739183, "miss": 1.2631785751200002, "though": 0.616088382158, "not": 0.093314478045, "might": 1.5366821530680002, "are": 0.17680484149620002, "some": 0.0395735090645, "omin": 4.76728903546, "alreadi": 0.670478380747, "imagin": 1.88684291737, "lead": 0.23620402986699998, "mayo": 3.90737271112, "scaleup": 7.033506484289999, "kdnugget": 14.067012968579998, "term": 0.33303898354600003, "power": 0.292396282715, "usher": 6.289211792560001, "for": 0.0022049327677790003, "model": 6.6370506579990005, "understand": 1.0880858756799998, "crucial": 2.04696874177, "direct": 0.6021170684880001, "stage": 0.733900940237, "deconstruct": 4.364296116499999, "dispar": 3.11720692209, "record": 0.353010356953, "develop": 0.178624694913, "seem": 4.14546516138, "just": 0.579062868218, "network": 3.8123322122079997, "transformerbas": 7.033506484289999, "succeed": 1.2104605888, "accept": 0.552585882007, "attrit": 3.9788316751, "relat": 0.21310030165399999, "day": 0.16865870631700003, "cuttingedg": 7.033506484289999, "approach": 1.4604672291620002, "takeaway": 6.27136643224, "schedul": 1.2987792057799998, "deep": 7.7320408188, "reason": 0.544301552962, "requir": 0.424253510675, "grave": 3.82078641352, "fun": 2.5561696698099996, "use": 0.0292080197316, "var": 9.5494480279, "advent": 4.9562539256, "distinct": 0.825779146958, "failur": 1.18954807429, "nonstat": 14.067012968579998, "drop": 0.8999535106219999, "follow": 0.09071382218839999, "late": 0.275660890876, "languag": 3.3227272976239997, "onli": 0.050648536658199995, "unlik": 0.885954358842, "spell": 1.90954250488, "small": 0.307101805059, "tie": 1.19932251002, "text": 2.28096401998, "mind": 2.5573376776599996, "new": 0.0354598937022, "simpl": 2.4464425787799997, "futur": 1.238690395398, "come": 0.28390990653000003, "may": 0.3042599717064, "those": 0.35709878174599996, "bayesian": 5.18392744417, "state": 0.0932200055336, "least": 0.480285584745, "run": 1.328144926617, "word": 2.34344432954, "caus": 0.977575702218, "year": 0.047402238894600005, "implement": 1.27437940907, "contain": 0.468845318236, "complet": 0.215285242047, "ourselv": 3.30266283107, "sens": 2.08515559002, "seri": 0.38193461069799994, "parri": 4.02007463363, "find": 1.095562660576, "point": 0.23103235903299998, "few": 0.275577913653, "intellig": 8.60099089278, "modern": 1.706267058876, "war": 0.345863640811, "endeavour": 3.50504732301, "code": 1.35601909597, "premis": 2.59844699771, "grammar": 2.68784749378, "sampl": 9.893132441950002, "prevent": 0.7706525875229999, "tech": 2.9535506595200003, "underpin": 3.7920308275, "page": 0.711326032411, "across": 0.549198455941, "ton": 2.35402426534, "nation": 0.408043349596, "featur": 0.423387418142, "them": 0.2825499807279, "true": 0.938325629634, "notebook": 3.693678049, "demand": 0.850831432969, "quick": 2.3721825266970002, "effort": 0.637887211057, "common": 0.338325805271, "now": 0.298185890042, "maintain": 0.572708175102, "april": 0.37183444219899997, "generat": 1.438364683472, "everyon": 1.8557438481400002, "need": 0.362740163442, "sequenc": 1.8035444374, "playground": 3.53483675982, "biocatalyt": 7.033506484289999, "belief": 1.37278064195, "which": 0.02589206922715, "num": 0.0015749519769850003, "financ": 1.44813224068, "overcom": 2.12664566269, "surround": 0.915723999073, "talent": 1.8437292863099999, "full": 1.022407248296, "default": 3.0511581621399997, "colab": 7.033506484289999, "finit": 3.33928418576, "oil": 1.5708860664500002, "everi": 0.782970854842, "search": 2.3597365081799997, "februari": 0.446054860765, "garbag": 3.78368585557, "help": 1.68103860672, "vast": 4.2007459978499995, "good": 0.418589404907, "normal": 0.959639378783, "prove": 0.899024430345, "amount": 1.639797772398, "here": 2.6551145651100003, "been": 0.023645982368400004, "billion": 1.5824680307199999, "such": 0.238783911224, "these": 0.2146008582024, "dsqasync": 7.033506484289999, "instruct": 1.42770441799, "someth": 2.37661424546, "hype": 4.38936008516, "fool": 2.9693757006599997, "therapi": 2.8139987791100003, "previous": 0.356602960063, "coopt": 7.033506484289999, "play": 0.38110439064199997, "never": 0.443205436091, "veri": 0.460319586476, "read": 1.67878536176}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  What Does GPT-2 Think About the AI Arms Race?</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb What Does GPT-2 Think About the AI Arms Race? Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html\" rel=\"prev\" title=\"Openclassrooms: Data Freelance Online course creator [Telecommute/Paris, France]\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=92450\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-92450 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 1-Apr, 2019  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/index.html\">Apr</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/opinions.html\">Opinions</a> \u00bb What Does GPT-2 Think About the AI Arms Race? (\u00a0<a href=\"/2019/n13.html\">19:n13</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">What Does GPT-2 Think About the AI Arms Race?</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n</div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/ai\" rel=\"tag\">AI</a>, <a href=\"https://www.kdnuggets.com/tag/natural-language-generation\" rel=\"tag\">Natural Language Generation</a>, <a href=\"https://www.kdnuggets.com/tag/nlp\" rel=\"tag\">NLP</a></div>\n<br/>\n<p class=\"excerpt\">\n     It may be April first, but that doesn't mean you will necessarily be fooled by GPT-2's views on the AI arms race. Why not have a read for fun and to see what the language generation model is capable of.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/matt-mayo\" rel=\"author\" title=\"Posts by Matthew Mayo\">Matthew Mayo</a>, KDnuggets.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><img alt=\"Header image\" class=\"aligncenter\" src=\"https://i.ibb.co/sRSd3q7/infinite-monkeys.jpg\" width=\"99%\"/></p>\n<p>In an obvious nod to April Fool's Day, we decided to ask the <a href=\"https://openai.com/blog/better-language-models/\" rel=\"noopener\" target=\"_blank\">GPT-2 language model</a>, which made news in February, what it thought of the impending <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race\" rel=\"noopener\" target=\"_blank\">artificial intelligence arms race</a>.</p>\n<p>In case you have not heard of GPT-2 or missed the <a href=\"/2019/03/openai-gpt-2-model-hype-controversy.html\" rel=\"noopener\" target=\"_blank\">controversy</a> surrounding its recent release, here's a quick overview:</p>\n<blockquote><p>\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n</p></blockquote>\n<p>A sample of text, or prompt, is fed to the model, which then predicts the next word (and so on, from this point). To prime the model, and set the stage as to the topic at hand, I fed the following prompt, written for no reason other than trying to highlight a few important terms for the model:</p>\n<p><b>The advent of modern neural networks and deep learning first resulted in the promise of state of the art results in a vast array of domains, but quickly helped usher in what is now referred to as the artificial intelligence arms race. While there is no evidence that modern approaches to neural networks will ultimately provide a direct line to so-called artificial general intelligence, nations and non-state actors, equipped with copious amounts of data, hope that these numerous techniques will help them advance their causes. This may or may not be the case, but it seems as though we are all overlooking something of grave importance in this discussion.</b></p>\n<p>Using defaults on the small GPT-2 model, I selected what I determined to be the \"best\" of 50 generated samples. What makes a sample the \"best?\" It's pretty subjective, but the one I read which seemed to, at a very cursory read, seem to make the most logical sense. Sure, I cherry-picked the results, but my goal isn't to prove or disprove that GPT-2 is a competent language generation model, I just wanted to have a bit of fun. A number of samples were garbage in every sense of the imagination, from spelling to grammar to word sequencing to sample structure.</p>\n<p>So, without further delay, here's what GPT-2 thinks of the AI arms race.</p>\n<blockquote><p>\nThe advent of modern neural networks and deep learning first resulted in the promise of state of the art results in a vast array of domains, but quickly helped usher in what is now referred to as the artificial intelligence arms race. While there is no evidence that modern approaches to neural networks will ultimately provide a direct line to so-called artificial general intelligence, nations and non-state actors, equipped with copious ammounts of data, hope that these numerous techniques will help them advance their causes. This may or may not be the case, but it seems as though we are all overlooking something of grave importance in this discussion.</p>\n<p>It may at the very least be a good idea for us to criticise the way in which humanity is waging a war of attrition against a number of distinct frontiers, trying to decide whether certain fronts are safe or not, deep learning may finally be able to merge disparate forces.</p>\n<p>Limiting the overall speed at which AI can deal with tasks, such as completely developing biocatalytic cutting-edge tech, can be fatal to civilization. A search for alternatives to oil and gas requires repeatedly searching for data, thus preventing perhaps the most powerful and successful AI from overtaking every other domain. But data on a short schedule can lead to higher readiness and anticipating an ill-fated future. Evaluating your ability to process long-term data demands if you can't find better ways of dealing with such questions than studying the hundreds of data sets tied to major carbon and energy sources in the world.</p>\n<p>Overcoming those hurdles will move us to the next phase of the intelligence process in the long run. The world is a vast place, full of resources that the normal human mind does not utilise to help determine how many places we want to live. But as the late Bayesian entrepreneur Robert Parry wrote, \"whether human therapies take place in museums or overseas, the human mind will ultimately follow the same process in influencing the mere retrieval of premises,\" and these processes will take many more years.</p>\n<p>Time is running out for human mental systems. The coming embrace of deep learning brings a series of important service releases that will be crucial to our understanding not only of the ways life machines perform but also the core theories underpinning our belief that machines are only capable of fully finite calculations.</p>\n<p>Martin Shkreli outlined the finance of deep learning: \"Everyone who gains might never stop spending time on it, of course. If we all genuinely pursue common endeavours to better our global futures, then it will no longer be an exercise to perform talents, maintain written records, contemplate how making parts of human needs will have produced such helix-like structures, envisaging new ways of instructing people out of our systems, and ultimately to set ourselves to work on them on a new reality.\"</p>\n<p>What happens next suggest that failure so far seems like such a mistake that those who might ultimately succeed in implementing deep learning within human systems are unlikely to accept it. This is because as it turns out, little-understood work has already been co-opted into the effort.\n</p></blockquote>\n<p>And there you have it. GPT-2 just dropped a ton of knowledge, with the key ominous takeaway being that \"<em>Time is running out for human mental systems.</em>\"</p>\n<p>If you would like to play around with the language model yourself, <a href=\"https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb\" rel=\"noopener\" target=\"_blank\">try out this GPT-2 playground Colab notebook</a>. You can find OpenAI's <a href=\"https://github.com/openai/gpt-2\" rel=\"noopener\" target=\"_blank\">full released code here</a>.</p>\n<p>\u00a0<br>\n<b>Related</b>:</br></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2019/03/openai-gpt-2-model-hype-controversy.html\">OpenAI\u2019s GPT-2: the model, the hype, and the controversy</a>\n<li><a href=\"/2019/02/bert-features-interbertible.html\">Are BERT Features InterBERTible?</a>\n<li><a href=\"/2019/02/deconstructing-bert-distilling-patterns-100-million-parameters.html\">Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n</div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-1-another-10');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-2-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/03/data-science-job-applications.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-3-tell-you');\"><b>What no one will tell you about data science job applications</b></a>\n<li> <a href=\"/2019/03/typical-data-scientist-2019.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-4-typical');\"><b>Who is a typical Data Scientist in 2019?</b></a>\n<li> <a href=\"/2019/02/asking-great-questions-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-5-great-questions');\"><b>Asking Great Questions as a Data Scientist</b></a>\n<li> <a href=\"/2019/03/pareto-principle-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-6-pareto');\"><b>The Pareto Principle for Data Scientists</b></a>\n<li> <a href=\"/2019/03/women-ai-big-data-science-machine-learning.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-7-19-inspiring-women');\"><b>19 Inspiring Women in AI, Big Data, Data Science, Machine Learning</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-1-ann-genetic');\"><b>Artificial Neural Networks Optimization using Genetic Algorithm with Python</b></a>\n<li> <a href=\"/2019/02/artificial-neural-network-implementation-using-numpy-and-image-classification.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-2-ann-numpy-images');\"><b>Artificial Neural Network Implementation using NumPy and Image Classification</b></a>\n<li> <a href=\"/2019/02/setup-python-environment-machine-learning.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-3-py-ml-setup');\"><b>How to Setup a Python Environment for Machine Learning</b></a>\n<li> <a href=\"/2019/03/typical-data-scientist-2019.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-4-typical');\"><b>Who is a typical Data Scientist in 2019?</b></a>\n<li> <a href=\"/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-5-azure-cert');\"><b>8 Reasons Why You Should Get a Microsoft Azure Certification</b></a>\n<li> <a href=\"/2019/03/pareto-principle-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-6-pareto');\"><b>The Pareto Principle for Data Scientists</b></a>\n<li> <a href=\"/2019/02/running-r-and-python-in-jupyter.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-7-r-python-jupyter');\"><b>Running R and Python in Jupyter</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html\">What Does GPT-2 Think About the AI Arms Race?</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-01-openclassrooms-data-freelance-online-course-creator-b.html\">Openclassrooms: Data Freelance Online course creator [T...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/gojek-calling-data-geeks.html\">Calling All Data Geeks to Come Home and Ignite Their Po...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/datathon-data-science-hackathon-april.html\">Datathon 2019: The International Data Science Hackathon...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/random-forest-python.html\">Explaining Random Forest (with Python Implementation)</a><li> <a href=\"https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html\">A Beginner\u2019s Guide to Linear Regression in Python wit...</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/index.html\">Apr</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/04/opinions.html\">Opinions</a> \u00bb What Does GPT-2 Think About the AI Arms Race? (\u00a0<a href=\"/2019/n13.html\">19:n13</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1554128635\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"bottom-left\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"bottom-left\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></body></html>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n\n\n<!-- Dynamic page generated in 0.604 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-01 10:23:55 -->\n<!-- Compression = gzip -->", "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/matt-mayo\" rel=\"author\" title=\"Posts by Matthew Mayo\">Matthew Mayo</a>, KDnuggets.</b></div>\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><img alt=\"Header image\" class=\"aligncenter\" src=\"https://i.ibb.co/sRSd3q7/infinite-monkeys.jpg\" width=\"99%\"/></p>\n<p>In an obvious nod to April Fool's Day, we decided to ask the <a href=\"https://openai.com/blog/better-language-models/\" rel=\"noopener\" target=\"_blank\">GPT-2 language model</a>, which made news in February, what it thought of the impending <a href=\"https://en.wikipedia.org/wiki/Artificial_intelligence_arms_race\" rel=\"noopener\" target=\"_blank\">artificial intelligence arms race</a>.</p>\n<p>In case you have not heard of GPT-2 or missed the <a href=\"/2019/03/openai-gpt-2-model-hype-controversy.html\" rel=\"noopener\" target=\"_blank\">controversy</a> surrounding its recent release, here's a quick overview:</p>\n<blockquote><p>\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n</p></blockquote>\n<p>A sample of text, or prompt, is fed to the model, which then predicts the next word (and so on, from this point). To prime the model, and set the stage as to the topic at hand, I fed the following prompt, written for no reason other than trying to highlight a few important terms for the model:</p>\n<p><b>The advent of modern neural networks and deep learning first resulted in the promise of state of the art results in a vast array of domains, but quickly helped usher in what is now referred to as the artificial intelligence arms race. While there is no evidence that modern approaches to neural networks will ultimately provide a direct line to so-called artificial general intelligence, nations and non-state actors, equipped with copious amounts of data, hope that these numerous techniques will help them advance their causes. This may or may not be the case, but it seems as though we are all overlooking something of grave importance in this discussion.</b></p>\n<p>Using defaults on the small GPT-2 model, I selected what I determined to be the \"best\" of 50 generated samples. What makes a sample the \"best?\" It's pretty subjective, but the one I read which seemed to, at a very cursory read, seem to make the most logical sense. Sure, I cherry-picked the results, but my goal isn't to prove or disprove that GPT-2 is a competent language generation model, I just wanted to have a bit of fun. A number of samples were garbage in every sense of the imagination, from spelling to grammar to word sequencing to sample structure.</p>\n<p>So, without further delay, here's what GPT-2 thinks of the AI arms race.</p>\n<blockquote><p>\nThe advent of modern neural networks and deep learning first resulted in the promise of state of the art results in a vast array of domains, but quickly helped usher in what is now referred to as the artificial intelligence arms race. While there is no evidence that modern approaches to neural networks will ultimately provide a direct line to so-called artificial general intelligence, nations and non-state actors, equipped with copious ammounts of data, hope that these numerous techniques will help them advance their causes. This may or may not be the case, but it seems as though we are all overlooking something of grave importance in this discussion.</p>\n<p>It may at the very least be a good idea for us to criticise the way in which humanity is waging a war of attrition against a number of distinct frontiers, trying to decide whether certain fronts are safe or not, deep learning may finally be able to merge disparate forces.</p>\n<p>Limiting the overall speed at which AI can deal with tasks, such as completely developing biocatalytic cutting-edge tech, can be fatal to civilization. A search for alternatives to oil and gas requires repeatedly searching for data, thus preventing perhaps the most powerful and successful AI from overtaking every other domain. But data on a short schedule can lead to higher readiness and anticipating an ill-fated future. Evaluating your ability to process long-term data demands if you can't find better ways of dealing with such questions than studying the hundreds of data sets tied to major carbon and energy sources in the world.</p>\n<p>Overcoming those hurdles will move us to the next phase of the intelligence process in the long run. The world is a vast place, full of resources that the normal human mind does not utilise to help determine how many places we want to live. But as the late Bayesian entrepreneur Robert Parry wrote, \"whether human therapies take place in museums or overseas, the human mind will ultimately follow the same process in influencing the mere retrieval of premises,\" and these processes will take many more years.</p>\n<p>Time is running out for human mental systems. The coming embrace of deep learning brings a series of important service releases that will be crucial to our understanding not only of the ways life machines perform but also the core theories underpinning our belief that machines are only capable of fully finite calculations.</p>\n<p>Martin Shkreli outlined the finance of deep learning: \"Everyone who gains might never stop spending time on it, of course. If we all genuinely pursue common endeavours to better our global futures, then it will no longer be an exercise to perform talents, maintain written records, contemplate how making parts of human needs will have produced such helix-like structures, envisaging new ways of instructing people out of our systems, and ultimately to set ourselves to work on them on a new reality.\"</p>\n<p>What happens next suggest that failure so far seems like such a mistake that those who might ultimately succeed in implementing deep learning within human systems are unlikely to accept it. This is because as it turns out, little-understood work has already been co-opted into the effort.\n</p></blockquote>\n<p>And there you have it. GPT-2 just dropped a ton of knowledge, with the key ominous takeaway being that \"<em>Time is running out for human mental systems.</em>\"</p>\n<p>If you would like to play around with the language model yourself, <a href=\"https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb\" rel=\"noopener\" target=\"_blank\">try out this GPT-2 playground Colab notebook</a>. You can find OpenAI's <a href=\"https://github.com/openai/gpt-2\" rel=\"noopener\" target=\"_blank\">full released code here</a>.</p>\n<p>\u00a0<br>\n<b>Related</b>:</br></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2019/03/openai-gpt-2-model-hype-controversy.html\">OpenAI\u2019s GPT-2: the model, the hype, and the controversy</a>\n<li><a href=\"/2019/02/bert-features-interbertible.html\">Are BERT Features InterBERTible?</a>\n<li><a href=\"/2019/02/deconstructing-bert-distilling-patterns-100-million-parameters.html\">Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>", "content": "By Matthew Mayo, KDnuggets. comments In an obvious nod to April Fool's Day, we decided to ask the GPT-2 language model, which made news in February, what it thought of the impending artificial intelligence arms race. In case you have not heard of GPT-2 or missed the controversy surrounding its recent release, here's a quick overview: GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data. A sample of text, or prompt, is fed to the model, which then predicts the next word (and so on, from this point). To prime the model, and set the stage as to the topic at hand, I fed the following prompt, written for no reason other than trying to highlight a few important terms for the model: The advent of modern neural networks and deep learning first resulted in the promise of state of the art results in a vast array of domains, but quickly helped usher in what is now referred to as the artificial intelligence arms race. While there is no evidence that modern approaches to neural networks will ultimately provide a direct line to so-called artificial general intelligence, nations and non-state actors, equipped with copious amounts of data, hope that these numerous techniques will help them advance their causes. This may or may not be the case, but it seems as though we are all overlooking something of grave importance in this discussion. Using defaults on the small GPT-2 model, I selected what I determined to be the \"best\" of 50 generated samples. What makes a sample the \"best?\" It's pretty subjective, but the one I read which seemed to, at a very cursory read, seem to make the most logical sense. Sure, I cherry-picked the results, but my goal isn't to prove or disprove that GPT-2 is a competent language generation model, I just wanted to have a bit of fun. A number of samples were garbage in every sense of the imagination, from spelling to grammar to word sequencing to sample structure. So, without further delay, here's what GPT-2 thinks of the AI arms race. The advent of modern neural networks and deep learning first resulted in the promise of state of the art results in a vast array of domains, but quickly helped usher in what is now referred to as the artificial intelligence arms race. While there is no evidence that modern approaches to neural networks will ultimately provide a direct line to so-called artificial general intelligence, nations and non-state actors, equipped with copious ammounts of data, hope that these numerous techniques will help them advance their causes. This may or may not be the case, but it seems as though we are all overlooking something of grave importance in this discussion. It may at the very least be a good idea for us to criticise the way in which humanity is waging a war of attrition against a number of distinct frontiers, trying to decide whether certain fronts are safe or not, deep learning may finally be able to merge disparate forces. Limiting the overall speed at which AI can deal with tasks, such as completely developing biocatalytic cutting-edge tech, can be fatal to civilization. A search for alternatives to oil and gas requires repeatedly searching for data, thus preventing perhaps the most powerful and successful AI from overtaking every other domain. But data on a short schedule can lead to higher readiness and anticipating an ill-fated future. Evaluating your ability to process long-term data demands if you can't find better ways of dealing with such questions than studying the hundreds of data sets tied to major carbon and energy sources in the world. Overcoming those hurdles will move us to the next phase of the intelligence process in the long run. The world is a vast place, full of resources that the normal human mind does not utilise to help determine how many places we want to live. But as the late Bayesian entrepreneur Robert Parry wrote, \"whether human therapies take place in museums or overseas, the human mind will ultimately follow the same process in influencing the mere retrieval of premises,\" and these processes will take many more years. Time is running out for human mental systems. The coming embrace of deep learning brings a series of important service releases that will be crucial to our understanding not only of the ways life machines perform but also the core theories underpinning our belief that machines are only capable of fully finite calculations. Martin Shkreli outlined the finance of deep learning: \"Everyone who gains might never stop spending time on it, of course. If we all genuinely pursue common endeavours to better our global futures, then it will no longer be an exercise to perform talents, maintain written records, contemplate how making parts of human needs will have produced such helix-like structures, envisaging new ways of instructing people out of our systems, and ultimately to set ourselves to work on them on a new reality.\" What happens next suggest that failure so far seems like such a mistake that those who might ultimately succeed in implementing deep learning within human systems are unlikely to accept it. This is because as it turns out, little-understood work has already been co-opted into the effort. And there you have it. GPT-2 just dropped a ton of knowledge, with the key ominous takeaway being that \"Time is running out for human mental systems.\" If you would like to play around with the language model yourself, try out this GPT-2 playground Colab notebook. You can find OpenAI's full released code here. \u00a0 Related: OpenAI\u2019s GPT-2: the model, the hype, and the controversy Are BERT Features InterBERTible? Deconstructing BERT: Distilling 6 Patterns from 100 Million Parameters var disqus_shortname = 'kdnuggets'; ( { var dsq = ; dsq.type = 'text/javascript'; dsq.async = true; dsq.src = '", "read_time": 314.4, "title_html": "<h1 id=\"title\">What Does GPT-2 Think About the AI Arms Race?</h1>", "url": "https://www.kdnuggets.com/2019/04/gpt-2-think-about-ai-arms-race.html"}