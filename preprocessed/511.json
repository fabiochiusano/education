{"content": "By Denny Britz , WildML. editor: devendra Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many\u00a0NLP tasks. But despite their recent popularity I\u2019ve only found a limited number of resources that thoroughly explain how RNNs work, and how to\u00a0implement them. That\u2019s what this tutorial is about. It\u2019s a multi-part series in which I\u2019m planning to cover the following: Introduction to RNNs (this post) Implementing a RNN using Python and Theano Understanding the Backpropagation Through Time (BPTT) algorithm and the vanishing gradient problem From RNNs to LSTM Networks As part of the tutorial we will implement a recurrent neural network based language model . The applications of\u00a0language models are two-fold: First, it allows us to score arbitrary sentences based on how likely they are to occur in the real world. This gives us a measure of\u00a0grammatical and semantic correctness. Such models are typically used as part of Machine Translation systems. Secondly, a language model allows us to generate new text (I think that\u2019s the much cooler application). Training a language model on Shakespeare allows us to generate\u00a0Shakespeare-like text.\u00a0 This fun post \u00a0by\u00a0Andrej Karpathy\u00a0demonstrates what character-level language models based\u00a0on RNNs are capable of. I\u2019m assuming that you are somewhat familiar with\u00a0basic Neural Networks. If you\u2019re not, you may want to head over to Implementing A Neural Network From Scratch , \u00a0which guides you through the ideas and implementation behind\u00a0non-recurrent networks. What are RNNs? The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other.\u00a0But for many tasks that\u2019s a very bad idea. If you want to predict the next word in\u00a0a sentence you better know which words came before it.\u00a0RNNs\u00a0are called recurrent \u00a0because they perform the same task for\u00a0every element of a\u00a0sequence, with the output being depended\u00a0on the previous computations. Another way to think about RNNs is that they have a \u201cmemory\u201d which captures information about what has been calculated so far. In theory RNNs can make use\u00a0of information in\u00a0arbitrarily\u00a0long sequences, but in\u00a0practice they are limited to looking back only a few steps (more on this later). Here\u00a0is what a typical RNN looks like: Fig. 1\u00a0 A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature The above diagram\u00a0shows a RNN being\u00a0 unrolled (or unfolded) into a full network. By unrolling we simply mean that we write out the network for the complete sequence.\u00a0For example, if the sequence we care about is a sentence of\u00a05 words, the network would be unrolled into a 5-layer neural network, one layer for each word.\u00a0The formulas that govern\u00a0the computation happening in a\u00a0RNN are\u00a0as follows: is the input at time step . For example, \u00a0could be a\u00a0one-hot vector corresponding to\u00a0the second word of a sentence. is the hidden state at time step . It\u2019s the \u201cmemory\u201d of the network. is calculated based on the previous hidden state and the input at the current step: . The function usually\u00a0is a nonlinearity such as tanh or ReLU . \u00a0 , which is required to calculate the first hidden state, is typically initialized to all zeroes. is the output at step . For example, if we wanted to predict the next word in a sentence it would\u00a0be a vector of probabilities across our vocabulary. . There are a few things to note here: You can think of the hidden state as the memory of the network. captures information about what happened in all the\u00a0previous time steps. The output at step is calculated solely based on the memory at time . As briefly mentioned above, it\u2019s a bit more complicated \u00a0in practice because typically\u00a0can\u2019t capture information from too many time steps ago. Unlike a traditional deep neural network, which uses\u00a0different parameters at each layer, a RNN shares the same parameters ( above) across\u00a0all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn. The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after\u00a0each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its\u00a0hidden state, which captures some information about a sequence. What can RNNs do? RNNs have shown great success in many NLP tasks. At this point I should mention that the most commonly used type of RNNs are LSTMs , which are much better at capturing long-term dependencies than vanilla RNNs are. But don\u2019t worry, LSTMs are essentially the same thing as the RNN we will develop in this tutorial, they\u00a0just have a different way of computing the hidden state. We\u2019ll cover LSTMs in more detail in a later post. Here are some example applications of RNNs in NLP (by non means an exhaustive list). Language Modeling\u00a0and Generating Text Given a sequence of words we want to predict the probability of each word given the previous words. Language Models allow us to measure how likely a sentence is, which is an important input for Machine Translation (since high-probability sentences are typically correct). A side-effect of being able to predict the next word is that we get a generative model, which allows us to generate new text by sampling from the output probabilities. And depending on what our training data is we can generate all kinds of stuff . In Language Modeling our input is typically a sequence of words (encoded as one-hot vectors for example), and our output is the sequence of predicted words. When training the network we set since we want the output at step to be the actual next word. Research papers about\u00a0Language Modeling and Generating Text: Recurrent neural network based language model Extensions of Recurrent neural network based language model Generating Text with Recurrent Neural Networks", "title_html": "<h1 id=\"title\">Recurrent Neural Networks Tutorial, Introduction</h1> ", "url": "https://www.kdnuggets.com/2015/10/recurrent-neural-networks-tutorial.html", "tfidf": {"tfidf": {"after": 1.02070207021, "base": 8.02397111915, "natur": 1.5392670157100001, "fact": 1.73375559681, "multipart": 933.882352941, "unlik": 2.42529789184, "too": 1.81585268215, "occur": 1.7453825857499998, "post": 6.71478922881, "here": 7.26923076924, "veri": 1.25880114177, "britz": 933.882352941, "kind": 2.5806241872599998, "through": 2.14149861738, "comput": 19.638792676900003, "long": 1.2657259028899999, "would": 2.1657458563599996, "layer": 16.28307692308, "encod": 29.0237659963, "been": 1.0239277652399998, "care": 4.98853102906, "about": 8.51888121272, "function": 2.495441685, "neural": 654.0674157303, "thing": 4.813096862219999, "sentenc": 46.76288659792, "python": 56.2978723404, "just": 2.67160286074, "onehot": 1867.764705882, "sequenti": 39.5910224439, "correct": 7.3262574988399995, "complet": 1.24021560816, "than": 1.03278688525, "know": 2.59327017315, "idea": 6.279235332900001, "should": 1.6643254009900001, "sole": 4.04175152749, "across": 3.4637285916800002, "translat": 5.71490280778, "measur": 4.82186788154, "few": 2.63458347162, "vector": 77.696574225, "how": 6.41001312204, "vanilla": 111.802816901, "numlay": 933.882352941, "guid": 2.49113447356, "they": 5.15086626435, "seri": 1.46511627907, "bad": 3.3944836433599996, "explain": 2.60049140049, "initi": 1.35, "found": 1.11387076405, "given": 2.70852170946, "word": 25.151522009739995, "usual": 1.72508964468, "abl": 1.8208510150200001, "deep": 3.6279707495399998, "mani": 4.17707031508, "will": 2.44962197192, "over": 1.02525024217, "generat": 16.42203258336, "full": 1.66729678639, "grammat": 40.8123393316, "second": 2.2261796256, "calcul": 24.51891891892, "wildml": 933.882352941, "text": 18.769655172420002, "worri": 10.302401038300001, "next": 5.9802241265600005, "perform": 3.0627954085000004, "but": 5.0816208949499995, "demonstr": 2.64997496244, "rnns": 14008.235294115, "need": 2.8745247148199997, "our": 9.43035343036, "there": 1.04091266719, "output": 69.09284332692, "final": 1.34008609775, "inform": 9.45187537212, "tanh": 933.882352941, "point": 1.25990000794, "num": 2.00063008002, "formula": 8.64235166032, "hidden": 46.87795275588, "exampl": 9.02900473932, "has": 2.0872995004, "have": 4.059579364559999, "paramet": 51.769565217300006, "thorough": 10.956521739100001, "use": 6.177832544279999, "follow": 2.09280253098, "govern": 1.50941243582, "resourc": 2.9487369985100003, "out": 1.06016694491, "model": 29.2683697656, "andrej": 369.209302326, "semant": 39.1034482759, "basic": 2.7301805675, "number": 2.20285833218, "reduc": 1.98698372966, "tutori": 178.3820224719, "sentiment": 19.845, "exhaust": 8.25585023401, "typic": 13.524918358679997, "forward": 3.66566612792, "mean": 2.89813800658, "not": 4.06269592476, "far": 1.71022298826, "involv": 1.4498630137000001, "arbitrari": 17.8181818182, "task": 23.31848225214, "them": 1.09876115994, "research": 1.9420183486200002, "plan": 1.5356935577500002, "are": 19.56821277982, "may": 4.20807103572, "the": 62.0, "real": 2.28103448276, "happen": 5.92719805862, "theano": 933.882352941, "shown": 5.53846153846, "previous": 5.71387439264, "want": 9.984905660399999, "captur": 14.40130624095, "sideeffect": 933.882352941, "problem": 1.76674827509, "complic": 5.6478121664900005, "sinc": 2.16737201366, "system": 1.38739840951, "vanish": 18.6556991774, "behind": 4.169117647059999, "extens": 1.99171998495, "essenti": 2.9280708225700005, "allow": 6.358029635550001, "world": 1.11340206186, "anoth": 1.13643521832, "which": 10.051918449999999, "cooler": 27.1384615385, "featur": 1.52712581762, "non": 16.979679144400002, "practic": 3.40869565218, "tradit": 3.2160437557, "other": 1.00992366412, "introduct": 2.7808723068799996, "algorithm": 27.9507042254, "one": 1.00627495722, "show": 1.26703910615, "longterm": 512.129032258, "develop": 1.1955719557200002, "some": 2.08073394496, "such": 2.12302754748, "becaus": 2.2990369994999997, "shakespearelik": 933.882352941, "get": 1.78562591385, "network": 49.28018297672, "like": 3.44755700325, "implement": 17.8824059473, "came": 1.46013059873, "diagram": 44.346368715, "sourc": 1.69760479042, "arbitrarili": 50.8846153846, "unrol": 1642.3448275859998, "call": 1.0676529926, "nonrecurr": 933.882352941, "independ": 1.58950740889, "backpropag": 933.882352941, "element": 2.36004162331, "memori": 10.29571984436, "despit": 1.60606980273, "understand": 2.96858638743, "vocabulari": 23.2785923754, "zero": 8.75192943771, "languag": 25.24371205554, "set": 1.18707940781, "simpli": 2.5192002538900002, "when": 2.0415353951, "much": 2.3884459154599997, "new": 2.0357761108, "from": 4.00226885988, "part": 2.08661365578, "back": 1.26070038911, "bit": 8.33385826772, "similar": 1.37514075357, "limit": 3.0373062942400004, "work": 1.11520089913, "detail": 2.26186066391, "differ": 3.7096347067499997, "editor": 4.33060556465, "somewhat": 4.29197080292, "relu": 933.882352941, "most": 1.02096463023, "what": 10.02747513024, "for": 10.003150400100001, "depend": 8.964426877480001, "predict": 31.109079033300002, "highprob": 933.882352941, "all": 5.05733944955, "input": 85.4204458107, "unfold": 67.1289640592, "with": 4.004792835959999, "denni": 9.50658682635, "assum": 5.915052160959999, "think": 8.721479582490002, "gradient": 41.889182058, "recent": 1.54405757635, "better": 4.0131445905000005, "current": 1.5325803649, "that": 13.05179282875, "look": 3.8172637653199994, "popular": 3.01538461538, "head": 1.57781753131, "more": 3.0515120451, "and": 12.000755905559998, "list": 1.36321483771, "correspond": 3.32481675393, "ago": 6.05954198473, "capabl": 3.6580645161300005, "karpathi": 933.882352941, "stuff": 23.3127753304, "later": 2.17300848618, "could": 1.2043695949, "promis": 3.5030891438699996, "befor": 1.10036041031, "familiar": 6.86381322957, "state": 6.286279944539999, "can": 5.8813069571, "fig": 54.5567010309, "twofold": 127.008, "give": 1.3653250774, "make": 2.1525320317200003, "way": 2.4381478922, "onli": 3.0769429549800007, "each": 9.51798561152, "their": 1.01547908405, "train": 5.8097096853, "great": 3.79778327088, "applic": 10.28016404058, "this": 10.037936267100001, "necessari": 2.8421052631599997, "step": 36.763092269279994, "scratch": 25.8146341463, "total": 1.5460122699399999, "shakespear": 17.4845814978, "devendra": 933.882352941, "common": 1.4025974025999999, "same": 4.47431832592, "type": 2.0281042411900003, "data": 3.37643555934, "sampl": 7.23280182232, "note": 1.42449528937, "sequenc": 48.56902485656, "probabl": 7.936677220469999, "machin": 8.04866920152, "into": 2.03004922958, "paper": 2.6628648104700003, "recurr": 249.17488789269999, "reflect": 2.3443591258099996, "time": 9.10147143132, "abov": 7.61530159492, "lstms": 2801.647058823, "requir": 1.52844902282, "import": 1.3401992233700002, "first": 2.01523229246, "everi": 1.47917637194, "nonlinear": 99.225, "share": 1.8566249561500001, "theori": 3.02745995423, "fun": 12.8863636364, "characterlevel": 933.882352941, "score": 4.2884927066500005, "main": 1.25303867403, "write": 2.0575427682700003, "briefli": 4.8669527897, "learn": 2.32275054865, "actual": 1.87482286254, "cover": 3.38760268858, "mention": 5.07788261634, "success": 1.32002993265}, "logtfidf": {"after": 0.020490694648099998, "base": 0.9556631160090001, "natur": 0.431306339292, "fact": 0.5502899207949999, "multipart": 6.83935046985, "unlik": 0.885954358842, "too": 0.5965551547219999, "occur": 0.556973778473, "post": 2.4171004581029996, "here": 2.6551145651100003, "veri": 0.230159793238, "britz": 6.83935046985, "kind": 0.948031302717, "through": 0.1367173837698, "comput": 6.8403445797, "long": 0.235645793878, "would": 0.1592352559294, "layer": 4.1939583247000005, "encod": 3.36811501148, "been": 0.023645982368400004, "care": 1.8279886058219998, "about": 0.5027478197968, "function": 0.914465741594, "neural": 44.9384667105, "thing": 1.7563870693599999, "sentenc": 14.125186601760001, "python": 4.03065674296, "just": 0.579062868218, "onehot": 13.6787009397, "sequenti": 3.6786023866, "correct": 2.59663526362, "complet": 0.215285242047, "than": 0.0322608622182, "know": 0.952919694398, "idea": 2.21590776636, "should": 0.509419876758, "sole": 1.3966781444299998, "across": 1.098396911882, "translat": 2.0998602200599996, "measur": 1.760028399452, "few": 0.551155827306, "vector": 9.76259663391, "how": 1.8862678277200002, "vanilla": 4.7167367562999996, "numlay": 6.83935046985, "guid": 0.912738218589, "they": 0.148634973838, "seri": 0.38193461069799994, "bad": 1.2221516561799999, "explain": 0.955700427358, "initi": 0.30010459245, "found": 0.107841124048, "given": 0.606511621662, "word": 8.20205515339, "usual": 0.545279017064, "abl": 0.599303982475, "deep": 1.2886734698, "mani": 0.1732630324884, "will": 0.40557306983, "over": 0.0249367214957, "generat": 5.753458733888, "full": 0.511203624148, "grammat": 3.708984470280001, "second": 0.21427952675999998, "calcul": 7.252602636839999, "wildml": 6.83935046985, "text": 6.84289205994, "worri": 2.3323769785799997, "next": 1.608654741996, "perform": 0.85236170116, "but": 0.0809618603595, "demonstr": 0.9745501918189999, "rnns": 102.59025704775, "need": 0.725480326884, "our": 3.4305568567280003, "there": 0.0400978929255, "output": 18.34403920443, "final": 0.292733863948, "inform": 2.726722227972, "tanh": 6.83935046985, "point": 0.23103235903299998, "num": 0.0006299807907940001, "formula": 2.15667472869, "hidden": 12.334728031200001, "exampl": 2.4520960499939997, "has": 0.0854478897096, "have": 0.0591400093648, "paramet": 8.544570431579999, "thorough": 2.39393487158, "use": 0.1752481183896, "follow": 0.09071382218839999, "govern": 0.411720459754, "resourc": 1.08137694258, "out": 0.0584263909193, "model": 10.324301023554002, "andrej": 5.91136369821, "semant": 3.6662106543, "basic": 1.00436774895, "number": 0.1932171568372, "reduc": 0.686617775143, "tutori": 12.2559454665, "sentiment": 4.58960981136, "exhaust": 2.11092206831, "typic": 4.876645914948, "forward": 1.29901007269, "mean": 0.74184256704, "not": 0.06220965203, "far": 0.536623764503, "involv": 0.371469078658, "arbitrari": 2.88021938643, "task": 8.144920839660001, "them": 0.0941833269093, "research": 0.663727818138, "plan": 0.428982108147, "are": 0.5598819980713, "may": 0.20283998113760002, "the": 0.0, "real": 0.824629060574, "happen": 2.17280883604, "theano": 6.83935046985, "shown": 2.03713916198, "previous": 1.426411840252, "want": 3.4581830312749995, "captur": 5.289405006050001, "sideeffect": 6.83935046985, "problem": 0.569140724273, "complic": 1.7312682430000002, "sinc": 0.1607363989154, "system": 0.327430345585, "vanish": 2.92615168533, "behind": 1.4691144748640002, "extens": 0.6889985794750001, "essenti": 1.07434378384, "allow": 1.2014030559450002, "world": 0.107420248621, "anoth": 0.127896361652, "which": 0.0517841384543, "cooler": 3.30095196667, "featur": 0.423387418142, "non": 2.8320172846099996, "practic": 1.066365061734, "tradit": 0.9500095525839999, "other": 0.00987474791976, "introduct": 1.02276465794, "algorithm": 3.33044239518, "one": 0.0062553516455, "show": 0.236682766013, "longterm": 6.238576609419999, "develop": 0.178624694913, "some": 0.079147018129, "such": 0.119391955612, "becaus": 0.27868631765, "shakespearelik": 6.83935046985, "get": 0.579769005782, "network": 18.108578007987997, "like": 0.417160729635, "implement": 6.37189704535, "came": 0.378525882905, "diagram": 6.19776729388, "sourc": 0.529218310751, "arbitrarili": 3.9295606260900002, "unrol": 18.915803951759997, "call": 0.0654627744488, "nonrecurr": 6.83935046985, "independ": 0.463424162503, "backpropag": 6.83935046985, "element": 0.8586792558769999, "memori": 3.7817355946399998, "despit": 0.473790078298, "understand": 1.0880858756799998, "vocabulari": 3.14753415606, "zero": 2.1692741832299998, "languag": 9.137500068465998, "set": 0.171496011289, "simpli": 0.923941491586, "when": 0.0411099777168, "much": 0.35499145860200004, "new": 0.0354598937022, "from": 0.002268216675464, "part": 0.08479062196560001, "back": 0.23166743089699998, "bit": 2.12032652634, "similar": 0.318556092114, "limit": 0.83564770926, "work": 0.109034567273, "detail": 0.816187777173, "differ": 0.6369633639360001, "editor": 1.4657073855, "somewhat": 1.4567460220700001, "relu": 6.83935046985, "most": 0.020747896295599998, "what": 1.807098374616, "for": 0.0031499039539700006, "depend": 3.22787926, "predict": 9.874441426139999, "highprob": 6.83935046985, "all": 0.057013160488999994, "input": 17.511727347729998, "unfold": 7.02693685082, "with": 0.00478996685356, "denni": 2.25198490849, "assum": 2.1687062705, "think": 3.2015298352499997, "gradient": 3.73502760882, "recent": 0.434413741288, "better": 1.3928558812, "current": 0.42695282784500005, "that": 0.05168992893532, "look": 1.2927733872, "popular": 0.8211604175499999, "head": 0.456042582852, "more": 0.05107479479999999, "and": 0.0007558817047632, "list": 0.309845761506, "correspond": 1.20141456099, "ago": 1.80163421715, "capabl": 1.2969341868100002, "karpathi": 6.83935046985, "stuff": 3.1490015077499995, "later": 0.1659308519756, "could": 0.18595627229000003, "promis": 1.25364519176, "befor": 0.0956377718795, "familiar": 1.92626315167, "state": 0.27966001660080003, "can": 0.8117054819699999, "fig": 3.9992405467300003, "twofold": 4.8442500766, "give": 0.311392552224, "make": 0.14699531564579998, "way": 0.39618301987000004, "onli": 0.0759728049873, "each": 1.389933514432, "their": 0.015360505122700001, "train": 1.982754938517, "great": 0.707415774237, "applic": 3.6948117854699998, "this": 0.037864490525, "necessari": 1.0445450673999999, "step": 13.51408574074, "scratch": 3.2509415461, "total": 0.43567888670500005, "shakespear": 2.8613194352999995, "devendra": 6.83935046985, "common": 0.338325805271, "same": 0.448238598416, "type": 0.707101485387, "data": 1.2168205848, "sampl": 1.9786264883900002, "note": 0.353817568083, "sequenc": 14.4283554992, "probabl": 2.918647238739, "machin": 2.78471916124, "into": 0.0298257264574, "paper": 0.979402539665, "recurr": 25.00571403316, "reflect": 0.85201207065, "time": 0.1009036697634, "abov": 2.575460919264, "lstms": 20.51805140955, "requir": 0.424253510675, "import": 0.292818277066, "first": 0.015174579624319999, "everi": 0.391485427421, "nonlinear": 4.59738999867, "share": 0.618760299747, "theori": 1.10772396902, "fun": 2.5561696698099996, "characterlevel": 6.83935046985, "score": 1.4559353207700003, "main": 0.225571540588, "write": 0.721512439877, "briefli": 1.5824680307199999, "learn": 0.842752064745, "actual": 0.628514181648, "cover": 1.053950638312, "mention": 1.863494372672, "success": 0.27765441259199997}, "logidf": {"after": 0.020490694648099998, "base": 0.13652330228700002, "natur": 0.431306339292, "fact": 0.5502899207949999, "multipart": 6.83935046985, "unlik": 0.885954358842, "too": 0.5965551547219999, "occur": 0.556973778473, "post": 0.8057001527009999, "here": 0.8850381883700001, "veri": 0.230159793238, "britz": 6.83935046985, "kind": 0.948031302717, "through": 0.0683586918849, "comput": 1.36806891594, "long": 0.235645793878, "would": 0.0796176279647, "layer": 2.0969791623500003, "encod": 3.36811501148, "been": 0.023645982368400004, "care": 0.9139943029109999, "about": 0.0628434774746, "function": 0.914465741594, "neural": 4.0853151555, "thing": 0.8781935346799999, "sentenc": 1.7656483252200001, "python": 4.03065674296, "just": 0.289531434109, "onehot": 6.83935046985, "sequenti": 3.6786023866, "correct": 1.29831763181, "complet": 0.215285242047, "than": 0.0322608622182, "know": 0.952919694398, "idea": 0.73863592212, "should": 0.509419876758, "sole": 1.3966781444299998, "across": 0.549198455941, "translat": 1.0499301100299998, "measur": 0.880014199726, "few": 0.275577913653, "vector": 3.25419887797, "how": 0.47156695693000006, "vanilla": 4.7167367562999996, "numlay": 6.83935046985, "guid": 0.912738218589, "they": 0.0297269947676, "seri": 0.38193461069799994, "bad": 1.2221516561799999, "explain": 0.955700427358, "initi": 0.30010459245, "found": 0.107841124048, "given": 0.303255810831, "word": 0.585861082385, "usual": 0.545279017064, "abl": 0.599303982475, "deep": 1.2886734698, "mani": 0.0433157581221, "will": 0.202786534915, "over": 0.0249367214957, "generat": 0.719182341736, "full": 0.511203624148, "grammat": 3.708984470280001, "second": 0.10713976337999999, "calcul": 1.8131506592099997, "wildml": 6.83935046985, "text": 1.14048200999, "worri": 2.3323769785799997, "next": 0.402163685499, "perform": 0.42618085058, "but": 0.0161923720719, "demonstr": 0.9745501918189999, "rnns": 6.83935046985, "need": 0.362740163442, "our": 0.8576392141820001, "there": 0.0400978929255, "output": 2.03822657827, "final": 0.292733863948, "inform": 0.454453704662, "tanh": 6.83935046985, "point": 0.23103235903299998, "num": 0.00031499039539700004, "formula": 2.15667472869, "hidden": 2.0557880052, "exampl": 0.40868267499899996, "has": 0.0427239448548, "have": 0.0147850023412, "paramet": 2.8481901438599997, "thorough": 2.39393487158, "use": 0.0292080197316, "follow": 0.045356911094199995, "govern": 0.411720459754, "resourc": 1.08137694258, "out": 0.0584263909193, "model": 0.7374500731110001, "andrej": 5.91136369821, "semant": 3.6662106543, "basic": 1.00436774895, "number": 0.0966085784186, "reduc": 0.686617775143, "tutori": 4.0853151555, "sentiment": 2.29480490568, "exhaust": 2.11092206831, "typic": 0.812774319158, "forward": 1.29901007269, "mean": 0.37092128352, "not": 0.0155524130075, "far": 0.536623764503, "involv": 0.371469078658, "arbitrari": 2.88021938643, "task": 1.35748680661, "them": 0.0941833269093, "research": 0.663727818138, "plan": 0.428982108147, "are": 0.0294674735827, "may": 0.050709995284400004, "the": 0.0, "real": 0.824629060574, "happen": 1.08640441802, "theano": 6.83935046985, "shown": 1.01856958099, "previous": 0.356602960063, "want": 0.6916366062549999, "captur": 1.0578810012100002, "sideeffect": 6.83935046985, "problem": 0.569140724273, "complic": 1.7312682430000002, "sinc": 0.0803681994577, "system": 0.327430345585, "vanish": 2.92615168533, "behind": 0.7345572374320001, "extens": 0.6889985794750001, "essenti": 1.07434378384, "allow": 0.24028061118900002, "world": 0.107420248621, "anoth": 0.127896361652, "which": 0.00517841384543, "cooler": 3.30095196667, "featur": 0.423387418142, "non": 2.8320172846099996, "practic": 0.533182530867, "tradit": 0.47500477629199994, "other": 0.00987474791976, "introduct": 1.02276465794, "algorithm": 3.33044239518, "one": 0.0062553516455, "show": 0.236682766013, "longterm": 6.238576609419999, "develop": 0.178624694913, "some": 0.0395735090645, "such": 0.059695977806, "becaus": 0.139343158825, "shakespearelik": 6.83935046985, "get": 0.579769005782, "network": 0.9530830530519999, "like": 0.139053576545, "implement": 1.27437940907, "came": 0.378525882905, "diagram": 3.09888364694, "sourc": 0.529218310751, "arbitrarili": 3.9295606260900002, "unrol": 6.305267983919999, "call": 0.0654627744488, "nonrecurr": 6.83935046985, "independ": 0.463424162503, "backpropag": 6.83935046985, "element": 0.8586792558769999, "memori": 0.9454338986599999, "despit": 0.473790078298, "understand": 1.0880858756799998, "vocabulari": 3.14753415606, "zero": 2.1692741832299998, "languag": 0.8306818244059999, "set": 0.171496011289, "simpli": 0.923941491586, "when": 0.0205549888584, "much": 0.17749572930100002, "new": 0.0177299468511, "from": 0.000567054168866, "part": 0.04239531098280001, "back": 0.23166743089699998, "bit": 2.12032652634, "similar": 0.318556092114, "limit": 0.41782385463, "work": 0.109034567273, "detail": 0.816187777173, "differ": 0.212321121312, "editor": 1.4657073855, "somewhat": 1.4567460220700001, "relu": 6.83935046985, "most": 0.020747896295599998, "what": 0.225887296827, "for": 0.00031499039539700004, "depend": 0.806969815, "predict": 1.6457402376899999, "highprob": 6.83935046985, "all": 0.011402632097799998, "input": 2.50167533539, "unfold": 3.51346842541, "with": 0.00119749171339, "denni": 2.25198490849, "assum": 1.08435313525, "think": 1.06717661175, "gradient": 3.73502760882, "recent": 0.434413741288, "better": 0.6964279406, "current": 0.42695282784500005, "that": 0.00397614837964, "look": 0.6463866936, "popular": 0.41058020877499996, "head": 0.456042582852, "more": 0.017024931599999998, "and": 6.29901420636e-05, "list": 0.309845761506, "correspond": 1.20141456099, "ago": 1.80163421715, "capabl": 1.2969341868100002, "karpathi": 6.83935046985, "stuff": 3.1490015077499995, "later": 0.0829654259878, "could": 0.18595627229000003, "promis": 1.25364519176, "befor": 0.0956377718795, "familiar": 1.92626315167, "state": 0.0466100027668, "can": 0.162341096394, "fig": 3.9992405467300003, "twofold": 4.8442500766, "give": 0.311392552224, "make": 0.07349765782289999, "way": 0.19809150993500002, "onli": 0.025324268329099998, "each": 0.173741689304, "their": 0.015360505122700001, "train": 0.660918312839, "great": 0.235805258079, "applic": 1.23160392849, "this": 0.0037864490525, "necessari": 1.0445450673999999, "step": 1.03954505698, "scratch": 3.2509415461, "total": 0.43567888670500005, "shakespear": 2.8613194352999995, "devendra": 6.83935046985, "common": 0.338325805271, "same": 0.112059649604, "type": 0.707101485387, "data": 1.2168205848, "sampl": 1.9786264883900002, "note": 0.353817568083, "sequenc": 1.8035444374, "probabl": 0.972882412913, "machin": 1.39235958062, "into": 0.0149128632287, "paper": 0.979402539665, "recurr": 3.5722448618800002, "reflect": 0.85201207065, "time": 0.0112115188626, "abov": 0.643865229816, "lstms": 6.83935046985, "requir": 0.424253510675, "import": 0.292818277066, "first": 0.0075872898121599995, "everi": 0.391485427421, "nonlinear": 4.59738999867, "share": 0.618760299747, "theori": 1.10772396902, "fun": 2.5561696698099996, "characterlevel": 6.83935046985, "score": 1.4559353207700003, "main": 0.225571540588, "write": 0.721512439877, "briefli": 1.5824680307199999, "learn": 0.842752064745, "actual": 0.628514181648, "cover": 0.526975319156, "mention": 0.931747186336, "success": 0.27765441259199997}, "freq": {"after": 1, "base": 7, "natur": 1, "fact": 1, "multipart": 1, "unlik": 1, "too": 1, "occur": 1, "post": 3, "here": 3, "veri": 1, "britz": 1, "kind": 1, "through": 2, "comput": 5, "long": 1, "would": 2, "layer": 2, "encod": 1, "been": 1, "care": 2, "about": 8, "function": 1, "neural": 11, "thing": 2, "sentenc": 8, "python": 1, "just": 2, "onehot": 2, "sequenti": 1, "correct": 2, "complet": 1, "than": 1, "know": 1, "idea": 3, "should": 1, "sole": 1, "across": 2, "translat": 2, "measur": 2, "few": 2, "vector": 3, "how": 4, "vanilla": 1, "numlay": 1, "guid": 1, "they": 5, "seri": 1, "bad": 1, "explain": 1, "initi": 1, "found": 1, "given": 2, "word": 14, "usual": 1, "abl": 1, "deep": 1, "mani": 4, "will": 2, "over": 1, "generat": 8, "full": 1, "grammat": 1, "second": 2, "calcul": 4, "wildml": 1, "text": 6, "worri": 1, "next": 4, "perform": 2, "but": 5, "demonstr": 1, "rnns": 15, "need": 2, "our": 4, "there": 1, "output": 9, "final": 1, "inform": 6, "tanh": 1, "point": 1, "num": 2, "formula": 1, "hidden": 6, "exampl": 6, "has": 2, "have": 4, "paramet": 3, "thorough": 1, "use": 6, "follow": 2, "govern": 1, "resourc": 1, "out": 1, "model": 14, "andrej": 1, "semant": 1, "basic": 1, "number": 2, "reduc": 1, "tutori": 3, "sentiment": 2, "exhaust": 1, "typic": 6, "forward": 1, "mean": 2, "not": 4, "far": 1, "involv": 1, "arbitrari": 1, "task": 6, "them": 1, "research": 1, "plan": 1, "are": 19, "may": 4, "the": 62, "real": 1, "happen": 2, "theano": 1, "shown": 2, "previous": 4, "want": 5, "captur": 5, "sideeffect": 1, "problem": 1, "complic": 1, "sinc": 2, "system": 1, "vanish": 1, "behind": 2, "extens": 1, "essenti": 1, "allow": 5, "world": 1, "anoth": 1, "which": 10, "cooler": 1, "featur": 1, "non": 1, "practic": 2, "tradit": 2, "other": 1, "introduct": 1, "algorithm": 1, "one": 1, "show": 1, "longterm": 1, "develop": 1, "some": 2, "such": 2, "becaus": 2, "shakespearelik": 1, "get": 1, "network": 19, "like": 3, "implement": 5, "came": 1, "diagram": 2, "sourc": 1, "arbitrarili": 1, "unrol": 3, "call": 1, "nonrecurr": 1, "independ": 1, "backpropag": 1, "element": 1, "memori": 4, "despit": 1, "understand": 1, "vocabulari": 1, "zero": 1, "languag": 11, "set": 1, "simpli": 1, "when": 2, "much": 2, "new": 2, "from": 4, "part": 2, "back": 1, "bit": 1, "similar": 1, "limit": 2, "work": 1, "detail": 1, "differ": 3, "editor": 1, "somewhat": 1, "relu": 1, "most": 1, "what": 8, "for": 10, "depend": 4, "predict": 6, "highprob": 1, "all": 5, "input": 7, "unfold": 2, "with": 4, "denni": 1, "assum": 2, "think": 3, "gradient": 1, "recent": 1, "better": 2, "current": 1, "that": 13, "look": 2, "popular": 2, "head": 1, "more": 3, "and": 12, "list": 1, "correspond": 1, "ago": 1, "capabl": 1, "karpathi": 1, "stuff": 1, "later": 2, "could": 1, "promis": 1, "befor": 1, "familiar": 1, "state": 6, "can": 5, "fig": 1, "twofold": 1, "give": 1, "make": 2, "way": 2, "onli": 3, "each": 8, "their": 1, "train": 3, "great": 3, "applic": 3, "this": 10, "necessari": 1, "step": 13, "scratch": 1, "total": 1, "shakespear": 1, "devendra": 1, "common": 1, "same": 4, "type": 1, "data": 1, "sampl": 1, "note": 1, "sequenc": 8, "probabl": 3, "machin": 2, "into": 2, "paper": 1, "recurr": 7, "reflect": 1, "time": 9, "abov": 4, "lstms": 3, "requir": 1, "import": 1, "first": 2, "everi": 1, "nonlinear": 1, "share": 1, "theori": 1, "fun": 1, "characterlevel": 1, "score": 1, "main": 1, "write": 1, "briefli": 1, "learn": 1, "actual": 1, "cover": 2, "mention": 2, "success": 1}, "idf": {"after": 1.02070207021, "base": 1.14628158845, "natur": 1.5392670157100001, "fact": 1.73375559681, "multipart": 933.882352941, "unlik": 2.42529789184, "too": 1.81585268215, "occur": 1.7453825857499998, "post": 2.23826307627, "here": 2.42307692308, "veri": 1.25880114177, "britz": 933.882352941, "kind": 2.5806241872599998, "through": 1.07074930869, "comput": 3.9277585353800006, "long": 1.2657259028899999, "would": 1.0828729281799998, "layer": 8.14153846154, "encod": 29.0237659963, "been": 1.0239277652399998, "care": 2.49426551453, "about": 1.06486015159, "function": 2.495441685, "neural": 59.4606741573, "thing": 2.4065484311099996, "sentenc": 5.84536082474, "python": 56.2978723404, "just": 1.33580143037, "onehot": 933.882352941, "sequenti": 39.5910224439, "correct": 3.6631287494199998, "complet": 1.24021560816, "than": 1.03278688525, "know": 2.59327017315, "idea": 2.0930784443, "should": 1.6643254009900001, "sole": 4.04175152749, "across": 1.7318642958400001, "translat": 2.85745140389, "measur": 2.41093394077, "few": 1.31729173581, "vector": 25.898858075, "how": 1.60250328051, "vanilla": 111.802816901, "numlay": 933.882352941, "guid": 2.49113447356, "they": 1.03017325287, "seri": 1.46511627907, "bad": 3.3944836433599996, "explain": 2.60049140049, "initi": 1.35, "found": 1.11387076405, "given": 1.35426085473, "word": 1.7965372864099998, "usual": 1.72508964468, "abl": 1.8208510150200001, "deep": 3.6279707495399998, "mani": 1.04426757877, "will": 1.22481098596, "over": 1.02525024217, "generat": 2.05275407292, "full": 1.66729678639, "grammat": 40.8123393316, "second": 1.1130898128, "calcul": 6.12972972973, "wildml": 933.882352941, "text": 3.12827586207, "worri": 10.302401038300001, "next": 1.4950560316400001, "perform": 1.5313977042500002, "but": 1.01632417899, "demonstr": 2.64997496244, "rnns": 933.882352941, "need": 1.4372623574099999, "our": 2.35758835759, "there": 1.04091266719, "output": 7.676982591880001, "final": 1.34008609775, "inform": 1.5753125620200001, "tanh": 933.882352941, "point": 1.25990000794, "num": 1.00031504001, "formula": 8.64235166032, "hidden": 7.81299212598, "exampl": 1.50483412322, "has": 1.0436497502, "have": 1.0148948411399998, "paramet": 17.256521739100002, "thorough": 10.956521739100001, "use": 1.0296387573799999, "follow": 1.04640126549, "govern": 1.50941243582, "resourc": 2.9487369985100003, "out": 1.06016694491, "model": 2.0905978404, "andrej": 369.209302326, "semant": 39.1034482759, "basic": 2.7301805675, "number": 1.10142916609, "reduc": 1.98698372966, "tutori": 59.4606741573, "sentiment": 9.9225, "exhaust": 8.25585023401, "typic": 2.2541530597799997, "forward": 3.66566612792, "mean": 1.44906900329, "not": 1.01567398119, "far": 1.71022298826, "involv": 1.4498630137000001, "arbitrari": 17.8181818182, "task": 3.88641370869, "them": 1.09876115994, "research": 1.9420183486200002, "plan": 1.5356935577500002, "are": 1.02990593578, "may": 1.05201775893, "the": 1.0, "real": 2.28103448276, "happen": 2.96359902931, "theano": 933.882352941, "shown": 2.76923076923, "previous": 1.42846859816, "want": 1.99698113208, "captur": 2.88026124819, "sideeffect": 933.882352941, "problem": 1.76674827509, "complic": 5.6478121664900005, "sinc": 1.08368600683, "system": 1.38739840951, "vanish": 18.6556991774, "behind": 2.0845588235299997, "extens": 1.99171998495, "essenti": 2.9280708225700005, "allow": 1.2716059271100002, "world": 1.11340206186, "anoth": 1.13643521832, "which": 1.005191845, "cooler": 27.1384615385, "featur": 1.52712581762, "non": 16.979679144400002, "practic": 1.70434782609, "tradit": 1.60802187785, "other": 1.00992366412, "introduct": 2.7808723068799996, "algorithm": 27.9507042254, "one": 1.00627495722, "show": 1.26703910615, "longterm": 512.129032258, "develop": 1.1955719557200002, "some": 1.04036697248, "such": 1.06151377374, "becaus": 1.1495184997499999, "shakespearelik": 933.882352941, "get": 1.78562591385, "network": 2.59369384088, "like": 1.14918566775, "implement": 3.57648118946, "came": 1.46013059873, "diagram": 22.1731843575, "sourc": 1.69760479042, "arbitrarili": 50.8846153846, "unrol": 547.448275862, "call": 1.0676529926, "nonrecurr": 933.882352941, "independ": 1.58950740889, "backpropag": 933.882352941, "element": 2.36004162331, "memori": 2.57392996109, "despit": 1.60606980273, "understand": 2.96858638743, "vocabulari": 23.2785923754, "zero": 8.75192943771, "languag": 2.29488291414, "set": 1.18707940781, "simpli": 2.5192002538900002, "when": 1.02076769755, "much": 1.1942229577299999, "new": 1.0178880554, "from": 1.00056721497, "part": 1.04330682789, "back": 1.26070038911, "bit": 8.33385826772, "similar": 1.37514075357, "limit": 1.5186531471200002, "work": 1.11520089913, "detail": 2.26186066391, "differ": 1.23654490225, "editor": 4.33060556465, "somewhat": 4.29197080292, "relu": 933.882352941, "most": 1.02096463023, "what": 1.25343439128, "for": 1.00031504001, "depend": 2.2411067193700003, "predict": 5.18484650555, "highprob": 933.882352941, "all": 1.01146788991, "input": 12.2029208301, "unfold": 33.5644820296, "with": 1.0011982089899998, "denni": 9.50658682635, "assum": 2.9575260804799997, "think": 2.90715986083, "gradient": 41.889182058, "recent": 1.54405757635, "better": 2.0065722952500002, "current": 1.5325803649, "that": 1.00398406375, "look": 1.9086318826599997, "popular": 1.50769230769, "head": 1.57781753131, "more": 1.0171706817, "and": 1.00006299213, "list": 1.36321483771, "correspond": 3.32481675393, "ago": 6.05954198473, "capabl": 3.6580645161300005, "karpathi": 933.882352941, "stuff": 23.3127753304, "later": 1.08650424309, "could": 1.2043695949, "promis": 3.5030891438699996, "befor": 1.10036041031, "familiar": 6.86381322957, "state": 1.0477133240899998, "can": 1.17626139142, "fig": 54.5567010309, "twofold": 127.008, "give": 1.3653250774, "make": 1.0762660158600001, "way": 1.2190739461, "onli": 1.0256476516600002, "each": 1.18974820144, "their": 1.01547908405, "train": 1.9365698950999999, "great": 1.26592775696, "applic": 3.42672134686, "this": 1.00379362671, "necessari": 2.8421052631599997, "step": 2.8279301745599996, "scratch": 25.8146341463, "total": 1.5460122699399999, "shakespear": 17.4845814978, "devendra": 933.882352941, "common": 1.4025974025999999, "same": 1.11857958148, "type": 2.0281042411900003, "data": 3.37643555934, "sampl": 7.23280182232, "note": 1.42449528937, "sequenc": 6.07112810707, "probabl": 2.64555907349, "machin": 4.02433460076, "into": 1.01502461479, "paper": 2.6628648104700003, "recurr": 35.5964125561, "reflect": 2.3443591258099996, "time": 1.01127460348, "abov": 1.90382539873, "lstms": 933.882352941, "requir": 1.52844902282, "import": 1.3401992233700002, "first": 1.00761614623, "everi": 1.47917637194, "nonlinear": 99.225, "share": 1.8566249561500001, "theori": 3.02745995423, "fun": 12.8863636364, "characterlevel": 933.882352941, "score": 4.2884927066500005, "main": 1.25303867403, "write": 2.0575427682700003, "briefli": 4.8669527897, "learn": 2.32275054865, "actual": 1.87482286254, "cover": 1.69380134429, "mention": 2.53894130817, "success": 1.32002993265}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Recurrent Neural Networks Tutorial, Introduction</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2015/10/recurrent-neural-networks-tutorial.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Recurrent Neural Networks Tutorial, Introduction Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2015/10/big-data-home-healthcare.html\" rel=\"prev\" title=\"How big data can help in home health care?\"/>\n<link href=\"https://www.kdnuggets.com/2015/10/baesens-elearning-course-credit-risk-modeling.html\" rel=\"next\" title=\"Online course: Credit Risk Modeling\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2015/10/recurrent-neural-networks-tutorial.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=39714\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2015/10/recurrent-neural-networks-tutorial.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-39714 single-format-standard post-template post-template-fullwidth-php\">\n<div class=\"main_wrapper\"><!-- publ: 7-Oct, 2015  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2015/index.html\">2015</a> \u00bb <a href=\"https://www.kdnuggets.com/2015/10/index.html\">Oct</a> \u00bb <a href=\"https://www.kdnuggets.com/2015/10/tutorials.html\">Tutorials, Overviews, How-Tos</a> \u00bb Recurrent Neural Networks Tutorial, Introduction (\u00a0<a href=\"/2015/n33.html\">15:n33</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Recurrent Neural Networks Tutorial, Introduction</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2015/10/big-data-home-healthcare.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2015/10/baesens-elearning-course-credit-risk-modeling.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/nlp\" rel=\"tag\">NLP</a>, <a href=\"https://www.kdnuggets.com/tag/recurrent-neural-networks\" rel=\"tag\">Recurrent Neural Networks</a></div>\n<br/>\n<p class=\"excerpt\">\n     Recurrent Neural Networks (RNNs) are popular models that have shown great promise in NLP and many other Machine Learning tasks. Here is a much-needed guide to key RNN models and a few brilliant research papers.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/denny-britz\" rel=\"author\" title=\"Posts by Denny Britz\">Denny Britz</a>, WildML.</b></div>\n<p><!-- editor: devendra -->Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many\u00a0NLP tasks. But despite their recent popularity I\u2019ve only found a limited number of resources that thoroughly explain how RNNs work, and how to\u00a0implement them. That\u2019s what this tutorial is about. It\u2019s a multi-part series in which I\u2019m planning to cover the following:</p>\n<ol class=\"three_ol\">\n<li>Introduction to RNNs (this post)</li>\n<li><a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\" target=\"_blank\">Implementing a RNN using Python and Theano</a></li>\n<li>Understanding the Backpropagation Through Time (BPTT) algorithm and the vanishing gradient problem</li>\n<li>From RNNs to LSTM Networks</li>\n</ol>\n<p>As part of the tutorial we will implement a <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" onclick=\"javascript:window.open('http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf'); return false;\">recurrent neural network based language model</a>. The applications of\u00a0language models are two-fold: First, it allows us to score arbitrary sentences based on how likely they are to occur in the real world. This gives us a measure of\u00a0grammatical and semantic correctness. Such models are typically used as part of Machine Translation systems. Secondly, a language model allows us to generate new text (I think that\u2019s the much cooler application). Training a language model on Shakespeare allows us to generate\u00a0Shakespeare-like text.\u00a0<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" onclick=\"javascript:window.open('http://karpathy.github.io/2015/05/21/rnn-effectiveness/'); return false;\">This fun post</a>\u00a0by\u00a0Andrej Karpathy\u00a0demonstrates what character-level language models based\u00a0on RNNs are capable of.</p>\n<p>I\u2019m assuming that you are somewhat familiar with\u00a0basic Neural Networks. If you\u2019re not, you may want to head over to <a href=\"http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\">Implementing A Neural Network From Scratch</a>, \u00a0which guides you through the ideas and implementation behind\u00a0non-recurrent networks.</p>\n<h3>What are RNNs?</h3>\n<p>The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other.\u00a0But for many tasks that\u2019s a very bad idea. If you want to predict the next word in\u00a0a sentence you better know which words came before it.\u00a0RNNs\u00a0are called <em>recurrent</em>\u00a0because they perform the same task for\u00a0every element of a\u00a0sequence, with the output being depended\u00a0on the previous computations. Another way to think about RNNs is that they have a \u201cmemory\u201d which captures information about what has been calculated so far. In theory RNNs can make use\u00a0of information in\u00a0arbitrarily\u00a0long sequences, but in\u00a0practice they are limited to looking back only a few steps (more on this later). Here\u00a0is what a typical RNN looks like: <a href=\"http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg\"><img alt=\"A recurrent neural network and the unfolding in time of the computation involved in its forward computation.\" class=\"wp-image-110\" height=\"280\" src=\"http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg\" width=\"595\"/></a><br>\n<b>Fig. 1\u00a0</b> A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature</br></p>\n<p>The above diagram\u00a0shows a RNN being\u00a0<em>unrolled</em> (or unfolded) into a full network. By unrolling we simply mean that we write out the network for the complete sequence.\u00a0For example, if the sequence we care about is a sentence of\u00a05 words, the network would be unrolled into a 5-layer neural network, one layer for each word.\u00a0The formulas that govern\u00a0the computation happening in a\u00a0RNN are\u00a0as follows:</p>\n<ul class=\"three_ul\">\n<li><img alt=\"x_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"x_t\"/> is the input at time step <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/>. For example, <img alt=\"x_1\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=x_1&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"x_1\"/>\u00a0could be a\u00a0one-hot vector corresponding to\u00a0the second word of a sentence.</li>\n<li><img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> is the hidden state at time step <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/>. It\u2019s the \u201cmemory\u201d of the network. <img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> is calculated based on the previous hidden state and the input at the current step: <img alt=\"s_t=f(Ux_t + Ws_{t-1})\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t%3Df%28Ux_t+%2B+Ws_%7Bt-1%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t=f(Ux_t + Ws_{t-1})\"/>. The function <img alt=\"f\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"f\"/> usually\u00a0is a nonlinearity such as <a href=\"https://reference.wolfram.com/language/ref/Tanh.html\" onclick=\"javascript:window.open('https://reference.wolfram.com/language/ref/Tanh.html'); return false;\">tanh</a> or <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\" onclick=\"javascript:window.open('https://en.wikipedia.org/wiki/Rectifier_(neural_networks)'); return false;\">ReLU</a>. \u00a0<img alt=\"s_{-1}\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_%7B-1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_{-1}\"/>, which is required to calculate the first hidden state, is typically initialized to all zeroes.</li>\n<li><img alt=\"o_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=o_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"o_t\"/> is the output at step <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/>. For example, if we wanted to predict the next word in a sentence it would\u00a0be a vector of probabilities across our vocabulary. <img alt=\"o_t = \\mathrm{softmax}(Vs_t)\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=o_t+%3D+%5Cmathrm%7Bsoftmax%7D%28Vs_t%29&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"o_t = \\mathrm{softmax}(Vs_t)\"/>.</li>\n</ul>\n<p>There are a few things to note here:</p>\n<ul class=\"three_ul\">\n<li>You can think of the hidden state <img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> as the memory of the network. <img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> captures information about what happened in all the\u00a0previous time steps. The output at step <img alt=\"o_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=o_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"o_t\"/> is calculated solely based on the memory at time <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/>. As briefly mentioned above, it\u2019s a bit more complicated \u00a0in practice because <img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> typically\u00a0can\u2019t capture information from too many time steps ago.</li>\n<li>Unlike a traditional deep neural network, which uses\u00a0different parameters at each layer, a RNN shares the same parameters (<img alt=\"U, V, W\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=U%2C+V%2C+W&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"U, V, W\"/> above) across\u00a0all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.</li>\n<li>The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after\u00a0each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its\u00a0hidden state, which captures some information about a sequence.</li>\n</ul>\n<h3>What can RNNs do?</h3>\n<p>RNNs have shown great success in many NLP tasks. At this point I should mention that the most commonly used type of RNNs are <a href=\"https://en.wikipedia.org/wiki/Long_short_term_memory\" onclick=\"javascript:window.open('https://en.wikipedia.org/wiki/Long_short_term_memory'); return false;\">LSTMs</a>, which are much better at capturing long-term dependencies than vanilla RNNs are. But don\u2019t worry, LSTMs are essentially the same thing as the RNN we will develop in this tutorial, they\u00a0just have a different way of computing the hidden state. We\u2019ll cover LSTMs in more detail in a later post. Here are some example applications of RNNs in NLP (by non means an exhaustive list).</p>\n<h4>Language Modeling\u00a0and Generating Text</h4>\n<p>Given a sequence of words we want to predict the probability of each word given the previous words. Language Models allow us to measure how likely a sentence is, which is an important input for Machine Translation (since high-probability sentences are typically correct). A side-effect of being able to predict the next word is that we get a <em>generative</em> model, which allows us to generate new text by sampling from the output probabilities. And depending on what our training data is we can generate <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" onclick=\"javascript:window.open('http://karpathy.github.io/2015/05/21/rnn-effectiveness/'); return false;\">all kinds of stuff</a>. In Language Modeling our input is typically a sequence of words (encoded as one-hot vectors for example), and our output is the sequence of predicted words. When training the network we set <img alt=\"o_t = x_{t+1}\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=o_t+%3D+x_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"o_t = x_{t+1}\"/> since we want the output at step <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/> to be the actual next word.</p>\n<p>Research papers about\u00a0Language Modeling and Generating Text:</p>\n<ul class=\"three_ul\">\n<li><a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" onclick=\"javascript:window.open('http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf'); return false;\" target=\"_blank\">Recurrent neural network based language model</a></li>\n<li><a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf\" onclick=\"javascript:window.open('http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf'); return false;\" target=\"_blank\">Extensions of Recurrent neural network based language model</a></li>\n<li><a href=\"http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf\" onclick=\"javascript:window.open('http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf'); return false;\" target=\"_blank\">Generating Text with Recurrent Neural Networks</a></li>\n</ul>\n</div>\n<div class=\"page-link\"><p>Pages: 1 <a href=\"https://www.kdnuggets.com/2015/10/recurrent-neural-networks-tutorial.html/2\">2</a></p></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2015/10/big-data-home-healthcare.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2015/10/baesens-elearning-course-credit-risk-modeling.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2015/index.html\">2015</a> \u00bb <a href=\"https://www.kdnuggets.com/2015/10/index.html\">Oct</a> \u00bb <a href=\"https://www.kdnuggets.com/2015/10/tutorials.html\">Tutorials, Overviews, How-Tos</a> \u00bb Recurrent Neural Networks Tutorial, Introduction (\u00a0<a href=\"/2015/n33.html\">15:n33</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556365389\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.787 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 07:43:09 -->\n<!-- Compression = gzip -->", "content_tokenized": ["denni", "britz", "wildml", "editor", "devendra", "recurr", "neural", "network", "rnns", "are", "popular", "model", "that", "have", "shown", "great", "promis", "mani", "task", "but", "despit", "their", "recent", "popular", "onli", "found", "limit", "number", "resourc", "that", "thorough", "explain", "how", "rnns", "work", "and", "how", "implement", "them", "that", "what", "this", "tutori", "about", "multipart", "seri", "which", "plan", "cover", "the", "follow", "introduct", "rnns", "this", "post", "implement", "use", "python", "and", "theano", "understand", "the", "backpropag", "through", "time", "algorithm", "and", "the", "vanish", "gradient", "problem", "from", "rnns", "network", "part", "the", "tutori", "will", "implement", "recurr", "neural", "network", "base", "languag", "model", "the", "applic", "languag", "model", "are", "twofold", "first", "allow", "score", "arbitrari", "sentenc", "base", "how", "like", "they", "are", "occur", "the", "real", "world", "this", "give", "measur", "grammat", "and", "semant", "correct", "such", "model", "are", "typic", "use", "part", "machin", "translat", "system", "second", "languag", "model", "allow", "generat", "new", "text", "think", "that", "the", "much", "cooler", "applic", "train", "languag", "model", "shakespear", "allow", "generat", "shakespearelik", "text", "this", "fun", "post", "andrej", "karpathi", "demonstr", "what", "characterlevel", "languag", "model", "base", "rnns", "are", "capabl", "assum", "that", "are", "somewhat", "familiar", "with", "basic", "neural", "network", "not", "may", "want", "head", "over", "implement", "neural", "network", "from", "scratch", "which", "guid", "through", "the", "idea", "and", "implement", "behind", "nonrecurr", "network", "what", "are", "rnns", "the", "idea", "behind", "rnns", "make", "use", "sequenti", "inform", "tradit", "neural", "network", "assum", "that", "all", "input", "and", "output", "are", "independ", "each", "other", "but", "for", "mani", "task", "that", "veri", "bad", "idea", "want", "predict", "the", "next", "word", "sentenc", "better", "know", "which", "word", "came", "befor", "rnns", "are", "call", "recurr", "becaus", "they", "perform", "the", "same", "task", "for", "everi", "element", "sequenc", "with", "the", "output", "depend", "the", "previous", "comput", "anoth", "way", "think", "about", "rnns", "that", "they", "have", "memori", "which", "captur", "inform", "about", "what", "has", "been", "calcul", "far", "theori", "rnns", "can", "make", "use", "inform", "arbitrarili", "long", "sequenc", "but", "practic", "they", "are", "limit", "look", "back", "onli", "few", "step", "more", "this", "later", "here", "what", "typic", "look", "like", "fig", "num", "recurr", "neural", "network", "and", "the", "unfold", "time", "the", "comput", "involv", "forward", "comput", "sourc", "natur", "the", "abov", "diagram", "show", "unrol", "unfold", "into", "full", "network", "unrol", "simpli", "mean", "that", "write", "out", "the", "network", "for", "the", "complet", "sequenc", "for", "exampl", "the", "sequenc", "care", "about", "sentenc", "num", "word", "the", "network", "would", "unrol", "into", "numlay", "neural", "network", "one", "layer", "for", "each", "word", "the", "formula", "that", "govern", "the", "comput", "happen", "are", "follow", "the", "input", "time", "step", "for", "exampl", "could", "onehot", "vector", "correspond", "the", "second", "word", "sentenc", "the", "hidden", "state", "time", "step", "the", "memori", "the", "network", "calcul", "base", "the", "previous", "hidden", "state", "and", "the", "input", "the", "current", "step", "the", "function", "usual", "nonlinear", "such", "tanh", "relu", "which", "requir", "calcul", "the", "first", "hidden", "state", "typic", "initi", "all", "zero", "the", "output", "step", "for", "exampl", "want", "predict", "the", "next", "word", "sentenc", "would", "vector", "probabl", "across", "our", "vocabulari", "there", "are", "few", "thing", "note", "here", "can", "think", "the", "hidden", "state", "the", "memori", "the", "network", "captur", "inform", "about", "what", "happen", "all", "the", "previous", "time", "step", "the", "output", "step", "calcul", "sole", "base", "the", "memori", "time", "briefli", "mention", "abov", "bit", "more", "complic", "practic", "becaus", "typic", "can", "captur", "inform", "from", "too", "mani", "time", "step", "ago", "unlik", "tradit", "deep", "neural", "network", "which", "use", "differ", "paramet", "each", "layer", "share", "the", "same", "paramet", "abov", "across", "all", "step", "this", "reflect", "the", "fact", "that", "are", "perform", "the", "same", "task", "each", "step", "just", "with", "differ", "input", "this", "great", "reduc", "the", "total", "number", "paramet", "need", "learn", "the", "abov", "diagram", "has", "output", "each", "time", "step", "but", "depend", "the", "task", "this", "may", "not", "necessari", "for", "exampl", "when", "predict", "the", "sentiment", "sentenc", "may", "onli", "care", "about", "the", "final", "output", "not", "the", "sentiment", "after", "each", "word", "similar", "may", "not", "need", "input", "each", "time", "step", "the", "main", "featur", "hidden", "state", "which", "captur", "some", "inform", "about", "sequenc", "what", "can", "rnns", "rnns", "have", "shown", "great", "success", "mani", "task", "this", "point", "should", "mention", "that", "the", "most", "common", "use", "type", "rnns", "are", "lstms", "which", "are", "much", "better", "captur", "longterm", "depend", "than", "vanilla", "rnns", "are", "but", "worri", "lstms", "are", "essenti", "the", "same", "thing", "the", "will", "develop", "this", "tutori", "they", "just", "have", "differ", "way", "comput", "the", "hidden", "state", "cover", "lstms", "more", "detail", "later", "post", "here", "are", "some", "exampl", "applic", "rnns", "non", "mean", "exhaust", "list", "languag", "model", "and", "generat", "text", "given", "sequenc", "word", "want", "predict", "the", "probabl", "each", "word", "given", "the", "previous", "word", "languag", "model", "allow", "measur", "how", "like", "sentenc", "which", "import", "input", "for", "machin", "translat", "sinc", "highprob", "sentenc", "are", "typic", "correct", "sideeffect", "abl", "predict", "the", "next", "word", "that", "get", "generat", "model", "which", "allow", "generat", "new", "text", "sampl", "from", "the", "output", "probabl", "and", "depend", "what", "our", "train", "data", "can", "generat", "all", "kind", "stuff", "languag", "model", "our", "input", "typic", "sequenc", "word", "encod", "onehot", "vector", "for", "exampl", "and", "our", "output", "the", "sequenc", "predict", "word", "when", "train", "the", "network", "set", "sinc", "want", "the", "output", "step", "the", "actual", "next", "word", "research", "paper", "about", "languag", "model", "and", "generat", "text", "recurr", "neural", "network", "base", "languag", "model", "extens", "recurr", "neural", "network", "base", "languag", "model", "generat", "text", "with", "recurr", "neural", "network"], "timestamp_scraper": 1556365389.718188, "title": "Recurrent Neural Networks Tutorial, Introduction", "read_time": 300.3, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/denny-britz\" rel=\"author\" title=\"Posts by Denny Britz\">Denny Britz</a>, WildML.</b></div>\n<p><!-- editor: devendra -->Recurrent Neural Networks (RNNs) are popular models that have shown great promise in many\u00a0NLP tasks. But despite their recent popularity I\u2019ve only found a limited number of resources that thoroughly explain how RNNs work, and how to\u00a0implement them. That\u2019s what this tutorial is about. It\u2019s a multi-part series in which I\u2019m planning to cover the following:</p>\n<ol class=\"three_ol\">\n<li>Introduction to RNNs (this post)</li>\n<li><a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/\" target=\"_blank\">Implementing a RNN using Python and Theano</a></li>\n<li>Understanding the Backpropagation Through Time (BPTT) algorithm and the vanishing gradient problem</li>\n<li>From RNNs to LSTM Networks</li>\n</ol>\n<p>As part of the tutorial we will implement a <a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" onclick=\"javascript:window.open('http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf'); return false;\">recurrent neural network based language model</a>. The applications of\u00a0language models are two-fold: First, it allows us to score arbitrary sentences based on how likely they are to occur in the real world. This gives us a measure of\u00a0grammatical and semantic correctness. Such models are typically used as part of Machine Translation systems. Secondly, a language model allows us to generate new text (I think that\u2019s the much cooler application). Training a language model on Shakespeare allows us to generate\u00a0Shakespeare-like text.\u00a0<a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" onclick=\"javascript:window.open('http://karpathy.github.io/2015/05/21/rnn-effectiveness/'); return false;\">This fun post</a>\u00a0by\u00a0Andrej Karpathy\u00a0demonstrates what character-level language models based\u00a0on RNNs are capable of.</p>\n<p>I\u2019m assuming that you are somewhat familiar with\u00a0basic Neural Networks. If you\u2019re not, you may want to head over to <a href=\"http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\">Implementing A Neural Network From Scratch</a>, \u00a0which guides you through the ideas and implementation behind\u00a0non-recurrent networks.</p>\n<h3>What are RNNs?</h3>\n<p>The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other.\u00a0But for many tasks that\u2019s a very bad idea. If you want to predict the next word in\u00a0a sentence you better know which words came before it.\u00a0RNNs\u00a0are called <em>recurrent</em>\u00a0because they perform the same task for\u00a0every element of a\u00a0sequence, with the output being depended\u00a0on the previous computations. Another way to think about RNNs is that they have a \u201cmemory\u201d which captures information about what has been calculated so far. In theory RNNs can make use\u00a0of information in\u00a0arbitrarily\u00a0long sequences, but in\u00a0practice they are limited to looking back only a few steps (more on this later). Here\u00a0is what a typical RNN looks like: <a href=\"http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg\"><img alt=\"A recurrent neural network and the unfolding in time of the computation involved in its forward computation.\" class=\"wp-image-110\" height=\"280\" src=\"http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg\" width=\"595\"/></a><br>\n<b>Fig. 1\u00a0</b> A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature</br></p>\n<p>The above diagram\u00a0shows a RNN being\u00a0<em>unrolled</em> (or unfolded) into a full network. By unrolling we simply mean that we write out the network for the complete sequence.\u00a0For example, if the sequence we care about is a sentence of\u00a05 words, the network would be unrolled into a 5-layer neural network, one layer for each word.\u00a0The formulas that govern\u00a0the computation happening in a\u00a0RNN are\u00a0as follows:</p>\n<ul class=\"three_ul\">\n<li><img alt=\"x_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=x_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"x_t\"/> is the input at time step <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/>. For example, <img alt=\"x_1\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=x_1&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"x_1\"/>\u00a0could be a\u00a0one-hot vector corresponding to\u00a0the second word of a sentence.</li>\n<li><img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> is the hidden state at time step <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/>. It\u2019s the \u201cmemory\u201d of the network. <img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> is calculated based on the previous hidden state and the input at the current step: <img alt=\"s_t=f(Ux_t + Ws_{t-1})\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t%3Df%28Ux_t+%2B+Ws_%7Bt-1%7D%29&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t=f(Ux_t + Ws_{t-1})\"/>. The function <img alt=\"f\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"f\"/> usually\u00a0is a nonlinearity such as <a href=\"https://reference.wolfram.com/language/ref/Tanh.html\" onclick=\"javascript:window.open('https://reference.wolfram.com/language/ref/Tanh.html'); return false;\">tanh</a> or <a href=\"https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\" onclick=\"javascript:window.open('https://en.wikipedia.org/wiki/Rectifier_(neural_networks)'); return false;\">ReLU</a>. \u00a0<img alt=\"s_{-1}\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_%7B-1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_{-1}\"/>, which is required to calculate the first hidden state, is typically initialized to all zeroes.</li>\n<li><img alt=\"o_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=o_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"o_t\"/> is the output at step <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/>. For example, if we wanted to predict the next word in a sentence it would\u00a0be a vector of probabilities across our vocabulary. <img alt=\"o_t = \\mathrm{softmax}(Vs_t)\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=o_t+%3D+%5Cmathrm%7Bsoftmax%7D%28Vs_t%29&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"o_t = \\mathrm{softmax}(Vs_t)\"/>.</li>\n</ul>\n<p>There are a few things to note here:</p>\n<ul class=\"three_ul\">\n<li>You can think of the hidden state <img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> as the memory of the network. <img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> captures information about what happened in all the\u00a0previous time steps. The output at step <img alt=\"o_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=o_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"o_t\"/> is calculated solely based on the memory at time <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/>. As briefly mentioned above, it\u2019s a bit more complicated \u00a0in practice because <img alt=\"s_t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=s_t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"s_t\"/> typically\u00a0can\u2019t capture information from too many time steps ago.</li>\n<li>Unlike a traditional deep neural network, which uses\u00a0different parameters at each layer, a RNN shares the same parameters (<img alt=\"U, V, W\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=U%2C+V%2C+W&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"U, V, W\"/> above) across\u00a0all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn.</li>\n<li>The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after\u00a0each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its\u00a0hidden state, which captures some information about a sequence.</li>\n</ul>\n<h3>What can RNNs do?</h3>\n<p>RNNs have shown great success in many NLP tasks. At this point I should mention that the most commonly used type of RNNs are <a href=\"https://en.wikipedia.org/wiki/Long_short_term_memory\" onclick=\"javascript:window.open('https://en.wikipedia.org/wiki/Long_short_term_memory'); return false;\">LSTMs</a>, which are much better at capturing long-term dependencies than vanilla RNNs are. But don\u2019t worry, LSTMs are essentially the same thing as the RNN we will develop in this tutorial, they\u00a0just have a different way of computing the hidden state. We\u2019ll cover LSTMs in more detail in a later post. Here are some example applications of RNNs in NLP (by non means an exhaustive list).</p>\n<h4>Language Modeling\u00a0and Generating Text</h4>\n<p>Given a sequence of words we want to predict the probability of each word given the previous words. Language Models allow us to measure how likely a sentence is, which is an important input for Machine Translation (since high-probability sentences are typically correct). A side-effect of being able to predict the next word is that we get a <em>generative</em> model, which allows us to generate new text by sampling from the output probabilities. And depending on what our training data is we can generate <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" onclick=\"javascript:window.open('http://karpathy.github.io/2015/05/21/rnn-effectiveness/'); return false;\">all kinds of stuff</a>. In Language Modeling our input is typically a sequence of words (encoded as one-hot vectors for example), and our output is the sequence of predicted words. When training the network we set <img alt=\"o_t = x_{t+1}\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=o_t+%3D+x_%7Bt%2B1%7D&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"o_t = x_{t+1}\"/> since we want the output at step <img alt=\"t\" class=\"latex\" src=\"//s0.wp.com/latex.php?latex=t&amp;bg=ffffff&amp;fg=000&amp;s=0\" title=\"t\"/> to be the actual next word.</p>\n<p>Research papers about\u00a0Language Modeling and Generating Text:</p>\n<ul class=\"three_ul\">\n<li><a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf\" onclick=\"javascript:window.open('http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf'); return false;\" target=\"_blank\">Recurrent neural network based language model</a></li>\n<li><a href=\"http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf\" onclick=\"javascript:window.open('http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf'); return false;\" target=\"_blank\">Extensions of Recurrent neural network based language model</a></li>\n<li><a href=\"http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf\" onclick=\"javascript:window.open('http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf'); return false;\" target=\"_blank\">Generating Text with Recurrent Neural Networks</a></li>\n</ul>\n</div> ", "website": "kdnuggets"}