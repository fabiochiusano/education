{"content": "By Matthew Rubashkin & Matt Mollison, Silicon Valley Data Science. On the deep learning R&D team at SVDS, we have investigated Recurrent Neural Networks (RNN) for exploring time series and developing speech recognition capabilities. Many products today rely on deep neural networks that implement recurrent layers, including products made by companies like Google , Baidu , and Amazon . However, when developing our own RNN pipelines, we did not find many simple and straightforward examples of using neural networks for sequence learning applications like speech recognition. Many examples were either powerful but quite complex, like the actively developed DeepSpeech project from Mozilla under Mozilla Public License, or were too simple and abstract to be used on real data. In this post, we\u2019ll provide a short tutorial for training a RNN for speech recognition; we\u2019re including code snippets throughout, and you can find the accompanying GitHub repository here . The software we\u2019re using is a mix of borrowed and inspired code from existing open source projects. Below is a video example of machine speech recognition on a 1906 Edison Phonograph advertisement . The video includes a running trace of sound amplitude, extracted spectrogram, and predicted text. Since we have extensive experience with Python, we used a well-documented package that has been advancing by leaps and bounds : TensorFlow . Before you get started, if you are brand new to RNNs, we highly recommend you read Christopher Olah\u2019s excellent overview of RNN Long Short-Term Memory (LSTM) networks here . Speech recognition: audio and transcriptions \u00a0 Until the 2010\u2019s, the state-of-the-art for speech recognition models were phonetic -based approaches including separate components for pronunciation, acoustic , and language models. Speech recognition in the past and today both rely on decomposing sound waves into frequency and amplitude using fourier transforms, yielding a spectrogram as shown below. Training the acoustic model for a traditional speech recognition pipeline that uses Hidden Markov Models (HMM) requires\u00a0speech+text data, as well as\u00a0a word to phoneme dictionary. HMMs are generative probabilistic models for sequential data, and are typically evaluated using Levenshtein word error distance , a string metric for measuring differences in strings. These models can be simplified and made more accurate with speech data that is aligned with phoneme transcriptions, but this a tedious manual task. Because of this effort, phoneme-level transcriptions are less likely to exist for large sets of speech data than word-level transcriptions. For more information on existing open source speech recognition tools and models, check out our colleague Cindi Thompson\u2019s recent post . Connectionist Temporal Classification (CTC) loss function \u00a0 We can discard the concept of phonemes when using neural networks for speech recognition by using an objective function that allows for the prediction of character-level transcriptions: Connectionist Temporal Classification (CTC). Briefly, CTC enables the computation of probabilities of multiple sequences, where the sequences are the set of all possible character-level transcriptions of the speech sample. The network uses the objective function to maximize the probability of the character sequence (i.e., chooses the most likely transcription), and calculates the error for the predicted result compared to the actual transcription to update network weights during training. It is important to note that the character -level error used by a CTC loss function differs from the Levenshtein word error distance often used in traditional speech recognition models. For character generating RNNs, the character and word error distance will be similar in phonetic languages such as Esperonto and Croatian, where individual sounds correspond to distinct characters. Conversely, the character versus word error will be quite different for a non-phonetic language like English. If you want to learn more about CTC, there are many papers and blog posts that explain it in more detail. We will use TensorFlow\u2019s CTC implementation , and there continues to be research and improvements on CTC-related implementations, such as this recent paper from Baidu. In order to utilize algorithms developed for traditional or deep learning speech recognition models, our team structured our speech recognition platform for modularity and fast prototyping: \u00a0 Importance of data \u00a0 It should be no surprise that creating a system that transforms speech into its textual representation requires having (1) digital audio files and (2) transcriptions of the words that were spoken. Because the model should generalize to decode any new speech samples, the more examples we can train the system on, the better it will perform. We researched freely available recordings of transcribed English speech; some examples that we have used for training are LibriSpeech (1000 hours), TED-LIUM (118 hours), and VoxForge (130 hours). The chart below includes information on these datasets including total size in hours, sampling rate, and annotation. In order to easily access data from any data source, we store all data in a flat format. This flat format has a single .wav and a single .txt per datum. For example, you can find example Librispeech Training datum \u2018211-122425-0059\u2019 in our GitHub repo as 211-122425-0059.wav and 211-122425-0059.txt. These data filenames are loaded into the TensorFlow graph using a datasets object class, that assists TensorFlow in efficiently loading, preprocessing the data, and loading individual batches of data from CPU to GPU memory. An example of the data fields in the datasets object is shown below: Feature representation \u00a0 In order for a machine to recognize audio data, the data must first be converted from the time to the frequency domain. There are several methods for creating features for machine learning of audio data, including binning by arbitrary frequencies (i.e., every 100Hz), or by using binning that matches the frequency bands of the human ear. This typical human-centric transformation for speech data is to compute Mel-frequency cepstral coefficients (MFCC), either 13 or 26 different cepstral features, as input for the model. After this transformation the data is stored as a matrix of frequency coefficients (rows) over time (columns). Because speech sounds do not occur in isolation and do not have a one-to-one mapping to characters, we can capture the effects of coarticulation (the articulation of one sound influencing the articulation of another) by training the network on overlapping windows (10s of milliseconds) of audio data that captures sound from before and after the current time index. Example code of how to obtain MFCC features, and how to create windows of audio data is shown below: For our RNN example, we use 9 time slices before and 9 after, for a total of 19 time points per window.With 26 cepstral coefficients, this is 494 data points per 25 ms observation.\u00a0Depending on the data sampling rate, we recommend 26 cepstral features for 16,000 Hz and 13 cepstral features for 8,000 hz. Below is an example of data loading windows on 8,000 Hz data: If you would like to learn more about converting analog to digital sound for RNN speech recognition, check out Adam Geitgey\u2019s machine learning post . Modeling the sequential nature of speech \u00a0 Long Short-Term Memory (LSTM) layers are a type of recurrent neural network (RNN) architecture that are useful for modeling data that has long-term sequential dependencies. They are important for time series data because they essentially remember past information at the current time point, which influences their output. This context is useful for speech recognition because of its temporal nature. If you would like to see how LSTM cells are instantiated in TensorFlow, we\u2019ve include example code below from the\u00a0LSTM layer of our\u00a0DeepSpeech-inspired\u00a0Bi-Directional Neural Network (BiRNN). For more details about this type of network architecture, there are some excellent overviews of how RNNs and LSTM cells work. Additionally, there continues to be research on alternatives to using RNNs for speech recognition, such as with convolutional layers which are more computationally efficient than RNNs. Network training and monitoring \u00a0 Because we trained our network using TensorFlow, we were able to visualize the computational graph as well as monitor the training, validation, and test performance from a web portal with very little extra effort using TensorBoard . Using tips from Dandelion Mane\u2019s great talk at the 2017TensorFlow Dev Summit, we utilize tf.name_scope to add node and layer names, and write out our summary to file. The results of this is an automatically generated, understandable computational graph, such as this example of a Bi-Directional Neural Network (BiRNN) below. The data is passed amongst different operations from bottom left to top right. The different nodes can be labelled and colored with namespaces for clarity. In this example, teal \u2018fc\u2019 boxes correspond to fully connected layers, and the green \u2018b\u2019 and \u2018h\u2019 boxes correspond to biases and weights, respectively. We utilized the TensorFlow provided tf.train.AdamOptimizer to control the learning rate. The AdamOptimizer improves on traditional gradient descent by using momentum ( moving averages of the parameters ), facilitating efficient dynamic adjustment of hyperparameters. We can track the loss and error rate by creating summary scalars of the label error rate: How to improve an RNN \u00a0 Now that we have built a simple LSTM RNN network, how do we improve our error rate? Luckily for the open source community, many large companies have published the math that underlies their best performing speech recognition models. In September 2016, Microsoft released a paper in arXiv describing how they achieved a 6.9% error rate on the NIST 200 Switchboard data. They utilized several different acoustic and language models on top of their convolutional+recurrent neural network. Several key improvements that have been made by the Microsoft team and other researchers in the past 4 years include: using language models on top of character based RNNs using convolutional neural nets (CNNs) for extracting features from the audio ensemble models that utilize multiple RNNs It is important to note that the language models that were pioneered in traditional speech recognition models of the past few decades, are again proving valuable in the deep learning speech recognition models. Modified From: A Historical Perspective of Speech Recognition, Xuedong Huang, James Baker, Raj Reddy Communications of the ACM, Vol. 57 No. 1, Pages 94-103, 2014 Training your first RNN \u00a0 We have provided a GitHub repository with a script that provides a working and straightforward implementation of the steps required to train an end-to-end speech recognition system using RNNs and the CTC loss function in TensorFlow. We have included example data from the LibriVox corpus in the repository . The data is separated into folders: Train: train-clean-100-wav (5 examples) Test: test-clean-wav (2 examples) Dev: dev-clean-wav (2 examples) When training these handful of examples, you will quickly notice that the training data will be overfit to ~0% word error rate (WER), while the Test and Dev sets will be at ~85% WER. The reason the test error rate is not 100% is because out of the 29 possible character choices (a-z, apostrophe, space, blank), the network will quickly learn that: certain characters (e, a, space, r, s, t) are more common consonant-vowel-consonant is a pattern in English increased signal amplitude of the MFCC input sound features corresponds to characters a-z The results of a training run using the default configurations in the github repository is shown below: If you would like to train a performant model, you can add additional .wav and .txt files to these folders, or create a new folder and update ` configs/neural_network.ini ` with the folder locations. Note that it can take quite a lot of computational power to process and train on just a few hundred hours of audio, even with a powerful GPU. We hope that our provided repo is a useful resource for getting started\u2014please share your experiences with adopting RNNs in the comments. To stay in touch, sign up for our newsletter or contact us . Matthew Rubashkin , with a background in optical physics and biomedical research, has a broad range of experiences in software development, database engineering, and data analytics. He enjoys working closely with clients to develop straightforward and robust solutions to difficult problems. Matt Mollison , with a background in cognitive psychology and neuroscience, has extensive experience in hypothesis testing and the analysis of complex datasets. He is excited about using predictive models and other statistical methods to solve real-world problems. Original . Reposted with permission. Related: Getting Started with Deep Learning The Anatomy of Deep Learning Frameworks Open Source Toolkits for Speech Recognition", "title_html": "<h1 id=\"title\">Building, Training, and Improving on Existing Recurrent Neural Networks</h1> ", "url": "https://www.kdnuggets.com/2017/05/building-training-improving-existing-recurrent-neural-networks.html", "tfidf": {"tfidf": {"after": 3.06210621063, "hand": 1.6152202665600002, "real": 2.28103448276, "matthew": 13.817232375979998, "christoph": 5.41473396999, "googl": 11.388809182200001, "web": 5.17133550489, "huang": 66.9873417722, "rubashkin": 3175.2, "occur": 1.7453825857499998, "realworld": 1587.6, "txt": 2886.54545454, "signal": 5.12459651388, "rememb": 4.88793103448, "new": 3.0536641662, "esperonto": 1587.6, "extra": 5.33826496301, "addit": 2.49269901084, "would": 3.2486187845399996, "touch": 5.45567010309, "tradit": 8.04010938925, "audio": 72.4517969196, "recogn": 2.54954231572, "the": 95.0, "dataset": 774.439024392, "function": 12.477208424999999, "class": 2.11651779763, "client": 14.1371326803, "descent": 8.494382022469999, "assist": 2.17300848618, "summit": 12.2311248074, "python": 56.2978723404, "well": 2.1311497416, "product": 3.24529844644, "sequenti": 118.7730673317, "amplitud": 200.11764705870002, "approach": 2.07556543339, "datum": 547.448275862, "yield": 6.46943765281, "their": 3.0464372521500005, "psycholog": 6.6846315789499995, "phonet": 87.2307692308, "dev": 445.12149532800004, "automat": 6.787516032490001, "momentum": 16.835630965, "measur": 2.41093394077, "bound": 5.40735694823, "creat": 6.2464589235000005, "enjoy": 3.3269069572500003, "multipl": 5.49627834516, "voxforg": 1587.6, "how": 11.21752296357, "test": 13.285355648549999, "repost": 933.882352941, "transcrib": 30.472168906, "cindi": 78.2068965517, "numwav": 1587.6, "origin": 1.13724928367, "jame": 1.9313868613099998, "increas": 1.32024948025, "tutori": 59.4606741573, "hundr": 2.4698195395099996, "devcleanwav": 1587.6, "for": 41.012916640410005, "librispeech": 3175.2, "deep": 21.76782449724, "difficult": 2.48957189901, "will": 9.79848788768, "overview": 25.3610223642, "birnn": 3175.2, "open": 4.98226894712, "contact": 3.2196309065099995, "accompani": 3.38146964856, "snippet": 135.692307692, "repo": 738.418604652, "amongst": 5.6478121664900005, "below": 20.30467528773, "perform": 6.125590817000001, "but": 2.03264835798, "certain": 1.8077886586200003, "obtain": 2.68629441624, "our": 28.29106029108, "classif": 16.134146341460003, "annot": 22.8760806916, "background": 8.05479452054, "object": 9.395472703080001, "point": 3.7797000238200003, "record": 1.42334588488, "video": 6.59439252336, "applic": 3.42672134686, "melfrequ": 1587.6, "hidden": 7.81299212598, "word": 12.575761004869998, "level": 1.6544393497299998, "decad": 2.1390460792200003, "has": 5.218248751, "have": 10.1489484114, "paramet": 17.256521739100002, "tensorflow": 12700.8, "adjust": 7.112903225810001, "pass": 1.61818367139, "neurosci": 66.7058823529, "experi": 7.48250265112, "default": 21.1398135819, "namespac": 466.941176471, "resourc": 2.9487369985100003, "green": 2.63065451533, "out": 4.24066777964, "xuedong": 1587.6, "model": 45.9931524888, "format": 5.0625, "quit": 8.65491550065, "excit": 9.818181818180001, "concept": 2.65707112971, "mollison": 3175.2, "under": 2.1563327674, "sound": 24.90352941176, "reli": 8.32293577982, "hyperparamet": 1587.6, "field": 1.7790228597, "shortterm": 3175.2, "loss": 9.70119156736, "short": 1.41295834817, "not": 4.06269592476, "arbitrari": 17.8181818182, "compon": 4.09491875161, "phonemelevel": 1587.6, "team": 6.82447342026, "altern": 2.1390460792200003, "read": 2.3149606299200003, "mane": 120.27272727299999, "quick": 4.41, "speechtext": 1587.6, "adopt": 2.0442956477000003, "metric": 22.235294117600002, "instanti": 240.545454545, "english": 5.2298232129, "index": 6.9969149405, "separ": 3.2024205748799996, "they": 4.12069301148, "anoth": 1.13643521832, "recurr": 106.78923766829999, "teal": 133.411764706, "effect": 1.3963060686000002, "allow": 1.2716059271100002, "wav": 3175.2, "scienc": 2.31969608416, "task": 3.88641370869, "nonphonet": 1587.6, "welldocu": 1587.6, "blog": 14.1876675603, "decompos": 49.6125, "github": 6350.4, "distanc": 10.426444833629999, "node": 88.6927374302, "featur": 12.21700654096, "connectionist": 3175.2, "wordlevel": 1587.6, "filenam": 496.125, "simpl": 10.19434931508, "influenc": 3.54493692084, "that": 27.107569721249998, "algorithm": 27.9507042254, "valley": 4.21673306773, "either": 3.1660185462199997, "longterm": 512.129032258, "develop": 7.173431734320001, "observ": 2.22446406053, "traincleannumwav": 1587.6, "architectur": 10.25581395348, "sever": 3.2172385841699995, "explor": 3.39593582888, "thompson": 9.91630231106, "network": 44.09279529496, "math": 22.0806675939, "started\u2014pleas": 1587.6, "lot": 4.40877534018, "box": 8.25370418508, "sourc": 8.4880239521, "control": 1.46959178006, "both": 1.05215720061, "locat": 1.59766529134, "result": 3.43834825296, "repositori": 179.89801699720002, "use": 30.889162721399998, "languag": 13.76929748484, "set": 3.56123822343, "ani": 2.26767604628, "licens": 5.79839298758, "seri": 2.93023255814, "and": 57.00359055141, "from": 15.00850822455, "apostroph": 191.277108434, "work": 3.34560269739, "detail": 4.52372132782, "num": 37.01165648037, "notic": 4.36994219653, "cepstral": 7938.0, "spectrogram": 3175.2, "left": 1.4398693996, "mozilla": 635.04, "depend": 4.4822134387400006, "predict": 20.7393860222, "output": 7.676982591880001, "match": 3.5676404494400002, "bottom": 6.27261951798, "fulli": 2.79015817223, "simplifi": 12.109839816900001, "with": 15.017973134849997, "acoust": 60.1363636365, "are": 17.50840090826, "solut": 4.7278141751, "better": 2.0065722952500002, "compani": 3.1047227926, "raj": 58.1538461538, "effort": 3.78495649064, "current": 3.0651607298, "wave": 4.4595505618, "preprocess": 1221.23076923, "stay": 2.6986231514499996, "histor": 1.6755672823199999, "shown": 11.07692307692, "summari": 15.60294840294, "public": 1.22424429365, "veri": 1.25880114177, "extract": 15.406113537120001, "valid": 6.61224489796, "brand": 5.5259310824900005, "discard": 19.1277108434, "align": 8.10413476263, "numtensorflow": 1587.6, "articul": 34.2894168466, "util": 23.299090108599998, "net": 6.96315789474, "column": 7.078020508250001, "again": 1.50883862384, "compar": 1.8662278123900002, "decod": 51.713355048900006, "map": 4.0728578758300005, "markov": 174.46153846200002, "even": 1.16461267606, "convers": 3.3486606201200004, "top": 5.516330785260001, "littl": 1.5499365420299998, "bidirect": 577.30909091, "rnns": 14288.4, "total": 3.0920245398799997, "tfnamescop": 1587.6, "train": 34.8582581118, "great": 1.26592775696, "windowwith": 1587.6, "step": 2.8279301745599996, "anatomi": 25.0410094637, "investig": 3.11721971333, "librivox": 661.5, "move": 1.29125660838, "store": 6.89361702128, "continu": 2.27857911734, "type": 4.056208482380001, "numhz": 1587.6, "note": 4.27348586811, "key": 2.28005170185, "phonem": 253.3404255318, "row": 5.549108703250001, "probabl": 5.29111814698, "machin": 16.09733840304, "into": 4.06009845916, "space": 4.79637462236, "paper": 7.988594431410001, "bias": 13.7335640138, "levenshtein": 3175.2, "numtxt": 1587.6, "requir": 4.58534706846, "coeffici": 109.4896551723, "edison": 41.2363636364, "choos": 4.17899447223, "humancentr": 1587.6, "coarticul": 1587.6, "overfit": 1587.6, "testcleanwav": 1587.6, "larg": 2.3714989917, "find": 5.188235294129999, "briefli": 4.8669527897, "overlap": 12.0913937548, "borrow": 8.02223345124, "actual": 1.87482286254, "matt": 22.744985673400002, "own": 1.17844418052, "base": 2.2925631769, "natur": 3.0785340314200003, "label": 8.95431472082, "hour": 11.2980358668, "vol": 2.92375690608, "permiss": 6.280063291139999, "post": 8.95305230508, "here": 4.84615384616, "hmms": 1587.6, "microsoft": 49.690140845, "were": 6.14753146176, "averag": 2.60390355913, "updat": 11.12933753944, "comput": 23.566551212280004, "long": 2.5314518057799997, "human": 1.8965476048299998, "layer": 48.849230769239995, "evalu": 6.9509632224199995, "than": 2.0655737705, "share": 1.8566249561500001, "about": 4.25944060636, "problem": 3.53349655018, "communiti": 1.96121062384, "neural": 535.1460674157, "distinct": 2.2836593786000003, "context": 4.25972632144, "when": 3.0623030926499997, "just": 1.33580143037, "perspect": 5.03520456708, "adam": 4.43092380687, "convolutionalrecurr": 1587.6, "pioneer": 4.74051955808, "olah": 1134.0, "dure": 1.0503473370799998, "frequenc": 44.051054384, "represent": 11.85660941, "blank": 23.3470588235, "reddi": 176.4, "check": 13.0131147541, "septemb": 1.45638014861, "recognit": 96.80487804878, "switchboard": 299.547169811, "recommend": 7.828402366860001, "dynam": 6.52527743527, "where": 2.13430127042, "ensembl": 16.746835443, "complex": 4.6804245283, "communic": 2.8395635843299996, "tool": 4.99716713881, "start": 2.53347163488, "run": 3.11385701676, "enabl": 3.5421686747, "load": 27.21988855552, "dictionari": 5.2292490118599995, "explain": 2.60049140049, "order": 3.7387550043299997, "digit": 8.832267037560001, "until": 1.14852058164, "track": 3.1276595744700004, "abl": 1.8208510150200001, "mani": 5.22133789385, "string": 16.75567282322, "geitgey": 1587.6, "colleagu": 8.23443983402, "analysi": 3.47852760736, "transcript": 119.8691275167, "provid": 6.07763570935, "monitor": 12.11446012972, "calcul": 6.12972972973, "error": 72.49315068492001, "broad": 4.27693965517, "biomed": 50.8846153846, "framework": 8.200413223139998, "too": 1.81585268215, "singl": 3.21897810218, "throughout": 1.5217099587799998, "graph": 113.1306413301, "there": 5.20456333595, "exist": 4.39413230004, "endtoend": 1587.6, "connect": 1.8843916913900003, "inform": 4.72593768606, "chart": 8.45367412141, "trace": 4.4408391608399995, "matrix": 22.6153846154, "baidu": 2268.0, "modifi": 4.45329593268, "best": 1.5828514456600002, "exampl": 28.59184834118, "right": 1.4054532577899999, "deepspeech": 1587.6, "configsneuralnetworkini": 1587.6, "baker": 10.5348374253, "softwar": 20.5248868778, "take": 1.13961668222, "visual": 5.22752716497, "now": 1.160780873, "robust": 19.9447236181, "configur": 11.504347826099998, "name": 1.10211732037, "code": 15.522855047679998, "per": 5.879274163679999, "tip": 9.42195845697, "high": 1.14777327935, "possibl": 2.8347468976, "color": 3.8255421686699997, "publish": 1.36885669943, "power": 4.018901358539999, "oper": 1.55479384977, "spoken": 7.06856634016, "surpris": 4.36633663366, "typic": 4.508306119559999, "hypothesi": 13.580838323399998, "pattern": 3.79173632673, "textual": 41.4516971279, "modular": 49.9245283019, "becaus": 8.046629498249999, "fourier": 88.6927374302, "portal": 16.9615384615, "batch": 35.6764044944, "research": 9.710091743100001, "script": 8.299006795610001, "over": 1.02525024217, "platform": 6.2332155476999995, "past": 8.06809808156, "facilit": 6.453658536590001, "newslett": 30.068181818200003, "want": 1.99698113208, "captur": 5.76052249638, "convert": 6.5481542586199994, "talk": 3.0303493033, "ear": 12.6, "system": 4.16219522853, "individu": 3.6008165116800006, "clariti": 31.3136094675, "extens": 3.9834399699, "howev": 1.0945191313299998, "essenti": 2.9280708225700005, "prove": 2.45720476706, "cognit": 21.454054054100002, "manual": 7.72930866602, "silicon": 31.8795180723, "maxim": 12.928338762200001, "which": 2.01038369, "databas": 8.24727272727, "such": 4.24605509496, "activ": 1.46403541129, "abstract": 9.966101694919999, "other": 2.01984732824, "one": 1.00627495722, "inspir": 2.8487349721900004, "fast": 4.8729281768, "hope": 2.50884955752, "see": 1.27242125511, "some": 2.08073394496, "bin": 44.6582278482, "corpus": 24.091047041, "get": 5.35687774155, "like": 10.342671009750001, "implement": 14.30592475784, "dandelion": 396.9, "croatian": 30.8871595331, "should": 3.3286508019800003, "file": 11.313064133009998, "process": 1.69524826482, "add": 9.22486926206, "transform": 13.68031021112, "analyt": 17.256521739100002, "memori": 7.7217898832700005, "includ": 10.190641247799999, "sign": 1.7606742819099999, "recent": 3.0881151527, "scalar": 98.6086956522, "slice": 35.3585746102, "tedious": 93.3882352941, "access": 1.8734953976900002, "consonantvowelconson": 1587.6, "deepspeechinspir": 1587.6, "sampl": 28.93120728928, "rang": 1.7848229342299997, "time": 8.09019682784, "engin": 2.47135740971, "relat": 1.23750876919, "flat": 11.35622317596, "similar": 1.37514075357, "pronunci": 21.6885245902, "project": 3.5069582505000003, "differ": 8.65581431575, "been": 2.0478555304799997, "statist": 4.24265098878, "valuabl": 7.46754468485, "most": 1.02096463023, "optic": 14.8930581614, "phonograph": 86.28260869569999, "domain": 9.39408284024, "excel": 9.68935001526, "all": 2.02293577982, "charact": 27.68923418426, "input": 24.4058416602, "today": 3.49922856514, "tftrainadamoptim": 1587.6, "reason": 1.72340425532, "physic": 2.39132399458, "gradient": 41.889182058, "cell": 14.2067114094, "less": 1.46904783936, "solv": 7.26923076923, "releas": 1.8377126982299998, "structur": 2.0580762250499998, "page": 2.03669018602, "weight": 9.757836508919999, "isol": 4.86397058824, "built": 1.99447236181, "rate": 19.264392611549997, "more": 9.154536135299999, "improv": 10.218846549950001, "leap": 19.6242274413, "correspond": 13.29926701572, "respect": 1.6443293630200002, "achiev": 1.87216981132, "these": 5.3707713126000005, "ctcrelat": 1587.6, "capabl": 3.6580645161300005, "general": 1.1218202374200001, "made": 3.21116504853, "straightforward": 83.2657342656, "befor": 3.30108123093, "comment": 3.05954904606, "probabilist": 127.008, "close": 1.2848818387799998, "window": 17.594384927969998, "can": 11.7626139142, "avail": 1.7288467821, "describ": 1.47027227264, "analog": 9.05131128848, "write": 2.0575427682700003, "folder": 675.574468084, "size": 2.49387370405, "few": 2.63458347162, "band": 3.44680851064, "amazon": 33.1440501044, "this": 13.049317147230001, "speech": 118.50614013958001, "accur": 5.768895348840001, "millisecond": 138.052173913, "packag": 7.828402366860001, "choic": 3.1319786940200003, "advanc": 1.9997480791, "common": 1.4025974025999999, "freeli": 11.3643521832, "data": 111.42237345822, "cnns": 1587.6, "method": 5.1428571428600005, "sequenc": 24.28451242828, "adamoptim": 1587.6, "versus": 7.77473065622, "sinc": 1.08368600683, "convolut": 202.242038216, "must": 1.9220338983099996, "mix": 2.7852631578900002, "stateoftheart": 1587.6, "advertis": 5.61187698834, "text": 3.12827586207, "understand": 2.96858638743, "import": 5.360796893480001, "first": 2.01523229246, "everi": 1.47917637194, "luckili": 191.277108434, "pipelin": 64.2753036438, "toolkit": 189.0, "generat": 6.15826221876, "characterlevel": 3175.2, "while": 1.0441988950299999, "tempor": 65.69379310349998, "onetoon": 1587.6, "year": 1.0485436893200002, "easili": 3.6938110749199997, "often": 1.29452054795, "prototyp": 11.7426035503, "tensorboard": 1587.6, "learn": 27.873006583800002, "arxiv": 441.0, "effici": 15.280076997120002}, "logtfidf": {"after": 0.061472083944299996, "hand": 0.479471335336, "real": 0.824629060574, "matthew": 3.8655387109800006, "christoph": 1.6891237509, "googl": 2.43263122258, "web": 1.6431309733200001, "huang": 4.20450367277, "rubashkin": 14.739957441820001, "occur": 0.556973778473, "realworld": 7.369978720910001, "txt": 14.549337082200001, "signal": 1.6340517929299998, "rememb": 1.5867691126199999, "new": 0.0531898405533, "esperonto": 7.369978720910001, "extra": 1.67490068688, "addit": 0.440437765944, "would": 0.23885288389409998, "touch": 1.6966554537399998, "tradit": 2.3750238814599998, "audio": 17.627839431840002, "recogn": 0.935913859031, "the": 0.0, "dataset": 21.06337826656, "function": 4.57232870797, "class": 0.7497721899330001, "client": 2.6488048591599997, "descent": 2.13940500645, "assist": 0.776112606548, "summit": 2.50398391664, "python": 4.03065674296, "well": 0.1270288766312, "product": 0.968120273072, "sequenti": 11.035807159800001, "amplitud": 12.600879420690001, "approach": 0.7302336145810001, "datum": 11.22424160672, "yield": 1.86708918863, "their": 0.046081515368100005, "psycholog": 1.89981109743, "phonet": 7.55081989254, "dev": 14.999204938320002, "automat": 1.9150850473199998, "momentum": 2.82349753127, "measur": 0.880014199726, "bound": 1.68776042417, "creat": 1.1128840925699999, "enjoy": 1.2020430306899998, "multipl": 2.02184803624, "voxforg": 7.369978720910001, "how": 3.30096869851, "test": 4.886122185515, "repost": 6.83935046985, "transcrib": 3.41681377215, "cindi": 4.35935783486, "numwav": 7.369978720910001, "origin": 0.128612437587, "jame": 0.658238325853, "increas": 0.277820718929, "tutori": 4.0853151555, "hundr": 0.904145087046, "devcleanwav": 7.369978720910001, "for": 0.012914606211277001, "librispeech": 14.739957441820001, "deep": 7.7320408188, "difficult": 0.912110767588, "will": 1.62229227932, "overview": 5.08013252448, "birnn": 14.739957441820001, "open": 0.878364152116, "contact": 1.16926672768, "accompani": 1.21831042226, "snippet": 4.91038987911, "repo": 11.82272739642, "amongst": 1.7312682430000002, "below": 7.322639327424, "perform": 1.70472340232, "but": 0.0323847441438, "certain": 0.592104362781, "obtain": 0.988162703503, "our": 10.291670570184001, "classif": 4.17558147258, "annot": 3.1300918533999997, "background": 2.7862406523799996, "object": 3.415734339212, "point": 0.6930970770989999, "record": 0.353010356953, "video": 2.38614497934, "applic": 1.23160392849, "melfrequ": 7.369978720910001, "hidden": 2.0557880052, "word": 4.101027576695, "level": 0.503462189943, "decad": 0.760359972282, "has": 0.213619724274, "have": 0.14785002341200001, "paramet": 2.8481901438599997, "tensorflow": 58.959829767280006, "adjust": 1.9619104904, "pass": 0.48130432974, "neurosci": 4.20029314023, "experi": 2.505091815732, "default": 3.0511581621399997, "namespac": 6.14620328929, "resourc": 1.08137694258, "green": 0.9672326803710001, "out": 0.2337055636772, "xuedong": 7.369978720910001, "model": 16.223901608442002, "format": 1.8574265037459998, "quit": 3.17854541052, "excit": 2.28423595433, "concept": 0.977224437103, "mollison": 14.739957441820001, "under": 0.15052361076639997, "sound": 9.08454396152, "reli": 2.85173574036, "hyperparamet": 7.369978720910001, "field": 0.5760642583510001, "shortterm": 14.739957441820001, "loss": 3.543817435368, "short": 0.345685625679, "not": 0.06220965203, "arbitrari": 2.88021938643, "compon": 1.40974687623, "phonemelevel": 7.369978720910001, "team": 2.465708684658, "altern": 0.760359972282, "read": 0.83939268088, "mane": 4.7897618913199995, "quick": 1.581455017798, "speechtext": 7.369978720910001, "adopt": 0.7150533036110001, "metric": 3.1016808515599994, "instanti": 5.482909071880001, "english": 1.6672955590050003, "index": 1.94546932912, "separ": 0.941519545898, "they": 0.1189079790704, "anoth": 0.127896361652, "recurr": 10.716734585640001, "teal": 4.89344032079, "effect": 0.333830227158, "allow": 0.24028061118900002, "wav": 14.739957441820001, "scienc": 0.841436178891, "task": 1.35748680661, "nonphonet": 7.369978720910001, "welldocu": 7.369978720910001, "blog": 2.65237310559, "decompos": 3.9042428181099997, "github": 29.479914883640003, "distanc": 3.73719918771, "node": 7.584061655, "featur": 3.387099345136, "connectionist": 14.739957441820001, "wordlevel": 7.369978720910001, "filenam": 6.2068279111, "simpl": 3.66966386817, "influenc": 1.144746370856, "that": 0.10735600625028, "algorithm": 3.33044239518, "valley": 1.4390606736700002, "either": 0.91865527763, "longterm": 6.238576609419999, "develop": 1.071748169478, "observ": 0.7995160149320001, "traincleannumwav": 7.369978720910001, "architectur": 3.26939515838, "sever": 0.20973336119069996, "explor": 1.22257937218, "thompson": 2.29418010091, "network": 16.202411901884, "math": 3.09470245618, "started\u2014pleas": 7.369978720910001, "lot": 1.4835969502500002, "box": 2.8350298223, "sourc": 2.6460915537550003, "control": 0.38498466158600003, "both": 0.050842533389300004, "locat": 0.46854337067199997, "result": 0.40913672514300004, "repositori": 15.224383027879998, "use": 0.8762405919480001, "languag": 4.984090946436, "set": 0.5144880338669999, "ani": 0.251216716732, "licens": 1.7575808080500002, "seri": 0.7638692213959999, "and": 0.0035904380976252, "from": 0.008505812532989999, "apostroph": 5.25372320611, "work": 0.327103701819, "detail": 1.632375554346, "num": 0.011654644629689001, "notic": 1.47474978168, "cepstral": 36.849893604550005, "spectrogram": 14.739957441820001, "left": 0.364552414753, "mozilla": 11.52108161694, "depend": 1.61393963, "predict": 6.5829609507599995, "output": 2.03822657827, "match": 1.27190443874, "bottom": 1.8361940533599999, "fulli": 1.02609828678, "simplifi": 2.4940183301400003, "with": 0.01796237570085, "acoust": 8.99400726627, "are": 0.5009470509059, "solut": 1.55346297627, "better": 0.6964279406, "compani": 0.879554506194, "raj": 4.06309201872, "effort": 1.275774422114, "current": 0.8539056556900001, "wave": 1.4950479900600002, "preprocess": 7.1076144564399995, "stay": 0.9927416990379999, "histor": 0.516151783952, "shown": 4.07427832396, "summari": 4.108625432059999, "public": 0.20232375048700002, "veri": 0.230159793238, "extract": 4.08323446602, "valid": 1.8889232176800002, "brand": 1.7094517549200001, "discard": 2.95113811311, "align": 2.09237439596, "numtensorflow": 7.369978720910001, "articul": 5.68337915852, "util": 7.694881981199999, "net": 1.9406330919499999, "column": 1.95699427938, "again": 0.411340231612, "compar": 0.6239191809269999, "decod": 3.9457160663199997, "map": 1.40434493384, "markov": 5.16170430739, "even": 0.152388564834, "convers": 1.2085604509999999, "top": 1.8273019133640003, "littl": 0.438213989466, "bidirect": 11.33046125734, "rnns": 66.32980848819001, "total": 0.8713577734100001, "tfnamescop": 7.369978720910001, "train": 11.896529631102, "great": 0.235805258079, "windowwith": 7.369978720910001, "step": 1.03954505698, "anatomi": 3.22051485947, "investig": 1.13694148702, "librivox": 6.4945099835599995, "move": 0.255615859253, "store": 2.4748974670400004, "continu": 0.26080974797400003, "type": 1.414202970774, "numhz": 7.369978720910001, "note": 1.061452704249, "key": 0.82419811896, "phonem": 13.30836555321, "row": 1.71363732085, "probabl": 1.945764825826, "machin": 5.56943832248, "into": 0.0596514529148, "space": 1.749426329944, "paper": 2.938207618995, "bias": 2.61984276467, "levenshtein": 14.739957441820001, "numtxt": 7.369978720910001, "requir": 1.272760532025, "coeffici": 10.79165334843, "edison": 3.7193204796199995, "choos": 1.43007066072, "humancentr": 7.369978720910001, "coarticul": 7.369978720910001, "overfit": 7.369978720910001, "testcleanwav": 7.369978720910001, "larg": 0.34075012121200005, "find": 1.643343990864, "briefli": 1.5824680307199999, "overlap": 2.4924939396, "borrow": 2.0822168683, "actual": 0.628514181648, "matt": 4.86239506116, "own": 0.164195077421, "base": 0.27304660457400004, "natur": 0.862612678584, "label": 2.99797665454, "hour": 4.075954905385, "vol": 1.07286940097, "permiss": 1.8373800586400002, "post": 3.2228006108039997, "here": 1.7700763767400003, "hmms": 7.369978720910001, "microsoft": 6.42531871906, "were": 0.14574686208659998, "averag": 0.957011687995, "updat": 3.4328749253799997, "comput": 8.208413495639999, "long": 0.471291587756, "human": 0.640035183243, "layer": 12.581874974100002, "evalu": 1.9388802431299998, "than": 0.0645217244364, "share": 0.618760299747, "about": 0.2513739098984, "problem": 1.138281448546, "communiti": 0.673561947791, "neural": 36.7678363995, "distinct": 0.825779146958, "context": 1.44920491442, "when": 0.0616649665752, "just": 0.289531434109, "perspect": 1.61645415436, "adam": 1.4886080966, "convolutionalrecurr": 7.369978720910001, "pioneer": 1.55614674111, "olah": 7.033506484289999, "dure": 0.0491209066894, "frequenc": 10.879556878649998, "represent": 3.5594765752999997, "blank": 3.15047101573, "reddi": 5.17275414357, "check": 3.74562099124, "septemb": 0.375954006775, "recognit": 32.596408519840004, "switchboard": 5.7022719003499995, "recommend": 2.72922253726, "dynam": 1.8756834711200001, "where": 0.1299842774914, "ensembl": 2.81820931165, "complex": 1.7004832728619999, "communic": 1.04365037288, "tool": 1.60887117963, "start": 0.472886738582, "run": 0.885429951078, "enabl": 1.26473915954, "load": 7.67061416752, "dictionari": 1.65426767539, "explain": 0.955700427358, "order": 0.6604211423790001, "digit": 2.9705290875, "until": 0.138474663439, "track": 1.14028498507, "abl": 0.599303982475, "mani": 0.2165787906105, "string": 4.251179392780001, "geitgey": 7.369978720910001, "colleagu": 2.10832533873, "analysi": 1.2466091029200002, "transcript": 23.30258369652, "provid": 0.9758892216250001, "monitor": 3.60250522116, "calcul": 1.8131506592099997, "error": 21.583025211600003, "broad": 1.45323772, "biomed": 3.9295606260900002, "framework": 2.10418454607, "too": 0.5965551547219999, "singl": 0.951833538118, "throughout": 0.41983467543499997, "graph": 10.88979294066, "there": 0.2004894646275, "exist": 1.144973382261, "endtoend": 7.369978720910001, "connect": 0.633605058682, "inform": 1.363361113986, "chart": 2.13460115413, "trace": 1.49084335877, "matrix": 3.1186304098799997, "baidu": 14.067012968579998, "modifi": 1.4936444810499998, "best": 0.459227932947, "exampl": 7.764970824980999, "right": 0.34035985417, "deepspeech": 7.369978720910001, "configsneuralnetworkini": 7.369978720910001, "baker": 2.35468761528, "softwar": 4.65698192666, "take": 0.130691962197, "visual": 1.6539383488600001, "now": 0.149092945021, "robust": 2.9929646280599997, "configur": 2.4427250357499997, "name": 0.09723316638430002, "code": 5.42407638388, "per": 2.018463072216, "tip": 2.2430429711200004, "high": 0.13782378654000002, "possibl": 0.697610949782, "color": 1.3417002006799998, "publish": 0.313975865467, "power": 0.8771888481450001, "oper": 0.441342964347, "spoken": 1.9556576786000002, "surpris": 1.47392435861, "typic": 1.625548638316, "hypothesi": 2.60865985243, "pattern": 1.33282404788, "textual": 3.7245288247199992, "modular": 3.91051243112, "becaus": 0.975402111775, "fourier": 4.48517800806, "portal": 2.8309483374299997, "batch": 3.5744895317400003, "research": 3.3186390906899996, "script": 2.1161358444599996, "over": 0.0249367214957, "platform": 1.8298923389200001, "past": 2.8064936630440003, "facilit": 1.86464718498, "newslett": 3.4034675302, "want": 0.6916366062549999, "captur": 2.1157620024200003, "convert": 2.3720720736, "talk": 1.10867789449, "ear": 2.53369681396, "system": 0.982291036755, "individu": 1.17602689597, "clariti": 3.4440528103099997, "extens": 1.3779971589500002, "howev": 0.0903151173475, "essenti": 1.07434378384, "prove": 0.899024430345, "cognit": 3.0659136276999996, "manual": 2.04501942341, "silicon": 3.46196373688, "maxim": 2.5594217052, "which": 0.01035682769086, "databas": 2.10988256718, "such": 0.238783911224, "activ": 0.381196603284, "abstract": 2.29918950399, "other": 0.01974949583952, "one": 0.0062553516455, "inspir": 1.04687502633, "fast": 1.5836950247400001, "hope": 0.919824304455, "see": 0.240921585492, "some": 0.079147018129, "bin": 6.211782768200001, "corpus": 3.1818402794, "get": 1.739307017346, "like": 1.251482188905, "implement": 5.09751763628, "dandelion": 5.98368435979, "croatian": 3.4303405484500002, "should": 1.018839753516, "file": 3.9820376616899997, "process": 0.527829199025, "add": 3.05751167426, "transform": 4.91865290828, "analyt": 2.8481901438599997, "memori": 2.8363016959799996, "includ": 0.18884681390500002, "sign": 0.565696850403, "recent": 0.868827482576, "scalar": 4.591159448919999, "slice": 3.56554092616, "tedious": 4.53676537685, "access": 0.627805882716, "consonantvowelconson": 7.369978720910001, "deepspeechinspir": 7.369978720910001, "sampl": 7.914505953560001, "rang": 0.579319213803, "time": 0.0896921509008, "engin": 0.904767558276, "relat": 0.21310030165399999, "flat": 3.4732374211000003, "similar": 0.318556092114, "pronunci": 3.07678329994, "project": 1.123203771814, "differ": 1.486247849184, "been": 0.04729196473680001, "statist": 1.4451883070700002, "valuabl": 2.010566255, "most": 0.020747896295599998, "optic": 2.70089520918, "phonograph": 4.45762805629, "domain": 2.24008000599, "excel": 3.1557603304, "all": 0.022805264195599997, "charact": 10.154632479629, "input": 5.00335067078, "today": 1.118790707358, "tftrainadamoptim": 7.369978720910001, "reason": 0.544301552962, "physic": 0.871847185184, "gradient": 3.73502760882, "cell": 3.9211346137199996, "less": 0.3846144626, "solv": 1.9836504770400003, "releas": 0.608521699544, "structur": 0.7217716751350001, "page": 0.711326032411, "weight": 3.16984705224, "isol": 1.5818550978200001, "built": 0.690379535065, "rate": 6.8493048494940005, "more": 0.15322438439999997, "improv": 3.5739790196599994, "leap": 2.9767648968400002, "correspond": 4.80565824396, "respect": 0.49733261904, "achiev": 0.6270980851169999, "these": 0.357668097004, "ctcrelat": 7.369978720910001, "capabl": 1.2969341868100002, "general": 0.114952578063, "made": 0.2040645782919, "straightforward": 9.970274467560001, "befor": 0.2869133156385, "comment": 1.11826753454, "probabilist": 4.8442500766, "close": 0.250666759864, "window": 5.306902572870001, "can": 1.6234109639399998, "avail": 0.547454586289, "describ": 0.385447603125, "analog": 2.20290964097, "write": 0.721512439877, "folder": 20.517076126520003, "size": 0.9138372060609999, "few": 0.551155827306, "band": 1.2374487335200002, "amazon": 3.50086321649, "this": 0.0492238376825, "speech": 41.57030467837001, "accur": 1.75248061485, "millisecond": 4.927631685540001, "packag": 2.0577584491900005, "choic": 1.14166497543, "advanc": 0.6930212121780001, "common": 0.338325805271, "freeli": 2.43048145465, "data": 40.1550792984, "cnns": 7.369978720910001, "method": 1.888923217682, "sequenc": 7.2141777496, "adamoptim": 7.369978720910001, "versus": 2.05087881518, "sinc": 0.0803681994577, "convolut": 9.2326360171, "must": 0.653383947388, "mix": 1.02434236008, "stateoftheart": 7.369978720910001, "advertis": 1.7248852425999999, "text": 1.14048200999, "understand": 1.0880858756799998, "import": 1.171273108264, "first": 0.015174579624319999, "everi": 0.391485427421, "luckili": 5.25372320611, "pipelin": 6.94005659344, "toolkit": 5.24174701506, "generat": 2.1575470252080002, "characterlevel": 14.739957441820001, "while": 0.04324998379380001, "tempor": 9.25917647715, "onetoon": 7.369978720910001, "year": 0.047402238894600005, "easili": 1.3066587367, "often": 0.258140393351, "prototyp": 2.4632235573, "tensorboard": 7.369978720910001, "learn": 10.11302477694, "arxiv": 6.08904487545, "effici": 4.88381260242}, "logidf": {"after": 0.020490694648099998, "hand": 0.479471335336, "real": 0.824629060574, "matthew": 1.9327693554900003, "christoph": 1.6891237509, "googl": 2.43263122258, "web": 1.6431309733200001, "huang": 4.20450367277, "rubashkin": 7.369978720910001, "occur": 0.556973778473, "realworld": 7.369978720910001, "txt": 7.2746685411000005, "signal": 1.6340517929299998, "rememb": 1.5867691126199999, "new": 0.0177299468511, "esperonto": 7.369978720910001, "extra": 1.67490068688, "addit": 0.220218882972, "would": 0.0796176279647, "touch": 1.6966554537399998, "tradit": 0.47500477629199994, "audio": 2.2034799289800002, "recogn": 0.935913859031, "the": 0.0, "dataset": 5.26584456664, "function": 0.914465741594, "class": 0.7497721899330001, "client": 2.6488048591599997, "descent": 2.13940500645, "assist": 0.776112606548, "summit": 2.50398391664, "python": 4.03065674296, "well": 0.0635144383156, "product": 0.484060136536, "sequenti": 3.6786023866, "amplitud": 4.20029314023, "approach": 0.7302336145810001, "datum": 5.61212080336, "yield": 1.86708918863, "their": 0.015360505122700001, "psycholog": 1.89981109743, "phonet": 3.77540994627, "dev": 4.99973497944, "automat": 1.9150850473199998, "momentum": 2.82349753127, "measur": 0.880014199726, "bound": 1.68776042417, "creat": 0.222576818514, "enjoy": 1.2020430306899998, "multipl": 1.01092401812, "voxforg": 7.369978720910001, "how": 0.47156695693000006, "test": 0.977224437103, "repost": 6.83935046985, "transcrib": 3.41681377215, "cindi": 4.35935783486, "numwav": 7.369978720910001, "origin": 0.128612437587, "jame": 0.658238325853, "increas": 0.277820718929, "tutori": 4.0853151555, "hundr": 0.904145087046, "devcleanwav": 7.369978720910001, "for": 0.00031499039539700004, "librispeech": 7.369978720910001, "deep": 1.2886734698, "difficult": 0.912110767588, "will": 0.202786534915, "overview": 2.54006626224, "birnn": 7.369978720910001, "open": 0.219591038029, "contact": 1.16926672768, "accompani": 1.21831042226, "snippet": 4.91038987911, "repo": 5.91136369821, "amongst": 1.7312682430000002, "below": 0.813626591936, "perform": 0.42618085058, "but": 0.0161923720719, "certain": 0.592104362781, "obtain": 0.988162703503, "our": 0.8576392141820001, "classif": 2.08779073629, "annot": 3.1300918533999997, "background": 1.3931203261899998, "object": 0.853933584803, "point": 0.23103235903299998, "record": 0.353010356953, "video": 1.19307248967, "applic": 1.23160392849, "melfrequ": 7.369978720910001, "hidden": 2.0557880052, "word": 0.585861082385, "level": 0.503462189943, "decad": 0.760359972282, "has": 0.0427239448548, "have": 0.0147850023412, "paramet": 2.8481901438599997, "tensorflow": 7.369978720910001, "adjust": 1.9619104904, "pass": 0.48130432974, "neurosci": 4.20029314023, "experi": 0.626272953933, "default": 3.0511581621399997, "namespac": 6.14620328929, "resourc": 1.08137694258, "green": 0.9672326803710001, "out": 0.0584263909193, "xuedong": 7.369978720910001, "model": 0.7374500731110001, "format": 0.9287132518729999, "quit": 1.05951513684, "excit": 2.28423595433, "concept": 0.977224437103, "mollison": 7.369978720910001, "under": 0.07526180538319999, "sound": 1.13556799519, "reli": 1.42586787018, "hyperparamet": 7.369978720910001, "field": 0.5760642583510001, "shortterm": 7.369978720910001, "loss": 0.885954358842, "short": 0.345685625679, "not": 0.0155524130075, "arbitrari": 2.88021938643, "compon": 1.40974687623, "phonemelevel": 7.369978720910001, "team": 0.821902894886, "altern": 0.760359972282, "read": 0.83939268088, "mane": 4.7897618913199995, "quick": 0.790727508899, "speechtext": 7.369978720910001, "adopt": 0.7150533036110001, "metric": 3.1016808515599994, "instanti": 5.482909071880001, "english": 0.555765186335, "index": 1.94546932912, "separ": 0.470759772949, "they": 0.0297269947676, "anoth": 0.127896361652, "recurr": 3.5722448618800002, "teal": 4.89344032079, "effect": 0.333830227158, "allow": 0.24028061118900002, "wav": 7.369978720910001, "scienc": 0.841436178891, "task": 1.35748680661, "nonphonet": 7.369978720910001, "welldocu": 7.369978720910001, "blog": 2.65237310559, "decompos": 3.9042428181099997, "github": 7.369978720910001, "distanc": 1.24573306257, "node": 3.7920308275, "featur": 0.423387418142, "connectionist": 7.369978720910001, "wordlevel": 7.369978720910001, "filenam": 6.2068279111, "simpl": 1.2232212893899999, "influenc": 0.572373185428, "that": 0.00397614837964, "algorithm": 3.33044239518, "valley": 1.4390606736700002, "either": 0.459327638815, "longterm": 6.238576609419999, "develop": 0.178624694913, "observ": 0.7995160149320001, "traincleannumwav": 7.369978720910001, "architectur": 1.63469757919, "sever": 0.06991112039689999, "explor": 1.22257937218, "thompson": 2.29418010091, "network": 0.9530830530519999, "math": 3.09470245618, "started\u2014pleas": 7.369978720910001, "lot": 1.4835969502500002, "box": 1.41751491115, "sourc": 0.529218310751, "control": 0.38498466158600003, "both": 0.050842533389300004, "locat": 0.46854337067199997, "result": 0.136378908381, "repositori": 3.8060957569699996, "use": 0.0292080197316, "languag": 0.8306818244059999, "set": 0.171496011289, "ani": 0.125608358366, "licens": 1.7575808080500002, "seri": 0.38193461069799994, "and": 6.29901420636e-05, "from": 0.000567054168866, "apostroph": 5.25372320611, "work": 0.109034567273, "detail": 0.816187777173, "num": 0.00031499039539700004, "notic": 1.47474978168, "cepstral": 7.369978720910001, "spectrogram": 7.369978720910001, "left": 0.364552414753, "mozilla": 5.76054080847, "depend": 0.806969815, "predict": 1.6457402376899999, "output": 2.03822657827, "match": 1.27190443874, "bottom": 1.8361940533599999, "fulli": 1.02609828678, "simplifi": 2.4940183301400003, "with": 0.00119749171339, "acoust": 2.99800242209, "are": 0.0294674735827, "solut": 1.55346297627, "better": 0.6964279406, "compani": 0.439777253097, "raj": 4.06309201872, "effort": 0.637887211057, "current": 0.42695282784500005, "wave": 1.4950479900600002, "preprocess": 7.1076144564399995, "stay": 0.9927416990379999, "histor": 0.516151783952, "shown": 1.01856958099, "summari": 2.0543127160299997, "public": 0.20232375048700002, "veri": 0.230159793238, "extract": 2.04161723301, "valid": 1.8889232176800002, "brand": 1.7094517549200001, "discard": 2.95113811311, "align": 2.09237439596, "numtensorflow": 7.369978720910001, "articul": 2.84168957926, "util": 1.5389763962399998, "net": 1.9406330919499999, "column": 1.95699427938, "again": 0.411340231612, "compar": 0.6239191809269999, "decod": 3.9457160663199997, "map": 1.40434493384, "markov": 5.16170430739, "even": 0.152388564834, "convers": 1.2085604509999999, "top": 0.609100637788, "littl": 0.438213989466, "bidirect": 5.66523062867, "rnns": 7.369978720910001, "total": 0.43567888670500005, "tfnamescop": 7.369978720910001, "train": 0.660918312839, "great": 0.235805258079, "windowwith": 7.369978720910001, "step": 1.03954505698, "anatomi": 3.22051485947, "investig": 1.13694148702, "librivox": 6.4945099835599995, "move": 0.255615859253, "store": 1.2374487335200002, "continu": 0.13040487398700001, "type": 0.707101485387, "numhz": 7.369978720910001, "note": 0.353817568083, "key": 0.82419811896, "phonem": 4.43612185107, "row": 1.71363732085, "probabl": 0.972882412913, "machin": 1.39235958062, "into": 0.0149128632287, "space": 0.874713164972, "paper": 0.979402539665, "bias": 2.61984276467, "levenshtein": 7.369978720910001, "numtxt": 7.369978720910001, "requir": 0.424253510675, "coeffici": 3.5972177828099996, "edison": 3.7193204796199995, "choos": 1.43007066072, "humancentr": 7.369978720910001, "coarticul": 7.369978720910001, "overfit": 7.369978720910001, "testcleanwav": 7.369978720910001, "larg": 0.17037506060600002, "find": 0.547781330288, "briefli": 1.5824680307199999, "overlap": 2.4924939396, "borrow": 2.0822168683, "actual": 0.628514181648, "matt": 2.43119753058, "own": 0.164195077421, "base": 0.13652330228700002, "natur": 0.431306339292, "label": 1.49898832727, "hour": 0.815190981077, "vol": 1.07286940097, "permiss": 1.8373800586400002, "post": 0.8057001527009999, "here": 0.8850381883700001, "hmms": 7.369978720910001, "microsoft": 3.21265935953, "were": 0.024291143681099997, "averag": 0.957011687995, "updat": 1.7164374626899999, "comput": 1.36806891594, "long": 0.235645793878, "human": 0.640035183243, "layer": 2.0969791623500003, "evalu": 1.9388802431299998, "than": 0.0322608622182, "share": 0.618760299747, "about": 0.0628434774746, "problem": 0.569140724273, "communiti": 0.673561947791, "neural": 4.0853151555, "distinct": 0.825779146958, "context": 1.44920491442, "when": 0.0205549888584, "just": 0.289531434109, "perspect": 1.61645415436, "adam": 1.4886080966, "convolutionalrecurr": 7.369978720910001, "pioneer": 1.55614674111, "olah": 7.033506484289999, "dure": 0.0491209066894, "frequenc": 2.1759113757299997, "represent": 1.7797382876499999, "blank": 3.15047101573, "reddi": 5.17275414357, "check": 1.87281049562, "septemb": 0.375954006775, "recognit": 1.4816549327200002, "switchboard": 5.7022719003499995, "recommend": 1.36461126863, "dynam": 1.8756834711200001, "where": 0.0649921387457, "ensembl": 2.81820931165, "complex": 0.8502416364309999, "communic": 1.04365037288, "tool": 1.60887117963, "start": 0.236443369291, "run": 0.442714975539, "enabl": 1.26473915954, "load": 1.91765354188, "dictionari": 1.65426767539, "explain": 0.955700427358, "order": 0.22014038079300002, "digit": 1.48526454375, "until": 0.138474663439, "track": 1.14028498507, "abl": 0.599303982475, "mani": 0.0433157581221, "string": 2.1255896963900005, "geitgey": 7.369978720910001, "colleagu": 2.10832533873, "analysi": 1.2466091029200002, "transcript": 2.58917596628, "provid": 0.19517784432500002, "monitor": 1.80125261058, "calcul": 1.8131506592099997, "error": 1.7985854343, "broad": 1.45323772, "biomed": 3.9295606260900002, "framework": 2.10418454607, "too": 0.5965551547219999, "singl": 0.475916769059, "throughout": 0.41983467543499997, "graph": 3.6299309802199997, "there": 0.0400978929255, "exist": 0.38165779408699996, "endtoend": 7.369978720910001, "connect": 0.633605058682, "inform": 0.454453704662, "chart": 2.13460115413, "trace": 1.49084335877, "matrix": 3.1186304098799997, "baidu": 7.033506484289999, "modifi": 1.4936444810499998, "best": 0.459227932947, "exampl": 0.40868267499899996, "right": 0.34035985417, "deepspeech": 7.369978720910001, "configsneuralnetworkini": 7.369978720910001, "baker": 2.35468761528, "softwar": 2.32849096333, "take": 0.130691962197, "visual": 1.6539383488600001, "now": 0.149092945021, "robust": 2.9929646280599997, "configur": 2.4427250357499997, "name": 0.09723316638430002, "code": 1.35601909597, "per": 0.672821024072, "tip": 2.2430429711200004, "high": 0.13782378654000002, "possibl": 0.348805474891, "color": 1.3417002006799998, "publish": 0.313975865467, "power": 0.292396282715, "oper": 0.441342964347, "spoken": 1.9556576786000002, "surpris": 1.47392435861, "typic": 0.812774319158, "hypothesi": 2.60865985243, "pattern": 1.33282404788, "textual": 3.7245288247199992, "modular": 3.91051243112, "becaus": 0.139343158825, "fourier": 4.48517800806, "portal": 2.8309483374299997, "batch": 3.5744895317400003, "research": 0.663727818138, "script": 2.1161358444599996, "over": 0.0249367214957, "platform": 1.8298923389200001, "past": 0.7016234157610001, "facilit": 1.86464718498, "newslett": 3.4034675302, "want": 0.6916366062549999, "captur": 1.0578810012100002, "convert": 1.1860360368, "talk": 1.10867789449, "ear": 2.53369681396, "system": 0.327430345585, "individu": 0.588013447985, "clariti": 3.4440528103099997, "extens": 0.6889985794750001, "howev": 0.0903151173475, "essenti": 1.07434378384, "prove": 0.899024430345, "cognit": 3.0659136276999996, "manual": 2.04501942341, "silicon": 3.46196373688, "maxim": 2.5594217052, "which": 0.00517841384543, "databas": 2.10988256718, "such": 0.059695977806, "activ": 0.381196603284, "abstract": 2.29918950399, "other": 0.00987474791976, "one": 0.0062553516455, "inspir": 1.04687502633, "fast": 1.5836950247400001, "hope": 0.919824304455, "see": 0.240921585492, "some": 0.0395735090645, "bin": 3.1058913841000004, "corpus": 3.1818402794, "get": 0.579769005782, "like": 0.139053576545, "implement": 1.27437940907, "dandelion": 5.98368435979, "croatian": 3.4303405484500002, "should": 0.509419876758, "file": 1.32734588723, "process": 0.527829199025, "add": 1.52875583713, "transform": 1.22966322707, "analyt": 2.8481901438599997, "memori": 0.9454338986599999, "includ": 0.0188846813905, "sign": 0.565696850403, "recent": 0.434413741288, "scalar": 4.591159448919999, "slice": 3.56554092616, "tedious": 4.53676537685, "access": 0.627805882716, "consonantvowelconson": 7.369978720910001, "deepspeechinspir": 7.369978720910001, "sampl": 1.9786264883900002, "rang": 0.579319213803, "time": 0.0112115188626, "engin": 0.904767558276, "relat": 0.21310030165399999, "flat": 1.7366187105500002, "similar": 0.318556092114, "pronunci": 3.07678329994, "project": 0.561601885907, "differ": 0.212321121312, "been": 0.023645982368400004, "statist": 1.4451883070700002, "valuabl": 2.010566255, "most": 0.020747896295599998, "optic": 2.70089520918, "phonograph": 4.45762805629, "domain": 2.24008000599, "excel": 1.5778801652, "all": 0.011402632097799998, "charact": 0.923148407239, "input": 2.50167533539, "today": 0.559395353679, "tftrainadamoptim": 7.369978720910001, "reason": 0.544301552962, "physic": 0.871847185184, "gradient": 3.73502760882, "cell": 1.9605673068599998, "less": 0.3846144626, "solv": 1.9836504770400003, "releas": 0.608521699544, "structur": 0.7217716751350001, "page": 0.711326032411, "weight": 1.58492352612, "isol": 1.5818550978200001, "built": 0.690379535065, "rate": 0.761033872166, "more": 0.017024931599999998, "improv": 0.7147958039319999, "leap": 2.9767648968400002, "correspond": 1.20141456099, "respect": 0.49733261904, "achiev": 0.6270980851169999, "these": 0.0715336194008, "ctcrelat": 7.369978720910001, "capabl": 1.2969341868100002, "general": 0.114952578063, "made": 0.0680215260973, "straightforward": 3.3234248225200003, "befor": 0.0956377718795, "comment": 1.11826753454, "probabilist": 4.8442500766, "close": 0.250666759864, "window": 1.7689675242900003, "can": 0.162341096394, "avail": 0.547454586289, "describ": 0.385447603125, "analog": 2.20290964097, "write": 0.721512439877, "folder": 5.129269031630001, "size": 0.9138372060609999, "few": 0.275577913653, "band": 1.2374487335200002, "amazon": 3.50086321649, "this": 0.0037864490525, "speech": 1.3409775702700002, "accur": 1.75248061485, "millisecond": 4.927631685540001, "packag": 2.0577584491900005, "choic": 1.14166497543, "advanc": 0.6930212121780001, "common": 0.338325805271, "freeli": 2.43048145465, "data": 1.2168205848, "cnns": 7.369978720910001, "method": 0.944461608841, "sequenc": 1.8035444374, "adamoptim": 7.369978720910001, "versus": 2.05087881518, "sinc": 0.0803681994577, "convolut": 4.61631800855, "must": 0.653383947388, "mix": 1.02434236008, "stateoftheart": 7.369978720910001, "advertis": 1.7248852425999999, "text": 1.14048200999, "understand": 1.0880858756799998, "import": 0.292818277066, "first": 0.0075872898121599995, "everi": 0.391485427421, "luckili": 5.25372320611, "pipelin": 3.47002829672, "toolkit": 5.24174701506, "generat": 0.719182341736, "characterlevel": 7.369978720910001, "while": 0.04324998379380001, "tempor": 3.08639215905, "onetoon": 7.369978720910001, "year": 0.047402238894600005, "easili": 1.3066587367, "often": 0.258140393351, "prototyp": 2.4632235573, "tensorboard": 7.369978720910001, "learn": 0.842752064745, "arxiv": 6.08904487545, "effici": 1.62793753414}, "freq": {"after": 3, "hand": 1, "real": 1, "matthew": 2, "christoph": 1, "googl": 1, "web": 1, "huang": 1, "rubashkin": 2, "occur": 1, "realworld": 1, "txt": 2, "signal": 1, "rememb": 1, "new": 3, "esperonto": 1, "extra": 1, "addit": 2, "would": 3, "touch": 1, "tradit": 5, "audio": 8, "recogn": 1, "the": 95, "dataset": 4, "function": 5, "class": 1, "client": 1, "descent": 1, "assist": 1, "summit": 1, "python": 1, "well": 2, "product": 2, "sequenti": 3, "amplitud": 3, "approach": 1, "datum": 2, "yield": 1, "their": 3, "psycholog": 1, "phonet": 2, "dev": 3, "automat": 1, "momentum": 1, "measur": 1, "bound": 1, "creat": 5, "enjoy": 1, "multipl": 2, "voxforg": 1, "how": 7, "test": 5, "repost": 1, "transcrib": 1, "cindi": 1, "numwav": 1, "origin": 1, "jame": 1, "increas": 1, "tutori": 1, "hundr": 1, "devcleanwav": 1, "for": 41, "librispeech": 2, "deep": 6, "difficult": 1, "will": 8, "overview": 2, "birnn": 2, "open": 4, "contact": 1, "accompani": 1, "snippet": 1, "repo": 2, "amongst": 1, "below": 9, "perform": 4, "but": 2, "certain": 1, "obtain": 1, "our": 12, "classif": 2, "annot": 1, "background": 2, "object": 4, "point": 3, "record": 1, "video": 2, "applic": 1, "melfrequ": 1, "hidden": 1, "word": 7, "level": 1, "decad": 1, "has": 5, "have": 10, "paramet": 1, "tensorflow": 8, "adjust": 1, "pass": 1, "neurosci": 1, "experi": 4, "default": 1, "namespac": 1, "resourc": 1, "green": 1, "out": 4, "xuedong": 1, "model": 22, "format": 2, "quit": 3, "excit": 1, "concept": 1, "mollison": 2, "under": 2, "sound": 8, "reli": 2, "hyperparamet": 1, "field": 1, "shortterm": 2, "loss": 4, "short": 1, "not": 4, "arbitrari": 1, "compon": 1, "phonemelevel": 1, "team": 3, "altern": 1, "read": 1, "mane": 1, "quick": 2, "speechtext": 1, "adopt": 1, "metric": 1, "instanti": 1, "english": 3, "index": 1, "separ": 2, "they": 4, "anoth": 1, "recurr": 3, "teal": 1, "effect": 1, "allow": 1, "wav": 2, "scienc": 1, "task": 1, "nonphonet": 1, "welldocu": 1, "blog": 1, "decompos": 1, "github": 4, "distanc": 3, "node": 2, "featur": 8, "connectionist": 2, "wordlevel": 1, "filenam": 1, "simpl": 3, "influenc": 2, "that": 27, "algorithm": 1, "valley": 1, "either": 2, "longterm": 1, "develop": 6, "observ": 1, "traincleannumwav": 1, "architectur": 2, "sever": 3, "explor": 1, "thompson": 1, "network": 17, "math": 1, "started\u2014pleas": 1, "lot": 1, "box": 2, "sourc": 5, "control": 1, "both": 1, "locat": 1, "result": 3, "repositori": 4, "use": 30, "languag": 6, "set": 3, "ani": 2, "licens": 1, "seri": 2, "and": 57, "from": 15, "apostroph": 1, "work": 3, "detail": 2, "num": 37, "notic": 1, "cepstral": 5, "spectrogram": 2, "left": 1, "mozilla": 2, "depend": 2, "predict": 4, "output": 1, "match": 1, "bottom": 1, "fulli": 1, "simplifi": 1, "with": 15, "acoust": 3, "are": 17, "solut": 1, "better": 1, "compani": 2, "raj": 1, "effort": 2, "current": 2, "wave": 1, "preprocess": 1, "stay": 1, "histor": 1, "shown": 4, "summari": 2, "public": 1, "veri": 1, "extract": 2, "valid": 1, "brand": 1, "discard": 1, "align": 1, "numtensorflow": 1, "articul": 2, "util": 5, "net": 1, "column": 1, "again": 1, "compar": 1, "decod": 1, "map": 1, "markov": 1, "even": 1, "convers": 1, "top": 3, "littl": 1, "bidirect": 2, "rnns": 9, "total": 2, "tfnamescop": 1, "train": 18, "great": 1, "windowwith": 1, "step": 1, "anatomi": 1, "investig": 1, "librivox": 1, "move": 1, "store": 2, "continu": 2, "type": 2, "numhz": 1, "note": 3, "key": 1, "phonem": 3, "row": 1, "probabl": 2, "machin": 4, "into": 4, "space": 2, "paper": 3, "bias": 1, "levenshtein": 2, "numtxt": 1, "requir": 3, "coeffici": 3, "edison": 1, "choos": 1, "humancentr": 1, "coarticul": 1, "overfit": 1, "testcleanwav": 1, "larg": 2, "find": 3, "briefli": 1, "overlap": 1, "borrow": 1, "actual": 1, "matt": 2, "own": 1, "base": 2, "natur": 2, "label": 2, "hour": 5, "vol": 1, "permiss": 1, "post": 4, "here": 2, "hmms": 1, "microsoft": 2, "were": 6, "averag": 1, "updat": 2, "comput": 6, "long": 2, "human": 1, "layer": 6, "evalu": 1, "than": 2, "share": 1, "about": 4, "problem": 2, "communiti": 1, "neural": 9, "distinct": 1, "context": 1, "when": 3, "just": 1, "perspect": 1, "adam": 1, "convolutionalrecurr": 1, "pioneer": 1, "olah": 1, "dure": 1, "frequenc": 5, "represent": 2, "blank": 1, "reddi": 1, "check": 2, "septemb": 1, "recognit": 22, "switchboard": 1, "recommend": 2, "dynam": 1, "where": 2, "ensembl": 1, "complex": 2, "communic": 1, "tool": 1, "start": 2, "run": 2, "enabl": 1, "load": 4, "dictionari": 1, "explain": 1, "order": 3, "digit": 2, "until": 1, "track": 1, "abl": 1, "mani": 5, "string": 2, "geitgey": 1, "colleagu": 1, "analysi": 1, "transcript": 9, "provid": 5, "monitor": 2, "calcul": 1, "error": 12, "broad": 1, "biomed": 1, "framework": 1, "too": 1, "singl": 2, "throughout": 1, "graph": 3, "there": 5, "exist": 3, "endtoend": 1, "connect": 1, "inform": 3, "chart": 1, "trace": 1, "matrix": 1, "baidu": 2, "modifi": 1, "best": 1, "exampl": 19, "right": 1, "deepspeech": 1, "configsneuralnetworkini": 1, "baker": 1, "softwar": 2, "take": 1, "visual": 1, "now": 1, "robust": 1, "configur": 1, "name": 1, "code": 4, "per": 3, "tip": 1, "high": 1, "possibl": 2, "color": 1, "publish": 1, "power": 3, "oper": 1, "spoken": 1, "surpris": 1, "typic": 2, "hypothesi": 1, "pattern": 1, "textual": 1, "modular": 1, "becaus": 7, "fourier": 1, "portal": 1, "batch": 1, "research": 5, "script": 1, "over": 1, "platform": 1, "past": 4, "facilit": 1, "newslett": 1, "want": 1, "captur": 2, "convert": 2, "talk": 1, "ear": 1, "system": 3, "individu": 2, "clariti": 1, "extens": 2, "howev": 1, "essenti": 1, "prove": 1, "cognit": 1, "manual": 1, "silicon": 1, "maxim": 1, "which": 2, "databas": 1, "such": 4, "activ": 1, "abstract": 1, "other": 2, "one": 1, "inspir": 1, "fast": 1, "hope": 1, "see": 1, "some": 2, "bin": 2, "corpus": 1, "get": 3, "like": 9, "implement": 4, "dandelion": 1, "croatian": 1, "should": 2, "file": 3, "process": 1, "add": 2, "transform": 4, "analyt": 1, "memori": 3, "includ": 10, "sign": 1, "recent": 2, "scalar": 1, "slice": 1, "tedious": 1, "access": 1, "consonantvowelconson": 1, "deepspeechinspir": 1, "sampl": 4, "rang": 1, "time": 8, "engin": 1, "relat": 1, "flat": 2, "similar": 1, "pronunci": 1, "project": 2, "differ": 7, "been": 2, "statist": 1, "valuabl": 1, "most": 1, "optic": 1, "phonograph": 1, "domain": 1, "excel": 2, "all": 2, "charact": 11, "input": 2, "today": 2, "tftrainadamoptim": 1, "reason": 1, "physic": 1, "gradient": 1, "cell": 2, "less": 1, "solv": 1, "releas": 1, "structur": 1, "page": 1, "weight": 2, "isol": 1, "built": 1, "rate": 9, "more": 9, "improv": 5, "leap": 1, "correspond": 4, "respect": 1, "achiev": 1, "these": 5, "ctcrelat": 1, "capabl": 1, "general": 1, "made": 3, "straightforward": 3, "befor": 3, "comment": 1, "probabilist": 1, "close": 1, "window": 3, "can": 10, "avail": 1, "describ": 1, "analog": 1, "write": 1, "folder": 4, "size": 1, "few": 2, "band": 1, "amazon": 1, "this": 13, "speech": 31, "accur": 1, "millisecond": 1, "packag": 1, "choic": 1, "advanc": 1, "common": 1, "freeli": 1, "data": 33, "cnns": 1, "method": 2, "sequenc": 4, "adamoptim": 1, "versus": 1, "sinc": 1, "convolut": 2, "must": 1, "mix": 1, "stateoftheart": 1, "advertis": 1, "text": 1, "understand": 1, "import": 4, "first": 2, "everi": 1, "luckili": 1, "pipelin": 2, "toolkit": 1, "generat": 3, "characterlevel": 2, "while": 1, "tempor": 3, "onetoon": 1, "year": 1, "easili": 1, "often": 1, "prototyp": 1, "tensorboard": 1, "learn": 12, "arxiv": 1, "effici": 3}, "idf": {"after": 1.02070207021, "hand": 1.6152202665600002, "real": 2.28103448276, "matthew": 6.908616187989999, "christoph": 5.41473396999, "googl": 11.388809182200001, "web": 5.17133550489, "huang": 66.9873417722, "rubashkin": 1587.6, "occur": 1.7453825857499998, "realworld": 1587.6, "txt": 1443.27272727, "signal": 5.12459651388, "rememb": 4.88793103448, "new": 1.0178880554, "esperonto": 1587.6, "extra": 5.33826496301, "addit": 1.24634950542, "would": 1.0828729281799998, "touch": 5.45567010309, "tradit": 1.60802187785, "audio": 9.05647461495, "recogn": 2.54954231572, "the": 1.0, "dataset": 193.609756098, "function": 2.495441685, "class": 2.11651779763, "client": 14.1371326803, "descent": 8.494382022469999, "assist": 2.17300848618, "summit": 12.2311248074, "python": 56.2978723404, "well": 1.0655748708, "product": 1.62264922322, "sequenti": 39.5910224439, "amplitud": 66.7058823529, "approach": 2.07556543339, "datum": 273.724137931, "yield": 6.46943765281, "their": 1.01547908405, "psycholog": 6.6846315789499995, "phonet": 43.6153846154, "dev": 148.373831776, "automat": 6.787516032490001, "momentum": 16.835630965, "measur": 2.41093394077, "bound": 5.40735694823, "creat": 1.2492917847, "enjoy": 3.3269069572500003, "multipl": 2.74813917258, "voxforg": 1587.6, "how": 1.60250328051, "test": 2.65707112971, "repost": 933.882352941, "transcrib": 30.472168906, "cindi": 78.2068965517, "numwav": 1587.6, "origin": 1.13724928367, "jame": 1.9313868613099998, "increas": 1.32024948025, "tutori": 59.4606741573, "hundr": 2.4698195395099996, "devcleanwav": 1587.6, "for": 1.00031504001, "librispeech": 1587.6, "deep": 3.6279707495399998, "difficult": 2.48957189901, "will": 1.22481098596, "overview": 12.6805111821, "birnn": 1587.6, "open": 1.24556723678, "contact": 3.2196309065099995, "accompani": 3.38146964856, "snippet": 135.692307692, "repo": 369.209302326, "amongst": 5.6478121664900005, "below": 2.25607503197, "perform": 1.5313977042500002, "but": 1.01632417899, "certain": 1.8077886586200003, "obtain": 2.68629441624, "our": 2.35758835759, "classif": 8.067073170730001, "annot": 22.8760806916, "background": 4.02739726027, "object": 2.3488681757700003, "point": 1.25990000794, "record": 1.42334588488, "video": 3.29719626168, "applic": 3.42672134686, "melfrequ": 1587.6, "hidden": 7.81299212598, "word": 1.7965372864099998, "level": 1.6544393497299998, "decad": 2.1390460792200003, "has": 1.0436497502, "have": 1.0148948411399998, "paramet": 17.256521739100002, "tensorflow": 1587.6, "adjust": 7.112903225810001, "pass": 1.61818367139, "neurosci": 66.7058823529, "experi": 1.87062566278, "default": 21.1398135819, "namespac": 466.941176471, "resourc": 2.9487369985100003, "green": 2.63065451533, "out": 1.06016694491, "xuedong": 1587.6, "model": 2.0905978404, "format": 2.53125, "quit": 2.8849718335500003, "excit": 9.818181818180001, "concept": 2.65707112971, "mollison": 1587.6, "under": 1.0781663837, "sound": 3.11294117647, "reli": 4.16146788991, "hyperparamet": 1587.6, "field": 1.7790228597, "shortterm": 1587.6, "loss": 2.42529789184, "short": 1.41295834817, "not": 1.01567398119, "arbitrari": 17.8181818182, "compon": 4.09491875161, "phonemelevel": 1587.6, "team": 2.2748244734200003, "altern": 2.1390460792200003, "read": 2.3149606299200003, "mane": 120.27272727299999, "quick": 2.205, "speechtext": 1587.6, "adopt": 2.0442956477000003, "metric": 22.235294117600002, "instanti": 240.545454545, "english": 1.7432744043000001, "index": 6.9969149405, "separ": 1.6012102874399998, "they": 1.03017325287, "anoth": 1.13643521832, "recurr": 35.5964125561, "teal": 133.411764706, "effect": 1.3963060686000002, "allow": 1.2716059271100002, "wav": 1587.6, "scienc": 2.31969608416, "task": 3.88641370869, "nonphonet": 1587.6, "welldocu": 1587.6, "blog": 14.1876675603, "decompos": 49.6125, "github": 1587.6, "distanc": 3.4754816112099998, "node": 44.3463687151, "featur": 1.52712581762, "connectionist": 1587.6, "wordlevel": 1587.6, "filenam": 496.125, "simpl": 3.3981164383599998, "influenc": 1.77246846042, "that": 1.00398406375, "algorithm": 27.9507042254, "valley": 4.21673306773, "either": 1.5830092731099998, "longterm": 512.129032258, "develop": 1.1955719557200002, "observ": 2.22446406053, "traincleannumwav": 1587.6, "architectur": 5.12790697674, "sever": 1.07241286139, "explor": 3.39593582888, "thompson": 9.91630231106, "network": 2.59369384088, "math": 22.0806675939, "started\u2014pleas": 1587.6, "lot": 4.40877534018, "box": 4.12685209254, "sourc": 1.69760479042, "control": 1.46959178006, "both": 1.05215720061, "locat": 1.59766529134, "result": 1.14611608432, "repositori": 44.974504249300004, "use": 1.0296387573799999, "languag": 2.29488291414, "set": 1.18707940781, "ani": 1.13383802314, "licens": 5.79839298758, "seri": 1.46511627907, "and": 1.00006299213, "from": 1.00056721497, "apostroph": 191.277108434, "work": 1.11520089913, "detail": 2.26186066391, "num": 1.00031504001, "notic": 4.36994219653, "cepstral": 1587.6, "spectrogram": 1587.6, "left": 1.4398693996, "mozilla": 317.52, "depend": 2.2411067193700003, "predict": 5.18484650555, "output": 7.676982591880001, "match": 3.5676404494400002, "bottom": 6.27261951798, "fulli": 2.79015817223, "simplifi": 12.109839816900001, "with": 1.0011982089899998, "acoust": 20.0454545455, "are": 1.02990593578, "solut": 4.7278141751, "better": 2.0065722952500002, "compani": 1.5523613963, "raj": 58.1538461538, "effort": 1.89247824532, "current": 1.5325803649, "wave": 4.4595505618, "preprocess": 1221.23076923, "stay": 2.6986231514499996, "histor": 1.6755672823199999, "shown": 2.76923076923, "summari": 7.80147420147, "public": 1.22424429365, "veri": 1.25880114177, "extract": 7.703056768560001, "valid": 6.61224489796, "brand": 5.5259310824900005, "discard": 19.1277108434, "align": 8.10413476263, "numtensorflow": 1587.6, "articul": 17.1447084233, "util": 4.65981802172, "net": 6.96315789474, "column": 7.078020508250001, "again": 1.50883862384, "compar": 1.8662278123900002, "decod": 51.713355048900006, "map": 4.0728578758300005, "markov": 174.46153846200002, "even": 1.16461267606, "convers": 3.3486606201200004, "top": 1.8387769284200002, "littl": 1.5499365420299998, "bidirect": 288.654545455, "rnns": 1587.6, "total": 1.5460122699399999, "tfnamescop": 1587.6, "train": 1.9365698950999999, "great": 1.26592775696, "windowwith": 1587.6, "step": 2.8279301745599996, "anatomi": 25.0410094637, "investig": 3.11721971333, "librivox": 661.5, "move": 1.29125660838, "store": 3.44680851064, "continu": 1.13928955867, "type": 2.0281042411900003, "numhz": 1587.6, "note": 1.42449528937, "key": 2.28005170185, "phonem": 84.44680851060001, "row": 5.549108703250001, "probabl": 2.64555907349, "machin": 4.02433460076, "into": 1.01502461479, "space": 2.39818731118, "paper": 2.6628648104700003, "bias": 13.7335640138, "levenshtein": 1587.6, "numtxt": 1587.6, "requir": 1.52844902282, "coeffici": 36.4965517241, "edison": 41.2363636364, "choos": 4.17899447223, "humancentr": 1587.6, "coarticul": 1587.6, "overfit": 1587.6, "testcleanwav": 1587.6, "larg": 1.18574949585, "find": 1.7294117647099998, "briefli": 4.8669527897, "overlap": 12.0913937548, "borrow": 8.02223345124, "actual": 1.87482286254, "matt": 11.372492836700001, "own": 1.17844418052, "base": 1.14628158845, "natur": 1.5392670157100001, "label": 4.47715736041, "hour": 2.25960717336, "vol": 2.92375690608, "permiss": 6.280063291139999, "post": 2.23826307627, "here": 2.42307692308, "hmms": 1587.6, "microsoft": 24.8450704225, "were": 1.02458857696, "averag": 2.60390355913, "updat": 5.56466876972, "comput": 3.9277585353800006, "long": 1.2657259028899999, "human": 1.8965476048299998, "layer": 8.14153846154, "evalu": 6.9509632224199995, "than": 1.03278688525, "share": 1.8566249561500001, "about": 1.06486015159, "problem": 1.76674827509, "communiti": 1.96121062384, "neural": 59.4606741573, "distinct": 2.2836593786000003, "context": 4.25972632144, "when": 1.02076769755, "just": 1.33580143037, "perspect": 5.03520456708, "adam": 4.43092380687, "convolutionalrecurr": 1587.6, "pioneer": 4.74051955808, "olah": 1134.0, "dure": 1.0503473370799998, "frequenc": 8.8102108768, "represent": 5.928304705, "blank": 23.3470588235, "reddi": 176.4, "check": 6.50655737705, "septemb": 1.45638014861, "recognit": 4.40022172949, "switchboard": 299.547169811, "recommend": 3.9142011834300003, "dynam": 6.52527743527, "where": 1.06715063521, "ensembl": 16.746835443, "complex": 2.34021226415, "communic": 2.8395635843299996, "tool": 4.99716713881, "start": 1.26673581744, "run": 1.55692850838, "enabl": 3.5421686747, "load": 6.80497213888, "dictionari": 5.2292490118599995, "explain": 2.60049140049, "order": 1.24625166811, "digit": 4.416133518780001, "until": 1.14852058164, "track": 3.1276595744700004, "abl": 1.8208510150200001, "mani": 1.04426757877, "string": 8.37783641161, "geitgey": 1587.6, "colleagu": 8.23443983402, "analysi": 3.47852760736, "transcript": 13.318791946300001, "provid": 1.21552714187, "monitor": 6.05723006486, "calcul": 6.12972972973, "error": 6.04109589041, "broad": 4.27693965517, "biomed": 50.8846153846, "framework": 8.200413223139998, "too": 1.81585268215, "singl": 1.60948905109, "throughout": 1.5217099587799998, "graph": 37.7102137767, "there": 1.04091266719, "exist": 1.4647107666799999, "endtoend": 1587.6, "connect": 1.8843916913900003, "inform": 1.5753125620200001, "chart": 8.45367412141, "trace": 4.4408391608399995, "matrix": 22.6153846154, "baidu": 1134.0, "modifi": 4.45329593268, "best": 1.5828514456600002, "exampl": 1.50483412322, "right": 1.4054532577899999, "deepspeech": 1587.6, "configsneuralnetworkini": 1587.6, "baker": 10.5348374253, "softwar": 10.2624434389, "take": 1.13961668222, "visual": 5.22752716497, "now": 1.160780873, "robust": 19.9447236181, "configur": 11.504347826099998, "name": 1.10211732037, "code": 3.8807137619199996, "per": 1.9597580545599997, "tip": 9.42195845697, "high": 1.14777327935, "possibl": 1.4173734488, "color": 3.8255421686699997, "publish": 1.36885669943, "power": 1.3396337861799998, "oper": 1.55479384977, "spoken": 7.06856634016, "surpris": 4.36633663366, "typic": 2.2541530597799997, "hypothesi": 13.580838323399998, "pattern": 3.79173632673, "textual": 41.4516971279, "modular": 49.9245283019, "becaus": 1.1495184997499999, "fourier": 88.6927374302, "portal": 16.9615384615, "batch": 35.6764044944, "research": 1.9420183486200002, "script": 8.299006795610001, "over": 1.02525024217, "platform": 6.2332155476999995, "past": 2.01702452039, "facilit": 6.453658536590001, "newslett": 30.068181818200003, "want": 1.99698113208, "captur": 2.88026124819, "convert": 3.2740771293099997, "talk": 3.0303493033, "ear": 12.6, "system": 1.38739840951, "individu": 1.8004082558400003, "clariti": 31.3136094675, "extens": 1.99171998495, "howev": 1.0945191313299998, "essenti": 2.9280708225700005, "prove": 2.45720476706, "cognit": 21.454054054100002, "manual": 7.72930866602, "silicon": 31.8795180723, "maxim": 12.928338762200001, "which": 1.005191845, "databas": 8.24727272727, "such": 1.06151377374, "activ": 1.46403541129, "abstract": 9.966101694919999, "other": 1.00992366412, "one": 1.00627495722, "inspir": 2.8487349721900004, "fast": 4.8729281768, "hope": 2.50884955752, "see": 1.27242125511, "some": 1.04036697248, "bin": 22.3291139241, "corpus": 24.091047041, "get": 1.78562591385, "like": 1.14918566775, "implement": 3.57648118946, "dandelion": 396.9, "croatian": 30.8871595331, "should": 1.6643254009900001, "file": 3.7710213776699995, "process": 1.69524826482, "add": 4.61243463103, "transform": 3.42007755278, "analyt": 17.256521739100002, "memori": 2.57392996109, "includ": 1.0190641247799999, "sign": 1.7606742819099999, "recent": 1.54405757635, "scalar": 98.6086956522, "slice": 35.3585746102, "tedious": 93.3882352941, "access": 1.8734953976900002, "consonantvowelconson": 1587.6, "deepspeechinspir": 1587.6, "sampl": 7.23280182232, "rang": 1.7848229342299997, "time": 1.01127460348, "engin": 2.47135740971, "relat": 1.23750876919, "flat": 5.67811158798, "similar": 1.37514075357, "pronunci": 21.6885245902, "project": 1.7534791252500002, "differ": 1.23654490225, "been": 1.0239277652399998, "statist": 4.24265098878, "valuabl": 7.46754468485, "most": 1.02096463023, "optic": 14.8930581614, "phonograph": 86.28260869569999, "domain": 9.39408284024, "excel": 4.84467500763, "all": 1.01146788991, "charact": 2.51720310766, "input": 12.2029208301, "today": 1.74961428257, "tftrainadamoptim": 1587.6, "reason": 1.72340425532, "physic": 2.39132399458, "gradient": 41.889182058, "cell": 7.1033557047, "less": 1.46904783936, "solv": 7.26923076923, "releas": 1.8377126982299998, "structur": 2.0580762250499998, "page": 2.03669018602, "weight": 4.878918254459999, "isol": 4.86397058824, "built": 1.99447236181, "rate": 2.14048806795, "more": 1.0171706817, "improv": 2.04376930999, "leap": 19.6242274413, "correspond": 3.32481675393, "respect": 1.6443293630200002, "achiev": 1.87216981132, "these": 1.07415426252, "ctcrelat": 1587.6, "capabl": 3.6580645161300005, "general": 1.1218202374200001, "made": 1.07038834951, "straightforward": 27.7552447552, "befor": 1.10036041031, "comment": 3.05954904606, "probabilist": 127.008, "close": 1.2848818387799998, "window": 5.86479497599, "can": 1.17626139142, "avail": 1.7288467821, "describ": 1.47027227264, "analog": 9.05131128848, "write": 2.0575427682700003, "folder": 168.893617021, "size": 2.49387370405, "few": 1.31729173581, "band": 3.44680851064, "amazon": 33.1440501044, "this": 1.00379362671, "speech": 3.8227787141800005, "accur": 5.768895348840001, "millisecond": 138.052173913, "packag": 7.828402366860001, "choic": 3.1319786940200003, "advanc": 1.9997480791, "common": 1.4025974025999999, "freeli": 11.3643521832, "data": 3.37643555934, "cnns": 1587.6, "method": 2.5714285714300003, "sequenc": 6.07112810707, "adamoptim": 1587.6, "versus": 7.77473065622, "sinc": 1.08368600683, "convolut": 101.121019108, "must": 1.9220338983099996, "mix": 2.7852631578900002, "stateoftheart": 1587.6, "advertis": 5.61187698834, "text": 3.12827586207, "understand": 2.96858638743, "import": 1.3401992233700002, "first": 1.00761614623, "everi": 1.47917637194, "luckili": 191.277108434, "pipelin": 32.1376518219, "toolkit": 189.0, "generat": 2.05275407292, "characterlevel": 1587.6, "while": 1.0441988950299999, "tempor": 21.897931034499997, "onetoon": 1587.6, "year": 1.0485436893200002, "easili": 3.6938110749199997, "often": 1.29452054795, "prototyp": 11.7426035503, "tensorboard": 1587.6, "learn": 2.32275054865, "arxiv": 441.0, "effici": 5.09335899904}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Building, Training, and Improving on Existing Recurrent Neural Networks</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/05/building-training-improving-existing-recurrent-neural-networks.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Building, Training, and Improving on Existing Recurrent Neural Networks Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/05/top-stories-2017-apr.html\" rel=\"prev\" title=\"Top April Stories: 10 Free Must-Read Books for Machine Learning and Data Science\"/>\n<link href=\"https://www.kdnuggets.com/2017/05/data-science-machine-learning-platforms-enterprise.html\" rel=\"next\" title=\"Data Science &amp; Machine Learning Platforms for the Enterprise\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2017/05/building-training-improving-existing-recurrent-neural-networks.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=65747\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2017/05/building-training-improving-existing-recurrent-neural-networks.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-65747 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 8-May, 2017  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/05/index.html\">May</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/05/tutorials.html\">Tutorials, Overviews</a> \u00bb Building, Training, and Improving on Existing Recurrent Neural Networks (\u00a0<a href=\"/2017/n18.html\">17:n18</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Building, Training, and Improving on Existing Recurrent Neural Networks</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/05/top-stories-2017-apr.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/05/data-science-machine-learning-platforms-enterprise.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <span class=\"http-likes\" style=\"float: left; font-size:14px\">http likes 77</span> <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/recurrent-neural-networks\" rel=\"tag\">Recurrent Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/svds\" rel=\"tag\">SVDS</a></div>\n<br/>\n<p class=\"excerpt\">\n     In this post, we\u2019ll provide a short tutorial for training a RNN for speech recognition, including code snippets throughout.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><b>By Matthew Rubashkin &amp; Matt Mollison, Silicon Valley Data Science.</b></p>\n<p><img alt=\"Header image\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/fractal-1765220_1280_header.jpg\" width=\"90%\"/></p>\n<p>On the deep learning R&amp;D team at SVDS, we have investigated Recurrent Neural Networks (RNN) for exploring time series and developing speech recognition capabilities. Many products today rely on deep neural networks that implement recurrent layers, including products made by companies like <a href=\"https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html\" target=\"_blank\">Google</a>, <a href=\"https://arxiv.org/abs/1412.5567\" target=\"_blank\">Baidu</a>, and <a href=\"https://www.slideshare.net/AmandaMackay4/amazon-deep-learning\" target=\"_blank\">Amazon</a>.</p>\n<p>However, when developing our own RNN pipelines, we did not find many simple and straightforward examples of using neural networks for sequence learning applications like speech recognition. Many examples were either powerful but quite complex, like the actively developed <a href=\"https://github.com/mozilla/DeepSpeech\" target=\"_blank\">DeepSpeech</a> project from Mozilla under Mozilla Public License, or were too simple and abstract to be used on real data.</p>\n<p>In this post, we\u2019ll provide a short tutorial for training a RNN for speech recognition; we\u2019re including code snippets throughout, and you can find the accompanying <a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial\" target=\"_blank\">GitHub repository here</a>. The software we\u2019re using is a mix of borrowed and inspired code from existing open source projects. Below is a video example of machine speech recognition on a <a href=\"https://en.wikipedia.org/wiki/File:Advertising_Record.ogg\" target=\"_blank\">1906 Edison Phonograph advertisement</a>. The video includes a running trace of sound amplitude, extracted spectrogram, and predicted text.</p>\n<p><center><iframe allowfullscreen=\"allowfullscreen\" data-mce-fragment=\"1\" frameborder=\"0\" height=\"500\" src=\"https://www.youtube.com/embed/y_bojXC_kBg?feature=oembed\" width=\"700\"></iframe></center></p>\n<p>Since we have extensive experience with Python, we used a well-documented package that has been <a href=\"http://svds.com/getting-started-deep-learning/\" target=\"_blank\">advancing by leaps and bounds</a>: <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a>. Before you get started, if you are brand new to RNNs, we highly recommend you read Christopher Olah\u2019s excellent overview of RNN Long Short-Term Memory (LSTM) networks <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\">here</a>.</p>\n<h3>Speech recognition: audio and transcriptions</h3>\n<p>\u00a0<br>\nUntil the 2010\u2019s, the state-of-the-art for speech recognition models were <a href=\"https://en.wikipedia.org/wiki/Phonetics\" target=\"_blank\">phonetic</a>-based approaches including separate components for pronunciation, <a href=\"https://en.wikipedia.org/wiki/Acoustic_model\" target=\"_blank\">acoustic</a>, and <a href=\"https://en.wikipedia.org/wiki/Language_model\" target=\"_blank\">language</a> models. Speech recognition in the past and today both rely on decomposing sound waves into frequency and amplitude using fourier transforms, yielding a spectrogram as shown below.</br></p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/spectrogram_edison.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/spectrogram_edison.png\" width=\"99%\"/></a></p>\n<p>Training the acoustic model for a traditional speech recognition pipeline that uses Hidden Markov Models (HMM) requires\u00a0speech+text data, as well as\u00a0a word to phoneme dictionary. HMMs are generative probabilistic models for sequential data, and are typically evaluated using <a href=\"https://en.wikipedia.org/wiki/Levenshtein_distance\" target=\"_blank\">Levenshtein word error distance</a>, a string metric for measuring differences in strings.</p>\n<p>These models can be simplified and made more accurate with speech data that is aligned with phoneme transcriptions, but this a tedious manual task. Because of this effort, phoneme-level transcriptions are less likely to exist for large sets of speech data than word-level transcriptions. For more information on existing open source speech recognition tools and models, check out our colleague Cindi Thompson\u2019s <a href=\"http://svds.com/open-source-toolkits-speech-recognition/\" target=\"_blank\">recent post</a>.</p>\n<h3>Connectionist Temporal Classification (CTC) loss function</h3>\n<p>\u00a0<br>\nWe can discard the concept of phonemes when using neural networks for speech recognition by using an objective function that allows for the prediction of character-level transcriptions: <a href=\"http://www.cs.toronto.edu/~graves/icml_2006.pdf\" target=\"_blank\">Connectionist Temporal Classification</a> (CTC). Briefly, CTC enables the computation of probabilities of multiple sequences, where the sequences are the set of all possible character-level transcriptions of the speech sample. The network uses the objective function to maximize the probability of the character sequence (i.e., chooses the most likely transcription), and calculates the error for the predicted result compared to the actual transcription to update network weights during training.</br></p>\n<p>It is important to note that the <strong>character</strong>-level error used by a CTC loss function differs from the Levenshtein <strong>word</strong> error distance often used in traditional speech recognition models. For character generating RNNs, the character and word error distance will be similar in phonetic languages such as Esperonto and Croatian, where individual sounds correspond to distinct characters. Conversely, the character versus word error will be quite different for a non-phonetic language like English.</p>\n<p>If you want to learn more about CTC, there are many papers and <a href=\"https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0?gi=24cd18fe52c#.wbsc6x23a\" target=\"_blank\">blog posts</a> that explain it in more detail. We will use TensorFlow\u2019s <a href=\"https://www.tensorflow.org/api_guides/python/nn#Connectionist_Temporal_Classification_CTC_\" target=\"_blank\">CTC implementation</a>, and there continues to be research and improvements on CTC-related implementations, such as <a href=\"https://arxiv.org/abs/1703.00096\" target=\"_blank\">this recent paper</a> from Baidu. In order to utilize algorithms developed for traditional or deep learning speech recognition models, our team structured our speech recognition platform for modularity and fast prototyping:</p>\n<h3><a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-2.40.46-PM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-2.40.46-PM.png\" width=\"99%\"/></a></h3>\n<p>\u00a0</p>\n<h3>Importance of data</h3>\n<p>\u00a0<br>\nIt should be no surprise that creating a system that transforms speech into its textual representation requires having (1) digital audio files and (2) transcriptions of the words that were spoken. Because the model should generalize to decode any new speech samples, the more examples we can train the system on, the better it will perform. We researched freely available recordings of transcribed English speech; some examples that we have used for training are <a href=\"http://www.openslr.org/12/\" target=\"_blank\">LibriSpeech</a> (1000 hours), <a href=\"http://www.openslr.org/7/\" target=\"_blank\">TED-LIUM</a> (118 hours), and <a href=\"http://www.voxforge.org/\" target=\"_blank\">VoxForge</a> (130 hours). The chart below includes information on these datasets including total size in hours, sampling rate, and annotation.</br></p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/RNN_datasets.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/RNN_datasets.png\" width=\"85%\"/></a></p>\n<p>In order to easily access data from any data source, we store all data in a flat format. This flat format has a single .wav and a single .txt per datum. For example, you can find example Librispeech Training datum \u2018211-122425-0059\u2019 in our <a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial\" target=\"_blank\">GitHub repo</a> as 211-122425-0059.wav and 211-122425-0059.txt. These data filenames are loaded into the TensorFlow graph using a datasets object class, that assists TensorFlow in efficiently loading, preprocessing the data, and loading individual batches of data from CPU to GPU memory. An example of the data fields in the<a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial/blob/master/src/data_manipulation/datasets.py\" target=\"_blank\"> datasets object</a> is shown below:</p>\n<p><script src=\"https://gist.github.com/mmmayo13/3ce61ff237f84587d16c9cbd4786392e.js\"></script></p>\n<h3>Feature representation</h3>\n<p>\u00a0<br>\nIn order for a machine to recognize audio data, the data must first be converted from the time to the frequency domain. There are several methods for creating features for machine learning of audio data, including binning by arbitrary frequencies (i.e., every 100Hz), or by using binning that matches the frequency bands of the human ear. This typical human-centric transformation for speech data is to compute <a href=\"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\" target=\"_blank\">Mel-frequency cepstral coefficients</a> (MFCC), either 13 or 26 different cepstral features, as input for the model. After this transformation the data is stored as a matrix of frequency coefficients (rows) over time (columns).</br></p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-3.34.51-PM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-3.34.51-PM.png\" width=\"75%\"/></a></p>\n<p>Because speech sounds do not occur in isolation and do not have a one-to-one mapping to characters, we can capture the effects of coarticulation (the articulation of one sound influencing the articulation of another) by training the network on overlapping windows (10s of milliseconds) of audio data that captures sound from before and after the current time index. Example code of how to obtain MFCC features, and how to create windows of audio data is shown below:</p>\n<p><script src=\"https://gist.github.com/mmmayo13/0b13793c99b41ce821ce49bceef3ecf4.js\"></script></p>\n<p>For our RNN example, we use 9 time slices before and 9 after, for a total of 19 time points per window.With 26 cepstral coefficients, this is 494 data points per 25 ms observation.\u00a0Depending on the data sampling rate, we recommend 26 cepstral features for 16,000 Hz and 13 cepstral features for 8,000 hz. Below is an example of data loading windows on 8,000 Hz data:</p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-21-at-10.45.40-AM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-21-at-10.45.40-AM.png\" width=\"60%\"/></a></p>\n<p>If you would like to learn more about converting analog to digital sound for RNN speech recognition, check out Adam Geitgey\u2019s <a href=\"https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a#.pwd1rl7cw\" target=\"_blank\">machine learning post</a>.</p>\n<h3>Modeling the sequential nature of speech</h3>\n<p>\u00a0<br/>\nLong Short-Term Memory (LSTM) layers are a type of recurrent neural network (RNN) architecture that are useful for modeling data that has long-term sequential dependencies. They are important for time series data because they essentially remember past information at the current time point, which influences their output. This context is useful for speech recognition because of its temporal nature. If you would like to see how LSTM cells are instantiated in TensorFlow, we\u2019ve include example code below from the\u00a0LSTM layer of our\u00a0DeepSpeech-inspired\u00a0Bi-Directional Neural Network (BiRNN).</p>\n<p><script src=\"https://gist.github.com/mmmayo13/a6f91c4a74f5fedfe1e88a2efe1892fc.js\"></script></p>\n<p>For more details about this type of network architecture, there are some excellent overviews of how <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" target=\"_blank\">RNNs</a> and <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\">LSTM cells</a> work. Additionally, there continues to be research on alternatives to using RNNs for speech recognition, such as with <a href=\"https://arxiv.org/abs/1701.02720\" target=\"_blank\">convolutional layers</a> which are more computationally efficient than RNNs.</p>\n<h3>Network training and monitoring</h3>\n<p>\u00a0<br/>\nBecause we trained our network using TensorFlow, we were able to visualize the computational graph as well as monitor the training, validation, and test performance from a web portal with very little extra effort using <a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\">TensorBoard</a>. Using tips from Dandelion Mane\u2019s <a href=\"https://www.youtube.com/watch?v=eBbEDRsCmv4\" target=\"_blank\">great talk</a> at the 2017TensorFlow Dev Summit, we utilize tf.name_scope to add node and layer names, and write out our summary to file. The results of this is an automatically generated, understandable computational graph, such as this example of a Bi-Directional Neural Network (BiRNN) below. The data is passed amongst different operations from bottom left to top right. The different nodes can be labelled and colored with namespaces for clarity. In this example, teal \u2018fc\u2019 boxes correspond to fully connected layers, and the green \u2018b\u2019 and \u2018h\u2019 boxes correspond to biases and weights, respectively.</p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/png-2.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/png-2.png\" width=\"99%\"/></a></p>\n<p>We utilized the TensorFlow provided tf.train.AdamOptimizer to control the learning rate. The AdamOptimizer improves on traditional gradient descent by using momentum (<a href=\"https://arxiv.org/abs/1206.5533\" target=\"_blank\">moving averages of the parameters</a>), facilitating efficient dynamic adjustment of hyperparameters. We can track the loss and error rate by creating summary scalars of the label error rate:</p>\n<p><script src=\"https://gist.github.com/mmmayo13/59274f4191d8e95224c03ea721b8ad67.js\"></script></p>\n<h3>How to improve an RNN</h3>\n<p>\u00a0<br/>\nNow that we have built a simple LSTM RNN network, how do we improve our error rate? Luckily for the open source community, many large companies have published the math that underlies their best performing speech recognition models. In September 2016, Microsoft released a <a href=\"https://arxiv.org/abs/1609.03528\" target=\"_blank\">paper in arXiv</a> describing how they achieved a 6.9% error rate on the NIST 200 Switchboard data. They utilized several different acoustic and language models on top of their convolutional+recurrent neural network. Several key improvements that have been made by the Microsoft team and other researchers in the past 4 years include:</p>\n<ul>\n<li>using language models on top of character based RNNs\n<li>using convolutional neural nets (CNNs) for extracting features from the audio\n<li>ensemble models that utilize multiple RNNs\n</li></li></li></ul>\n<p>It is important to note that the language models that were pioneered in traditional speech recognition models of the past few decades, are again proving valuable in the deep learning speech recognition models.<br/>\n<a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-21-at-10.59.56-AM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-21-at-10.59.56-AM.png\" width=\"75%\"/></a><br/>\nModified From: A Historical Perspective of Speech Recognition, Xuedong Huang, James Baker, Raj Reddy Communications of the ACM, Vol. 57 No. 1, Pages 94-103, 2014</p>\n<h3>Training your first RNN</h3>\n<p>\u00a0<br/>\nWe have provided a <a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial\" target=\"_blank\">GitHub repository</a> with a script that provides a working and straightforward implementation of the steps required to train an end-to-end speech recognition system using RNNs and the CTC loss function in TensorFlow. We have included example data from the <a href=\"https://librivox.org/\" target=\"_blank\">LibriVox corpus</a> in the <a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial/tree/master/data/raw/librivox/LibriSpeech\" target=\"_blank\">repository</a>. The data is separated into folders:</p>\n<ul>\n<li>Train: train-clean-100-wav (5 examples)\n<li>Test: test-clean-wav (2 examples)\n<li>Dev: dev-clean-wav (2 examples)\n</li></li></li></ul>\n<p>When training these handful of examples, you will quickly notice that the training data will be overfit to ~0% word error rate (WER), while the Test and Dev sets will be at ~85% WER. The reason the test error rate is not 100% is because out of the 29 possible character choices (a-z, apostrophe, space, blank), the network will quickly learn that:</p>\n<ul>\n<li>certain characters (e, a, space, r, s, t) are more common\n<li>consonant-vowel-consonant is a pattern in English\n<li>increased signal amplitude of the MFCC input sound features corresponds to characters a-z\n</li></li></li></ul>\n<p>The results of a training run using the default configurations in the github repository is shown below:</p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-2.08.26-PM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-2.08.26-PM.png\" width=\"99%\"/></a></p>\n<p>If you would like to train a performant model, you can add additional .wav and .txt files to these folders, or create a new folder and update `<a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial/blob/master/configs/neural_network.ini\" target=\"_blank\">configs/neural_network.ini</a>` with the folder locations. Note that it can take quite a lot of computational power to process and train on just a few hundred hours of audio, even with a powerful GPU.</p>\n<p>We hope that our provided repo is a useful resource for getting started\u2014please share your experiences with adopting RNNs in the comments. To stay in touch, sign up for our <a href=\"https://svds.com/newsletter/\" target=\"_blank\">newsletter</a> or <a href=\"https://svds.com/contact/\" target=\"_blank\">contact us</a>.</p>\n<p><b><a href=\"https://www.linkedin.com/in/mrubash1/\" target=\"_blank\">Matthew Rubashkin</a></b>, with a background in optical physics and biomedical research, has a broad range of experiences in software development, database engineering, and data analytics. He enjoys working closely with clients to develop straightforward and robust solutions to difficult problems.</p>\n<p><b><a href=\"https://www.linkedin.com/in/mattmollison/\" target=\"_blank\">Matt Mollison</a></b>, with a background in cognitive psychology and neuroscience, has extensive experience in hypothesis testing and the analysis of complex datasets. He is excited about using predictive models and other statistical methods to solve real-world problems.</p>\n<p><a href=\"https://www.svds.com/tensorflow-rnn-tutorial/?utm_campaign=KDNuggets%20Blog&amp;utm_source=KDNuggets\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/03/getting-started-deep-learning.html\">Getting Started with Deep Learning</a>\n<li><a href=\"/2017/02/anatomy-deep-learning-frameworks.html\">The Anatomy of Deep Learning Frameworks</a>\n<li><a href=\"/2017/03/open-source-toolkits-speech-recognition.html\">Open Source Toolkits for Speech Recognition</a>\n</li></li></li></ul>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/05/top-stories-2017-apr.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/05/data-science-machine-learning-platforms-enterprise.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/kdnuggets-editor.html\">Looking for a KDnuggets Editor</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning Experts</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/05/index.html\">May</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/05/tutorials.html\">Tutorials, Overviews</a> \u00bb Building, Training, and Improving on Existing Recurrent Neural Networks (\u00a0<a href=\"/2017/n18.html\">17:n18</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<div>\n<br/><span style=\"font-size:9px\">By subscribing, you agree to KDnuggets <a href=\"https://www.kdnuggets.com/news/privacy-policy.html\">privacy policy</a></span>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556484228\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.713 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-28 16:43:48 -->\n<!-- Compression = gzip -->", "content_tokenized": ["matthew", "rubashkin", "matt", "mollison", "silicon", "valley", "data", "scienc", "the", "deep", "learn", "team", "have", "investig", "recurr", "neural", "network", "for", "explor", "time", "seri", "and", "develop", "speech", "recognit", "capabl", "mani", "product", "today", "reli", "deep", "neural", "network", "that", "implement", "recurr", "layer", "includ", "product", "made", "compani", "like", "googl", "baidu", "and", "amazon", "howev", "when", "develop", "our", "own", "pipelin", "not", "find", "mani", "simpl", "and", "straightforward", "exampl", "use", "neural", "network", "for", "sequenc", "learn", "applic", "like", "speech", "recognit", "mani", "exampl", "were", "either", "power", "but", "quit", "complex", "like", "the", "activ", "develop", "deepspeech", "project", "from", "mozilla", "under", "mozilla", "public", "licens", "were", "too", "simpl", "and", "abstract", "use", "real", "data", "this", "post", "provid", "short", "tutori", "for", "train", "for", "speech", "recognit", "includ", "code", "snippet", "throughout", "and", "can", "find", "the", "accompani", "github", "repositori", "here", "the", "softwar", "use", "mix", "borrow", "and", "inspir", "code", "from", "exist", "open", "sourc", "project", "below", "video", "exampl", "machin", "speech", "recognit", "num", "edison", "phonograph", "advertis", "the", "video", "includ", "run", "trace", "sound", "amplitud", "extract", "spectrogram", "and", "predict", "text", "sinc", "have", "extens", "experi", "with", "python", "use", "welldocu", "packag", "that", "has", "been", "advanc", "leap", "and", "bound", "tensorflow", "befor", "get", "start", "are", "brand", "new", "rnns", "high", "recommend", "read", "christoph", "olah", "excel", "overview", "long", "shortterm", "memori", "network", "here", "speech", "recognit", "audio", "and", "transcript", "until", "the", "num", "the", "stateoftheart", "for", "speech", "recognit", "model", "were", "phonet", "base", "approach", "includ", "separ", "compon", "for", "pronunci", "acoust", "and", "languag", "model", "speech", "recognit", "the", "past", "and", "today", "both", "reli", "decompos", "sound", "wave", "into", "frequenc", "and", "amplitud", "use", "fourier", "transform", "yield", "spectrogram", "shown", "below", "train", "the", "acoust", "model", "for", "tradit", "speech", "recognit", "pipelin", "that", "use", "hidden", "markov", "model", "requir", "speechtext", "data", "well", "word", "phonem", "dictionari", "hmms", "are", "generat", "probabilist", "model", "for", "sequenti", "data", "and", "are", "typic", "evalu", "use", "levenshtein", "word", "error", "distanc", "string", "metric", "for", "measur", "differ", "string", "these", "model", "can", "simplifi", "and", "made", "more", "accur", "with", "speech", "data", "that", "align", "with", "phonem", "transcript", "but", "this", "tedious", "manual", "task", "becaus", "this", "effort", "phonemelevel", "transcript", "are", "less", "like", "exist", "for", "larg", "set", "speech", "data", "than", "wordlevel", "transcript", "for", "more", "inform", "exist", "open", "sourc", "speech", "recognit", "tool", "and", "model", "check", "out", "our", "colleagu", "cindi", "thompson", "recent", "post", "connectionist", "tempor", "classif", "loss", "function", "can", "discard", "the", "concept", "phonem", "when", "use", "neural", "network", "for", "speech", "recognit", "use", "object", "function", "that", "allow", "for", "the", "predict", "characterlevel", "transcript", "connectionist", "tempor", "classif", "briefli", "enabl", "the", "comput", "probabl", "multipl", "sequenc", "where", "the", "sequenc", "are", "the", "set", "all", "possibl", "characterlevel", "transcript", "the", "speech", "sampl", "the", "network", "use", "the", "object", "function", "maxim", "the", "probabl", "the", "charact", "sequenc", "choos", "the", "most", "like", "transcript", "and", "calcul", "the", "error", "for", "the", "predict", "result", "compar", "the", "actual", "transcript", "updat", "network", "weight", "dure", "train", "import", "note", "that", "the", "charact", "level", "error", "use", "loss", "function", "differ", "from", "the", "levenshtein", "word", "error", "distanc", "often", "use", "tradit", "speech", "recognit", "model", "for", "charact", "generat", "rnns", "the", "charact", "and", "word", "error", "distanc", "will", "similar", "phonet", "languag", "such", "esperonto", "and", "croatian", "where", "individu", "sound", "correspond", "distinct", "charact", "convers", "the", "charact", "versus", "word", "error", "will", "quit", "differ", "for", "nonphonet", "languag", "like", "english", "want", "learn", "more", "about", "there", "are", "mani", "paper", "and", "blog", "post", "that", "explain", "more", "detail", "will", "use", "tensorflow", "implement", "and", "there", "continu", "research", "and", "improv", "ctcrelat", "implement", "such", "this", "recent", "paper", "from", "baidu", "order", "util", "algorithm", "develop", "for", "tradit", "deep", "learn", "speech", "recognit", "model", "our", "team", "structur", "our", "speech", "recognit", "platform", "for", "modular", "and", "fast", "prototyp", "import", "data", "should", "surpris", "that", "creat", "system", "that", "transform", "speech", "into", "textual", "represent", "requir", "have", "num", "digit", "audio", "file", "and", "num", "transcript", "the", "word", "that", "were", "spoken", "becaus", "the", "model", "should", "general", "decod", "ani", "new", "speech", "sampl", "the", "more", "exampl", "can", "train", "the", "system", "the", "better", "will", "perform", "research", "freeli", "avail", "record", "transcrib", "english", "speech", "some", "exampl", "that", "have", "use", "for", "train", "are", "librispeech", "num", "hour", "num", "hour", "and", "voxforg", "num", "hour", "the", "chart", "below", "includ", "inform", "these", "dataset", "includ", "total", "size", "hour", "sampl", "rate", "and", "annot", "order", "easili", "access", "data", "from", "ani", "data", "sourc", "store", "all", "data", "flat", "format", "this", "flat", "format", "has", "singl", "wav", "and", "singl", "txt", "per", "datum", "for", "exampl", "can", "find", "exampl", "librispeech", "train", "datum", "num", "our", "github", "repo", "numwav", "and", "numtxt", "these", "data", "filenam", "are", "load", "into", "the", "tensorflow", "graph", "use", "dataset", "object", "class", "that", "assist", "tensorflow", "effici", "load", "preprocess", "the", "data", "and", "load", "individu", "batch", "data", "from", "memori", "exampl", "the", "data", "field", "the", "dataset", "object", "shown", "below", "featur", "represent", "order", "for", "machin", "recogn", "audio", "data", "the", "data", "must", "first", "convert", "from", "the", "time", "the", "frequenc", "domain", "there", "are", "sever", "method", "for", "creat", "featur", "for", "machin", "learn", "audio", "data", "includ", "bin", "arbitrari", "frequenc", "everi", "numhz", "use", "bin", "that", "match", "the", "frequenc", "band", "the", "human", "ear", "this", "typic", "humancentr", "transform", "for", "speech", "data", "comput", "melfrequ", "cepstral", "coeffici", "either", "num", "num", "differ", "cepstral", "featur", "input", "for", "the", "model", "after", "this", "transform", "the", "data", "store", "matrix", "frequenc", "coeffici", "row", "over", "time", "column", "becaus", "speech", "sound", "not", "occur", "isol", "and", "not", "have", "onetoon", "map", "charact", "can", "captur", "the", "effect", "coarticul", "the", "articul", "one", "sound", "influenc", "the", "articul", "anoth", "train", "the", "network", "overlap", "window", "num", "millisecond", "audio", "data", "that", "captur", "sound", "from", "befor", "and", "after", "the", "current", "time", "index", "exampl", "code", "how", "obtain", "featur", "and", "how", "creat", "window", "audio", "data", "shown", "below", "for", "our", "exampl", "use", "num", "time", "slice", "befor", "and", "num", "after", "for", "total", "num", "time", "point", "per", "windowwith", "num", "cepstral", "coeffici", "this", "num", "data", "point", "per", "num", "observ", "depend", "the", "data", "sampl", "rate", "recommend", "num", "cepstral", "featur", "for", "num", "and", "num", "cepstral", "featur", "for", "num", "below", "exampl", "data", "load", "window", "num", "data", "would", "like", "learn", "more", "about", "convert", "analog", "digit", "sound", "for", "speech", "recognit", "check", "out", "adam", "geitgey", "machin", "learn", "post", "model", "the", "sequenti", "natur", "speech", "long", "shortterm", "memori", "layer", "are", "type", "recurr", "neural", "network", "architectur", "that", "are", "use", "for", "model", "data", "that", "has", "longterm", "sequenti", "depend", "they", "are", "import", "for", "time", "seri", "data", "becaus", "they", "essenti", "rememb", "past", "inform", "the", "current", "time", "point", "which", "influenc", "their", "output", "this", "context", "use", "for", "speech", "recognit", "becaus", "tempor", "natur", "would", "like", "see", "how", "cell", "are", "instanti", "tensorflow", "includ", "exampl", "code", "below", "from", "the", "layer", "our", "deepspeechinspir", "bidirect", "neural", "network", "birnn", "for", "more", "detail", "about", "this", "type", "network", "architectur", "there", "are", "some", "excel", "overview", "how", "rnns", "and", "cell", "work", "addit", "there", "continu", "research", "altern", "use", "rnns", "for", "speech", "recognit", "such", "with", "convolut", "layer", "which", "are", "more", "comput", "effici", "than", "rnns", "network", "train", "and", "monitor", "becaus", "train", "our", "network", "use", "tensorflow", "were", "abl", "visual", "the", "comput", "graph", "well", "monitor", "the", "train", "valid", "and", "test", "perform", "from", "web", "portal", "with", "veri", "littl", "extra", "effort", "use", "tensorboard", "use", "tip", "from", "dandelion", "mane", "great", "talk", "the", "numtensorflow", "dev", "summit", "util", "tfnamescop", "add", "node", "and", "layer", "name", "and", "write", "out", "our", "summari", "file", "the", "result", "this", "automat", "generat", "understand", "comput", "graph", "such", "this", "exampl", "bidirect", "neural", "network", "birnn", "below", "the", "data", "pass", "amongst", "differ", "oper", "from", "bottom", "left", "top", "right", "the", "differ", "node", "can", "label", "and", "color", "with", "namespac", "for", "clariti", "this", "exampl", "teal", "box", "correspond", "fulli", "connect", "layer", "and", "the", "green", "and", "box", "correspond", "bias", "and", "weight", "respect", "util", "the", "tensorflow", "provid", "tftrainadamoptim", "control", "the", "learn", "rate", "the", "adamoptim", "improv", "tradit", "gradient", "descent", "use", "momentum", "move", "averag", "the", "paramet", "facilit", "effici", "dynam", "adjust", "hyperparamet", "can", "track", "the", "loss", "and", "error", "rate", "creat", "summari", "scalar", "the", "label", "error", "rate", "how", "improv", "now", "that", "have", "built", "simpl", "network", "how", "improv", "our", "error", "rate", "luckili", "for", "the", "open", "sourc", "communiti", "mani", "larg", "compani", "have", "publish", "the", "math", "that", "under", "their", "best", "perform", "speech", "recognit", "model", "septemb", "num", "microsoft", "releas", "paper", "arxiv", "describ", "how", "they", "achiev", "num", "error", "rate", "the", "num", "switchboard", "data", "they", "util", "sever", "differ", "acoust", "and", "languag", "model", "top", "their", "convolutionalrecurr", "neural", "network", "sever", "key", "improv", "that", "have", "been", "made", "the", "microsoft", "team", "and", "other", "research", "the", "past", "num", "year", "includ", "use", "languag", "model", "top", "charact", "base", "rnns", "use", "convolut", "neural", "net", "cnns", "for", "extract", "featur", "from", "the", "audio", "ensembl", "model", "that", "util", "multipl", "rnns", "import", "note", "that", "the", "languag", "model", "that", "were", "pioneer", "tradit", "speech", "recognit", "model", "the", "past", "few", "decad", "are", "again", "prove", "valuabl", "the", "deep", "learn", "speech", "recognit", "model", "modifi", "from", "histor", "perspect", "speech", "recognit", "xuedong", "huang", "jame", "baker", "raj", "reddi", "communic", "the", "vol", "num", "num", "page", "num", "num", "train", "first", "have", "provid", "github", "repositori", "with", "script", "that", "provid", "work", "and", "straightforward", "implement", "the", "step", "requir", "train", "endtoend", "speech", "recognit", "system", "use", "rnns", "and", "the", "loss", "function", "tensorflow", "have", "includ", "exampl", "data", "from", "the", "librivox", "corpus", "the", "repositori", "the", "data", "separ", "into", "folder", "train", "traincleannumwav", "num", "exampl", "test", "testcleanwav", "num", "exampl", "dev", "devcleanwav", "num", "exampl", "when", "train", "these", "hand", "exampl", "will", "quick", "notic", "that", "the", "train", "data", "will", "overfit", "num", "word", "error", "rate", "while", "the", "test", "and", "dev", "set", "will", "num", "the", "reason", "the", "test", "error", "rate", "not", "num", "becaus", "out", "the", "num", "possibl", "charact", "choic", "apostroph", "space", "blank", "the", "network", "will", "quick", "learn", "that", "certain", "charact", "space", "are", "more", "common", "consonantvowelconson", "pattern", "english", "increas", "signal", "amplitud", "the", "input", "sound", "featur", "correspond", "charact", "the", "result", "train", "run", "use", "the", "default", "configur", "the", "github", "repositori", "shown", "below", "would", "like", "train", "perform", "model", "can", "add", "addit", "wav", "and", "txt", "file", "these", "folder", "creat", "new", "folder", "and", "updat", "configsneuralnetworkini", "with", "the", "folder", "locat", "note", "that", "can", "take", "quit", "lot", "comput", "power", "process", "and", "train", "just", "few", "hundr", "hour", "audio", "even", "with", "power", "hope", "that", "our", "provid", "repo", "use", "resourc", "for", "get", "started\u2014pleas", "share", "experi", "with", "adopt", "rnns", "the", "comment", "stay", "touch", "sign", "for", "our", "newslett", "contact", "matthew", "rubashkin", "with", "background", "optic", "physic", "and", "biomed", "research", "has", "broad", "rang", "experi", "softwar", "develop", "databas", "engin", "and", "data", "analyt", "enjoy", "work", "close", "with", "client", "develop", "straightforward", "and", "robust", "solut", "difficult", "problem", "matt", "mollison", "with", "background", "cognit", "psycholog", "and", "neurosci", "has", "extens", "experi", "hypothesi", "test", "and", "the", "analysi", "complex", "dataset", "excit", "about", "use", "predict", "model", "and", "other", "statist", "method", "solv", "realworld", "problem", "origin", "repost", "with", "permiss", "relat", "get", "start", "with", "deep", "learn", "the", "anatomi", "deep", "learn", "framework", "open", "sourc", "toolkit", "for", "speech", "recognit"], "timestamp_scraper": 1556484229.261881, "title": "Building, Training, and Improving on Existing Recurrent Neural Networks", "read_time": 610.5, "content_html": "<div class=\"post\" id=\"post-\">\n<p><b>By Matthew Rubashkin &amp; Matt Mollison, Silicon Valley Data Science.</b></p>\n<p><img alt=\"Header image\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/fractal-1765220_1280_header.jpg\" width=\"90%\"/></p>\n<p>On the deep learning R&amp;D team at SVDS, we have investigated Recurrent Neural Networks (RNN) for exploring time series and developing speech recognition capabilities. Many products today rely on deep neural networks that implement recurrent layers, including products made by companies like <a href=\"https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html\" target=\"_blank\">Google</a>, <a href=\"https://arxiv.org/abs/1412.5567\" target=\"_blank\">Baidu</a>, and <a href=\"https://www.slideshare.net/AmandaMackay4/amazon-deep-learning\" target=\"_blank\">Amazon</a>.</p>\n<p>However, when developing our own RNN pipelines, we did not find many simple and straightforward examples of using neural networks for sequence learning applications like speech recognition. Many examples were either powerful but quite complex, like the actively developed <a href=\"https://github.com/mozilla/DeepSpeech\" target=\"_blank\">DeepSpeech</a> project from Mozilla under Mozilla Public License, or were too simple and abstract to be used on real data.</p>\n<p>In this post, we\u2019ll provide a short tutorial for training a RNN for speech recognition; we\u2019re including code snippets throughout, and you can find the accompanying <a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial\" target=\"_blank\">GitHub repository here</a>. The software we\u2019re using is a mix of borrowed and inspired code from existing open source projects. Below is a video example of machine speech recognition on a <a href=\"https://en.wikipedia.org/wiki/File:Advertising_Record.ogg\" target=\"_blank\">1906 Edison Phonograph advertisement</a>. The video includes a running trace of sound amplitude, extracted spectrogram, and predicted text.</p>\n<p><center><iframe allowfullscreen=\"allowfullscreen\" data-mce-fragment=\"1\" frameborder=\"0\" height=\"500\" src=\"https://www.youtube.com/embed/y_bojXC_kBg?feature=oembed\" width=\"700\"></iframe></center></p>\n<p>Since we have extensive experience with Python, we used a well-documented package that has been <a href=\"http://svds.com/getting-started-deep-learning/\" target=\"_blank\">advancing by leaps and bounds</a>: <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a>. Before you get started, if you are brand new to RNNs, we highly recommend you read Christopher Olah\u2019s excellent overview of RNN Long Short-Term Memory (LSTM) networks <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\">here</a>.</p>\n<h3>Speech recognition: audio and transcriptions</h3>\n<p>\u00a0<br>\nUntil the 2010\u2019s, the state-of-the-art for speech recognition models were <a href=\"https://en.wikipedia.org/wiki/Phonetics\" target=\"_blank\">phonetic</a>-based approaches including separate components for pronunciation, <a href=\"https://en.wikipedia.org/wiki/Acoustic_model\" target=\"_blank\">acoustic</a>, and <a href=\"https://en.wikipedia.org/wiki/Language_model\" target=\"_blank\">language</a> models. Speech recognition in the past and today both rely on decomposing sound waves into frequency and amplitude using fourier transforms, yielding a spectrogram as shown below.</br></p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/spectrogram_edison.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/spectrogram_edison.png\" width=\"99%\"/></a></p>\n<p>Training the acoustic model for a traditional speech recognition pipeline that uses Hidden Markov Models (HMM) requires\u00a0speech+text data, as well as\u00a0a word to phoneme dictionary. HMMs are generative probabilistic models for sequential data, and are typically evaluated using <a href=\"https://en.wikipedia.org/wiki/Levenshtein_distance\" target=\"_blank\">Levenshtein word error distance</a>, a string metric for measuring differences in strings.</p>\n<p>These models can be simplified and made more accurate with speech data that is aligned with phoneme transcriptions, but this a tedious manual task. Because of this effort, phoneme-level transcriptions are less likely to exist for large sets of speech data than word-level transcriptions. For more information on existing open source speech recognition tools and models, check out our colleague Cindi Thompson\u2019s <a href=\"http://svds.com/open-source-toolkits-speech-recognition/\" target=\"_blank\">recent post</a>.</p>\n<h3>Connectionist Temporal Classification (CTC) loss function</h3>\n<p>\u00a0<br>\nWe can discard the concept of phonemes when using neural networks for speech recognition by using an objective function that allows for the prediction of character-level transcriptions: <a href=\"http://www.cs.toronto.edu/~graves/icml_2006.pdf\" target=\"_blank\">Connectionist Temporal Classification</a> (CTC). Briefly, CTC enables the computation of probabilities of multiple sequences, where the sequences are the set of all possible character-level transcriptions of the speech sample. The network uses the objective function to maximize the probability of the character sequence (i.e., chooses the most likely transcription), and calculates the error for the predicted result compared to the actual transcription to update network weights during training.</br></p>\n<p>It is important to note that the <strong>character</strong>-level error used by a CTC loss function differs from the Levenshtein <strong>word</strong> error distance often used in traditional speech recognition models. For character generating RNNs, the character and word error distance will be similar in phonetic languages such as Esperonto and Croatian, where individual sounds correspond to distinct characters. Conversely, the character versus word error will be quite different for a non-phonetic language like English.</p>\n<p>If you want to learn more about CTC, there are many papers and <a href=\"https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0?gi=24cd18fe52c#.wbsc6x23a\" target=\"_blank\">blog posts</a> that explain it in more detail. We will use TensorFlow\u2019s <a href=\"https://www.tensorflow.org/api_guides/python/nn#Connectionist_Temporal_Classification_CTC_\" target=\"_blank\">CTC implementation</a>, and there continues to be research and improvements on CTC-related implementations, such as <a href=\"https://arxiv.org/abs/1703.00096\" target=\"_blank\">this recent paper</a> from Baidu. In order to utilize algorithms developed for traditional or deep learning speech recognition models, our team structured our speech recognition platform for modularity and fast prototyping:</p>\n<h3><a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-2.40.46-PM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-2.40.46-PM.png\" width=\"99%\"/></a></h3>\n<p>\u00a0</p>\n<h3>Importance of data</h3>\n<p>\u00a0<br>\nIt should be no surprise that creating a system that transforms speech into its textual representation requires having (1) digital audio files and (2) transcriptions of the words that were spoken. Because the model should generalize to decode any new speech samples, the more examples we can train the system on, the better it will perform. We researched freely available recordings of transcribed English speech; some examples that we have used for training are <a href=\"http://www.openslr.org/12/\" target=\"_blank\">LibriSpeech</a> (1000 hours), <a href=\"http://www.openslr.org/7/\" target=\"_blank\">TED-LIUM</a> (118 hours), and <a href=\"http://www.voxforge.org/\" target=\"_blank\">VoxForge</a> (130 hours). The chart below includes information on these datasets including total size in hours, sampling rate, and annotation.</br></p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/RNN_datasets.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/RNN_datasets.png\" width=\"85%\"/></a></p>\n<p>In order to easily access data from any data source, we store all data in a flat format. This flat format has a single .wav and a single .txt per datum. For example, you can find example Librispeech Training datum \u2018211-122425-0059\u2019 in our <a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial\" target=\"_blank\">GitHub repo</a> as 211-122425-0059.wav and 211-122425-0059.txt. These data filenames are loaded into the TensorFlow graph using a datasets object class, that assists TensorFlow in efficiently loading, preprocessing the data, and loading individual batches of data from CPU to GPU memory. An example of the data fields in the<a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial/blob/master/src/data_manipulation/datasets.py\" target=\"_blank\"> datasets object</a> is shown below:</p>\n<p><script src=\"https://gist.github.com/mmmayo13/3ce61ff237f84587d16c9cbd4786392e.js\"></script></p>\n<h3>Feature representation</h3>\n<p>\u00a0<br>\nIn order for a machine to recognize audio data, the data must first be converted from the time to the frequency domain. There are several methods for creating features for machine learning of audio data, including binning by arbitrary frequencies (i.e., every 100Hz), or by using binning that matches the frequency bands of the human ear. This typical human-centric transformation for speech data is to compute <a href=\"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\" target=\"_blank\">Mel-frequency cepstral coefficients</a> (MFCC), either 13 or 26 different cepstral features, as input for the model. After this transformation the data is stored as a matrix of frequency coefficients (rows) over time (columns).</br></p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-3.34.51-PM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-3.34.51-PM.png\" width=\"75%\"/></a></p>\n<p>Because speech sounds do not occur in isolation and do not have a one-to-one mapping to characters, we can capture the effects of coarticulation (the articulation of one sound influencing the articulation of another) by training the network on overlapping windows (10s of milliseconds) of audio data that captures sound from before and after the current time index. Example code of how to obtain MFCC features, and how to create windows of audio data is shown below:</p>\n<p><script src=\"https://gist.github.com/mmmayo13/0b13793c99b41ce821ce49bceef3ecf4.js\"></script></p>\n<p>For our RNN example, we use 9 time slices before and 9 after, for a total of 19 time points per window.With 26 cepstral coefficients, this is 494 data points per 25 ms observation.\u00a0Depending on the data sampling rate, we recommend 26 cepstral features for 16,000 Hz and 13 cepstral features for 8,000 hz. Below is an example of data loading windows on 8,000 Hz data:</p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-21-at-10.45.40-AM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-21-at-10.45.40-AM.png\" width=\"60%\"/></a></p>\n<p>If you would like to learn more about converting analog to digital sound for RNN speech recognition, check out Adam Geitgey\u2019s <a href=\"https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a#.pwd1rl7cw\" target=\"_blank\">machine learning post</a>.</p>\n<h3>Modeling the sequential nature of speech</h3>\n<p>\u00a0<br/>\nLong Short-Term Memory (LSTM) layers are a type of recurrent neural network (RNN) architecture that are useful for modeling data that has long-term sequential dependencies. They are important for time series data because they essentially remember past information at the current time point, which influences their output. This context is useful for speech recognition because of its temporal nature. If you would like to see how LSTM cells are instantiated in TensorFlow, we\u2019ve include example code below from the\u00a0LSTM layer of our\u00a0DeepSpeech-inspired\u00a0Bi-Directional Neural Network (BiRNN).</p>\n<p><script src=\"https://gist.github.com/mmmayo13/a6f91c4a74f5fedfe1e88a2efe1892fc.js\"></script></p>\n<p>For more details about this type of network architecture, there are some excellent overviews of how <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\" target=\"_blank\">RNNs</a> and <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\">LSTM cells</a> work. Additionally, there continues to be research on alternatives to using RNNs for speech recognition, such as with <a href=\"https://arxiv.org/abs/1701.02720\" target=\"_blank\">convolutional layers</a> which are more computationally efficient than RNNs.</p>\n<h3>Network training and monitoring</h3>\n<p>\u00a0<br/>\nBecause we trained our network using TensorFlow, we were able to visualize the computational graph as well as monitor the training, validation, and test performance from a web portal with very little extra effort using <a href=\"https://www.tensorflow.org/get_started/summaries_and_tensorboard\" target=\"_blank\">TensorBoard</a>. Using tips from Dandelion Mane\u2019s <a href=\"https://www.youtube.com/watch?v=eBbEDRsCmv4\" target=\"_blank\">great talk</a> at the 2017TensorFlow Dev Summit, we utilize tf.name_scope to add node and layer names, and write out our summary to file. The results of this is an automatically generated, understandable computational graph, such as this example of a Bi-Directional Neural Network (BiRNN) below. The data is passed amongst different operations from bottom left to top right. The different nodes can be labelled and colored with namespaces for clarity. In this example, teal \u2018fc\u2019 boxes correspond to fully connected layers, and the green \u2018b\u2019 and \u2018h\u2019 boxes correspond to biases and weights, respectively.</p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/png-2.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/png-2.png\" width=\"99%\"/></a></p>\n<p>We utilized the TensorFlow provided tf.train.AdamOptimizer to control the learning rate. The AdamOptimizer improves on traditional gradient descent by using momentum (<a href=\"https://arxiv.org/abs/1206.5533\" target=\"_blank\">moving averages of the parameters</a>), facilitating efficient dynamic adjustment of hyperparameters. We can track the loss and error rate by creating summary scalars of the label error rate:</p>\n<p><script src=\"https://gist.github.com/mmmayo13/59274f4191d8e95224c03ea721b8ad67.js\"></script></p>\n<h3>How to improve an RNN</h3>\n<p>\u00a0<br/>\nNow that we have built a simple LSTM RNN network, how do we improve our error rate? Luckily for the open source community, many large companies have published the math that underlies their best performing speech recognition models. In September 2016, Microsoft released a <a href=\"https://arxiv.org/abs/1609.03528\" target=\"_blank\">paper in arXiv</a> describing how they achieved a 6.9% error rate on the NIST 200 Switchboard data. They utilized several different acoustic and language models on top of their convolutional+recurrent neural network. Several key improvements that have been made by the Microsoft team and other researchers in the past 4 years include:</p>\n<ul>\n<li>using language models on top of character based RNNs\n<li>using convolutional neural nets (CNNs) for extracting features from the audio\n<li>ensemble models that utilize multiple RNNs\n</li></li></li></ul>\n<p>It is important to note that the language models that were pioneered in traditional speech recognition models of the past few decades, are again proving valuable in the deep learning speech recognition models.<br/>\n<a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-21-at-10.59.56-AM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-21-at-10.59.56-AM.png\" width=\"75%\"/></a><br/>\nModified From: A Historical Perspective of Speech Recognition, Xuedong Huang, James Baker, Raj Reddy Communications of the ACM, Vol. 57 No. 1, Pages 94-103, 2014</p>\n<h3>Training your first RNN</h3>\n<p>\u00a0<br/>\nWe have provided a <a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial\" target=\"_blank\">GitHub repository</a> with a script that provides a working and straightforward implementation of the steps required to train an end-to-end speech recognition system using RNNs and the CTC loss function in TensorFlow. We have included example data from the <a href=\"https://librivox.org/\" target=\"_blank\">LibriVox corpus</a> in the <a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial/tree/master/data/raw/librivox/LibriSpeech\" target=\"_blank\">repository</a>. The data is separated into folders:</p>\n<ul>\n<li>Train: train-clean-100-wav (5 examples)\n<li>Test: test-clean-wav (2 examples)\n<li>Dev: dev-clean-wav (2 examples)\n</li></li></li></ul>\n<p>When training these handful of examples, you will quickly notice that the training data will be overfit to ~0% word error rate (WER), while the Test and Dev sets will be at ~85% WER. The reason the test error rate is not 100% is because out of the 29 possible character choices (a-z, apostrophe, space, blank), the network will quickly learn that:</p>\n<ul>\n<li>certain characters (e, a, space, r, s, t) are more common\n<li>consonant-vowel-consonant is a pattern in English\n<li>increased signal amplitude of the MFCC input sound features corresponds to characters a-z\n</li></li></li></ul>\n<p>The results of a training run using the default configurations in the github repository is shown below:</p>\n<p><a href=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-2.08.26-PM.png\" target=\"_blank\"><img alt=\"\" class=\"aligncenter\" src=\"https://www.svds.com/wp-content/uploads/2017/03/Screen-Shot-2017-03-22-at-2.08.26-PM.png\" width=\"99%\"/></a></p>\n<p>If you would like to train a performant model, you can add additional .wav and .txt files to these folders, or create a new folder and update `<a href=\"https://github.com/silicon-valley-data-science/RNN-Tutorial/blob/master/configs/neural_network.ini\" target=\"_blank\">configs/neural_network.ini</a>` with the folder locations. Note that it can take quite a lot of computational power to process and train on just a few hundred hours of audio, even with a powerful GPU.</p>\n<p>We hope that our provided repo is a useful resource for getting started\u2014please share your experiences with adopting RNNs in the comments. To stay in touch, sign up for our <a href=\"https://svds.com/newsletter/\" target=\"_blank\">newsletter</a> or <a href=\"https://svds.com/contact/\" target=\"_blank\">contact us</a>.</p>\n<p><b><a href=\"https://www.linkedin.com/in/mrubash1/\" target=\"_blank\">Matthew Rubashkin</a></b>, with a background in optical physics and biomedical research, has a broad range of experiences in software development, database engineering, and data analytics. He enjoys working closely with clients to develop straightforward and robust solutions to difficult problems.</p>\n<p><b><a href=\"https://www.linkedin.com/in/mattmollison/\" target=\"_blank\">Matt Mollison</a></b>, with a background in cognitive psychology and neuroscience, has extensive experience in hypothesis testing and the analysis of complex datasets. He is excited about using predictive models and other statistical methods to solve real-world problems.</p>\n<p><a href=\"https://www.svds.com/tensorflow-rnn-tutorial/?utm_campaign=KDNuggets%20Blog&amp;utm_source=KDNuggets\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/03/getting-started-deep-learning.html\">Getting Started with Deep Learning</a>\n<li><a href=\"/2017/02/anatomy-deep-learning-frameworks.html\">The Anatomy of Deep Learning Frameworks</a>\n<li><a href=\"/2017/03/open-source-toolkits-speech-recognition.html\">Open Source Toolkits for Speech Recognition</a>\n</li></li></li></ul>\n</div> ", "website": "kdnuggets"}