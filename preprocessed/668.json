{"content": "By Matthew Mayo , KDnuggets. At the intersection of computational linguistics and artificial intelligence is where we find natural language processing. Very broadly, natural language processing (NLP) is a discipline which is interested in how human languages, and, to some extent, the humans who speak them, interact with technology. NLP is an interdisciplinary topic which has historically been the equal domain of artificial intelligence researchers and linguistics alike; perhaps obviously, those approaching the discipline from the linguistics side must get up to speed on technology, while those entering the discipline from the technology realm need to learn the linguistic concepts. It is this second group that this post aims to serve at an introductory level, as we take a no-nonsense approach to defining some key NLP terminology. While you certainly won't be a linguistic expert after reading this, we hope that you are better able to understand some of the NLP-related discourse, and gain perspective as to how to proceed with learning more on the topics herein. So here they are, 18 select natural language processing terms, concisely defined, with links to further reading where appropriate. 1. Natural Language Processing (NLP) Natural language processing (NLP) concerns itself with the interaction between natural human languages and computing devices. NLP is a major aspect of computational linguistics, and also falls within the realms of computer science and artificial intelligence. 2. Tokenization Tokenization is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or tokens . Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized. 3. Normalization Before further processing, text needs to be normalized. Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly. 4. Stemming Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem. running \u2192 run 5. Lemmatization Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma . For example, stemming the word \"better\" would fail to return its citation form (another word for lemma); however, lemmatization would result in the following: better \u2192 good It should be easy to see why the implementation of a stemmer would be the less difficult feat of the two. 6. Corpus In linguistics and NLP, corpus (literally Latin for body ) refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful. Corpora may also consist of themed texts (historical, Biblical, etc.). Corpora are generally solely used for statistical linguistic analysis and hypothesis testing. 7. Stop Words Stop words are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" while all required words in a particular passage, don't generally contribute greatly to one's understanding of content. As a simple example, the following panagram is just as legible if the stop words are removed: The quick brown fox jumps over the lazy dog. 8. Parts-of-speech (POS) Tagging POS tagging consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc. 9. Statistical Language Modeling Statistical Language Modeling is the process of building a statistical language model which is meant to provide an estimate of a natural language. For a sequence of input words, the model would assign a probability to the entire sequence, which contributes to the estimated likelihood of various possible sequences. This can be especially useful for NLP applications which generate text. 10. Bag of Words Bag of words is a particular representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order, but is interested in the number of occurrences of words within the text. The ultimate representation of the text selection is that of a bag of words ( bag referring to the set theory concept of multisets , which differ from simple sets). Actual storage mechanisms for the bag of words representation can vary, but the following is a simple example using a dictionary for intuitiveness. Sample text: \"Well, well, well,\" said John. \"There, there,\" said James. \"There, there.\"", "title_html": "<h1 id=\"title\">Natural Language Processing Key Terms, Explained</h1> ", "url": "https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html", "tfidf": {"tfidf": {"after": 2.04140414042, "base": 1.14628158845, "natur": 10.774869109970002, "matthew": 6.908616187989999, "understand": 5.93717277486, "equival": 4.09175257732, "enter": 1.75813953488, "fall": 1.6945244956799999, "smaller": 2.59369384088, "panagram": 481.09090909099996, "piec": 6.48264597796, "here": 2.42307692308, "lower": 2.10055570257, "form": 3.38267045454, "foot": 5.65384615385, "punctuat": 42.1114058355, "num": 11.00346544011, "assign": 7.67327211214, "uniform": 5.7231434751300005, "alik": 16.2, "would": 5.414364640899999, "human": 5.68964281449, "number": 2.20285833218, "been": 2.0478555304799997, "etc": 12.6200317965, "upper": 3.41052631579, "run": 3.11385701676, "omit": 13.0131147541, "dog": 6.26272189349, "sentenc": 17.536082474220002, "just": 1.33580143037, "perspect": 5.03520456708, "entir": 1.59365589239, "devic": 5.00820189274, "speed": 3.8703071672400005, "fox": 6.53064582476, "topic": 10.915091096600001, "approach": 4.15113086678, "lemmat": 1924.3636363639998, "collect": 3.28219971056, "sole": 4.04175152749, "contribut": 5.77659187386, "term": 1.39520168732, "equal": 5.08438751, "noun": 30.068181818200003, "post": 2.23826307627, "their": 1.01547908405, "multipl": 2.74813917258, "intuit": 27.7068062827, "how": 3.20500656102, "test": 2.65707112971, "singl": 1.60948905109, "them": 1.09876115994, "verb": 29.291512915100004, "chunk": 81.0, "seri": 1.46511627907, "dictionari": 5.2292490118599995, "simpl": 10.19434931508, "whi": 3.2566153846200003, "jame": 1.9313868613099998, "especi": 1.66712170534, "expert": 5.36713995943, "mechan": 3.41492794149, "expand": 2.2260235558000003, "given": 1.35426085473, "word": 43.116894873839996, "interest": 3.20662492426, "abl": 3.6417020300400003, "string": 8.37783641161, "over": 1.02525024217, "concept": 5.31414225942, "perhap": 3.14812611541, "build": 1.6341739578, "analysi": 3.47852760736, "second": 1.1130898128, "herein": 217.479452055, "select": 6.07035432066, "broad": 4.27693965517, "one": 1.00627495722, "general": 6.730921424520001, "earli": 1.12468121281, "perform": 1.5313977042500002, "but": 2.03264835798, "certain": 1.8077886586200003, "obtain": 2.68629441624, "there": 5.20456333595, "longer": 2.02319357716, "scienc": 2.31969608416, "where": 2.13430127042, "overal": 3.0442953020099996, "likelihood": 23.4852071006, "applic": 3.42672134686, "linguist": 77.16160388824001, "level": 3.3088786994599997, "exampl": 4.51450236966, "has": 2.0872995004, "generat": 2.05275407292, "introductori": 30.297709923699998, "categori": 3.98194130926, "take": 1.13961668222, "follow": 3.1392037964699995, "further": 5.447246526, "out": 1.06016694491, "model": 12.5435870424, "interact": 8.8371834122, "see": 1.27242125511, "good": 1.51981619759, "lemma": 538.169491526, "disciplin": 20.78917503273, "circumfix": 481.09090909099996, "brown": 3.2215909090900006, "numer": 1.83325635104, "also": 2.02953020134, "field": 1.7790228597, "hypothesi": 13.580838323399998, "mean": 1.44906900329, "interdisciplinari": 37.8902147971, "grammar": 14.7, "corpora": 1443.2727272729999, "storag": 8.623574144489998, "task": 3.88641370869, "major": 1.14852058164, "research": 1.9420183486200002, "theori": 3.02745995423, "technolog": 7.8104296490700005, "the": 42.0, "may": 3.15605327679, "less": 1.46904783936, "mayo": 49.7680250784, "aim": 2.8960233491400005, "tag": 78.9850746268, "occurr": 13.805217391300001, "partsofspeech": 481.09090909099996, "easi": 5.2937645882, "citat": 16.8177966102, "those": 3.58644578313, "said": 3.09503850278, "appropri": 8.62826086956, "convert": 6.5481542586199994, "latin": 3.80902111324, "sinc": 1.08368600683, "they": 2.06034650574, "anoth": 1.13643521832, "remov": 4.011623499680001, "howev": 1.0945191313299998, "allow": 1.2716059271100002, "get": 1.78562591385, "elimin": 3.67670217693, "kdnugget": 481.09090909099996, "token": 235.94904458579998, "represent": 17.784914115, "larger": 2.2407904022599996, "span": 6.17262830482, "which": 9.046726605, "stem": 37.2676056338, "plural": 14.2898289829, "estim": 4.6998223801, "must": 1.9220338983099996, "liter": 5.468825353080001, "that": 5.01992031875, "refer": 3.9007371007500002, "itself": 1.74557449148, "provid": 1.21552714187, "consist": 2.9802890932999997, "terminolog": 17.6989966555, "hope": 2.50884955752, "captur": 2.88026124819, "intersect": 12.660287081300002, "such": 1.06151377374, "result": 1.14611608432, "corpus": 72.273141123, "affix": 58.3676470588, "intellig": 12.580031695710002, "implement": 3.57648118946, "text": 46.924137931050005, "some": 3.1211009174399997, "multilingu": 78.2068965517, "put": 3.31613577024, "discours": 16.468879668, "concern": 1.8852867830400002, "play": 1.46390041494, "content": 7.0843373494, "filter": 16.8893617021, "who": 1.06279287723, "well": 3.1967246123999997, "languag": 32.12836079796, "set": 2.37415881562, "this": 4.01517450684, "two": 1.01379310345, "sampl": 7.23280182232, "should": 1.6643254009900001, "from": 4.00226885988, "part": 1.04330682789, "relat": 2.47501753838, "lazi": 48.5504587156, "littl": 1.5499365420299998, "differ": 2.4730898045, "instanc": 3.2572835453400004, "statist": 16.97060395512, "most": 2.04192926046, "between": 1.03453668708, "return": 1.39532431007, "domain": 9.39408284024, "for": 10.003150400100001, "difficult": 2.48957189901, "obvious": 6.44841592201, "quick": 2.205, "fail": 1.9281029876099998, "need": 2.8745247148199997, "all": 4.04587155964, "simplifi": 12.109839816900001, "with": 4.004792835959999, "requir": 1.52844902282, "reason": 1.72340425532, "are": 8.23924748624, "speak": 2.89127663449, "split": 3.4709226060300002, "better": 6.01971688575, "feat": 17.070967741900002, "case": 1.48498737256, "normal": 10.44301924024, "popular": 1.50769230769, "histor": 3.3511345646399997, "bag": 95.1608391606, "more": 1.0171706817, "and": 14.000881889819999, "suffix": 28.4516129032, "veri": 1.25880114177, "these": 1.07415426252, "theme": 3.8255421686699997, "gain": 1.84819557625, "nononsens": 481.09090909099996, "befor": 2.20072082062, "realm": 21.5560081466, "artifici": 24.949188056580002, "multiset": 481.09090909099996, "group": 1.20996875238, "serv": 1.4668760972, "can": 5.8813069571, "process": 20.34297917784, "john": 1.35553278689, "comput": 15.711034141520003, "possibl": 1.4173734488, "side": 1.5989525632, "defin": 5.45660766454, "great": 1.26592775696, "prefix": 25.8146341463, "key": 2.28005170185, "bodi": 1.8618505922400002, "legibl": 223.605633803, "step": 5.655860349119999, "read": 4.629921259840001, "link": 2.15151104486, "common": 1.4025974025999999, "contract": 3.0165304959099997, "while": 3.1325966850899993, "sequenc": 18.21338432121, "probabl": 2.64555907349, "vari": 2.4970116388799997, "passag": 4.629921259840001, "into": 3.04507384437, "ultim": 2.58524670249, "biblic": 15.3986420951, "nlprelat": 481.09090909099996, "stemmer": 481.09090909099996, "aspect": 3.0893169877399997, "adject": 32.5327868852, "jump": 8.07117437722, "identifi": 2.30187037843, "concis": 22.647646219699997, "meant": 6.41195476576, "canon": 10.8220858896, "infix": 360.818181818, "proceed": 6.866782006919999, "input": 12.2029208301, "various": 1.3323262839899999, "extent": 4.09491875161, "use": 5.148193786899999, "particular": 2.7629655412400003, "order": 2.49250333622, "find": 1.7294117647099998, "within": 2.4738605376, "learn": 4.6455010973, "actual": 1.87482286254, "same": 1.11857958148, "stop": 6.535126234920001}, "logtfidf": {"after": 0.040981389296199995, "base": 0.13652330228700002, "natur": 3.019144375044, "matthew": 1.9327693554900003, "understand": 2.1761717513599996, "equival": 1.40897338129, "enter": 0.564256167492, "fall": 0.527402167952, "smaller": 0.9530830530519999, "panagram": 6.17605625244, "piec": 2.35196315278, "here": 0.8850381883700001, "lower": 0.742201929994, "form": 0.36015955257300003, "foot": 1.73233604876, "punctuat": 3.7403186264499997, "num": 0.0034648943493670007, "assign": 2.6891919112, "uniform": 1.74451821303, "alik": 2.7850112422400004, "would": 0.3980881398235, "human": 1.920105549729, "number": 0.1932171568372, "been": 0.04729196473680001, "etc": 4.310019263910001, "upper": 1.22686662419, "run": 0.885429951078, "omit": 2.56595767618, "dog": 1.8346148978799999, "sentenc": 5.296944975660001, "just": 0.289531434109, "perspect": 1.61645415436, "entir": 0.46603068026999994, "devic": 1.6110769470299997, "speed": 1.3533338752700002, "fox": 1.8765058395900003, "topic": 3.3939983108200003, "approach": 1.4604672291620002, "lemmat": 24.70422500976, "collect": 0.99073332104, "sole": 1.3966781444299998, "contribut": 1.965604736727, "term": 0.33303898354600003, "equal": 1.866054782686, "noun": 3.4034675302, "post": 0.8057001527009999, "their": 0.015360505122700001, "multipl": 1.01092401812, "intuit": 3.3216780971900004, "how": 0.9431339138600001, "test": 0.977224437103, "singl": 0.475916769059, "them": 0.0941833269093, "verb": 3.3772978124599997, "chunk": 4.394449154669999, "seri": 0.38193461069799994, "dictionari": 1.65426767539, "simpl": 3.66966386817, "whi": 1.18068843047, "jame": 0.658238325853, "especi": 0.511098609709, "expert": 1.68029517063, "mechan": 1.22815639221, "expand": 0.80021683492, "given": 0.303255810831, "word": 14.06066597724, "interest": 0.9441435559639999, "abl": 1.19860796495, "string": 2.1255896963900005, "over": 0.0249367214957, "concept": 1.954448874206, "perhap": 1.14680739183, "build": 0.491137452091, "analysi": 1.2466091029200002, "second": 0.10713976337999999, "herein": 5.38210437275, "select": 2.114414061399, "broad": 1.45323772, "one": 0.0062553516455, "general": 0.689715468378, "earli": 0.117499629108, "perform": 0.42618085058, "but": 0.0323847441438, "certain": 0.592104362781, "obtain": 0.988162703503, "there": 0.2004894646275, "longer": 0.7046772417749999, "scienc": 0.841436178891, "where": 0.1299842774914, "overal": 1.1132694464700001, "likelihood": 3.15637073786, "applic": 1.23160392849, "linguist": 18.13168346136, "level": 1.006924379886, "exampl": 1.2260480249969998, "has": 0.0854478897096, "generat": 0.719182341736, "introductori": 3.41107212958, "categori": 1.38176946652, "take": 0.130691962197, "follow": 0.1360707332826, "further": 1.235263581188, "out": 0.0584263909193, "model": 4.424700438666001, "interact": 2.9716420535799997, "see": 0.240921585492, "good": 0.418589404907, "lemma": 11.19005274, "disciplin": 5.8074603943500005, "circumfix": 6.17605625244, "brown": 1.16987530869, "numer": 0.606093812346, "also": 0.0293143156, "field": 0.5760642583510001, "hypothesi": 2.60865985243, "mean": 0.37092128352, "interdisciplinari": 3.63469289398, "grammar": 2.68784749378, "corpora": 18.52816875732, "storag": 2.1544996326700003, "task": 1.35748680661, "major": 0.138474663439, "research": 0.663727818138, "theori": 1.10772396902, "technolog": 2.8705430590649996, "the": 0.0, "may": 0.1521299858532, "less": 0.3846144626, "mayo": 3.90737271112, "aim": 1.06333853704, "tag": 11.93185817888, "occurr": 2.62504659255, "partsofspeech": 6.17605625244, "easi": 1.6665296351499999, "citat": 2.8224376477599997, "those": 0.5356481726189999, "said": 0.87330633163, "appropri": 2.9237915654799997, "convert": 2.3720720736, "latin": 1.33737223047, "sinc": 0.0803681994577, "they": 0.0594539895352, "anoth": 0.127896361652, "remov": 1.3920976831760001, "howev": 0.0903151173475, "allow": 0.24028061118900002, "get": 0.579769005782, "elimin": 1.30201620283, "kdnugget": 6.17605625244, "token": 24.623940039230003, "represent": 5.33921486295, "larger": 0.806828661778, "span": 1.82012472855, "which": 0.04660572460887, "stem": 10.043432776000001, "plural": 2.65954802426, "estim": 1.70875507195, "must": 0.653383947388, "liter": 1.6990638498800001, "that": 0.019880741898199997, "refer": 0.787659740394, "itself": 0.5570837229510001, "provid": 0.19517784432500002, "consist": 0.797746252852, "terminolog": 2.87350795184, "hope": 0.919824304455, "captur": 1.0578810012100002, "intersect": 2.53847009271, "such": 0.059695977806, "result": 0.136378908381, "corpus": 9.5455208382, "affix": 4.06676174761, "intellig": 4.30049544639, "implement": 1.27437940907, "text": 17.10723014985, "some": 0.11872052719350001, "multilingu": 4.35935783486, "put": 1.011305999708, "discours": 2.8014725192900003, "concern": 0.634079948873, "play": 0.38110439064199997, "content": 2.52947831908, "filter": 2.82668393864, "who": 0.0609002329859, "well": 0.1905433149468, "languag": 11.629545541684, "set": 0.342992022578, "this": 0.01514579621, "two": 0.0136988443582, "sampl": 1.9786264883900002, "should": 0.509419876758, "from": 0.002268216675464, "part": 0.04239531098280001, "relat": 0.42620060330799997, "lazi": 3.88260364301, "littl": 0.438213989466, "differ": 0.424642242624, "instanc": 1.18089357972, "statist": 5.780753228280001, "most": 0.041495792591199995, "between": 0.033953681165299995, "return": 0.333126868592, "domain": 2.24008000599, "for": 0.0031499039539700006, "difficult": 0.912110767588, "obvious": 1.86383450716, "quick": 0.790727508899, "fail": 0.656536611573, "need": 0.725480326884, "all": 0.04561052839119999, "simplifi": 2.4940183301400003, "with": 0.00478996685356, "requir": 0.424253510675, "reason": 0.544301552962, "are": 0.2357397886616, "speak": 1.06169814662, "split": 1.24442043932, "better": 2.0892838218, "feat": 2.8373792277599996, "case": 0.395406268889, "normal": 3.838557515132, "popular": 0.41058020877499996, "histor": 1.032303567904, "bag": 16.58285420754, "more": 0.017024931599999998, "and": 0.0008818619888904, "suffix": 3.3482048515200002, "veri": 0.230159793238, "these": 0.0715336194008, "theme": 1.3417002006799998, "gain": 0.6142097989249999, "nononsens": 6.17605625244, "befor": 0.191275543759, "realm": 4.75501479488, "artifici": 6.3546869705399995, "multiset": 6.17605625244, "group": 0.190594534797, "serv": 0.383135035608, "can": 0.8117054819699999, "process": 6.3339503883, "john": 0.304194577702, "comput": 5.47227566376, "possibl": 0.348805474891, "side": 0.46934876686899996, "defin": 2.0073602185, "great": 0.235805258079, "prefix": 3.2509415461, "key": 0.82419811896, "bodi": 0.6215709351609999, "legibl": 5.40988393686, "step": 2.07909011396, "read": 1.67878536176, "link": 0.7661704068449999, "common": 0.338325805271, "contract": 1.10410732855, "while": 0.12974995138140002, "sequenc": 5.4106333122, "probabl": 0.972882412913, "vari": 0.915094672432, "passag": 1.5325398614399999, "into": 0.0447385896861, "ultim": 0.9498209395739999, "biblic": 2.73427932989, "nlprelat": 6.17605625244, "stemmer": 6.17605625244, "aspect": 1.12795002691, "adject": 3.4822484080500002, "jump": 2.08829899551, "identifi": 0.833722000472, "concis": 3.1200559268700006, "meant": 2.33003399908, "canon": 2.38158903576, "infix": 5.8883741799800005, "proceed": 2.4670968071, "input": 2.50167533539, "various": 0.28692650007, "extent": 1.40974687623, "use": 0.146040098658, "particular": 0.646314787608, "order": 0.44028076158600005, "find": 0.547781330288, "within": 0.42526544119599996, "learn": 1.68550412949, "actual": 0.628514181648, "same": 0.112059649604, "stop": 2.335738124889}, "logidf": {"after": 0.020490694648099998, "base": 0.13652330228700002, "natur": 0.431306339292, "matthew": 1.9327693554900003, "understand": 1.0880858756799998, "equival": 1.40897338129, "enter": 0.564256167492, "fall": 0.527402167952, "smaller": 0.9530830530519999, "panagram": 6.17605625244, "piec": 1.17598157639, "here": 0.8850381883700001, "lower": 0.742201929994, "form": 0.120053184191, "foot": 1.73233604876, "punctuat": 3.7403186264499997, "num": 0.00031499039539700004, "assign": 1.3445959556, "uniform": 1.74451821303, "alik": 2.7850112422400004, "would": 0.0796176279647, "human": 0.640035183243, "number": 0.0966085784186, "been": 0.023645982368400004, "etc": 1.4366730879700003, "upper": 1.22686662419, "run": 0.442714975539, "omit": 2.56595767618, "dog": 1.8346148978799999, "sentenc": 1.7656483252200001, "just": 0.289531434109, "perspect": 1.61645415436, "entir": 0.46603068026999994, "devic": 1.6110769470299997, "speed": 1.3533338752700002, "fox": 1.8765058395900003, "topic": 1.6969991554100001, "approach": 0.7302336145810001, "lemmat": 6.17605625244, "collect": 0.49536666052, "sole": 1.3966781444299998, "contribut": 0.655201578909, "term": 0.33303898354600003, "equal": 0.933027391343, "noun": 3.4034675302, "post": 0.8057001527009999, "their": 0.015360505122700001, "multipl": 1.01092401812, "intuit": 3.3216780971900004, "how": 0.47156695693000006, "test": 0.977224437103, "singl": 0.475916769059, "them": 0.0941833269093, "verb": 3.3772978124599997, "chunk": 4.394449154669999, "seri": 0.38193461069799994, "dictionari": 1.65426767539, "simpl": 1.2232212893899999, "whi": 1.18068843047, "jame": 0.658238325853, "especi": 0.511098609709, "expert": 1.68029517063, "mechan": 1.22815639221, "expand": 0.80021683492, "given": 0.303255810831, "word": 0.585861082385, "interest": 0.47207177798199995, "abl": 0.599303982475, "string": 2.1255896963900005, "over": 0.0249367214957, "concept": 0.977224437103, "perhap": 1.14680739183, "build": 0.491137452091, "analysi": 1.2466091029200002, "second": 0.10713976337999999, "herein": 5.38210437275, "select": 0.704804687133, "broad": 1.45323772, "one": 0.0062553516455, "general": 0.114952578063, "earli": 0.117499629108, "perform": 0.42618085058, "but": 0.0161923720719, "certain": 0.592104362781, "obtain": 0.988162703503, "there": 0.0400978929255, "longer": 0.7046772417749999, "scienc": 0.841436178891, "where": 0.0649921387457, "overal": 1.1132694464700001, "likelihood": 3.15637073786, "applic": 1.23160392849, "linguist": 2.26646043267, "level": 0.503462189943, "exampl": 0.40868267499899996, "has": 0.0427239448548, "generat": 0.719182341736, "introductori": 3.41107212958, "categori": 1.38176946652, "take": 0.130691962197, "follow": 0.045356911094199995, "further": 0.308815895297, "out": 0.0584263909193, "model": 0.7374500731110001, "interact": 1.4858210267899998, "see": 0.240921585492, "good": 0.418589404907, "lemma": 5.59502637, "disciplin": 1.93582013145, "circumfix": 6.17605625244, "brown": 1.16987530869, "numer": 0.606093812346, "also": 0.0146571578, "field": 0.5760642583510001, "hypothesi": 2.60865985243, "mean": 0.37092128352, "interdisciplinari": 3.63469289398, "grammar": 2.68784749378, "corpora": 6.17605625244, "storag": 2.1544996326700003, "task": 1.35748680661, "major": 0.138474663439, "research": 0.663727818138, "theori": 1.10772396902, "technolog": 0.956847686355, "the": 0.0, "may": 0.050709995284400004, "less": 0.3846144626, "mayo": 3.90737271112, "aim": 1.06333853704, "tag": 2.98296454472, "occurr": 2.62504659255, "partsofspeech": 6.17605625244, "easi": 1.6665296351499999, "citat": 2.8224376477599997, "those": 0.17854939087299998, "said": 0.436653165815, "appropri": 1.4618957827399999, "convert": 1.1860360368, "latin": 1.33737223047, "sinc": 0.0803681994577, "they": 0.0297269947676, "anoth": 0.127896361652, "remov": 0.6960488415880001, "howev": 0.0903151173475, "allow": 0.24028061118900002, "get": 0.579769005782, "elimin": 1.30201620283, "kdnugget": 6.17605625244, "token": 3.5177057198900004, "represent": 1.7797382876499999, "larger": 0.806828661778, "span": 1.82012472855, "which": 0.00517841384543, "stem": 2.0086865552, "plural": 2.65954802426, "estim": 0.854377535975, "must": 0.653383947388, "liter": 1.6990638498800001, "that": 0.00397614837964, "refer": 0.262553246798, "itself": 0.5570837229510001, "provid": 0.19517784432500002, "consist": 0.398873126426, "terminolog": 2.87350795184, "hope": 0.919824304455, "captur": 1.0578810012100002, "intersect": 2.53847009271, "such": 0.059695977806, "result": 0.136378908381, "corpus": 3.1818402794, "affix": 4.06676174761, "intellig": 1.43349848213, "implement": 1.27437940907, "text": 1.14048200999, "some": 0.0395735090645, "multilingu": 4.35935783486, "put": 0.505652999854, "discours": 2.8014725192900003, "concern": 0.634079948873, "play": 0.38110439064199997, "content": 1.26473915954, "filter": 2.82668393864, "who": 0.0609002329859, "well": 0.0635144383156, "languag": 0.8306818244059999, "set": 0.171496011289, "this": 0.0037864490525, "two": 0.0136988443582, "sampl": 1.9786264883900002, "should": 0.509419876758, "from": 0.000567054168866, "part": 0.04239531098280001, "relat": 0.21310030165399999, "lazi": 3.88260364301, "littl": 0.438213989466, "differ": 0.212321121312, "instanc": 1.18089357972, "statist": 1.4451883070700002, "most": 0.020747896295599998, "between": 0.033953681165299995, "return": 0.333126868592, "domain": 2.24008000599, "for": 0.00031499039539700004, "difficult": 0.912110767588, "obvious": 1.86383450716, "quick": 0.790727508899, "fail": 0.656536611573, "need": 0.362740163442, "all": 0.011402632097799998, "simplifi": 2.4940183301400003, "with": 0.00119749171339, "requir": 0.424253510675, "reason": 0.544301552962, "are": 0.0294674735827, "speak": 1.06169814662, "split": 1.24442043932, "better": 0.6964279406, "feat": 2.8373792277599996, "case": 0.395406268889, "normal": 0.959639378783, "popular": 0.41058020877499996, "histor": 0.516151783952, "bag": 2.76380903459, "more": 0.017024931599999998, "and": 6.29901420636e-05, "suffix": 3.3482048515200002, "veri": 0.230159793238, "these": 0.0715336194008, "theme": 1.3417002006799998, "gain": 0.6142097989249999, "nononsens": 6.17605625244, "befor": 0.0956377718795, "realm": 2.37750739744, "artifici": 2.11822899018, "multiset": 6.17605625244, "group": 0.190594534797, "serv": 0.383135035608, "can": 0.162341096394, "process": 0.527829199025, "john": 0.304194577702, "comput": 1.36806891594, "possibl": 0.348805474891, "side": 0.46934876686899996, "defin": 1.00368010925, "great": 0.235805258079, "prefix": 3.2509415461, "key": 0.82419811896, "bodi": 0.6215709351609999, "legibl": 5.40988393686, "step": 1.03954505698, "read": 0.83939268088, "link": 0.7661704068449999, "common": 0.338325805271, "contract": 1.10410732855, "while": 0.04324998379380001, "sequenc": 1.8035444374, "probabl": 0.972882412913, "vari": 0.915094672432, "passag": 1.5325398614399999, "into": 0.0149128632287, "ultim": 0.9498209395739999, "biblic": 2.73427932989, "nlprelat": 6.17605625244, "stemmer": 6.17605625244, "aspect": 1.12795002691, "adject": 3.4822484080500002, "jump": 2.08829899551, "identifi": 0.833722000472, "concis": 3.1200559268700006, "meant": 1.16501699954, "canon": 2.38158903576, "infix": 5.8883741799800005, "proceed": 1.23354840355, "input": 2.50167533539, "various": 0.28692650007, "extent": 1.40974687623, "use": 0.0292080197316, "particular": 0.323157393804, "order": 0.22014038079300002, "find": 0.547781330288, "within": 0.21263272059799998, "learn": 0.842752064745, "actual": 0.628514181648, "same": 0.112059649604, "stop": 0.778579374963}, "freq": {"after": 2, "base": 1, "natur": 7, "matthew": 1, "understand": 2, "equival": 1, "enter": 1, "fall": 1, "smaller": 1, "panagram": 1, "piec": 2, "here": 1, "lower": 1, "form": 3, "foot": 1, "punctuat": 1, "num": 11, "assign": 2, "uniform": 1, "alik": 1, "would": 5, "human": 3, "number": 2, "been": 2, "etc": 3, "upper": 1, "run": 2, "omit": 1, "dog": 1, "sentenc": 3, "just": 1, "perspect": 1, "entir": 1, "devic": 1, "speed": 1, "fox": 1, "topic": 2, "approach": 2, "lemmat": 4, "collect": 2, "sole": 1, "contribut": 3, "term": 1, "equal": 2, "noun": 1, "post": 1, "their": 1, "multipl": 1, "intuit": 1, "how": 2, "test": 1, "singl": 1, "them": 1, "verb": 1, "chunk": 1, "seri": 1, "dictionari": 1, "simpl": 3, "whi": 1, "jame": 1, "especi": 1, "expert": 1, "mechan": 1, "expand": 1, "given": 1, "word": 24, "interest": 2, "abl": 2, "string": 1, "over": 1, "concept": 2, "perhap": 1, "build": 1, "analysi": 1, "second": 1, "herein": 1, "select": 3, "broad": 1, "one": 1, "general": 6, "earli": 1, "perform": 1, "but": 2, "certain": 1, "obtain": 1, "there": 5, "longer": 1, "scienc": 1, "where": 2, "overal": 1, "likelihood": 1, "applic": 1, "linguist": 8, "level": 2, "exampl": 3, "has": 2, "generat": 1, "introductori": 1, "categori": 1, "take": 1, "follow": 3, "further": 4, "out": 1, "model": 6, "interact": 2, "see": 1, "good": 1, "lemma": 2, "disciplin": 3, "circumfix": 1, "brown": 1, "numer": 1, "also": 2, "field": 1, "hypothesi": 1, "mean": 1, "interdisciplinari": 1, "grammar": 1, "corpora": 3, "storag": 1, "task": 1, "major": 1, "research": 1, "theori": 1, "technolog": 3, "the": 42, "may": 3, "less": 1, "mayo": 1, "aim": 1, "tag": 4, "occurr": 1, "partsofspeech": 1, "easi": 1, "citat": 1, "those": 3, "said": 2, "appropri": 2, "convert": 2, "latin": 1, "sinc": 1, "they": 2, "anoth": 1, "remov": 2, "howev": 1, "allow": 1, "get": 1, "elimin": 1, "kdnugget": 1, "token": 7, "represent": 3, "larger": 1, "span": 1, "which": 9, "stem": 5, "plural": 1, "estim": 2, "must": 1, "liter": 1, "that": 5, "refer": 3, "itself": 1, "provid": 1, "consist": 2, "terminolog": 1, "hope": 1, "captur": 1, "intersect": 1, "such": 1, "result": 1, "corpus": 3, "affix": 1, "intellig": 3, "implement": 1, "text": 15, "some": 3, "multilingu": 1, "put": 2, "discours": 1, "concern": 1, "play": 1, "content": 2, "filter": 1, "who": 1, "well": 3, "languag": 14, "set": 2, "this": 4, "two": 1, "sampl": 1, "should": 1, "from": 4, "part": 1, "relat": 2, "lazi": 1, "littl": 1, "differ": 2, "instanc": 1, "statist": 4, "most": 2, "between": 1, "return": 1, "domain": 1, "for": 10, "difficult": 1, "obvious": 1, "quick": 1, "fail": 1, "need": 2, "all": 4, "simplifi": 1, "with": 4, "requir": 1, "reason": 1, "are": 8, "speak": 1, "split": 1, "better": 3, "feat": 1, "case": 1, "normal": 4, "popular": 1, "histor": 2, "bag": 6, "more": 1, "and": 14, "suffix": 1, "veri": 1, "these": 1, "theme": 1, "gain": 1, "nononsens": 1, "befor": 2, "realm": 2, "artifici": 3, "multiset": 1, "group": 1, "serv": 1, "can": 5, "process": 12, "john": 1, "comput": 4, "possibl": 1, "side": 1, "defin": 2, "great": 1, "prefix": 1, "key": 1, "bodi": 1, "legibl": 1, "step": 2, "read": 2, "link": 1, "common": 1, "contract": 1, "while": 3, "sequenc": 3, "probabl": 1, "vari": 1, "passag": 1, "into": 3, "ultim": 1, "biblic": 1, "nlprelat": 1, "stemmer": 1, "aspect": 1, "adject": 1, "jump": 1, "identifi": 1, "concis": 1, "meant": 2, "canon": 1, "infix": 1, "proceed": 2, "input": 1, "various": 1, "extent": 1, "use": 5, "particular": 2, "order": 2, "find": 1, "within": 2, "learn": 2, "actual": 1, "same": 1, "stop": 3}, "idf": {"after": 1.02070207021, "base": 1.14628158845, "natur": 1.5392670157100001, "matthew": 6.908616187989999, "understand": 2.96858638743, "equival": 4.09175257732, "enter": 1.75813953488, "fall": 1.6945244956799999, "smaller": 2.59369384088, "panagram": 481.09090909099996, "piec": 3.24132298898, "here": 2.42307692308, "lower": 2.10055570257, "form": 1.12755681818, "foot": 5.65384615385, "punctuat": 42.1114058355, "num": 1.00031504001, "assign": 3.83663605607, "uniform": 5.7231434751300005, "alik": 16.2, "would": 1.0828729281799998, "human": 1.8965476048299998, "number": 1.10142916609, "been": 1.0239277652399998, "etc": 4.2066772655, "upper": 3.41052631579, "run": 1.55692850838, "omit": 13.0131147541, "dog": 6.26272189349, "sentenc": 5.84536082474, "just": 1.33580143037, "perspect": 5.03520456708, "entir": 1.59365589239, "devic": 5.00820189274, "speed": 3.8703071672400005, "fox": 6.53064582476, "topic": 5.457545548300001, "approach": 2.07556543339, "lemmat": 481.09090909099996, "collect": 1.64109985528, "sole": 4.04175152749, "contribut": 1.9255306246200001, "term": 1.39520168732, "equal": 2.542193755, "noun": 30.068181818200003, "post": 2.23826307627, "their": 1.01547908405, "multipl": 2.74813917258, "intuit": 27.7068062827, "how": 1.60250328051, "test": 2.65707112971, "singl": 1.60948905109, "them": 1.09876115994, "verb": 29.291512915100004, "chunk": 81.0, "seri": 1.46511627907, "dictionari": 5.2292490118599995, "simpl": 3.3981164383599998, "whi": 3.2566153846200003, "jame": 1.9313868613099998, "especi": 1.66712170534, "expert": 5.36713995943, "mechan": 3.41492794149, "expand": 2.2260235558000003, "given": 1.35426085473, "word": 1.7965372864099998, "interest": 1.60331246213, "abl": 1.8208510150200001, "string": 8.37783641161, "over": 1.02525024217, "concept": 2.65707112971, "perhap": 3.14812611541, "build": 1.6341739578, "analysi": 3.47852760736, "second": 1.1130898128, "herein": 217.479452055, "select": 2.02345144022, "broad": 4.27693965517, "one": 1.00627495722, "general": 1.1218202374200001, "earli": 1.12468121281, "perform": 1.5313977042500002, "but": 1.01632417899, "certain": 1.8077886586200003, "obtain": 2.68629441624, "there": 1.04091266719, "longer": 2.02319357716, "scienc": 2.31969608416, "where": 1.06715063521, "overal": 3.0442953020099996, "likelihood": 23.4852071006, "applic": 3.42672134686, "linguist": 9.645200486030001, "level": 1.6544393497299998, "exampl": 1.50483412322, "has": 1.0436497502, "generat": 2.05275407292, "introductori": 30.297709923699998, "categori": 3.98194130926, "take": 1.13961668222, "follow": 1.04640126549, "further": 1.3618116315, "out": 1.06016694491, "model": 2.0905978404, "interact": 4.4185917061, "see": 1.27242125511, "good": 1.51981619759, "lemma": 269.084745763, "disciplin": 6.92972501091, "circumfix": 481.09090909099996, "brown": 3.2215909090900006, "numer": 1.83325635104, "also": 1.01476510067, "field": 1.7790228597, "hypothesi": 13.580838323399998, "mean": 1.44906900329, "interdisciplinari": 37.8902147971, "grammar": 14.7, "corpora": 481.09090909099996, "storag": 8.623574144489998, "task": 3.88641370869, "major": 1.14852058164, "research": 1.9420183486200002, "theori": 3.02745995423, "technolog": 2.6034765496900003, "the": 1.0, "may": 1.05201775893, "less": 1.46904783936, "mayo": 49.7680250784, "aim": 2.8960233491400005, "tag": 19.7462686567, "occurr": 13.805217391300001, "partsofspeech": 481.09090909099996, "easi": 5.2937645882, "citat": 16.8177966102, "those": 1.19548192771, "said": 1.54751925139, "appropri": 4.31413043478, "convert": 3.2740771293099997, "latin": 3.80902111324, "sinc": 1.08368600683, "they": 1.03017325287, "anoth": 1.13643521832, "remov": 2.0058117498400003, "howev": 1.0945191313299998, "allow": 1.2716059271100002, "get": 1.78562591385, "elimin": 3.67670217693, "kdnugget": 481.09090909099996, "token": 33.7070063694, "represent": 5.928304705, "larger": 2.2407904022599996, "span": 6.17262830482, "which": 1.005191845, "stem": 7.453521126760001, "plural": 14.2898289829, "estim": 2.34991119005, "must": 1.9220338983099996, "liter": 5.468825353080001, "that": 1.00398406375, "refer": 1.30024570025, "itself": 1.74557449148, "provid": 1.21552714187, "consist": 1.4901445466499998, "terminolog": 17.6989966555, "hope": 2.50884955752, "captur": 2.88026124819, "intersect": 12.660287081300002, "such": 1.06151377374, "result": 1.14611608432, "corpus": 24.091047041, "affix": 58.3676470588, "intellig": 4.19334389857, "implement": 3.57648118946, "text": 3.12827586207, "some": 1.04036697248, "multilingu": 78.2068965517, "put": 1.65806788512, "discours": 16.468879668, "concern": 1.8852867830400002, "play": 1.46390041494, "content": 3.5421686747, "filter": 16.8893617021, "who": 1.06279287723, "well": 1.0655748708, "languag": 2.29488291414, "set": 1.18707940781, "this": 1.00379362671, "two": 1.01379310345, "sampl": 7.23280182232, "should": 1.6643254009900001, "from": 1.00056721497, "part": 1.04330682789, "relat": 1.23750876919, "lazi": 48.5504587156, "littl": 1.5499365420299998, "differ": 1.23654490225, "instanc": 3.2572835453400004, "statist": 4.24265098878, "most": 1.02096463023, "between": 1.03453668708, "return": 1.39532431007, "domain": 9.39408284024, "for": 1.00031504001, "difficult": 2.48957189901, "obvious": 6.44841592201, "quick": 2.205, "fail": 1.9281029876099998, "need": 1.4372623574099999, "all": 1.01146788991, "simplifi": 12.109839816900001, "with": 1.0011982089899998, "requir": 1.52844902282, "reason": 1.72340425532, "are": 1.02990593578, "speak": 2.89127663449, "split": 3.4709226060300002, "better": 2.0065722952500002, "feat": 17.070967741900002, "case": 1.48498737256, "normal": 2.61075481006, "popular": 1.50769230769, "histor": 1.6755672823199999, "bag": 15.8601398601, "more": 1.0171706817, "and": 1.00006299213, "suffix": 28.4516129032, "veri": 1.25880114177, "these": 1.07415426252, "theme": 3.8255421686699997, "gain": 1.84819557625, "nononsens": 481.09090909099996, "befor": 1.10036041031, "realm": 10.7780040733, "artifici": 8.31639601886, "multiset": 481.09090909099996, "group": 1.20996875238, "serv": 1.4668760972, "can": 1.17626139142, "process": 1.69524826482, "john": 1.35553278689, "comput": 3.9277585353800006, "possibl": 1.4173734488, "side": 1.5989525632, "defin": 2.72830383227, "great": 1.26592775696, "prefix": 25.8146341463, "key": 2.28005170185, "bodi": 1.8618505922400002, "legibl": 223.605633803, "step": 2.8279301745599996, "read": 2.3149606299200003, "link": 2.15151104486, "common": 1.4025974025999999, "contract": 3.0165304959099997, "while": 1.0441988950299999, "sequenc": 6.07112810707, "probabl": 2.64555907349, "vari": 2.4970116388799997, "passag": 4.629921259840001, "into": 1.01502461479, "ultim": 2.58524670249, "biblic": 15.3986420951, "nlprelat": 481.09090909099996, "stemmer": 481.09090909099996, "aspect": 3.0893169877399997, "adject": 32.5327868852, "jump": 8.07117437722, "identifi": 2.30187037843, "concis": 22.647646219699997, "meant": 3.20597738288, "canon": 10.8220858896, "infix": 360.818181818, "proceed": 3.4333910034599997, "input": 12.2029208301, "various": 1.3323262839899999, "extent": 4.09491875161, "use": 1.0296387573799999, "particular": 1.3814827706200001, "order": 1.24625166811, "find": 1.7294117647099998, "within": 1.2369302688, "learn": 2.32275054865, "actual": 1.87482286254, "same": 1.11857958148, "stop": 2.1783754116400003}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Natural Language Processing Key Terms, Explained</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Natural Language Processing Key Terms, Explained Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/02/impetus-spark-streaming-innovation-contest.html\" rel=\"prev\" title=\"Spark Streaming Innovation Contest\"/>\n<link href=\"https://www.kdnuggets.com/2017/02/cazena-data-science-sandbox-as-service-webinar.html\" rel=\"next\" title=\"Webinar: The Data Science Sandbox as a Service, March 8\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=61688\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-61688 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 16-Feb, 2017  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/02/index.html\">Feb</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/02/tutorials.html\">Tutorials, Overviews</a> \u00bb Natural Language Processing Key Terms, Explained (\u00a0<a href=\"/2017/n07.html\">17:n07</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Natural Language Processing Key Terms, Explained</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/02/impetus-spark-streaming-innovation-contest.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/02/cazena-data-science-sandbox-as-service-webinar.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <span class=\"http-likes\" style=\"float: left; font-size:14px\">http likes 421</span> <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/explained\" rel=\"tag\">Explained</a>, <a href=\"https://www.kdnuggets.com/tag/information-retrieval\" rel=\"tag\">Information Retrieval</a>, <a href=\"https://www.kdnuggets.com/tag/key-terms\" rel=\"tag\">Key Terms</a>, <a href=\"https://www.kdnuggets.com/tag/natural-language-processing\" rel=\"tag\">Natural Language Processing</a>, <a href=\"https://www.kdnuggets.com/tag/nlp\" rel=\"tag\">NLP</a>, <a href=\"https://www.kdnuggets.com/tag/semantic-analysis\" rel=\"tag\">Semantic Analysis</a>, <a href=\"https://www.kdnuggets.com/tag/sentiment-analysis\" rel=\"tag\">Sentiment Analysis</a></div>\n<br/>\n<p class=\"excerpt\">\n     This post provides a concise overview of 18 natural language processing terms, intended as an entry point for the beginner looking for some orientation on the topic.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/matt-mayo\" rel=\"author\" title=\"Posts by Matthew Mayo\">Matthew Mayo</a>, KDnuggets.</b></div>\n<p>At the intersection of computational linguistics and artificial intelligence is where we find natural language processing. Very broadly, natural language processing (NLP) is a discipline which is interested in how human languages, and, to some extent, the humans who speak them, interact with technology. NLP is an interdisciplinary topic which has historically been the equal domain of artificial intelligence researchers and linguistics alike; perhaps obviously, those approaching the discipline from the linguistics side must get up to speed on technology, while those entering the discipline from the technology realm need to learn the linguistic concepts.</p>\n<p>It is this second group that this post aims to serve at an introductory level, as we take a no-nonsense approach to defining some key NLP terminology. While you certainly won't be a linguistic expert after reading this, we hope that you are better able to understand some of the NLP-related discourse, and gain perspective as to how to proceed with learning more on the topics herein.</p>\n<p>So here they are, 18 select natural language processing terms, concisely defined, with links to further reading where appropriate.</p>\n<p><img alt=\"NLP wordcloud\" class=\"aligncenter\" src=\"/wp-content/uploads/nlp-word-cloud.jpg\" width=\"75%\"/></p>\n<p><b>1. <a href=\"/tag/natural-language-processing\">Natural Language Processing (NLP)</a></b> </p>\n<p>Natural language processing (NLP) concerns itself with the interaction between natural human languages and computing devices. NLP is a major aspect of computational linguistics, and also falls within the realms of computer science and artificial intelligence.</p>\n<p><b>2. Tokenization</b></p>\n<p>Tokenization is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or <strong>tokens</strong>. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized.</p>\n<p><b>3. Normalization</b></p>\n<p>Before further processing, text needs to be normalized. Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.</p>\n<p><b>4. Stemming</b></p>\n<p>Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.</p>\n<p><center><font size=\"+2\">running \u2192 run</font></center></p>\n<p><b>5. Lemmatization</b></p>\n<p>Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's <a href=\"https://en.wikipedia.org/wiki/Lemma_(morphology)\" target=\"_blank\">lemma</a>.</p>\n<p>For example, stemming the word \"better\" would fail to return its citation form (another word for lemma); however, lemmatization would result in the following:</p>\n<p><center><font size=\"+2\">better \u2192 good</font></center></p>\n<p>It should be easy to see why the implementation of a stemmer would be the less difficult feat of the two.</p>\n<p><b>6. Corpus</b></p>\n<p>In linguistics and NLP, corpus (literally Latin for <em>body</em>) refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful. Corpora may also consist of themed texts (historical, Biblical, etc.). Corpora are generally solely used for statistical linguistic analysis and hypothesis testing.</p>\n<p><b>7. Stop Words</b></p>\n<p>Stop words are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" while all required words in a particular passage, don't generally contribute greatly to one's understanding of content. As a simple example, the following <a href=\"https://en.wikipedia.org/wiki/Pangram\" target=\"_blank\">panagram</a> is just as legible if the stop words are removed:</p>\n<p><center><font size=\"+2\"><strike>The</strike> quick brown fox jumps over <strike>the</strike> lazy dog.</font></center></p>\n<p><b>8. Parts-of-speech (POS) Tagging</b></p>\n<p>POS tagging consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc.</p>\n<p><img alt=\"POS Tagging\" class=\"aligncenter\" src=\"/wp-content/uploads/pos-tagging.jpg\" width=\"450\"/></p>\n<p><b>9. Statistical Language Modeling</b></p>\n<p>Statistical Language Modeling is the process of building a statistical language model which is meant to provide an estimate of a natural language. For a sequence of input words, the model would assign a probability to the entire sequence, which contributes to the estimated likelihood of various possible sequences. This can be especially useful for NLP applications which generate text.</p>\n<p><b>10. Bag of Words</b></p>\n<p>Bag of words is a particular representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order, but is interested in the number of occurrences of words within the text. The ultimate representation of the text selection is that of a bag of words (<b>bag</b> referring to the set theory concept of <a href=\"https://en.wikipedia.org/wiki/Multiset\" target=\"_blank\">multisets</a>, which differ from simple sets).</p>\n<p>Actual storage mechanisms for the bag of words representation can vary, but the following is a simple example using a dictionary for intuitiveness. Sample text:</p>\n<p><center><font size=\"+2\">\"Well, well, well,\" said John.</font></center></p>\n<p>\"There, there,\" said James. \"There, there.\"</p></div></div></div></div></div></body></html>\n<p>The resulting bag of words representation as a dictionary:</p>\n<pre><code>   {\r\n      'well': 3,\r\n      'said': 2,\r\n      'john': 1,\r\n      'there': 4,\r\n      'james': 1\r\n   }\r\n</code></pre>\n<p><br class=\"blank\"/></p>\n<p><b>11. n-grams</b></p>\n<p>n-grams is another representation model for simplifying text selection contents. As opposed to the orderless representation of bag of words, n-grams modeling is interested in preserving contiguous sequences of <em>N</em> items from the text selection.</p>\n<p>An example of trigram (3-gram) model of the second sentence of the above example (\"There, there,\" said James. \"There, there.\") appears as a list representation below:</p>\n<pre><code>   [\r\n      \"there there said\",\r\n      \"there said james\",\r\n      \"said james there\",\r\n      \"james there there\",\r\n   ]\r\n</code></pre>\n<p><br class=\"blank\"/></p>\n<p><b>12. Regular Expressions</b></p>\n<p>Regular expressions, often abbreviated <em>regexp</em> or <em>regexp</em>, are a tried and true method of concisely describing patterns of text. A regular expression is represented as a special text string itself, and is meant for developing search patterns on selections of text. Regular expressions can be thought of as an expanded set of rules beyond the wildcard characters of <strong>?</strong> and <strong>*</strong>. Though often cited as frustrating to learn, regular expressions are incredibly powerful text searching tools.</p>\n<p><b>13. Zipf's Law</b></p>\n<p>Zipf's Law is used to describe the relationship between word frequencies in document collections. If a document collection's words are ordered by frequency, and <em>y</em> is used to describe the number of times that the <em>x</em>th word appears, Zipf's observation is concisely captured as <font size=\"+1\"><em>y = cx<sup>-1/2</sup></em></font> (item frequency is inversely proportional to item rank). More generally, <a href=\"https://en.wikipedia.org/wiki/Zipf's_law\" target=\"_blank\">Wikipedia says</a>:</p>\n<blockquote><p>\nZipf's law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\n</p></blockquote>\n<p><img alt=\"Zipf's Law\" class=\"aligncenter\" src=\"/wp-content/uploads/zipf.jpg\" width=\"75%\"/></p>\n<p><b>14. Similarity Measures</b></p>\n<p>There are numerous similarity measures which can be applied to NLP. What are we measuring the similarity of? Generally, strings.</p>\n<ul>\n<li><strong>Levenshtein</strong> - the number of characters that must be deleted, inserted, or substituted in order to make a pair of strings equal\n<li><strong>Jaccard</strong> - the measure of overlap between 2 sets; in the case of NLP, generally, documents are sets of words\n<li><strong>Smith Waterman</strong> - similar to Levenshtein, but with costs assigned to substitution, insertion, and deletion\n</li></li></li></ul>\n<p><b>15. Syntactic Analysis</b></p>\n<p>Also referred to as <strong>parsing</strong>, syntactic analysis is the task of analyzing strings as symbols, and ensuring their conformance to a established set of grammatical rules. This step must, out of necessity, come before any further analysis which attempts to extract insight from text -- semantic, sentiment, etc. -- treating it as something beyond symbols.</p>\n<p><b>16. <a href=\"/tag/semantic-analysis\">Semantic Analysis</a></b></p>\n<p>Also known as <strong>meaning generation</strong>, semantic analysis is interested in determining the meaning of text selections (either character or word sequences). After an input selection of text is read and parsed (analyzed syntactically), the text selection can then be interpreted for meaning. Simply put, syntactic analysis is concerned with what words a text selection was made up of, while semantic analysis wants to know what the collection of words actually <b>means</b>. The topic of semantic analysis is both broad and deep, with a wide variety of tools and techniques at the researcher's disposal.</p>\n<p><b>17. <a href=\"/tag/sentiment-analysis\">Sentiment Analysis</a></b></p>\n<p>Sentiment analysis is the process of evaluating and determining the sentiment captured in a selection of text, with sentiment defined as feeling or emotion. This sentiment can be simply positive (happy), negative (sad or angry), or neutral, or can be some more precise measurement along a scale, with neutral in the middle, and positive and negative increasing in either direction.</p>\n<p><img alt=\"Sentiment analysis\" class=\"aligncenter\" src=\"/wp-content/uploads/sentiment-analysis.jpg\" width=\"600\"/></p>\n<p><b>18. <a href=\"/tag/information-retrieval\">Information Retrieval</a></b></p>\n<p>Information retrieval is the process of accessing and retrieving the most appropriate information from text based on a particular query, using context-based indexing or metadata. One of the most famous examples of information retrieval would be Google Search.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2016/05/machine-learning-key-terms-explained.html\">Machine Learning Key Terms, Explained</a>\n<li><a href=\"/2016/10/deep-learning-key-terms-explained.html\">Deep Learning Key Terms, Explained</a>\n<li><a href=\"/2017/01/deep-learning-review-natural-language-processing.html\">Deep Learning Research Review: Natural Language Processing</a>\n</li></li></li></ul>\n\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/02/impetus-spark-streaming-innovation-contest.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2017/02/cazena-data-science-sandbox-as-service-webinar.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end-->\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/02/index.html\">Feb</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/02/tutorials.html\">Tutorials, Overviews</a> \u00bb Natural Language Processing Key Terms, Explained (\u00a0<a href=\"/2017/n07.html\">17:n07</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end-->\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556323915\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper-->\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n\n\n<!-- Dynamic page generated in 0.711 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 20:11:55 -->\n<!-- Compression = gzip -->", "content_tokenized": ["matthew", "mayo", "kdnugget", "the", "intersect", "comput", "linguist", "and", "artifici", "intellig", "where", "find", "natur", "languag", "process", "veri", "broad", "natur", "languag", "process", "disciplin", "which", "interest", "how", "human", "languag", "and", "some", "extent", "the", "human", "who", "speak", "them", "interact", "with", "technolog", "interdisciplinari", "topic", "which", "has", "histor", "been", "the", "equal", "domain", "artifici", "intellig", "research", "and", "linguist", "alik", "perhap", "obvious", "those", "approach", "the", "disciplin", "from", "the", "linguist", "side", "must", "get", "speed", "technolog", "while", "those", "enter", "the", "disciplin", "from", "the", "technolog", "realm", "need", "learn", "the", "linguist", "concept", "this", "second", "group", "that", "this", "post", "aim", "serv", "introductori", "level", "take", "nononsens", "approach", "defin", "some", "key", "terminolog", "while", "certain", "linguist", "expert", "after", "read", "this", "hope", "that", "are", "better", "abl", "understand", "some", "the", "nlprelat", "discours", "and", "gain", "perspect", "how", "proceed", "with", "learn", "more", "the", "topic", "herein", "here", "they", "are", "num", "select", "natur", "languag", "process", "term", "concis", "defin", "with", "link", "further", "read", "where", "appropri", "num", "natur", "languag", "process", "natur", "languag", "process", "concern", "itself", "with", "the", "interact", "between", "natur", "human", "languag", "and", "comput", "devic", "major", "aspect", "comput", "linguist", "and", "also", "fall", "within", "the", "realm", "comput", "scienc", "and", "artifici", "intellig", "num", "token", "token", "general", "earli", "step", "the", "process", "step", "which", "split", "longer", "string", "text", "into", "smaller", "piec", "token", "larger", "chunk", "text", "can", "token", "into", "sentenc", "sentenc", "can", "token", "into", "word", "etc", "further", "process", "general", "perform", "after", "piec", "text", "has", "been", "appropri", "token", "num", "normal", "befor", "further", "process", "text", "need", "normal", "normal", "general", "refer", "seri", "relat", "task", "meant", "put", "all", "text", "level", "play", "field", "convert", "all", "text", "the", "same", "case", "upper", "lower", "remov", "punctuat", "expand", "contract", "convert", "number", "their", "word", "equival", "and", "normal", "put", "all", "word", "equal", "foot", "and", "allow", "process", "proceed", "uniform", "num", "stem", "stem", "the", "process", "elimin", "affix", "suffix", "prefix", "infix", "circumfix", "from", "word", "order", "obtain", "word", "stem", "run", "run", "num", "lemmat", "lemmat", "relat", "stem", "differ", "that", "lemmat", "abl", "captur", "canon", "form", "base", "word", "lemma", "for", "exampl", "stem", "the", "word", "better", "would", "fail", "return", "citat", "form", "anoth", "word", "for", "lemma", "howev", "lemmat", "would", "result", "the", "follow", "better", "good", "should", "easi", "see", "whi", "the", "implement", "stemmer", "would", "the", "less", "difficult", "feat", "the", "two", "num", "corpus", "linguist", "and", "corpus", "liter", "latin", "for", "bodi", "refer", "collect", "text", "such", "collect", "may", "form", "singl", "languag", "text", "can", "span", "multipl", "languag", "there", "are", "numer", "reason", "for", "which", "multilingu", "corpora", "the", "plural", "corpus", "may", "use", "corpora", "may", "also", "consist", "theme", "text", "histor", "biblic", "etc", "corpora", "are", "general", "sole", "use", "for", "statist", "linguist", "analysi", "and", "hypothesi", "test", "num", "stop", "word", "stop", "word", "are", "those", "word", "which", "are", "filter", "out", "befor", "further", "process", "text", "sinc", "these", "word", "contribut", "littl", "overal", "mean", "given", "that", "they", "are", "general", "the", "most", "common", "word", "languag", "for", "instanc", "the", "and", "and", "while", "all", "requir", "word", "particular", "passag", "general", "contribut", "great", "one", "understand", "content", "simpl", "exampl", "the", "follow", "panagram", "just", "legibl", "the", "stop", "word", "are", "remov", "the", "quick", "brown", "fox", "jump", "over", "the", "lazi", "dog", "num", "partsofspeech", "tag", "tag", "consist", "assign", "categori", "tag", "the", "token", "part", "sentenc", "the", "most", "popular", "tag", "would", "identifi", "word", "noun", "verb", "adject", "etc", "num", "statist", "languag", "model", "statist", "languag", "model", "the", "process", "build", "statist", "languag", "model", "which", "meant", "provid", "estim", "natur", "languag", "for", "sequenc", "input", "word", "the", "model", "would", "assign", "probabl", "the", "entir", "sequenc", "which", "contribut", "the", "estim", "likelihood", "various", "possibl", "sequenc", "this", "can", "especi", "use", "for", "applic", "which", "generat", "text", "num", "bag", "word", "bag", "word", "particular", "represent", "model", "use", "simplifi", "the", "content", "select", "text", "the", "bag", "word", "model", "omit", "grammar", "and", "word", "order", "but", "interest", "the", "number", "occurr", "word", "within", "the", "text", "the", "ultim", "represent", "the", "text", "select", "that", "bag", "word", "bag", "refer", "the", "set", "theori", "concept", "multiset", "which", "differ", "from", "simpl", "set", "actual", "storag", "mechan", "for", "the", "bag", "word", "represent", "can", "vari", "but", "the", "follow", "simpl", "exampl", "use", "dictionari", "for", "intuit", "sampl", "text", "well", "well", "well", "said", "john", "there", "there", "said", "jame", "there", "there"], "timestamp_scraper": 1556379504.038703, "title": "Natural Language Processing Key Terms, Explained", "read_time": 243.29999999999998, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/matt-mayo\" rel=\"author\" title=\"Posts by Matthew Mayo\">Matthew Mayo</a>, KDnuggets.</b></div>\n<p>At the intersection of computational linguistics and artificial intelligence is where we find natural language processing. Very broadly, natural language processing (NLP) is a discipline which is interested in how human languages, and, to some extent, the humans who speak them, interact with technology. NLP is an interdisciplinary topic which has historically been the equal domain of artificial intelligence researchers and linguistics alike; perhaps obviously, those approaching the discipline from the linguistics side must get up to speed on technology, while those entering the discipline from the technology realm need to learn the linguistic concepts.</p>\n<p>It is this second group that this post aims to serve at an introductory level, as we take a no-nonsense approach to defining some key NLP terminology. While you certainly won't be a linguistic expert after reading this, we hope that you are better able to understand some of the NLP-related discourse, and gain perspective as to how to proceed with learning more on the topics herein.</p>\n<p>So here they are, 18 select natural language processing terms, concisely defined, with links to further reading where appropriate.</p>\n<p><img alt=\"NLP wordcloud\" class=\"aligncenter\" src=\"/wp-content/uploads/nlp-word-cloud.jpg\" width=\"75%\"/></p>\n<p><b>1. <a href=\"/tag/natural-language-processing\">Natural Language Processing (NLP)</a></b> </p>\n<p>Natural language processing (NLP) concerns itself with the interaction between natural human languages and computing devices. NLP is a major aspect of computational linguistics, and also falls within the realms of computer science and artificial intelligence.</p>\n<p><b>2. Tokenization</b></p>\n<p>Tokenization is, generally, an early step in the NLP process, a step which splits longer strings of text into smaller pieces, or <strong>tokens</strong>. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. Further processing is generally performed after a piece of text has been appropriately tokenized.</p>\n<p><b>3. Normalization</b></p>\n<p>Before further processing, text needs to be normalized. Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.</p>\n<p><b>4. Stemming</b></p>\n<p>Stemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.</p>\n<p><center><font size=\"+2\">running \u2192 run</font></center></p>\n<p><b>5. Lemmatization</b></p>\n<p>Lemmatization is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's <a href=\"https://en.wikipedia.org/wiki/Lemma_(morphology)\" target=\"_blank\">lemma</a>.</p>\n<p>For example, stemming the word \"better\" would fail to return its citation form (another word for lemma); however, lemmatization would result in the following:</p>\n<p><center><font size=\"+2\">better \u2192 good</font></center></p>\n<p>It should be easy to see why the implementation of a stemmer would be the less difficult feat of the two.</p>\n<p><b>6. Corpus</b></p>\n<p>In linguistics and NLP, corpus (literally Latin for <em>body</em>) refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages -- there are numerous reasons for which multilingual corpora (the plural of corpus) may be useful. Corpora may also consist of themed texts (historical, Biblical, etc.). Corpora are generally solely used for statistical linguistic analysis and hypothesis testing.</p>\n<p><b>7. Stop Words</b></p>\n<p>Stop words are those words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" while all required words in a particular passage, don't generally contribute greatly to one's understanding of content. As a simple example, the following <a href=\"https://en.wikipedia.org/wiki/Pangram\" target=\"_blank\">panagram</a> is just as legible if the stop words are removed:</p>\n<p><center><font size=\"+2\"><strike>The</strike> quick brown fox jumps over <strike>the</strike> lazy dog.</font></center></p>\n<p><b>8. Parts-of-speech (POS) Tagging</b></p>\n<p>POS tagging consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc.</p>\n<p><img alt=\"POS Tagging\" class=\"aligncenter\" src=\"/wp-content/uploads/pos-tagging.jpg\" width=\"450\"/></p>\n<p><b>9. Statistical Language Modeling</b></p>\n<p>Statistical Language Modeling is the process of building a statistical language model which is meant to provide an estimate of a natural language. For a sequence of input words, the model would assign a probability to the entire sequence, which contributes to the estimated likelihood of various possible sequences. This can be especially useful for NLP applications which generate text.</p>\n<p><b>10. Bag of Words</b></p>\n<p>Bag of words is a particular representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order, but is interested in the number of occurrences of words within the text. The ultimate representation of the text selection is that of a bag of words (<b>bag</b> referring to the set theory concept of <a href=\"https://en.wikipedia.org/wiki/Multiset\" target=\"_blank\">multisets</a>, which differ from simple sets).</p>\n<p>Actual storage mechanisms for the bag of words representation can vary, but the following is a simple example using a dictionary for intuitiveness. Sample text:</p>\n<p><center><font size=\"+2\">\"Well, well, well,\" said John.</font></center></p>\n<p>\"There, there,\" said James. \"There, there.\"</p></div> ", "website": "kdnuggets"}