{"content": "By Sebastian Raschka , Michigan State University. That's an interesting question, and I try to answer this is a very general way. The tl;dr version of this is: Deep learning is essentially a set of techniques that help we to parameterize deep neural network structures, neural networks with many, many layers and parameters. And if we are interested, a more concrete example: Let's start with multi-layer perceptrons (MLPs)... On a tangent: The term \"perceptron\" in MLPs may be a bit confusing since we don't really want only linear neurons in our network. Using MLPs, we want to learn complex functions to solve non-linear problems. Thus, our network is conventionally composed of one or multiple \"hidden\" layers that connect the input and output layer. Those hidden layers normally have some sort of sigmoid activation function (log-sigmoid or the hyperbolic tangent etc.). For example, think of a log-sigmoid unit in our network as a logistic regression unit that returns continuous values outputs in the range 0-1. A simple MLP could look like this where y_hat is the final class label that we return as the prediction based on the inputs (x) if this are classification tasks. The \"a\"s are our activated neurons and the \"w\"s are the weight coefficients. Now, if we add multiple hidden layers to this MLP, we'd also call the network \"deep.\" The problem with such \"deep\" networks is that it becomes tougher and tougher to learn \"good\" weights for this network. When we start training our network, we typically assign random values as initial weights, which can be terribly off from the \"optimal\" solution we want to find. During training, we then use the popular backpropagation algorithm (think of it as reverse-mode auto-differentiation) to propagate the \"errors\" from right to left and calculate the partial derivatives with respect to each weight to take a step into the opposite direction of the cost (or \"error\") gradient.\u00a0 Now, the problem with deep neural networks is the so-called \"vanishing gradient\" -- the more layers we add, the harder it becomes to \"update\" our weights because the signal becomes weaker and weaker. Since our network's weights can be terribly off in the beginning (random initialization) it can become almost impossible to parameterize a \"deep\" neural network with backpropagation. Deep Learning Now, this is where \"deep learning\" comes into play. Roughly speaking, we can think of deep learning as \"clever\" tricks or algorithms that can help we with the training of such \"deep\" neural network structures. There are many, many different neural network architectures, but to continue with the example of the MLP, let me introduce the idea of convolutional neural networks (ConvNets). We can think of those as an \"add-on\" to our MLP that helps we to detect features as \"good\" inputs for our MLP. In applications of \"usual\" machine learning, there is typically a strong focus on the feature engineering part; the model learned by an algorithm can only be so good as its input data. Of course, there must be sufficient discriminatory information in our dataset, however, the performance of machine learning algorithms can suffer substantially when the information is buried in meaningless features. The goal behind deep learning is to automatically learn the features from (somewhat) noisy data; it's about algorithms that do the feature engineering for us to provide deep neural network structures with meaningful information so that it can learn more effectively.\u00a0 We can think of deep learning as algorithms for automatic \"feature engineering,\" or we could simply call them \"feature detectors,\" which help us to overcome the vanishing gradient challenge and facilitate the learning in neural networks with many layers. Let's consider a ConvNet in context of image classification. Here, we use so-called \"receptive fields\" (think of them as \"windows\") that slide over our image. We then connect those \"receptive fields\" (for example of the size of 5x5 pixel) with 1 unit in the next layer, this is the so-called \"feature map.\" After this mapping, we have constructed a so-called convolutional layer. Note that our feature detectors are basically replicates of one another -- they share the same weights. The idea is that if a feature detector is useful in one part of the imagine it is likely that it is useful somewhere else, but at the same time it allows each patch of image to be represented in several ways. Next, we have a \"pooling\" layer, where we reduce neighboring features from our feature map into single units (by taking the max feature or by averaging them, for example). We do this over many rounds and eventually arrive at an almost scale invariant representation of our image (the exact term is \"equivariant\"). This is very powerful since we can detect objects in an image no matter where they are located. In essence, the \"convolutional\" add-on that acts as a feature extractor or filter to our MLP. Via the convolutional layers we aim to extract the useful features from the images, and via the pooling layers, we aim to make the features somewhat equivariant to scale and translation. Bio: Sebastian Raschka is a 'Data Scientist' and Machine Learning enthusiast with a big passion for Python & open source. Author of ' Python Machine Learning '. Michigan State University. Original . Reposted with permission. Related: When Does Deep Learning Work Better Than SVMs or Random Forests? The Development of Classification as a Learning Machine Why Implement Machine Learning Algorithms From Scratch?", "title_html": "<h1 id=\"title\"><img align=\"right\" alt=\"2016 Silver Blog\" src=\"/images/top-kdnuggets-blog-2016-silver.png\" width=\"120\"/>What is the Difference Between Deep Learning and \u201cRegular\u201d Machine Learning?</h1> ", "url": "https://www.kdnuggets.com/2016/06/difference-between-deep-learning-regular-machine-learning.html", "tfidf": {"tfidf": {"after": 1.02070207021, "base": 1.14628158845, "relat": 1.23750876919, "label": 4.47715736041, "pool": 14.105730786320002, "pixel": 86.28260869569999, "univers": 2.49779735682, "permiss": 6.280063291139999, "the": 54.0, "sebastian": 53.6351351352, "here": 2.42307692308, "signal": 5.12459651388, "techniqu": 3.7293868921800004, "num": 2.00063008002, "assign": 3.83663605607, "averag": 2.60390355913, "updat": 5.56466876972, "paramet": 17.256521739100002, "regress": 51.2129032258, "layer": 97.69846153847999, "logsigmoid": 2268.0, "than": 1.03278688525, "noisi": 62.2588235294, "etc": 4.2066772655, "dataset": 193.609756098, "about": 1.06486015159, "function": 4.99088337, "class": 2.11651779763, "neural": 535.1460674157, "somewhat": 8.58394160584, "buri": 5.13122171946, "python": 112.5957446808, "work": 1.11520089913, "terribl": 29.1035747022, "author": 1.4229631621399998, "unit": 4.61578717836, "simpli": 2.5192002538900002, "essenti": 2.9280708225700005, "dure": 1.0503473370799998, "essenc": 14.659279778399998, "enthusiast": 9.39408284024, "state": 2.0954266481799997, "scale": 7.493981590739999, "automat": 13.575032064980002, "term": 2.79040337464, "where": 4.26860254084, "train": 5.8097096853, "translat": 2.85745140389, "complex": 2.34021226415, "start": 2.53347163488, "multipl": 5.49627834516, "usual": 1.72508964468, "speak": 2.89127663449, "repost": 933.882352941, "them": 3.29628347982, "whi": 3.2566153846200003, "good": 4.55944859277, "michigan": 17.9592760181, "initi": 2.7, "singl": 1.60948905109, "raschka": 2268.0, "interest": 3.20662492426, "invari": 22.4237288136, "deep": 50.791590493559994, "mani": 6.26560547262, "weaker": 44.595505618000004, "over": 2.05050048434, "numxnum": 52.0524590164, "consid": 1.2397313759200002, "propag": 18.8104265403, "somewher": 15.5647058824, "provid": 1.21552714187, "confus": 4.1451697127900005, "calcul": 6.12972972973, "power": 1.3396337861799998, "linear": 13.8776223776, "next": 2.9901120632800002, "repres": 1.46972782818, "perform": 1.5313977042500002, "but": 2.03264835798, "tangent": 208.89473684200001, "represent": 5.928304705, "our": 35.36382536385, "classif": 24.201219512190004, "cost": 2.31935719503, "off": 3.0242880274400004, "inform": 4.72593768606, "applic": 3.42672134686, "hidden": 23.43897637794, "exampl": 7.5241706161, "big": 2.7400759406299997, "have": 3.0446845234199995, "arriv": 2.03173790632, "think": 17.442959164980003, "neuron": 128.5506072874, "take": 2.27923336444, "now": 3.4823426189999998, "tri": 1.8544562551099997, "then": 2.17315721032, "model": 2.0905978404, "suffer": 2.16117615029, "object": 2.3488681757700003, "basic": 2.7301805675, "play": 1.46390041494, "realli": 4.7476076555, "perceptron": 2268.0, "matter": 2.44773358002, "cours": 2.15092805853, "weight": 34.15242778122, "also": 1.01476510067, "optim": 11.5377906977, "typic": 4.508306119559999, "field": 3.5580457194, "into": 3.04507384437, "yhat": 1134.0, "there": 3.12273800157, "autodifferenti": 1134.0, "recept": 17.868317388860003, "window": 5.86479497599, "gradient": 125.66754617400001, "may": 1.05201775893, "becom": 4.49968114504, "via": 4.595744680859999, "implement": 3.57648118946, "logist": 14.0994671403, "facilit": 6.453658536590001, "open": 1.24556723678, "challeng": 2.55816951337, "eventu": 1.63653231626, "want": 5.99094339624, "better": 2.0065722952500002, "problem": 5.30024482527, "concret": 10.0100882724, "they": 2.06034650574, "vanish": 37.3113983548, "act": 1.4318181818200002, "imposs": 4.96125, "howev": 1.0945191313299998, "effect": 1.3963060686000002, "let": 10.45849802373, "anoth": 1.13643521832, "opposit": 2.4663663197099996, "equivari": 2268.0, "construct": 1.9320920043799998, "which": 2.01038369, "passion": 8.14571575167, "random": 21.5706521739, "featur": 24.43401308192, "suffici": 4.3117870722400005, "activ": 2.92807082258, "thus": 1.6463756092500001, "simpl": 3.3981164383599998, "meaning": 21.8076923077, "deriv": 2.78379800105, "algorithm": 195.6549295778, "one": 3.01882487166, "valu": 4.555523672880001, "share": 1.8566249561500001, "begin": 1.3305397251100002, "parameter": 661.5, "develop": 1.1955719557200002, "discriminatori": 53.6351351351, "such": 2.12302754748, "becaus": 1.1495184997499999, "imag": 16.20826952526, "popular": 1.50769230769, "sever": 1.07241286139, "context": 4.25972632144, "clever": 30.5307692308, "network": 44.09279529496, "like": 2.2983713355, "convent": 2.64159733777, "respect": 1.6443293630200002, "some": 1.04036697248, "sourc": 1.69760479042, "imagin": 6.598503740650001, "add": 9.22486926206, "step": 2.8279301745599996, "locat": 1.59766529134, "detect": 10.82577565632, "part": 2.08661365578, "backpropag": 2268.0, "that": 15.059760956249999, "version": 2.0083491461099996, "structur": 6.174228675149999, "filter": 16.8893617021, "introduc": 1.7258397651900002, "set": 1.18707940781, "this": 11.04172989381, "question": 2.20408163265, "trick": 14.7272727273, "rang": 1.7848229342299997, "substanti": 3.4777656078900003, "from": 6.00340328982, "engin": 7.414072229129999, "aim": 5.792046698280001, "answer": 4.64890190337, "those": 3.58644578313, "differ": 1.23654490225, "same": 2.23715916296, "partial": 3.6131087847099996, "reversemod": 1134.0, "els": 5.44444444444, "idea": 4.1861568886, "left": 1.4398693996, "return": 2.79064862014, "for": 8.00252032008, "predict": 5.18484650555, "output": 15.353965183760002, "input": 48.8116833204, "exact": 3.46864758575, "with": 13.015576716869997, "strong": 1.6439888163999998, "replic": 20.889473684200002, "are": 7.20934155046, "solut": 4.7278141751, "max": 7.474576271189999, "almost": 3.07168424108, "slide": 15.1056137012, "socal": 4536.0, "normal": 2.61075481006, "sort": 5.188235294119999, "look": 1.9086318826599997, "architectur": 5.12790697674, "call": 2.1353059852, "final": 1.34008609775, "more": 3.0515120451, "and": 13.00081889769, "reduc": 1.98698372966, "veri": 2.51760228354, "extract": 7.703056768560001, "addon": 1984.5, "task": 3.88641370869, "harder": 17.1262135922, "rough": 3.29582727839, "forest": 4.89546716004, "general": 1.1218202374200001, "could": 2.4087391898, "focus": 2.01012914662, "sigmoid": 1058.4, "map": 12.218573627490002, "detector": 136.8620689656, "patch": 16.764519535399998, "doe": 1.70581282905, "can": 12.93887530562, "make": 1.0762660158600001, "way": 2.4381478922, "extractor": 299.547169811, "onli": 2.0512953033200003, "each": 2.37949640288, "size": 2.49387370405, "hyperbol": 85.8162162162, "compos": 2.5060773480700003, "goal": 3.28152128979, "scratch": 25.8146341463, "error": 12.08219178082, "neighbor": 5.781500364169999, "continu": 2.27857911734, "solv": 7.26923076923, "data": 10.12930667802, "overcom": 8.38668779715, "note": 1.42449528937, "connect": 3.7687833827800006, "time": 1.01127460348, "behind": 2.0845588235299997, "scientist": 4.69426374926, "machin": 24.146007604559998, "mlps": 3402.0, "convolut": 404.484076432, "bio": 42.336000000000006, "come": 1.32831325301, "must": 1.9220338983099996, "origin": 1.13724928367, "convnet": 2268.0, "sinc": 3.2510580204900004, "right": 1.4054532577899999, "tougher": 175.4254143646, "direct": 1.22226499346, "coeffici": 36.4965517241, "nonlinear": 99.225, "allow": 1.2716059271100002, "use": 6.177832544279999, "meaningless": 51.0482315113, "svms": 1134.0, "help": 5.59851891036, "find": 1.7294117647099998, "round": 3.3843530164099995, "bit": 8.33385826772, "learn": 44.13226042435, "when": 3.0623030926499997, "multilay": 324.0}, "logtfidf": {"after": 0.020490694648099998, "base": 0.13652330228700002, "relat": 0.21310030165399999, "label": 1.49898832727, "pool": 3.906867946, "pixel": 4.45762805629, "univers": 0.444524211372, "permiss": 1.8373800586400002, "the": 0.0, "sebastian": 6.5781143580400006, "here": 0.8850381883700001, "signal": 1.6340517929299998, "techniqu": 1.31624384807, "num": 0.0006299807907940001, "assign": 1.3445959556, "averag": 0.957011687995, "updat": 1.7164374626899999, "paramet": 2.8481901438599997, "regress": 3.9359915164199997, "layer": 25.163749948200003, "logsigmoid": 14.067012968579998, "than": 0.0322608622182, "noisi": 4.1313002687400004, "etc": 1.4366730879700003, "dataset": 5.26584456664, "about": 0.0628434774746, "function": 1.828931483188, "class": 0.7497721899330001, "neural": 36.7678363995, "somewhat": 2.9134920441400003, "buri": 1.6353437827700001, "python": 8.06131348592, "work": 0.109034567273, "terribl": 5.35542765614, "author": 0.35274143130999996, "unit": 0.572752247268, "simpli": 0.923941491586, "essenti": 1.07434378384, "dure": 0.0491209066894, "essenc": 2.6850735669, "enthusiast": 2.24008000599, "state": 0.0932200055336, "scale": 2.64190612656, "automat": 3.8301700946399997, "term": 0.6660779670920001, "where": 0.2599685549828, "train": 1.982754938517, "translat": 1.0499301100299998, "complex": 0.8502416364309999, "start": 0.472886738582, "multipl": 2.02184803624, "usual": 0.545279017064, "speak": 1.06169814662, "repost": 6.83935046985, "them": 0.2825499807279, "whi": 1.18068843047, "good": 1.2557682147209999, "michigan": 4.389919141419999, "initi": 0.6002091849, "singl": 0.475916769059, "raschka": 14.067012968579998, "interest": 0.9441435559639999, "invari": 3.1101197202099997, "deep": 18.0414285772, "mani": 0.2598945487326, "weaker": 6.20897180498, "over": 0.0498734429914, "numxnum": 3.9522520373, "consid": 0.214894723824, "propag": 2.93441131931, "somewher": 2.7450059076200004, "provid": 0.19517784432500002, "confus": 1.4219437317299999, "calcul": 1.8131506592099997, "power": 0.292396282715, "linear": 2.63027764196, "next": 0.804327370998, "repres": 0.38507723275, "perform": 0.42618085058, "but": 0.0323847441438, "tangent": 9.297366586119999, "represent": 1.7797382876499999, "our": 12.86458821273, "classif": 6.263372208870001, "cost": 0.84129007618, "off": 0.8270570407760001, "inform": 1.363361113986, "applic": 1.23160392849, "hidden": 6.1673640156000005, "exampl": 2.0434133749949996, "big": 1.00798563557, "have": 0.0443550070236, "arriv": 0.7088915382879999, "think": 6.403059670499999, "neuron": 8.32635095454, "take": 0.261383924394, "now": 0.44727883506300004, "tri": 0.61759152916, "then": 0.16606773046179998, "model": 0.7374500731110001, "suffer": 0.7706525875229999, "object": 0.853933584803, "basic": 1.00436774895, "play": 0.38110439064199997, "realli": 1.5576408397, "perceptron": 14.067012968579998, "matter": 0.8951625270360001, "cours": 0.765899404133, "weight": 11.09446468284, "also": 0.0146571578, "optim": 2.4456277954099996, "typic": 1.625548638316, "field": 1.1521285167020001, "into": 0.0447385896861, "yhat": 7.033506484289999, "there": 0.12029367877649999, "autodifferenti": 7.033506484289999, "recept": 4.3797639715, "window": 1.7689675242900003, "gradient": 11.20508282646, "may": 0.050709995284400004, "becom": 0.47084870595600004, "via": 1.663967250828, "implement": 1.27437940907, "logist": 2.6461370052, "facilit": 1.86464718498, "open": 0.219591038029, "challeng": 0.9392919688950001, "eventu": 0.49257956194200003, "want": 2.0749098187649997, "better": 0.6964279406, "problem": 1.707422172819, "concret": 2.3035934117099996, "they": 0.0594539895352, "vanish": 5.85230337066, "act": 0.358945092473, "imposs": 1.60165772512, "howev": 0.0903151173475, "effect": 0.333830227158, "let": 3.7464077018399995, "anoth": 0.127896361652, "opposit": 0.90274594185, "equivari": 14.067012968579998, "construct": 0.658603355972, "which": 0.01035682769086, "passion": 2.0974921144, "random": 5.9181642195299995, "featur": 6.774198690272, "suffici": 1.4613524521099999, "activ": 0.762393206568, "thus": 0.49857627139300004, "simpl": 1.2232212893899999, "meaning": 3.08226276571, "deriv": 1.02381618275, "algorithm": 23.31309676626, "one": 0.0187660549365, "valu": 1.646386620296, "share": 0.618760299747, "begin": 0.285584668268, "parameter": 11.602725606, "develop": 0.178624694913, "discriminatori": 3.98220435958, "such": 0.119391955612, "becaus": 0.139343158825, "imag": 5.96257264374, "popular": 0.41058020877499996, "sever": 0.06991112039689999, "context": 1.44920491442, "clever": 3.4187350023299996, "network": 16.202411901884, "like": 0.27810715309, "convent": 0.971383786374, "respect": 0.49733261904, "some": 0.0395735090645, "sourc": 0.529218310751, "imagin": 1.88684291737, "add": 3.05751167426, "step": 1.03954505698, "locat": 0.46854337067199997, "detect": 3.37756548986, "part": 0.08479062196560001, "backpropag": 14.067012968579998, "that": 0.059642225694599996, "version": 0.697313064259, "structur": 2.1653150254050004, "filter": 2.82668393864, "introduc": 0.5457137524260001, "set": 0.171496011289, "this": 0.0416509395775, "question": 0.790310929014, "trick": 2.6897010624299997, "rang": 0.579319213803, "substanti": 1.24639002087, "from": 0.0034023250131959997, "engin": 2.7143026748279997, "aim": 2.12667707408, "answer": 1.5366310419, "those": 0.5356481726189999, "differ": 0.212321121312, "same": 0.224119299208, "partial": 1.28456856096, "reversemod": 7.033506484289999, "els": 1.6945957207700002, "idea": 1.47727184424, "left": 0.364552414753, "return": 0.666253737184, "for": 0.0025199231631760004, "predict": 1.6457402376899999, "output": 4.07645315654, "input": 10.00670134156, "exact": 1.2437647732500001, "with": 0.01556739227407, "strong": 0.49712549393600003, "replic": 3.03924538062, "are": 0.2062723150789, "solut": 1.55346297627, "max": 2.01150743154, "almost": 0.8581576866680001, "slide": 2.7150664430299996, "socal": 28.134025937159997, "normal": 0.959639378783, "sort": 1.64639361896, "look": 0.6463866936, "architectur": 1.63469757919, "call": 0.1309255488976, "final": 0.292733863948, "more": 0.05107479479999999, "and": 0.0008188718468268, "reduc": 0.686617775143, "veri": 0.460319586476, "extract": 2.04161723301, "addon": 13.79995018332, "task": 1.35748680661, "harder": 2.84061024834, "rough": 1.1926572072700001, "forest": 1.5883097076, "general": 0.114952578063, "could": 0.37191254458000006, "focus": 0.6981989720559999, "sigmoid": 6.964513612799999, "map": 4.21303480152, "detector": 11.461084002390002, "patch": 2.81926472072, "doe": 0.5340417297169999, "can": 1.7857520603339998, "make": 0.07349765782289999, "way": 0.39618301987000004, "extractor": 5.7022719003499995, "onli": 0.050648536658199995, "each": 0.347483378608, "size": 0.9138372060609999, "hyperbol": 4.45220798882, "compos": 0.918718721148, "goal": 1.18830712273, "scratch": 3.2509415461, "error": 3.5971708686, "neighbor": 1.7546632275799998, "continu": 0.26080974797400003, "solv": 1.9836504770400003, "data": 3.6504617544, "overcom": 2.12664566269, "note": 0.353817568083, "connect": 1.267210117364, "time": 0.0112115188626, "behind": 0.7345572374320001, "scientist": 1.54634128444, "machin": 8.35415748372, "mlps": 21.100519452869996, "convolut": 18.4652720342, "bio": 3.7456377879300002, "come": 0.28390990653000003, "must": 0.653383947388, "origin": 0.128612437587, "convnet": 14.067012968579998, "sinc": 0.2411045983731, "right": 0.34035985417, "tougher": 8.94813356528, "direct": 0.200705689496, "coeffici": 3.5972177828099996, "nonlinear": 4.59738999867, "allow": 0.24028061118900002, "use": 0.1752481183896, "meaningless": 3.93277090172, "svms": 7.033506484289999, "help": 1.344830885376, "find": 0.547781330288, "round": 1.2191627555700002, "bit": 2.12032652634, "learn": 16.012289230154998, "when": 0.0616649665752, "multilay": 5.78074351579}, "logidf": {"after": 0.020490694648099998, "base": 0.13652330228700002, "relat": 0.21310030165399999, "label": 1.49898832727, "pool": 1.953433973, "pixel": 4.45762805629, "univers": 0.222262105686, "permiss": 1.8373800586400002, "the": 0.0, "sebastian": 3.2890571790200003, "here": 0.8850381883700001, "signal": 1.6340517929299998, "techniqu": 1.31624384807, "num": 0.00031499039539700004, "assign": 1.3445959556, "averag": 0.957011687995, "updat": 1.7164374626899999, "paramet": 2.8481901438599997, "regress": 3.9359915164199997, "layer": 2.0969791623500003, "logsigmoid": 7.033506484289999, "than": 0.0322608622182, "noisi": 4.1313002687400004, "etc": 1.4366730879700003, "dataset": 5.26584456664, "about": 0.0628434774746, "function": 0.914465741594, "class": 0.7497721899330001, "neural": 4.0853151555, "somewhat": 1.4567460220700001, "buri": 1.6353437827700001, "python": 4.03065674296, "work": 0.109034567273, "terribl": 2.67771382807, "author": 0.35274143130999996, "unit": 0.143188061817, "simpli": 0.923941491586, "essenti": 1.07434378384, "dure": 0.0491209066894, "essenc": 2.6850735669, "enthusiast": 2.24008000599, "state": 0.0466100027668, "scale": 1.32095306328, "automat": 1.9150850473199998, "term": 0.33303898354600003, "where": 0.0649921387457, "train": 0.660918312839, "translat": 1.0499301100299998, "complex": 0.8502416364309999, "start": 0.236443369291, "multipl": 1.01092401812, "usual": 0.545279017064, "speak": 1.06169814662, "repost": 6.83935046985, "them": 0.0941833269093, "whi": 1.18068843047, "good": 0.418589404907, "michigan": 2.1949595707099996, "initi": 0.30010459245, "singl": 0.475916769059, "raschka": 7.033506484289999, "interest": 0.47207177798199995, "invari": 3.1101197202099997, "deep": 1.2886734698, "mani": 0.0433157581221, "weaker": 3.10448590249, "over": 0.0249367214957, "numxnum": 3.9522520373, "consid": 0.214894723824, "propag": 2.93441131931, "somewher": 2.7450059076200004, "provid": 0.19517784432500002, "confus": 1.4219437317299999, "calcul": 1.8131506592099997, "power": 0.292396282715, "linear": 2.63027764196, "next": 0.402163685499, "repres": 0.38507723275, "perform": 0.42618085058, "but": 0.0161923720719, "tangent": 4.6486832930599995, "represent": 1.7797382876499999, "our": 0.8576392141820001, "classif": 2.08779073629, "cost": 0.84129007618, "off": 0.41352852038800003, "inform": 0.454453704662, "applic": 1.23160392849, "hidden": 2.0557880052, "exampl": 0.40868267499899996, "big": 1.00798563557, "have": 0.0147850023412, "arriv": 0.7088915382879999, "think": 1.06717661175, "neuron": 4.16317547727, "take": 0.130691962197, "now": 0.149092945021, "tri": 0.61759152916, "then": 0.08303386523089999, "model": 0.7374500731110001, "suffer": 0.7706525875229999, "object": 0.853933584803, "basic": 1.00436774895, "play": 0.38110439064199997, "realli": 1.5576408397, "perceptron": 7.033506484289999, "matter": 0.8951625270360001, "cours": 0.765899404133, "weight": 1.58492352612, "also": 0.0146571578, "optim": 2.4456277954099996, "typic": 0.812774319158, "field": 0.5760642583510001, "into": 0.0149128632287, "yhat": 7.033506484289999, "there": 0.0400978929255, "autodifferenti": 7.033506484289999, "recept": 2.18988198575, "window": 1.7689675242900003, "gradient": 3.73502760882, "may": 0.050709995284400004, "becom": 0.11771217648900001, "via": 0.831983625414, "implement": 1.27437940907, "logist": 2.6461370052, "facilit": 1.86464718498, "open": 0.219591038029, "challeng": 0.9392919688950001, "eventu": 0.49257956194200003, "want": 0.6916366062549999, "better": 0.6964279406, "problem": 0.569140724273, "concret": 2.3035934117099996, "they": 0.0297269947676, "vanish": 2.92615168533, "act": 0.358945092473, "imposs": 1.60165772512, "howev": 0.0903151173475, "effect": 0.333830227158, "let": 1.2488025672799998, "anoth": 0.127896361652, "opposit": 0.90274594185, "equivari": 7.033506484289999, "construct": 0.658603355972, "which": 0.00517841384543, "passion": 2.0974921144, "random": 1.9727214065099998, "featur": 0.423387418142, "suffici": 1.4613524521099999, "activ": 0.381196603284, "thus": 0.49857627139300004, "simpl": 1.2232212893899999, "meaning": 3.08226276571, "deriv": 1.02381618275, "algorithm": 3.33044239518, "one": 0.0062553516455, "valu": 0.823193310148, "share": 0.618760299747, "begin": 0.285584668268, "parameter": 5.801362803, "develop": 0.178624694913, "discriminatori": 3.98220435958, "such": 0.059695977806, "becaus": 0.139343158825, "imag": 0.99376210729, "popular": 0.41058020877499996, "sever": 0.06991112039689999, "context": 1.44920491442, "clever": 3.4187350023299996, "network": 0.9530830530519999, "like": 0.139053576545, "convent": 0.971383786374, "respect": 0.49733261904, "some": 0.0395735090645, "sourc": 0.529218310751, "imagin": 1.88684291737, "add": 1.52875583713, "step": 1.03954505698, "locat": 0.46854337067199997, "detect": 1.68878274493, "part": 0.04239531098280001, "backpropag": 7.033506484289999, "that": 0.00397614837964, "version": 0.697313064259, "structur": 0.7217716751350001, "filter": 2.82668393864, "introduc": 0.5457137524260001, "set": 0.171496011289, "this": 0.0037864490525, "question": 0.790310929014, "trick": 2.6897010624299997, "rang": 0.579319213803, "substanti": 1.24639002087, "from": 0.000567054168866, "engin": 0.904767558276, "aim": 1.06333853704, "answer": 1.5366310419, "those": 0.17854939087299998, "differ": 0.212321121312, "same": 0.112059649604, "partial": 1.28456856096, "reversemod": 7.033506484289999, "els": 1.6945957207700002, "idea": 0.73863592212, "left": 0.364552414753, "return": 0.333126868592, "for": 0.00031499039539700004, "predict": 1.6457402376899999, "output": 2.03822657827, "input": 2.50167533539, "exact": 1.2437647732500001, "with": 0.00119749171339, "strong": 0.49712549393600003, "replic": 3.03924538062, "are": 0.0294674735827, "solut": 1.55346297627, "max": 2.01150743154, "almost": 0.42907884333400004, "slide": 2.7150664430299996, "socal": 7.033506484289999, "normal": 0.959639378783, "sort": 1.64639361896, "look": 0.6463866936, "architectur": 1.63469757919, "call": 0.0654627744488, "final": 0.292733863948, "more": 0.017024931599999998, "and": 6.29901420636e-05, "reduc": 0.686617775143, "veri": 0.230159793238, "extract": 2.04161723301, "addon": 6.89997509166, "task": 1.35748680661, "harder": 2.84061024834, "rough": 1.1926572072700001, "forest": 1.5883097076, "general": 0.114952578063, "could": 0.18595627229000003, "focus": 0.6981989720559999, "sigmoid": 6.964513612799999, "map": 1.40434493384, "detector": 3.8203613341300007, "patch": 2.81926472072, "doe": 0.5340417297169999, "can": 0.162341096394, "make": 0.07349765782289999, "way": 0.19809150993500002, "extractor": 5.7022719003499995, "onli": 0.025324268329099998, "each": 0.173741689304, "size": 0.9138372060609999, "hyperbol": 4.45220798882, "compos": 0.918718721148, "goal": 1.18830712273, "scratch": 3.2509415461, "error": 1.7985854343, "neighbor": 1.7546632275799998, "continu": 0.13040487398700001, "solv": 1.9836504770400003, "data": 1.2168205848, "overcom": 2.12664566269, "note": 0.353817568083, "connect": 0.633605058682, "time": 0.0112115188626, "behind": 0.7345572374320001, "scientist": 1.54634128444, "machin": 1.39235958062, "mlps": 7.033506484289999, "convolut": 4.61631800855, "bio": 3.7456377879300002, "come": 0.28390990653000003, "must": 0.653383947388, "origin": 0.128612437587, "convnet": 7.033506484289999, "sinc": 0.0803681994577, "right": 0.34035985417, "tougher": 4.47406678264, "direct": 0.200705689496, "coeffici": 3.5972177828099996, "nonlinear": 4.59738999867, "allow": 0.24028061118900002, "use": 0.0292080197316, "meaningless": 3.93277090172, "svms": 7.033506484289999, "help": 0.336207721344, "find": 0.547781330288, "round": 1.2191627555700002, "bit": 2.12032652634, "learn": 0.842752064745, "when": 0.0205549888584, "multilay": 5.78074351579}, "freq": {"after": 1, "base": 1, "relat": 1, "label": 1, "pool": 2, "pixel": 1, "univers": 2, "permiss": 1, "the": 54, "sebastian": 2, "here": 1, "signal": 1, "techniqu": 1, "num": 2, "assign": 1, "averag": 1, "updat": 1, "paramet": 1, "regress": 1, "layer": 12, "logsigmoid": 2, "than": 1, "noisi": 1, "etc": 1, "dataset": 1, "about": 1, "function": 2, "class": 1, "neural": 9, "somewhat": 2, "buri": 1, "python": 2, "work": 1, "terribl": 2, "author": 1, "unit": 4, "simpli": 1, "essenti": 1, "dure": 1, "essenc": 1, "enthusiast": 1, "state": 2, "scale": 2, "automat": 2, "term": 2, "where": 4, "train": 3, "translat": 1, "complex": 1, "start": 2, "multipl": 2, "usual": 1, "speak": 1, "repost": 1, "them": 3, "whi": 1, "good": 3, "michigan": 2, "initi": 2, "singl": 1, "raschka": 2, "interest": 2, "invari": 1, "deep": 14, "mani": 6, "weaker": 2, "over": 2, "numxnum": 1, "consid": 1, "propag": 1, "somewher": 1, "provid": 1, "confus": 1, "calcul": 1, "power": 1, "linear": 1, "next": 2, "repres": 1, "perform": 1, "but": 2, "tangent": 2, "represent": 1, "our": 15, "classif": 3, "cost": 1, "off": 2, "inform": 3, "applic": 1, "hidden": 3, "exampl": 5, "big": 1, "have": 3, "arriv": 1, "think": 6, "neuron": 2, "take": 2, "now": 3, "tri": 1, "then": 2, "model": 1, "suffer": 1, "object": 1, "basic": 1, "play": 1, "realli": 1, "perceptron": 2, "matter": 1, "cours": 1, "weight": 7, "also": 1, "optim": 1, "typic": 2, "field": 2, "into": 3, "yhat": 1, "there": 3, "autodifferenti": 1, "recept": 2, "window": 1, "gradient": 3, "may": 1, "becom": 4, "via": 2, "implement": 1, "logist": 1, "facilit": 1, "open": 1, "challeng": 1, "eventu": 1, "want": 3, "better": 1, "problem": 3, "concret": 1, "they": 2, "vanish": 2, "act": 1, "imposs": 1, "howev": 1, "effect": 1, "let": 3, "anoth": 1, "opposit": 1, "equivari": 2, "construct": 1, "which": 2, "passion": 1, "random": 3, "featur": 16, "suffici": 1, "activ": 2, "thus": 1, "simpl": 1, "meaning": 1, "deriv": 1, "algorithm": 7, "one": 3, "valu": 2, "share": 1, "begin": 1, "parameter": 2, "develop": 1, "discriminatori": 1, "such": 2, "becaus": 1, "imag": 6, "popular": 1, "sever": 1, "context": 1, "clever": 1, "network": 17, "like": 2, "convent": 1, "respect": 1, "some": 1, "sourc": 1, "imagin": 1, "add": 2, "step": 1, "locat": 1, "detect": 2, "part": 2, "backpropag": 2, "that": 15, "version": 1, "structur": 3, "filter": 1, "introduc": 1, "set": 1, "this": 11, "question": 1, "trick": 1, "rang": 1, "substanti": 1, "from": 6, "engin": 3, "aim": 2, "answer": 1, "those": 3, "differ": 1, "same": 2, "partial": 1, "reversemod": 1, "els": 1, "idea": 2, "left": 1, "return": 2, "for": 8, "predict": 1, "output": 2, "input": 4, "exact": 1, "with": 13, "strong": 1, "replic": 1, "are": 7, "solut": 1, "max": 1, "almost": 2, "slide": 1, "socal": 4, "normal": 1, "sort": 1, "look": 1, "architectur": 1, "call": 2, "final": 1, "more": 3, "and": 13, "reduc": 1, "veri": 2, "extract": 1, "addon": 2, "task": 1, "harder": 1, "rough": 1, "forest": 1, "general": 1, "could": 2, "focus": 1, "sigmoid": 1, "map": 3, "detector": 3, "patch": 1, "doe": 1, "can": 11, "make": 1, "way": 2, "extractor": 1, "onli": 2, "each": 2, "size": 1, "hyperbol": 1, "compos": 1, "goal": 1, "scratch": 1, "error": 2, "neighbor": 1, "continu": 2, "solv": 1, "data": 3, "overcom": 1, "note": 1, "connect": 2, "time": 1, "behind": 1, "scientist": 1, "machin": 6, "mlps": 3, "convolut": 4, "bio": 1, "come": 1, "must": 1, "origin": 1, "convnet": 2, "sinc": 3, "right": 1, "tougher": 2, "direct": 1, "coeffici": 1, "nonlinear": 1, "allow": 1, "use": 6, "meaningless": 1, "svms": 1, "help": 4, "find": 1, "round": 1, "bit": 1, "learn": 19, "when": 3, "multilay": 1}, "idf": {"after": 1.02070207021, "base": 1.14628158845, "relat": 1.23750876919, "label": 4.47715736041, "pool": 7.052865393160001, "pixel": 86.28260869569999, "univers": 1.24889867841, "permiss": 6.280063291139999, "the": 1.0, "sebastian": 26.8175675676, "here": 2.42307692308, "signal": 5.12459651388, "techniqu": 3.7293868921800004, "num": 1.00031504001, "assign": 3.83663605607, "averag": 2.60390355913, "updat": 5.56466876972, "paramet": 17.256521739100002, "regress": 51.2129032258, "layer": 8.14153846154, "logsigmoid": 1134.0, "than": 1.03278688525, "noisi": 62.2588235294, "etc": 4.2066772655, "dataset": 193.609756098, "about": 1.06486015159, "function": 2.495441685, "class": 2.11651779763, "neural": 59.4606741573, "somewhat": 4.29197080292, "buri": 5.13122171946, "python": 56.2978723404, "work": 1.11520089913, "terribl": 14.5517873511, "author": 1.4229631621399998, "unit": 1.15394679459, "simpli": 2.5192002538900002, "essenti": 2.9280708225700005, "dure": 1.0503473370799998, "essenc": 14.659279778399998, "enthusiast": 9.39408284024, "state": 1.0477133240899998, "scale": 3.7469907953699995, "automat": 6.787516032490001, "term": 1.39520168732, "where": 1.06715063521, "train": 1.9365698950999999, "translat": 2.85745140389, "complex": 2.34021226415, "start": 1.26673581744, "multipl": 2.74813917258, "usual": 1.72508964468, "speak": 2.89127663449, "repost": 933.882352941, "them": 1.09876115994, "whi": 3.2566153846200003, "good": 1.51981619759, "michigan": 8.97963800905, "initi": 1.35, "singl": 1.60948905109, "raschka": 1134.0, "interest": 1.60331246213, "invari": 22.4237288136, "deep": 3.6279707495399998, "mani": 1.04426757877, "weaker": 22.297752809000002, "over": 1.02525024217, "numxnum": 52.0524590164, "consid": 1.2397313759200002, "propag": 18.8104265403, "somewher": 15.5647058824, "provid": 1.21552714187, "confus": 4.1451697127900005, "calcul": 6.12972972973, "power": 1.3396337861799998, "linear": 13.8776223776, "next": 1.4950560316400001, "repres": 1.46972782818, "perform": 1.5313977042500002, "but": 1.01632417899, "tangent": 104.44736842100001, "represent": 5.928304705, "our": 2.35758835759, "classif": 8.067073170730001, "cost": 2.31935719503, "off": 1.5121440137200002, "inform": 1.5753125620200001, "applic": 3.42672134686, "hidden": 7.81299212598, "exampl": 1.50483412322, "big": 2.7400759406299997, "have": 1.0148948411399998, "arriv": 2.03173790632, "think": 2.90715986083, "neuron": 64.2753036437, "take": 1.13961668222, "now": 1.160780873, "tri": 1.8544562551099997, "then": 1.08657860516, "model": 2.0905978404, "suffer": 2.16117615029, "object": 2.3488681757700003, "basic": 2.7301805675, "play": 1.46390041494, "realli": 4.7476076555, "perceptron": 1134.0, "matter": 2.44773358002, "cours": 2.15092805853, "weight": 4.878918254459999, "also": 1.01476510067, "optim": 11.5377906977, "typic": 2.2541530597799997, "field": 1.7790228597, "into": 1.01502461479, "yhat": 1134.0, "there": 1.04091266719, "autodifferenti": 1134.0, "recept": 8.934158694430002, "window": 5.86479497599, "gradient": 41.889182058, "may": 1.05201775893, "becom": 1.12492028626, "via": 2.2978723404299997, "implement": 3.57648118946, "logist": 14.0994671403, "facilit": 6.453658536590001, "open": 1.24556723678, "challeng": 2.55816951337, "eventu": 1.63653231626, "want": 1.99698113208, "better": 2.0065722952500002, "problem": 1.76674827509, "concret": 10.0100882724, "they": 1.03017325287, "vanish": 18.6556991774, "act": 1.4318181818200002, "imposs": 4.96125, "howev": 1.0945191313299998, "effect": 1.3963060686000002, "let": 3.48616600791, "anoth": 1.13643521832, "opposit": 2.4663663197099996, "equivari": 1134.0, "construct": 1.9320920043799998, "which": 1.005191845, "passion": 8.14571575167, "random": 7.1902173913, "featur": 1.52712581762, "suffici": 4.3117870722400005, "activ": 1.46403541129, "thus": 1.6463756092500001, "simpl": 3.3981164383599998, "meaning": 21.8076923077, "deriv": 2.78379800105, "algorithm": 27.9507042254, "one": 1.00627495722, "valu": 2.2777618364400003, "share": 1.8566249561500001, "begin": 1.3305397251100002, "parameter": 330.75, "develop": 1.1955719557200002, "discriminatori": 53.6351351351, "such": 1.06151377374, "becaus": 1.1495184997499999, "imag": 2.70137825421, "popular": 1.50769230769, "sever": 1.07241286139, "context": 4.25972632144, "clever": 30.5307692308, "network": 2.59369384088, "like": 1.14918566775, "convent": 2.64159733777, "respect": 1.6443293630200002, "some": 1.04036697248, "sourc": 1.69760479042, "imagin": 6.598503740650001, "add": 4.61243463103, "step": 2.8279301745599996, "locat": 1.59766529134, "detect": 5.41288782816, "part": 1.04330682789, "backpropag": 1134.0, "that": 1.00398406375, "version": 2.0083491461099996, "structur": 2.0580762250499998, "filter": 16.8893617021, "introduc": 1.7258397651900002, "set": 1.18707940781, "this": 1.00379362671, "question": 2.20408163265, "trick": 14.7272727273, "rang": 1.7848229342299997, "substanti": 3.4777656078900003, "from": 1.00056721497, "engin": 2.47135740971, "aim": 2.8960233491400005, "answer": 4.64890190337, "those": 1.19548192771, "differ": 1.23654490225, "same": 1.11857958148, "partial": 3.6131087847099996, "reversemod": 1134.0, "els": 5.44444444444, "idea": 2.0930784443, "left": 1.4398693996, "return": 1.39532431007, "for": 1.00031504001, "predict": 5.18484650555, "output": 7.676982591880001, "input": 12.2029208301, "exact": 3.46864758575, "with": 1.0011982089899998, "strong": 1.6439888163999998, "replic": 20.889473684200002, "are": 1.02990593578, "solut": 4.7278141751, "max": 7.474576271189999, "almost": 1.53584212054, "slide": 15.1056137012, "socal": 1134.0, "normal": 2.61075481006, "sort": 5.188235294119999, "look": 1.9086318826599997, "architectur": 5.12790697674, "call": 1.0676529926, "final": 1.34008609775, "more": 1.0171706817, "and": 1.00006299213, "reduc": 1.98698372966, "veri": 1.25880114177, "extract": 7.703056768560001, "addon": 992.25, "task": 3.88641370869, "harder": 17.1262135922, "rough": 3.29582727839, "forest": 4.89546716004, "general": 1.1218202374200001, "could": 1.2043695949, "focus": 2.01012914662, "sigmoid": 1058.4, "map": 4.0728578758300005, "detector": 45.6206896552, "patch": 16.764519535399998, "doe": 1.70581282905, "can": 1.17626139142, "make": 1.0762660158600001, "way": 1.2190739461, "extractor": 299.547169811, "onli": 1.0256476516600002, "each": 1.18974820144, "size": 2.49387370405, "hyperbol": 85.8162162162, "compos": 2.5060773480700003, "goal": 3.28152128979, "scratch": 25.8146341463, "error": 6.04109589041, "neighbor": 5.781500364169999, "continu": 1.13928955867, "solv": 7.26923076923, "data": 3.37643555934, "overcom": 8.38668779715, "note": 1.42449528937, "connect": 1.8843916913900003, "time": 1.01127460348, "behind": 2.0845588235299997, "scientist": 4.69426374926, "machin": 4.02433460076, "mlps": 1134.0, "convolut": 101.121019108, "bio": 42.336000000000006, "come": 1.32831325301, "must": 1.9220338983099996, "origin": 1.13724928367, "convnet": 1134.0, "sinc": 1.08368600683, "right": 1.4054532577899999, "tougher": 87.7127071823, "direct": 1.22226499346, "coeffici": 36.4965517241, "nonlinear": 99.225, "allow": 1.2716059271100002, "use": 1.0296387573799999, "meaningless": 51.0482315113, "svms": 1134.0, "help": 1.39962972759, "find": 1.7294117647099998, "round": 3.3843530164099995, "bit": 8.33385826772, "learn": 2.32275054865, "when": 1.02076769755, "multilay": 324.0}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  What is the Difference Between Deep Learning and \u201cRegular\u201d Machine Learning?</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2016/06/difference-between-deep-learning-regular-machine-learning.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb What is the Difference Between Deep Learning and \u201cRegular\u201d Machine Learning? Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2016/06/doing-data-science-kaggle-walkthrough-data-cleaning.html\" rel=\"prev\" title=\"Doing Data Science: A Kaggle Walkthrough Part 3 \u2013 Cleaning Data\"/>\n<link href=\"https://www.kdnuggets.com/2016/06/building-data-systems-need.html\" rel=\"next\" title=\"Building Data Systems: What Do You Need?\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2016/06/difference-between-deep-learning-regular-machine-learning.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=50252\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2016/06/difference-between-deep-learning-regular-machine-learning.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-50252 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 3-Jun, 2016  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2016/index.html\">2016</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/06/index.html\">Jun</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/06/tutorials.html\">Tutorials, Overviews</a> \u00bb What is the Difference Between Deep Learning and \u201cRegular\u201d Machine Learning? (\u00a0<a href=\"/2016/n20.html\">16:n20</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\"><img align=\"right\" alt=\"2016 Silver Blog\" src=\"/images/top-kdnuggets-blog-2016-silver.png\" width=\"120\"/>What is the Difference Between Deep Learning and \u201cRegular\u201d Machine Learning?</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2016/06/doing-data-science-kaggle-walkthrough-data-cleaning.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2016/06/building-data-systems-need.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <span class=\"http-likes\" style=\"float: left; font-size:14px\">http likes 2018</span> <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/convolutional-neural-networks\" rel=\"tag\">Convolutional Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a></div>\n<br/>\n<p class=\"excerpt\">\n     Another concise explanation of a machine learning concept by Sebastian Raschka. This time, Sebastian explains the difference between Deep Learning and \"regular\" machine learning.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/sebastian-raschka\" rel=\"author\" title=\"Posts by Sebastian Raschka\">Sebastian Raschka</a>, Michigan State University.</b></div>\n<p>That's an interesting question, and I try to answer this is a very general way. The tl;dr version of this is: Deep learning is essentially a set of techniques that help we to parameterize deep neural network structures, neural networks with many, many layers and parameters.</p>\n<p>And if we are interested, a more concrete example: Let's start with multi-layer perceptrons (MLPs)...</p>\n<p>On a tangent: The term \"perceptron\" in MLPs may be a bit confusing since we don't really want only linear neurons in our network. Using MLPs, we want to learn complex functions to solve non-linear problems. Thus, our network is conventionally composed of one or multiple \"hidden\" layers that connect the input and output layer. Those hidden layers normally have some sort of sigmoid activation function (log-sigmoid or the hyperbolic tangent etc.). For example, think of a log-sigmoid unit in our network as a logistic regression unit that returns continuous values outputs in the range 0-1. A simple MLP could look like this</p>\n<p><a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning/mlp.png\" target=\"_blank\"><img alt=\"\" src=\"https://github.com/rasbt/python-machine-learning-book/raw/master/faq/difference-deep-and-normal-learning/mlp.png\"/></a></p>\n<p>where y_hat is the final class label that we return as the prediction based on the inputs (x) if this are classification tasks. The \"a\"s are our activated neurons and the \"w\"s are the weight coefficients. Now, if we add multiple hidden layers to this MLP, we'd also call the network \"deep.\" The problem with such \"deep\" networks is that it becomes tougher and tougher to learn \"good\" weights for this network. When we start training our network, we typically assign random values as initial weights, which can be terribly off from the \"optimal\" solution we want to find. During training, we then use the popular backpropagation algorithm (think of it as reverse-mode auto-differentiation) to propagate the \"errors\" from right to left and calculate the partial derivatives with respect to each weight to take a step into the opposite direction of the cost (or \"error\") gradient.\u00a0<b>Now, the problem with deep neural networks is the so-called \"vanishing gradient\" -- the more layers we add, the harder it becomes to \"update\" our weights because the signal becomes weaker and weaker. Since our network's weights can be terribly off in the beginning (random initialization) it can become almost impossible to parameterize a \"deep\" neural network with backpropagation.</b></p>\n<p><b>Deep Learning</b></p>\n<p>Now, this is where \"deep learning\" comes into play. Roughly speaking, we can think of deep learning as \"clever\" tricks or algorithms that can help we with the training of such \"deep\" neural network structures. There are many, many different neural network architectures, but to continue with the example of the MLP, let me introduce the idea of convolutional neural networks (ConvNets). We can think of those as an \"add-on\" to our MLP that helps we to detect features as \"good\" inputs for our MLP.</p>\n<p>In applications of \"usual\" machine learning, there is typically a strong focus on the feature engineering part; the model learned by an algorithm can only be so good as its input data. Of course, there must be sufficient discriminatory information in our dataset, however, the performance of machine learning algorithms can suffer substantially when the information is buried in meaningless features. The goal behind deep learning is to automatically learn the features from (somewhat) noisy data; it's about algorithms that do the feature engineering for us to provide deep neural network structures with meaningful information so that it can learn more effectively.\u00a0<b>We can think of deep learning as algorithms for automatic \"feature engineering,\" or we could simply call them \"feature detectors,\" which help us to overcome the vanishing gradient challenge and facilitate the learning in neural networks with many layers.</b></p>\n<p>Let's consider a ConvNet in context of image classification. Here, we use so-called \"receptive fields\" (think of them as \"windows\") that slide over our image. We then connect those \"receptive fields\" (for example of the size of 5x5 pixel) with 1 unit in the next layer, this is the so-called \"feature map.\" After this mapping, we have constructed a so-called convolutional layer. Note that our feature detectors are basically replicates of one another -- they share the same weights. The idea is that if a feature detector is useful in one part of the imagine it is likely that it is useful somewhere else, but at the same time it allows each patch of image to be represented in several ways.</p>\n<p><a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning/convolution.png\" target=\"_blank\"><img alt=\"\" src=\"https://github.com/rasbt/python-machine-learning-book/raw/master/faq/difference-deep-and-normal-learning/convolution.png\"/></a></p>\n<p>Next, we have a \"pooling\" layer, where we reduce neighboring features from our feature map into single units (by taking the max feature or by averaging them, for example). We do this over many rounds and eventually arrive at an almost scale invariant representation of our image (the exact term is \"equivariant\"). This is very powerful since we can detect objects in an image no matter where they are located.</p>\n<p><a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning/convnet.png\" target=\"_blank\"><img alt=\"\" src=\"https://github.com/rasbt/python-machine-learning-book/raw/master/faq/difference-deep-and-normal-learning/convnet.png\"/></a></p>\n<p>In essence, the \"convolutional\" add-on that acts as a feature extractor or filter to our MLP. Via the convolutional layers we aim to extract the useful features from the images, and via the pooling layers, we aim to make the features somewhat equivariant to scale and translation.</p>\n<p><b>Bio: <a href=\"https://twitter.com/rasbt\">Sebastian Raschka</a></b> is a 'Data Scientist' and Machine Learning enthusiast with a big passion for Python &amp; open source. Author of '<a href=\"https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning\">Python Machine Learning</a>'. Michigan State University.</p>\n<p><a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2016/04/deep-learning-vs-svm-random-forest.html\">When Does Deep Learning Work Better Than SVMs or Random Forests?</a></li>\n<li><a href=\"/2016/04/development-classification-learning-machine.html\">The Development of Classification as a Learning Machine</a></li>\n<li><a href=\"/2016/05/implement-machine-learning-algorithms-scratch.html\">Why Implement Machine Learning Algorithms From Scratch?</a></li>\n</ul>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2016/06/doing-data-science-kaggle-walkthrough-data-cleaning.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2016/06/building-data-systems-need.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2016/index.html\">2016</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/06/index.html\">Jun</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/06/tutorials.html\">Tutorials, Overviews</a> \u00bb What is the Difference Between Deep Learning and \u201cRegular\u201d Machine Learning? (\u00a0<a href=\"/2016/n20.html\">16:n20</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556357652\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></body></html>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n\n\n<!-- Dynamic page generated in 0.603 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 05:34:12 -->\n<!-- Compression = gzip -->", "content_tokenized": ["sebastian", "raschka", "michigan", "state", "univers", "that", "interest", "question", "and", "tri", "answer", "this", "veri", "general", "way", "the", "version", "this", "deep", "learn", "essenti", "set", "techniqu", "that", "help", "parameter", "deep", "neural", "network", "structur", "neural", "network", "with", "mani", "mani", "layer", "and", "paramet", "and", "are", "interest", "more", "concret", "exampl", "let", "start", "with", "multilay", "perceptron", "mlps", "tangent", "the", "term", "perceptron", "mlps", "may", "bit", "confus", "sinc", "realli", "want", "onli", "linear", "neuron", "our", "network", "use", "mlps", "want", "learn", "complex", "function", "solv", "nonlinear", "problem", "thus", "our", "network", "convent", "compos", "one", "multipl", "hidden", "layer", "that", "connect", "the", "input", "and", "output", "layer", "those", "hidden", "layer", "normal", "have", "some", "sort", "sigmoid", "activ", "function", "logsigmoid", "the", "hyperbol", "tangent", "etc", "for", "exampl", "think", "logsigmoid", "unit", "our", "network", "logist", "regress", "unit", "that", "return", "continu", "valu", "output", "the", "rang", "num", "simpl", "could", "look", "like", "this", "where", "yhat", "the", "final", "class", "label", "that", "return", "the", "predict", "base", "the", "input", "this", "are", "classif", "task", "the", "are", "our", "activ", "neuron", "and", "the", "are", "the", "weight", "coeffici", "now", "add", "multipl", "hidden", "layer", "this", "also", "call", "the", "network", "deep", "the", "problem", "with", "such", "deep", "network", "that", "becom", "tougher", "and", "tougher", "learn", "good", "weight", "for", "this", "network", "when", "start", "train", "our", "network", "typic", "assign", "random", "valu", "initi", "weight", "which", "can", "terribl", "off", "from", "the", "optim", "solut", "want", "find", "dure", "train", "then", "use", "the", "popular", "backpropag", "algorithm", "think", "reversemod", "autodifferenti", "propag", "the", "error", "from", "right", "left", "and", "calcul", "the", "partial", "deriv", "with", "respect", "each", "weight", "take", "step", "into", "the", "opposit", "direct", "the", "cost", "error", "gradient", "now", "the", "problem", "with", "deep", "neural", "network", "the", "socal", "vanish", "gradient", "the", "more", "layer", "add", "the", "harder", "becom", "updat", "our", "weight", "becaus", "the", "signal", "becom", "weaker", "and", "weaker", "sinc", "our", "network", "weight", "can", "terribl", "off", "the", "begin", "random", "initi", "can", "becom", "almost", "imposs", "parameter", "deep", "neural", "network", "with", "backpropag", "deep", "learn", "now", "this", "where", "deep", "learn", "come", "into", "play", "rough", "speak", "can", "think", "deep", "learn", "clever", "trick", "algorithm", "that", "can", "help", "with", "the", "train", "such", "deep", "neural", "network", "structur", "there", "are", "mani", "mani", "differ", "neural", "network", "architectur", "but", "continu", "with", "the", "exampl", "the", "let", "introduc", "the", "idea", "convolut", "neural", "network", "convnet", "can", "think", "those", "addon", "our", "that", "help", "detect", "featur", "good", "input", "for", "our", "applic", "usual", "machin", "learn", "there", "typic", "strong", "focus", "the", "featur", "engin", "part", "the", "model", "learn", "algorithm", "can", "onli", "good", "input", "data", "cours", "there", "must", "suffici", "discriminatori", "inform", "our", "dataset", "howev", "the", "perform", "machin", "learn", "algorithm", "can", "suffer", "substanti", "when", "the", "inform", "buri", "meaningless", "featur", "the", "goal", "behind", "deep", "learn", "automat", "learn", "the", "featur", "from", "somewhat", "noisi", "data", "about", "algorithm", "that", "the", "featur", "engin", "for", "provid", "deep", "neural", "network", "structur", "with", "meaning", "inform", "that", "can", "learn", "more", "effect", "can", "think", "deep", "learn", "algorithm", "for", "automat", "featur", "engin", "could", "simpli", "call", "them", "featur", "detector", "which", "help", "overcom", "the", "vanish", "gradient", "challeng", "and", "facilit", "the", "learn", "neural", "network", "with", "mani", "layer", "let", "consid", "convnet", "context", "imag", "classif", "here", "use", "socal", "recept", "field", "think", "them", "window", "that", "slide", "over", "our", "imag", "then", "connect", "those", "recept", "field", "for", "exampl", "the", "size", "numxnum", "pixel", "with", "num", "unit", "the", "next", "layer", "this", "the", "socal", "featur", "map", "after", "this", "map", "have", "construct", "socal", "convolut", "layer", "note", "that", "our", "featur", "detector", "are", "basic", "replic", "one", "anoth", "they", "share", "the", "same", "weight", "the", "idea", "that", "featur", "detector", "use", "one", "part", "the", "imagin", "like", "that", "use", "somewher", "els", "but", "the", "same", "time", "allow", "each", "patch", "imag", "repres", "sever", "way", "next", "have", "pool", "layer", "where", "reduc", "neighbor", "featur", "from", "our", "featur", "map", "into", "singl", "unit", "take", "the", "max", "featur", "averag", "them", "for", "exampl", "this", "over", "mani", "round", "and", "eventu", "arriv", "almost", "scale", "invari", "represent", "our", "imag", "the", "exact", "term", "equivari", "this", "veri", "power", "sinc", "can", "detect", "object", "imag", "matter", "where", "they", "are", "locat", "essenc", "the", "convolut", "addon", "that", "act", "featur", "extractor", "filter", "our", "via", "the", "convolut", "layer", "aim", "extract", "the", "use", "featur", "from", "the", "imag", "and", "via", "the", "pool", "layer", "aim", "make", "the", "featur", "somewhat", "equivari", "scale", "and", "translat", "bio", "sebastian", "raschka", "data", "scientist", "and", "machin", "learn", "enthusiast", "with", "big", "passion", "for", "python", "open", "sourc", "author", "python", "machin", "learn", "michigan", "state", "univers", "origin", "repost", "with", "permiss", "relat", "when", "doe", "deep", "learn", "work", "better", "than", "svms", "random", "forest", "the", "develop", "classif", "learn", "machin", "whi", "implement", "machin", "learn", "algorithm", "from", "scratch"], "timestamp_scraper": 1556365401.50103, "title": "What is the Difference Between Deep Learning and \u201cRegular\u201d Machine Learning?", "read_time": 271.2, "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/sebastian-raschka\" rel=\"author\" title=\"Posts by Sebastian Raschka\">Sebastian Raschka</a>, Michigan State University.</b></div>\n<p>That's an interesting question, and I try to answer this is a very general way. The tl;dr version of this is: Deep learning is essentially a set of techniques that help we to parameterize deep neural network structures, neural networks with many, many layers and parameters.</p>\n<p>And if we are interested, a more concrete example: Let's start with multi-layer perceptrons (MLPs)...</p>\n<p>On a tangent: The term \"perceptron\" in MLPs may be a bit confusing since we don't really want only linear neurons in our network. Using MLPs, we want to learn complex functions to solve non-linear problems. Thus, our network is conventionally composed of one or multiple \"hidden\" layers that connect the input and output layer. Those hidden layers normally have some sort of sigmoid activation function (log-sigmoid or the hyperbolic tangent etc.). For example, think of a log-sigmoid unit in our network as a logistic regression unit that returns continuous values outputs in the range 0-1. A simple MLP could look like this</p>\n<p><a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning/mlp.png\" target=\"_blank\"><img alt=\"\" src=\"https://github.com/rasbt/python-machine-learning-book/raw/master/faq/difference-deep-and-normal-learning/mlp.png\"/></a></p>\n<p>where y_hat is the final class label that we return as the prediction based on the inputs (x) if this are classification tasks. The \"a\"s are our activated neurons and the \"w\"s are the weight coefficients. Now, if we add multiple hidden layers to this MLP, we'd also call the network \"deep.\" The problem with such \"deep\" networks is that it becomes tougher and tougher to learn \"good\" weights for this network. When we start training our network, we typically assign random values as initial weights, which can be terribly off from the \"optimal\" solution we want to find. During training, we then use the popular backpropagation algorithm (think of it as reverse-mode auto-differentiation) to propagate the \"errors\" from right to left and calculate the partial derivatives with respect to each weight to take a step into the opposite direction of the cost (or \"error\") gradient.\u00a0<b>Now, the problem with deep neural networks is the so-called \"vanishing gradient\" -- the more layers we add, the harder it becomes to \"update\" our weights because the signal becomes weaker and weaker. Since our network's weights can be terribly off in the beginning (random initialization) it can become almost impossible to parameterize a \"deep\" neural network with backpropagation.</b></p>\n<p><b>Deep Learning</b></p>\n<p>Now, this is where \"deep learning\" comes into play. Roughly speaking, we can think of deep learning as \"clever\" tricks or algorithms that can help we with the training of such \"deep\" neural network structures. There are many, many different neural network architectures, but to continue with the example of the MLP, let me introduce the idea of convolutional neural networks (ConvNets). We can think of those as an \"add-on\" to our MLP that helps we to detect features as \"good\" inputs for our MLP.</p>\n<p>In applications of \"usual\" machine learning, there is typically a strong focus on the feature engineering part; the model learned by an algorithm can only be so good as its input data. Of course, there must be sufficient discriminatory information in our dataset, however, the performance of machine learning algorithms can suffer substantially when the information is buried in meaningless features. The goal behind deep learning is to automatically learn the features from (somewhat) noisy data; it's about algorithms that do the feature engineering for us to provide deep neural network structures with meaningful information so that it can learn more effectively.\u00a0<b>We can think of deep learning as algorithms for automatic \"feature engineering,\" or we could simply call them \"feature detectors,\" which help us to overcome the vanishing gradient challenge and facilitate the learning in neural networks with many layers.</b></p>\n<p>Let's consider a ConvNet in context of image classification. Here, we use so-called \"receptive fields\" (think of them as \"windows\") that slide over our image. We then connect those \"receptive fields\" (for example of the size of 5x5 pixel) with 1 unit in the next layer, this is the so-called \"feature map.\" After this mapping, we have constructed a so-called convolutional layer. Note that our feature detectors are basically replicates of one another -- they share the same weights. The idea is that if a feature detector is useful in one part of the imagine it is likely that it is useful somewhere else, but at the same time it allows each patch of image to be represented in several ways.</p>\n<p><a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning/convolution.png\" target=\"_blank\"><img alt=\"\" src=\"https://github.com/rasbt/python-machine-learning-book/raw/master/faq/difference-deep-and-normal-learning/convolution.png\"/></a></p>\n<p>Next, we have a \"pooling\" layer, where we reduce neighboring features from our feature map into single units (by taking the max feature or by averaging them, for example). We do this over many rounds and eventually arrive at an almost scale invariant representation of our image (the exact term is \"equivariant\"). This is very powerful since we can detect objects in an image no matter where they are located.</p>\n<p><a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning/convnet.png\" target=\"_blank\"><img alt=\"\" src=\"https://github.com/rasbt/python-machine-learning-book/raw/master/faq/difference-deep-and-normal-learning/convnet.png\"/></a></p>\n<p>In essence, the \"convolutional\" add-on that acts as a feature extractor or filter to our MLP. Via the convolutional layers we aim to extract the useful features from the images, and via the pooling layers, we aim to make the features somewhat equivariant to scale and translation.</p>\n<p><b>Bio: <a href=\"https://twitter.com/rasbt\">Sebastian Raschka</a></b> is a 'Data Scientist' and Machine Learning enthusiast with a big passion for Python &amp; open source. Author of '<a href=\"https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning\">Python Machine Learning</a>'. Michigan State University.</p>\n<p><a href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2016/04/deep-learning-vs-svm-random-forest.html\">When Does Deep Learning Work Better Than SVMs or Random Forests?</a></li>\n<li><a href=\"/2016/04/development-classification-learning-machine.html\">The Development of Classification as a Learning Machine</a></li>\n<li><a href=\"/2016/05/implement-machine-learning-algorithms-scratch.html\">Why Implement Machine Learning Algorithms From Scratch?</a></li>\n</ul>\n</div> ", "website": "kdnuggets"}