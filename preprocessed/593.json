{"content": "Gokula Krishnan Santhanam, ETH Zurich. Deep Learning, whether you like it or not is here to stay, and with any tech gold-rush comes a plethora of options that can seem daunting to newcomers. If you were to start off with deep learning, one of the first questions to ask is, which framework to learn? I\u2019d say instead of a simple trial-and-error, if you try to understand the building blocks of all these frameworks, it would help you make an informed decision. Common choices include Theano , TensorFlow , Torch , and Keras . All of these choices have their own pros and cons and have their own way of doing things. After exploring the white papers and the dev docs, I could understand the design choices and was able to abstract the fundamental concepts that are common to all of these. In this post, I have tried to sketch out these common principles which would help you better understand the frameworks and for the brave hearts among you, provide a guide on how to implement your own deep learning framework. The interesting thing about these principles is that they are not specific to DL alone, they are applicable whenever you want to do a series of computations on data. Hence, most DL frameworks can be used for non-DL tasks as well (see www.tensorflow.org/tutorials/mandelbrot/ ) There are a few core components of a DL framework, these are: A Tensor Object Operations on the Tensor Object A Computation Graph and Optimizations Auto-differentiation tools BLAS / cuBLAS and cuDNN extensions These should make your framework complete, but you would need to polish it to make it more easy to use. I will be using references to the Python NumPy package in this article to make it easier to understand. If you haven\u2019t used NumPy before, fret not, this article should be easy to understand even if you skip the numpy parts. I\u2019m a believer of understanding a system at multiple levels of abstractions so you will be seeing discussions of low-level optimizations interspersed with high-level Calculus and Linear Algebra. Drop a comment below if something needs more explanation! NOTE : I have been a contributor to Theano and hence might be biased towards it in citing references. Having said that, theano also has one of the most informative websites of all the frameworks I\u2019ve come across. A Tensor Object \u00a0 At the heart of the framework is the tensor object. A tensor is a generalization of a matrix to n-dimensions (think numpy\u2019s ndarrays). In other words, a matrix is a two-dimensional tensor with (rows, columns). An easy way to understand tensors is to consider them as N-D Arrays. As an example, take a color image. Let\u2019s say it\u2019s an RGB Bitmap image of size 258 x 320 (Height x Width). This is a three-dimensional tensor (height, width, channels). Take a look at the following images to understand this better (taken from: github.com/parambharat/CarND-helpers/blob/master/image_processing/Image_processing_tutorial.ipynb ) A Normal RGB\u00a0Image Red, Green and Blue Channels of the same\u00a0image The same image represented as a 3D\u00a0Tensor As an extension, a set of 100 images can be represented as a 4D tensor (ID of image, height, width, channels). Similarly, we represent all input data as tensors\u00b9 before feeding them into the neural net. This is an abstraction necessary before we can feed data into a net, else we would have to define operations that work on each type and that\u2019s a lot of wasted effort\u00b2. We would also need to be able to get back data in a form we want. So, we need a Tensor Object that supports storing the data in form of tensors. Not just that, we would like the object to be able to convert other data types (images, text, video) into tensors and back. Think of something like numpy.imread and numpy.imsave, they read images as ndarrays and store ndarrays as images respectively. The basic Tensor object needs to support representing data in form of a tensor. This means supporting indexing, overloading operators, having a space efficient way to store the data and so on. Depending on further design choices, you might have to add more features as well. Operations on the Tensor Object \u00a0 A neural network can be considered as a series of Operations performed on an input tensor to give an output. Learning is done by correcting the errors between the output created by the net and the expected output\u00b3. These operations could be something simple like matrix multiplication (in sigmoids) or more complicated like convolutions, pooling or LSTMs. Sigmoid layer represented as Matrix Operations, from www.datasciencecentral.com/profiles/blogs/matrix-multiplication-in-neural-networks Check out the following links to get an idea of how extensive these operations could be: NumPy: www.scipy-lectures.org/intro/numpy/operations.html Theano: deeplearning.net/software/theano/library/tensor/basic.html TensorFlow: www.tensorflow.org/api_docs/python/math_ops/ You could just skip this part and ask the user to implement these operations themselves, but it is too cumbersome and outright inefficient. Moreover, most operations are common enough that one could justify making them a part of the framework to spare headaches for the users. NumPy does a pretty good job of having a lot of operations already implemented (it\u2019s insanely fast as well) and there is a running theano issue about incorporating more operations which show how important it is to have more operation supported by the framework. Instead of implementing operations as functions, they are usually implemented as classes. This allows us to store more information about the operation like calculated shape of the output (useful for sanity checks), how to compute the gradient or the gradient itself (for the auto-differentiation), have ways to be able to decide whether to compute the op on GPU or CPU\u2074 and so on. Again, this idea is similar to classes used for various algorithms implemented by scikit-learn. You could define a method called compute that does the actual computation and returns the tensor after the computation is done. These classes are usually derived from an abstract class (in theano, it\u2019s the Op class). This enforces a unified interface across the Ops and also provide a way to add new ops later on. This makes the framework very flexible and ensures people can use it even as new network architectures and nonlinearities come along. Computation Graph and Optimizations \u00a0 So far, we have classes for representing tensors and operations on them. The power of neural networks lies in the ability to chain multiple such operations to form a powerful approximator. Therefore, the standard use case is that you can initialize a tensor, perform actions after actions on them and finally interpret the resulting tensor as labels or real values. Sounds simple enough? Chaining together different operations, taken from colah.github.io/posts/2015-08-Backprop/ Unfortunately, as you chain more and more operations together, several issues arise that can drastically slow down your code and introduce bugs as well. Start later ops only after the previous one is done or do them in parallel? How to assign to different devices and coordinate between them? How do you avoid redundant operations (multiplying with ones, adding zeros), cache useful intermediate values, and reduce multiple operations into one ( replace ,2),2) with one  ) There are more such issues and it becomes necessary to be able to get a bigger picture to even notice that these issues exist. We need a way to optimize the resultant chain of operations for both space and time. In order to get a bigger picture, we introduce a Computation Graph which is basically an object that contains links to the instances of various Ops and the relations between which operation takes the output of which operation as well as additional information. Depending on the framework in question, this can be implemented in different ways. For example, Theano deeplearning.net/software/theano/extending/graphstructures.html TensorFlow download.tensorflow.org/paper/whitepaper2015.pdf Caffe /tutorial/net_layer_blob.html Like many ideas in Deep Learning, the idea of computation graphs has been around for quite some time. Take a look at any Compilers textbook and you can find similar ideas in Abstract Syntax Trees and Intermediate Representations used for Optimization. These ideas have been extended and adapted to the Deep Learning scenario to give us the computational graph. The idea of optimizing the graph before code generation (will be covered later) is straightforward. The optimizations themselves could again be implemented as classes or functions\u2075 and could be selectively applied depending on whether you want the code to compile fast or run fast\u2076. Additionally, since you get a bird\u2019s eye view of what will be happening in the network, the graph class can then decide on how to allocate GPU memory (like Register Allocation in Compilers) and coordinate between various machines when deployed in a distributed environment. This helps us to effectively solve the three problems mentioned above. Auto-differentiation tools \u00a0 Another benefit of having the computational graph is that calculating gradients used in the learning phase becomes modular and straightforward to compute. This is thanks to the chain rule\u2077 that lets you calculate derivatives of composition of functions in a systematic way. As we saw earlier, neural nets can be thought of composition of simple nonlinearities giving rise to more complex functions. Differentiating these functions is simply traversing the graph from the outputs back to the inputs\u2078. Symbolic Differentiation or Automatic Differentiation\u2079 is a programmatic way by which gradients can be computed in a computation graph. Symbolic differentiation refers to calculating the derivatives analytically i.e., you get an expression of what the gradient is. To use it, you simply plug in the values into the derivative and use it. Unfortunately, some nonlinearities like ReLU (Rectified Linear Units) are not differentiable at some points. So, we instead calculate the gradient in an iterative manner. Since the second method could be used universally, most computational graph packages like Computation Graph Toolkit ( rll.berkeley.edu/cgt/ ) implement auto-differentiation but you can use symbolic differentiation if you are creating your own. It is usually not a good idea to roll out your own gradient computation module because it is easier and faster for the toolkit to provide it as part of the package. So, either have your own Computation Graph toolkit and auto-differentiation module or use an external package for both. Since the derivative at each node has to computed with respect to only its adjacent nodes, the method to compute gradients can be added to the class\u00b9\u2070 \u00b9\u00b9 and can be invoked by the differentiation module. BLAS / cuBLAS and cuDNN extensions \u00a0 With all the above components, you can stop right now and have a fully functional Deep Learning framework. It would be able to take data as input and convert to tensors, perform operations on them in an efficient way, compute gradients to learn and return back results for the test dataset. The problem, however, lies in the fact that since you most likely implemented it in a high-level language (Java / Python / Lua), there is an inherent upper limit to the speedups you can get. This is because even the simplest operations in a high-level language take more time (CPU cycles) than when done in a low-level language. In these situations, there are two different approaches we could take. The first one is an another analogy from compilers. The last step of a compilation process is hardware specific code generation in Assembly. Similarly, instead of running the graph written in the high-level language, the corresponding code for the network is generated in C and this is compiled and executed. The code for this is stored in each of the Ops and can be combined together in the compilation phase\u00b9\u00b2. Transferring data to and from the low-level to high-level code is done by wrappers like pyCUDA and Cython. The second approach is to have a backend implemented in a low-level language like C++, this means that the low-level language\u200a\u2014\u200ahigh-level language interaction is internal to the framework unlike the previous approach and could be faster because we don\u2019t need to compile the entire graph every time. Instead, we could just call the compiled methods with the appropriate arguments\u00b9\u00b3. Another source of non-optimal behavior comes from slow implementations at the low-level language. It is difficult to write efficient code\u00b9\u2074 and we will be better off using libraries that have optimized implementations of these methods. BLAS or B asic L inear A lgebra S ubprograms are a collection of optimized matrix operations, initially written in Fortran\u00b9\u2075. These can be leveraged to do very fast matrix (tensor) operations and can provide significant speedups. There are many other software packages like Intel MKL, ATLAS which also perform similar functions. Which one to choose is a personal preference. BLAS packages are usually optimized assuming that the instructions will be run on a CPU. In the deep learning situation, this is not the case and BLAS may not be able to fully exploit the parallelism offered by GPGPUs. To solve this issue, NVIDIA has released cuBLAS\u00b9\u2076 which is optimized for GPUs. This is now included with the CUDA toolkit and is probably why not many people have heard of it. Finally, cuDNN\u00b9\u2077 is a library that builds on the featureset of cuBLAS and provides optimized Neural Network specific operations like Winograd Convolution and RNNs. So, by using these packages, you could gain significant speed-ups in your framework. Speed-ups are important in DL because it is the difference between training a neural network in four hours instead of four days. In the fast moving world of AI startups, that\u2019s the difference between being a pioneer and playing a game of catch-up. So exploit parallelism and optimized libraries wherever you can! Conclusion \u00a0 We have finally come to the end of a pretty long post, thanks a lot for reading it. I hope I have demystified the anatomy of deep learning frameworks for many of you. My main goal in writing this post was to make concrete my understanding of how different frameworks are doing essentially the same thing. This will be a very helpful exercise for anyone who\u2019s above a novice level but below a pro level (a semi-pro, if you will). Once you can understand how things work behind the scenes, they become easier to approach and master. Frameworks do a great job in abstracting away most of these ideas in order to provide a simple interface for the programmers. No wonder that most of these concepts are not very evident when you are learning a framework. As someone who\u2019s interested in not just the applications of Deep Learning but also in the fundamental challenges of the field, I believe that knowing how things work under the hood is an important step towards mastery of the topic as it clears out many of the misunderstandings and provides a simpler way to think about why things are the way they are. I sincerely believe that a good worker knows not just which tool to use but also why that tool is the best choice. This blog is a step in that direction. Hope you enjoyed reading this post as much as I did writing it. Please do let me know your thoughts in the comments below! If you found this interesting, and want to know more about me/hire me as an intern, I\u2019d love to hear from you! You can find my CV here. Footnotes [1] It\u2019s not straightforward how you would represent text as tensors. The first way is to use a one-hot encoding, which is a very sparse matrix and wastes a lot of space. A more dense representation is word vectors. These are pretty cool and I probably might write another post on them if enough people are interested! [2] Also, as you will see in the Auto-differentiation part, it\u2019s not clear how you would calculate the derivatives of words. They\u2019re not even continuous! [3] This is the (in)famous backpropagation algorithm and is central to learning in Multilayered neural networks. [4] This also means moving the data to GPU or back. I have noticed that in Theano (possibly other frameworks as well), this is the most time-consuming part during execution. [5] /software/theano/optimizations.html [6] deeplearning.net/software/theano/library/config.html#config.mode [7] ) / dx = (df / dg) * (dg / dx). cf. en.wikipedia.org/wiki/Chain_rule [8] Check out /posts/2015-08-Backprop/ for a more detailed discussion of derivatives on computational graphs [9] /wiki/Automatic_differentiation [10] /how_tos/adding_an_op/ [11] /software/theano/tutorial/gradients.html [12] deeplearning.net/software/theano/extending/extending_theano_c.html#methods-the-c-op-needs-to-define [13] Both TensorFlow and Caffe do this. [14] If you are interested, you can check out the materials of www.inf.ethz.ch/personal/markusp/teaching/263-2300-ETH-spring11/course.html, will be taking this next sem\u00a0:) [15] /blas/ [16] /cublas [17] /cudnn Bio: Gokula Krishnan Santhanam is a Masters Student in CS at ETH Zurich, a Deep Learning Researcher, and a Pythonista. Original . Reposted with permission. Related: Deep Learning, Artificial Intuition and the Quest for AGI The Gentlest Introduction to Tensorflow \u2013 Part 1 3 practical thoughts on why deep learning performs so well", "title_html": "<h1 id=\"title\"><img align=\"right\" alt=\"Silver Blog, Feb 2017\" src=\"/images/top-kdnuggets-blog-2017-feb-silver.png\" width=\"100\"/>The Anatomy of Deep Learning Frameworks</h1> ", "url": "https://www.kdnuggets.com/2017/02/anatomy-deep-learning-frameworks.html", "tfidf": {"tfidf": {"after": 4.08280828084, "lowlevel": 9525.599999999999, "pro": 12.7008, "toolkit": 756.0, "limit": 1.5186531471200002, "extern": 5.24133377352, "tensorflow": 7938.0, "onc": 1.4974533106999999, "this": 32.12139605472, "real": 2.28103448276, "play": 1.46390041494, "form": 4.51022727272, "red": 2.22228443449, "lstms": 1587.6, "new": 2.0357761108, "assign": 3.83663605607, "space": 7.19456193354, "with": 11.013180298889997, "addit": 2.49269901084, "way": 15.8479612993, "would": 9.745856353619999, "thank": 12.01362088536, "previous": 2.85693719632, "multipli": 20.4061696658, "the": 141.0, "dataset": 193.609756098, "cudnn\u00b9\u2077": 1587.6, "function": 14.97265011, "class": 16.93214238104, "pictur": 6.990752972260001, "python": 112.5957446808, "demystifi": 337.787234043, "hear": 4.17899447223, "well": 7.459024095599999, "onehot": 1587.6, "done": 11.651254953749998, "four": 2.41901569404, "assembl": 3.0011342155, "complet": 1.24021560816, "approach": 8.30226173356, "rllberkeleyeducgt": 1587.6, "toward": 3.2606284658, "know": 10.3730806926, "featureset": 1587.6, "their": 2.0309581681, "dev": 148.373831776, "automat": 6.787516032490001, "modul": 50.8303094985, "calculus": 62.2588235294, "run": 6.22771403352, "origin": 1.13724928367, "instead": 9.56769787062, "creat": 2.4985835694, "misunderstand": 32.4, "express": 1.9120799710900003, "extend": 1.9604840701400004, "how": 19.23003936612, "programm": 5.181462140990001, "instruct": 4.169117647059999, "repost": 933.882352941, "make": 7.533862111020001, "spars": 21.0, "quit": 2.8849718335500003, "multilay": 324.0, "initi": 2.7, "distribut": 2.7396031061299997, "krishnan": 933.882352942, "cubla": 6350.4, "found": 1.11387076405, "for": 23.00724592023, "word": 5.3896118592299995, "interest": 8.016562310649999, "deep": 43.53564899448, "difficult": 2.48957189901, "will": 12.2481098596, "exploit": 11.58832116788, "specif": 5.615847187829999, "consid": 2.4794627518400003, "someon": 4.9350326391, "work": 3.34560269739, "unit": 1.15394679459, "alloc": 21.1117021276, "second": 2.2261796256, "incorpor": 2.62847682119, "phase\u00b9\u00b2": 1587.6, "next": 1.4950560316400001, "below": 6.76822509591, "clear": 3.70847932726, "perform": 7.6569885212500015, "but": 6.09794507394, "threedimension": 1587.6, "need": 10.06083650187, "nonoptim": 1587.6, "all": 6.06880733946, "final": 4.02025829325, "overload": 61.0615384615, "expect": 2.20011086475, "point": 1.25990000794, "quest": 12.6805111821, "caff": 1984.5, "applic": 6.85344269372, "releas": 1.8377126982299998, "numpyimread": 1587.6, "level": 4.96331804919, "various": 3.99697885197, "has": 4.1745990008, "enough": 6.695908899210001, "replac": 1.5602948402899999, "polish": 6.59576235978, "differentiation\u2079": 1587.6, "famous": 2.28201811125, "saniti": 103.764705882, "sound": 3.11294117647, "green": 2.63065451533, "out": 6.36100166946, "enjoy": 3.3269069572500003, "theano": 12700.8, "good": 4.55944859277, "daunt": 102.425806452, "slow": 8.09586945436, "concept": 5.31414225942, "manner": 3.93164933135, "channel": 11.03521779426, "pool": 7.052865393160001, "algebra": 41.4516971279, "spare": 9.997481108310001, "optim": 149.9912790701, "field": 1.7790228597, "sourc": 1.69760479042, "togeth": 4.74287990439, "view": 1.6407606448899998, "wikiautomaticdifferenti": 1587.6, "not": 15.23510971785, "far": 1.71022298826, "henc": 10.781663837019998, "tree": 4.127925117, "autodifferenti": 9525.599999999999, "compon": 8.18983750322, "respect": 3.2886587260400004, "mehir": 1587.6, "store": 17.2340425532, "worker": 3.6843815270399998, "happen": 2.96359902931, "unifi": 7.60709151893, "pleas": 9.12938470385, "easi": 15.8812937646, "width": 51.882352941300006, "test": 2.65707112971, "notic": 8.73988439306, "asic": 330.75, "better": 6.01971688575, "than": 1.03278688525, "problem": 3.53349655018, "complic": 5.6478121664900005, "they": 7.21121277009, "design": 2.9165059245, "travers": 22.8103448276, "deploy": 7.41869158879, "core": 4.623179965059999, "blue": 3.07019918778, "last": 1.2117234010100002, "let": 10.45849802373, "world": 1.11340206186, "task": 3.88641370869, "textbook": 18.290322580599998, "rectifi": 54.3698630137, "eye": 3.39375801625, "blog": 14.1876675603, "away": 1.85142857143, "node": 88.6927374302, "featur": 1.52712581762, "feed": 15.55707986282, "practic": 1.70434782609, "neural": 416.2247191011, "alreadi": 1.9551724137900002, "normal": 2.61075481006, "itself": 1.74557449148, "algorithm": 55.9014084508, "array": 10.1444089457, "offer": 1.53896859248, "coordin": 11.31172069826, "either": 1.5830092731099998, "startup": 68.4310344828, "effort\u00b2": 1587.6, "moreov": 7.56, "compil": 51.26803013997001, "such": 2.12302754748, "signific": 2.9058295964200003, "sever": 1.07241286139, "colahgithubiopostsnumbackprop": 1587.6, "explor": 3.39593582888, "therefor": 2.33401940606, "lie": 6.4314360948000004, "network": 20.74955072704, "deriv": 19.48658600735, "fast": 19.4917127072, "some": 3.1211009174399997, "rule\u2077": 1587.6, "drop": 2.4594887684, "both": 3.15647160183, "aris": 7.54921540656, "result": 3.43834825296, "thought": 5.95647823911, "off": 3.0242880274400004, "configmod": 1587.6, "becom": 3.37476085878, "use": 20.592775147599998, "devic": 5.00820189274, "stop": 2.1783754116400003, "languag": 18.35906331312, "set": 1.18707940781, "ubprogram": 1587.6, "ani": 2.26767604628, "outright": 18.227324913900002, "from": 9.00510493473, "tensors\u00b9": 1587.6, "allow": 1.2716059271100002, "behavior": 5.52978056426, "whether": 6.62051709759, "detail": 2.26186066391, "num": 24.00756096024, "pioneer": 4.74051955808, "instanc": 3.2572835453400004, "redund": 29.7861163227, "relu": 1587.6, "els": 5.44444444444, "idea": 18.837705998700002, "environ": 3.43561999567, "nondl": 1587.6, "invok": 18.227324913900002, "color": 3.8255421686699997, "functions\u2075": 1587.6, "wwwtensorfloworgapidocspythonmathop": 1587.6, "output": 38.384912959400005, "correct": 3.6631287494199998, "height": 12.3069767442, "fulli": 5.58031634446, "roll": 4.27578777269, "unlik": 2.42529789184, "written": 3.9146837627999997, "plethora": 91.2413793103, "much": 1.1942229577299999, "assum": 2.9575260804799997, "fortran\u00b9\u2075": 1587.6, "are": 24.71774245872, "topic": 5.457545548300001, "explan": 6.50922509225, "intel": 54.5567010309, "analog": 9.05131128848, "power": 2.6792675723599997, "look": 3.8172637653199994, "option": 4.04896710023, "video": 3.29719626168, "backend": 933.882352941, "tutorialnetlayerblobhtml": 1587.6, "numpi": 9525.599999999999, "refer": 3.9007371007500002, "intern": 2.60711060022, "exercis": 4.73627684964, "think": 8.721479582490002, "said": 1.54751925139, "combin": 1.69760479042, "enwikipediaorgwikichainrul": 1587.6, "later": 3.25951272927, "net": 27.85263157896, "general": 1.1218202374200001, "column": 7.078020508250001, "methodsthecopneedstodefin": 1587.6, "iter": 37.4433962264, "sigmoid": 2116.8, "then": 1.08657860516, "goldrush": 882.0, "drastic": 14.0620017715, "even": 5.8230633803, "index": 6.9969149405, "upper": 3.41052631579, "could": 15.6568047337, "simpler": 17.9187358916, "bug": 27.372413793099998, "check": 26.0262295082, "train": 1.9365698950999999, "great": 1.26592775696, "master": 6.30250099246, "avoid": 2.45986984816, "goal": 3.28152128979, "anatomi": 25.0410094637, "framework": 172.20867768593996, "downloadtensorfloworgpaperwhitepapernumpdf": 1587.6, "ask": 4.3489932886, "librari": 8.04798918555, "taken": 3.2024205748799996, "anoth": 4.54574087328, "heart": 6.00681044268, "adjac": 6.240566037740001, "pythonista": 1587.6, "probabl": 5.29111814698, "machin": 4.02433460076, "into": 5.0751230739499995, "paper": 2.6628648104700003, "bias": 13.7335640138, "howtosaddinganop": 1587.6, "lua": 387.219512195, "abov": 5.7114761961900005, "inear": 1587.6, "justifi": 7.578042959430001, "symbol": 10.253606028, "cumbersom": 79.38, "effect": 1.3963060686000002, "transfer": 2.72549356223, "earlier": 1.86776470588, "cite": 2.73253012048, "choos": 4.17899447223, "wonder": 7.265903890160001, "trialanderror": 1587.6, "interpret": 3.2150668286799995, "flexibl": 9.68639414277, "help": 5.59851891036, "main": 1.25303867403, "type": 4.056208482380001, "believ": 4.9350326391, "find": 3.4588235294199996, "bird": 6.46416938111, "abil": 2.70875277256, "conclus": 4.84615384615, "doe": 3.4116256581, "pros": 141.75, "own": 7.070665083120001, "approxim": 2.2132998745299997, "relat": 2.47501753838, "label": 4.47715736041, "fact": 1.73375559681, "hour": 2.25960717336, "who": 2.12558575446, "decid": 3.8515283842800003, "univers": 1.24889867841, "permiss": 6.280063291139999, "bigger": 26.46, "output\u00b3": 1587.6, "post": 11.19131538135, "here": 4.84615384616, "bitmap": 407.07692307699995, "veri": 6.29400570885, "softwar": 10.2624434389, "multipl": 10.99255669032, "were": 1.02458857696, "wrapper": 252.0, "give": 4.095975232200001, "long": 1.2657259028899999, "layer": 8.14153846154, "encod": 29.0237659963, "deeplearningnetsoftwaretheanolibrarytensorbasichtml": 1587.6, "contain": 1.59814777532, "parallel": 13.73752523796, "about": 5.324300757950001, "evid": 2.24872521246, "intuit": 27.7068062827, "job": 6.507890961259999, "scene": 3.45055422734, "brave": 15.7970149254, "thing": 14.439290586659997, "catchup": 1587.6, "when": 3.0623030926499997, "just": 6.6790071518500005, "entir": 1.59365589239, "adapt": 3.32272917539, "twodimension": 1587.6, "heard": 4.45204711161, "inputs\u2078": 1587.6, "ensur": 3.4127257093700005, "what": 2.50686878256, "simpli": 5.0384005077800005, "dure": 1.0503473370799998, "code\u00b9\u2074": 1587.6, "them": 9.88885043946, "under": 1.0781663837, "defin": 5.45660766454, "enforc": 4.93810264386, "sincer": 28.2491103203, "across": 3.4637285916800002, "newcom": 27.5147313692, "complex": 2.34021226415, "materi": 2.13014893332, "tool": 19.98866855524, "start": 2.53347163488, "whenev": 11.622254758399999, "among": 1.25670862028, "articl": 4.03610016524, "love": 2.97303370787, "sketch": 10.956521739100001, "guid": 2.49113447356, "softwaretheanooptimizationshtml": 1587.6, "seri": 2.93023255814, "simpl": 16.990582191799998, "semipro": 1587.6, "systemat": 8.338235294119999, "execut": 4.4727426398, "can": 29.4065347855, "whi": 13.026461538480001, "central": 1.6121039805000001, "support": 5.074230922880001, "abl": 12.74595710514, "mani": 5.22133789385, "other": 4.03969465648, "appli": 2.2972073506, "question": 4.4081632653, "step": 8.48379052368, "generat": 6.15826221876, "build": 3.2683479156, "provid": 8.50868999309, "calcul": 36.77837837838, "error": 6.04109589041, "linear": 27.7552447552, "unfortun": 19.932203389839998, "repres": 10.28809479726, "too": 1.81585268215, "order": 2.49250333622, "rnns": 1587.6, "there": 6.24547600314, "exist": 1.4647107666799999, "valu": 6.833285509320001, "inform": 6.301250248080001, "move": 2.58251321676, "end": 1.10680423871, "vector": 25.898858075, "con": 22.744985673400002, "hardwar": 18.8104265403, "gokula": 3175.2, "say": 3.5088960106, "best": 1.5828514456600002, "exampl": 3.00966824644, "continu": 1.13928955867, "now": 2.321561746, "prefer": 3.0216977540900003, "again": 3.01767724768, "follow": 2.09280253098, "data": 37.14079115274, "themselv": 4.11935651272, "further": 1.3618116315, "white": 1.86930413282, "code": 27.164996333439998, "lgebra": 1587.6, "usual": 6.90035857872, "interact": 4.4185917061, "represent": 11.85660941, "basic": 5.460361135, "haven": 12.690647482000001, "possibl": 1.4173734488, "ndarray": 4762.799999999999, "leverag": 35.7567567568, "reduc": 1.98698372966, "issu": 7.19608376395, "also": 8.11812080536, "person": 1.40520446097, "simplest": 28.0494699647, "around": 1.21394708671, "mean": 4.34720700987, "novic": 68.4310344828, "seem": 2.29123971713, "which": 12.06230214, "modular": 49.9245283019, "pycuda": 1587.6, "masteri": 41.889182058, "method": 12.857142857150002, "straightforward": 83.2657342656, "user": 15.4210781933, "research": 1.9420183486200002, "cudnn": 4762.799999999999, "student": 2.47174217655, "scenario": 15.3986420951, "may": 1.05201775893, "doc": 34.8157894737, "take": 9.11693345776, "wwwtensorfloworgtutorialsmandelbrot": 1587.6, "understand": 29.685863874299997, "peopl": 3.6396148555800005, "deeplearningnetsoftwaretheanoextendinggraphstructureshtml": 1587.6, "zero": 8.75192943771, "challeng": 2.55816951337, "highlevel": 9525.599999999999, "intermedi": 22.8760806916, "want": 7.98792452832, "wast": 14.998582900339999, "convert": 6.5481542586199994, "concret": 10.0100882724, "system": 1.38739840951, "cublas\u00b9\u2076": 1587.6, "behind": 2.0845588235299997, "wherev": 21.867768595, "extens": 7.9668799398, "howev": 1.0945191313299998, "essenti": 2.9280708225700005, "regist": 3.9620663838300003, "actual": 1.87482286254, "gpgpus": 1587.6, "plug": 30.297709923699998, "cython": 1587.6, "contributor": 14.4721969006, "saw": 1.94845360825, "sinc": 4.33474402732, "stay": 2.6986231514499996, "ndimens": 1587.6, "insan": 23.6602086438, "three": 1.06621893889, "class\u00b9\u2070": 1587.6, "one": 9.05647461498, "graph": 603.3634204272, "lot": 17.63510136072, "hope": 5.01769911504, "see": 3.81726376533, "becaus": 4.5980739989999995, "imag": 29.715160796310002, "cach": 49.0, "ineffici": 26.154859967100002, "scikitlearn": 1587.6, "get": 12.49938139695, "easier": 23.52, "like": 17.23778501625, "two": 1.01379310345, "implement": 46.49425546298, "text": 6.25655172414, "select": 2.02345144022, "collect": 1.64109985528, "alon": 2.99716820842, "blas": 260.262295082, "part": 7.30314779523, "add": 9.22486926206, "analyt": 17.256521739100002, "memori": 2.57392996109, "import": 4.020597670110001, "includ": 2.0381282495599997, "cool": 6.8578833693300005, "anyon": 5.37440758294, "kera": 835.5789473680001, "pretti": 47.25, "choic": 15.659893470100002, "principl": 6.904109589039999, "should": 3.3286508019800003, "time": 4.04509841392, "process": 1.69524826482, "back": 6.30350194555, "hood": 19.1507840772, "similar": 6.87570376785, "row": 5.549108703250001, "differ": 8.65581431575, "been": 3.0717832957199995, "wwwdatasciencecentralcomprofilesblogsmatrixmultiplicationinneuralnetwork": 1587.6, "decis": 2.16, "numpyimsav": 1587.6, "most": 8.16771704184, "between": 6.207220122479999, "backpropag": 1587.6, "return": 2.79064862014, "everi": 1.47917637194, "programmat": 214.54054054099998, "postsnumbackprop": 1587.6, "shape": 3.20338983051, "java": 31.625498008, "along": 1.2973768080399999, "footnot": 25.320574162699998, "syntax": 45.6206896552, "nonlinear": 297.67499999999995, "action": 3.63711340206, "situat": 4.13222280062, "someth": 9.84456386937, "gradient": 377.00263852200004, "have": 22.327686505079996, "appropri": 4.31413043478, "tri": 3.7089125102199993, "case": 2.96997474512, "day": 1.18371607516, "architectur": 5.12790697674, "arguments\u00b9\u00b3": 1587.6, "fast\u2076": 1587.6, "call": 2.1353059852, "cycl": 5.40919931857, "object": 21.139813581930003, "more": 15.257560225499999, "and": 73.00459842549, "correspond": 3.32481675393, "interfac": 41.8339920948, "discuss": 4.39352428394, "torch": 30.1825095057, "these": 22.55723951292, "benefit": 3.06841901817, "gain": 1.84819557625, "artifici": 8.31639601886, "websit": 2.52160101652, "faster": 15.22877697842, "winograd": 1587.6, "softwaretheanotutorialgradientshtml": 1587.6, "befor": 4.40144164124, "comment": 6.11909809212, "wwwscipylecturesorgintronumpyoperationshtml": 1587.6, "matrix": 158.3076923078, "standard": 1.8915763135900003, "speedup": 3024.0, "introduc": 3.4516795303800003, "zurich": 125.50197628459999, "comput": 90.33844631374001, "fundament": 10.6586102719, "gentlest": 1587.6, "depend": 6.723320158110001, "onli": 2.0512953033200003, "each": 3.56924460432, "introduct": 2.7808723068799996, "size": 2.49387370405, "santhanam": 3175.2, "few": 1.31729173581, "tensor": 3663.692307696, "deeplearningnetsoftwaretheanoextendingextendingtheanochtml": 1587.6, "necessari": 5.684210526319999, "skip": 52.223684210600005, "rise": 2.02940048575, "read": 6.944881889760001, "packag": 54.79881656802, "link": 4.30302208972, "common": 5.6103896103999995, "chain": 25.88196935115, "solv": 14.53846153846, "block": 3.20274359492, "wwwinfethzchpersonalmarkuspteachingnumethspringnumcoursehtml": 1587.6, "oper": 45.08902164333, "composit": 9.259842519680001, "phase": 4.3012733676499995, "convolut": 202.242038216, "bio": 42.336000000000006, "come": 6.64156626505, "abstract": 59.796610169519994, "might": 6.468559011270001, "timeconsum": 1587.6, "that": 28.111553784999998, "githubcomparambharatcarndhelpersblobmasterimageprocessingimageprocessingtutorialipynb": 1587.6, "headach": 54.0, "right": 1.4054532577899999, "gpus": 1058.4, "differenti": 46.55718475073999, "sem": 317.52, "direct": 1.22226499346, "interspers": 47.25, "first": 3.0228484386899996, "game": 2.57978550536, "input": 36.6087624903, "cover": 1.69380134429, "inher": 10.7706919946, "show": 1.26703910615, "mention": 2.53894130817, "deeplearningnetsoftwaretheanolibraryconfightml": 1587.6, "write": 8.230171073080001, "fret": 145.651376147, "tech": 19.1739130435, "down": 1.35889754344, "learn": 41.8095098757, "same": 3.35573874444, "dens": 10.4173228346, "effici": 15.280076997120002}, "logtfidf": {"after": 0.08196277859239999, "lowlevel": 44.21987232546, "pro": 2.5416649836099996, "toolkit": 20.96698806024, "limit": 0.41782385463, "extern": 1.6565760028799998, "tensorflow": 36.849893604550005, "onc": 0.403765872355, "this": 0.12116636968, "real": 0.824629060574, "play": 0.38110439064199997, "form": 0.480212736764, "red": 0.798535691347, "lstms": 7.369978720910001, "new": 0.0354598937022, "assign": 1.3445959556, "space": 2.624139494916, "with": 0.01317240884729, "addit": 0.440437765944, "way": 2.575189629155, "would": 0.7165586516822999, "thank": 3.5857877986, "previous": 0.713205920126, "multipli": 3.01583728972, "the": 0.0, "dataset": 5.26584456664, "cudnn\u00b9\u2077": 7.369978720910001, "function": 5.486794449564, "class": 5.998177519464001, "pictur": 2.50288218248, "python": 8.06131348592, "demystifi": 5.822416212189999, "hear": 1.43007066072, "well": 0.4446010682092, "onehot": 7.369978720910001, "done": 4.229879915645, "four": 0.380427077738, "assembl": 1.09899028905, "complet": 0.215285242047, "approach": 2.9209344583240004, "rllberkeleyeducgt": 7.369978720910001, "toward": 0.9775455543200001, "know": 3.811678777592, "featureset": 7.369978720910001, "their": 0.030721010245400002, "dev": 4.99973497944, "automat": 1.9150850473199998, "modul": 8.48964159498, "calculus": 4.1313002687400004, "run": 1.770859902156, "origin": 0.128612437587, "instead": 2.79979890249, "creat": 0.445153637028, "misunderstand": 3.4781584227999995, "express": 0.648191639641, "extend": 0.673191417311, "how": 5.658803483160001, "programm": 1.6450872830399998, "instruct": 1.42770441799, "repost": 6.83935046985, "make": 0.5144836047603, "spars": 3.04452243772, "quit": 1.05951513684, "multilay": 5.78074351579, "initi": 0.6002091849, "distribut": 1.00781305813, "krishnan": 12.29240657858, "cubla": 29.479914883640003, "found": 0.107841124048, "for": 0.007244779094131001, "word": 1.757583247155, "interest": 2.3603588899099996, "deep": 15.4640816376, "difficult": 0.912110767588, "will": 2.0278653491500003, "exploit": 3.5137012290400005, "specif": 1.8809405026230002, "consid": 0.429789447648, "someon": 1.59635928666, "work": 0.327103701819, "unit": 0.143188061817, "alloc": 4.71336061878, "second": 0.21427952675999998, "incorpor": 0.9664045229739999, "phase\u00b9\u00b2": 7.369978720910001, "next": 0.402163685499, "below": 2.440879775808, "clear": 1.234949454396, "perform": 2.1309042528999997, "but": 0.0971542324314, "threedimension": 7.369978720910001, "need": 2.539181144094, "nonoptim": 7.369978720910001, "all": 0.06841579258679999, "final": 0.878201591844, "overload": 4.1118821828900005, "expect": 0.78850775216, "point": 0.23103235903299998, "quest": 2.54006626224, "caff": 13.79995018332, "applic": 2.46320785698, "releas": 0.608521699544, "numpyimread": 7.369978720910001, "level": 1.510386569829, "various": 0.8607795002099999, "has": 0.1708957794192, "enough": 2.408653317507, "replac": 0.444874803592, "polish": 1.88642737612, "differentiation\u2079": 7.369978720910001, "famous": 0.825060187979, "saniti": 4.64212589251, "sound": 1.13556799519, "green": 0.9672326803710001, "out": 0.3505583455158, "enjoy": 1.2020430306899998, "theano": 58.959829767280006, "good": 1.2557682147209999, "daunt": 4.62913869698, "slow": 2.7964136143, "concept": 1.954448874206, "manner": 1.36905901503, "channel": 3.90743846256, "pool": 1.953433973, "algebra": 3.7245288247199992, "spare": 2.3023331721, "optim": 31.793161340329995, "field": 0.5760642583510001, "sourc": 0.529218310751, "togeth": 1.374096711924, "view": 0.49515994217299997, "wikiautomaticdifferenti": 7.369978720910001, "not": 0.2332861951125, "far": 0.536623764503, "henc": 3.36939943564, "tree": 1.41777488775, "autodifferenti": 44.21987232546, "compon": 2.81949375246, "respect": 0.99466523808, "mehir": 7.369978720910001, "store": 6.187243667600001, "worker": 1.3041026762899999, "happen": 1.08640441802, "unifi": 2.02908090683, "pleas": 2.21149829955, "easi": 4.99958890545, "width": 8.55109926984, "test": 0.977224437103, "notic": 2.94949956336, "asic": 5.801362803, "better": 2.0892838218, "than": 0.0322608622182, "problem": 1.138281448546, "complic": 1.7312682430000002, "they": 0.20808896337320001, "design": 0.754478236044, "travers": 3.1272141535699998, "deploy": 2.00400270589, "core": 1.53108277245, "blue": 1.1217424415100001, "last": 0.19204364461100001, "let": 3.7464077018399995, "world": 0.107420248621, "task": 1.35748680661, "textbook": 2.9063720992400004, "rectifi": 3.9958100116300006, "eye": 1.22193786676, "blog": 2.65237310559, "away": 0.615957541869, "node": 7.584061655, "featur": 0.423387418142, "feed": 4.10273730218, "practic": 0.533182530867, "neural": 28.597206088500002, "alreadi": 0.670478380747, "normal": 0.959639378783, "itself": 0.5570837229510001, "algorithm": 6.66088479036, "array": 2.31692271093, "offer": 0.431112446902, "coordin": 3.46538447508, "either": 0.459327638815, "startup": 4.225826442240001, "effort\u00b2": 7.369978720910001, "moreov": 2.02287119019, "compil": 15.658585077780002, "such": 0.119391955612, "signific": 0.747143488664, "sever": 0.06991112039689999, "colahgithubiopostsnumbackprop": 7.369978720910001, "explor": 1.22257937218, "therefor": 0.847591848336, "lie": 2.33610135128, "network": 7.624664424415999, "deriv": 7.166713279250001, "fast": 6.3347800989600005, "some": 0.11872052719350001, "rule\u2077": 7.369978720910001, "drop": 0.8999535106219999, "both": 0.1525276001679, "aris": 2.0214436382, "result": 0.40913672514300004, "thought": 2.057601354849, "off": 0.8270570407760001, "configmod": 7.369978720910001, "becom": 0.353136529467, "use": 0.584160394632, "devic": 1.6110769470299997, "stop": 0.778579374963, "languag": 6.645454595247999, "set": 0.171496011289, "ubprogram": 7.369978720910001, "ani": 0.251216716732, "outright": 2.9029218370499996, "from": 0.005103487519794, "tensors\u00b9": 7.369978720910001, "allow": 0.24028061118900002, "behavior": 1.71014813378, "whether": 2.3746835689409997, "detail": 0.816187777173, "num": 0.007559769489528001, "pioneer": 1.55614674111, "instanc": 1.18089357972, "redund": 3.3940423897400005, "relu": 7.369978720910001, "els": 1.6945957207700002, "idea": 6.64772329908, "environ": 1.2341974030299998, "nondl": 7.369978720910001, "invok": 2.9029218370499996, "color": 1.3417002006799998, "functions\u2075": 7.369978720910001, "wwwtensorfloworgapidocspythonmathop": 7.369978720910001, "output": 10.191132891350001, "correct": 1.29831763181, "height": 4.23466208364, "fulli": 2.05219657356, "roll": 1.4529683597299998, "unlik": 0.885954358842, "written": 1.343174739666, "plethora": 4.513508514690001, "much": 0.17749572930100002, "assum": 1.08435313525, "fortran\u00b9\u2075": 7.369978720910001, "are": 0.7072193659848001, "topic": 1.6969991554100001, "explan": 1.87322041569, "intel": 3.9992405467300003, "analog": 2.20290964097, "power": 0.58479256543, "look": 1.2927733872, "option": 1.39846181161, "video": 1.19307248967, "backend": 6.83935046985, "tutorialnetlayerblobhtml": 7.369978720910001, "numpi": 44.21987232546, "refer": 0.787659740394, "intern": 0.530190755632, "exercis": 1.5552513523, "think": 3.2015298352499997, "said": 0.436653165815, "combin": 0.529218310751, "enwikipediaorgwikichainrul": 7.369978720910001, "later": 0.24889627796340003, "net": 7.7625323677999996, "general": 0.114952578063, "column": 1.95699427938, "methodsthecopneedstodefin": 7.369978720910001, "iter": 3.62283035867, "sigmoid": 13.929027225599999, "then": 0.08303386523089999, "goldrush": 6.7821920560099995, "drastic": 2.64347624975, "even": 0.76194282417, "index": 1.94546932912, "upper": 1.22686662419, "could": 2.4174315397700004, "simpler": 2.8858468633, "bug": 3.30953571036, "check": 7.49124198248, "train": 0.660918312839, "great": 0.235805258079, "master": 2.29559871398, "avoid": 0.900108441291, "goal": 1.18830712273, "anatomi": 3.22051485947, "framework": 44.18787546747, "downloadtensorfloworgpaperwhitepapernumpdf": 7.369978720910001, "ask": 1.553594419694, "librari": 2.960429942829, "taken": 0.941519545898, "anoth": 0.511585446608, "heart": 2.19949343748, "adjac": 1.83107088944, "pythonista": 7.369978720910001, "probabl": 1.945764825826, "machin": 1.39235958062, "into": 0.0745643161435, "paper": 0.979402539665, "bias": 2.61984276467, "howtosaddinganop": 7.369978720910001, "lua": 5.958991747200001, "abov": 1.9315956894480002, "inear": 7.369978720910001, "justifi": 2.02525498155, "symbol": 3.68705148798, "cumbersom": 4.37424644735, "effect": 0.333830227158, "transfer": 1.00264953547, "earlier": 0.624742371425, "cite": 1.00522796406, "choos": 1.43007066072, "wonder": 1.98319270637, "trialanderror": 7.369978720910001, "interpret": 1.1678481440000001, "flexibl": 2.2707222351599996, "help": 1.344830885376, "main": 0.225571540588, "type": 1.414202970774, "believ": 1.493240993988, "find": 1.095562660576, "bird": 1.86627452464, "abil": 0.996488297427, "conclus": 1.57818536893, "doe": 1.0680834594339998, "pros": 4.95406494261, "own": 0.985170464526, "approxim": 0.7944845577770001, "relat": 0.42620060330799997, "label": 1.49898832727, "fact": 0.5502899207949999, "hour": 0.815190981077, "who": 0.1218004659718, "decid": 1.310645743786, "univers": 0.222262105686, "permiss": 1.8373800586400002, "bigger": 5.16497395626, "output\u00b3": 7.369978720910001, "post": 4.028500763505, "here": 1.7700763767400003, "bitmap": 6.009002167769999, "veri": 1.15079896619, "softwar": 2.32849096333, "multipl": 4.04369607248, "were": 0.024291143681099997, "wrapper": 5.52942908751, "give": 0.9341776566719999, "long": 0.235645793878, "layer": 2.0969791623500003, "encod": 3.36811501148, "deeplearningnetsoftwaretheanolibrarytensorbasichtml": 7.369978720910001, "contain": 0.468845318236, "parallel": 4.56455660466, "about": 0.31421738737300003, "evid": 0.8103634834160001, "intuit": 3.3216780971900004, "job": 2.3597365081799997, "scene": 1.23853486375, "brave": 2.7598209934099995, "thing": 5.26916120808, "catchup": 7.369978720910001, "when": 0.0616649665752, "just": 1.447657170545, "entir": 0.46603068026999994, "adapt": 1.2007864860200002, "twodimension": 7.369978720910001, "heard": 1.4933640154799999, "inputs\u2078": 7.369978720910001, "ensur": 1.22751130026, "what": 0.451774593654, "simpli": 1.847882983172, "dure": 0.0491209066894, "code\u00b9\u2074": 7.369978720910001, "them": 0.8476499421836999, "under": 0.07526180538319999, "defin": 2.0073602185, "enforc": 1.59698117723, "sincer": 3.3410619640099997, "across": 1.098396911882, "newcom": 3.31472154739, "complex": 0.8502416364309999, "materi": 0.7561918990209999, "tool": 6.43548471852, "start": 0.472886738582, "whenev": 2.45292177377, "among": 0.228496097073, "articl": 1.404263479148, "love": 1.08958288195, "sketch": 2.39393487158, "guid": 0.912738218589, "softwaretheanooptimizationshtml": 7.369978720910001, "seri": 0.7638692213959999, "simpl": 6.116106446949999, "semipro": 7.369978720910001, "systemat": 2.12085159855, "execut": 1.609709211728, "can": 4.05852740985, "whi": 4.72275372188, "central": 0.477540146039, "support": 0.951522440148, "abl": 4.1951278773250005, "mani": 0.2165787906105, "other": 0.03949899167904, "appli": 0.8316941898119999, "question": 1.580621858028, "step": 3.11863517094, "generat": 2.1575470252080002, "build": 0.982274904182, "provid": 1.366244910275, "calcul": 10.878903955259998, "error": 1.7985854343, "linear": 5.26055528392, "unfortun": 4.59837900798, "repres": 2.69554062925, "too": 0.5965551547219999, "order": 0.44028076158600005, "rnns": 7.369978720910001, "there": 0.24058735755299998, "exist": 0.38165779408699996, "valu": 2.469579930444, "inform": 1.817814818648, "move": 0.511231718506, "end": 0.101476798618, "vector": 3.25419887797, "con": 3.12434471114, "hardwar": 2.93441131931, "gokula": 14.739957441820001, "say": 1.124308561104, "best": 0.459227932947, "exampl": 0.8173653499979999, "continu": 0.13040487398700001, "now": 0.298185890042, "prefer": 1.10581884366, "again": 0.822680463224, "follow": 0.09071382218839999, "data": 13.3850264328, "themselv": 1.4450995687380002, "further": 0.308815895297, "white": 0.625566240123, "code": 9.49213367179, "lgebra": 7.369978720910001, "usual": 2.181116068256, "interact": 1.4858210267899998, "represent": 3.5594765752999997, "basic": 2.0087354979, "haven": 2.54086530344, "possibl": 0.348805474891, "ndarray": 22.10993616273, "leverag": 3.5767392514699994, "reduc": 0.686617775143, "issu": 1.82049521967, "also": 0.1172572624, "person": 0.34018281601800004, "simplest": 3.3339697356999998, "around": 0.19387710578200001, "mean": 1.11276385056, "novic": 4.225826442240001, "seem": 0.829093032276, "which": 0.06214096614516, "modular": 3.91051243112, "pycuda": 7.369978720910001, "masteri": 3.73502760882, "method": 4.7223080442050005, "straightforward": 9.970274467560001, "user": 4.08517621376, "research": 0.663727818138, "cudnn": 22.10993616273, "student": 0.904923236645, "scenario": 2.73427932989, "may": 0.050709995284400004, "doc": 3.55007100439, "take": 1.045535697576, "wwwtensorfloworgtutorialsmandelbrot": 7.369978720910001, "understand": 10.880858756799999, "peopl": 0.579796735419, "deeplearningnetsoftwaretheanoextendinggraphstructureshtml": 7.369978720910001, "zero": 2.1692741832299998, "challeng": 0.9392919688950001, "highlevel": 44.21987232546, "intermedi": 4.87388934568, "want": 2.7665464250199996, "wast": 4.02961708554, "convert": 2.3720720736, "concret": 2.3035934117099996, "system": 0.327430345585, "cublas\u00b9\u2076": 7.369978720910001, "behind": 0.7345572374320001, "wherev": 3.08501379908, "extens": 2.7559943179000004, "howev": 0.0903151173475, "essenti": 1.07434378384, "regist": 1.3767657032700003, "actual": 0.628514181648, "gpgpus": 7.369978720910001, "plug": 3.41107212958, "cython": 7.369978720910001, "contributor": 2.67222935363, "saw": 0.667036036556, "sinc": 0.3214727978308, "stay": 0.9927416990379999, "ndimens": 7.369978720910001, "insan": 3.1637946769300003, "three": 0.06411868822490001, "class\u00b9\u2070": 7.369978720910001, "one": 0.0562981648095, "graph": 58.078895683519995, "lot": 5.934387801000001, "hope": 1.83964860891, "see": 0.722764756476, "becaus": 0.5573726353, "imag": 10.93138318019, "cach": 3.89182029811, "ineffici": 3.2640350228400004, "scikitlearn": 7.369978720910001, "get": 4.058383040474, "easier": 6.177716503079999, "like": 2.085803648175, "two": 0.0136988443582, "implement": 16.566932317910002, "text": 2.28096401998, "select": 0.704804687133, "collect": 0.49536666052, "alon": 1.09766791236, "blas": 5.561689949730001, "part": 0.2967671768796, "add": 3.05751167426, "analyt": 2.8481901438599997, "memori": 0.9454338986599999, "import": 0.878454831198, "includ": 0.037769362781, "cool": 1.9253988473800001, "anyon": 1.68164835081, "kera": 6.72812483474, "pretti": 8.27052109581, "choic": 5.70832487715, "principl": 2.4779392927200004, "should": 1.018839753516, "time": 0.0448460754504, "process": 0.527829199025, "back": 1.1583371544849999, "hood": 2.9523436587700003, "similar": 1.59278046057, "row": 1.71363732085, "differ": 1.486247849184, "been": 0.07093794710520002, "wwwdatasciencecentralcomprofilesblogsmatrixmultiplicationinneuralnetwork": 7.369978720910001, "decis": 0.7701082216959999, "numpyimsav": 7.369978720910001, "most": 0.16598317036479998, "between": 0.20372208699179997, "backpropag": 7.369978720910001, "return": 0.666253737184, "everi": 0.391485427421, "programmat": 5.3684987207, "postsnumbackprop": 7.369978720910001, "shape": 1.16420957115, "java": 3.45396369421, "along": 0.260344385917, "footnot": 3.2316172732700004, "syntax": 3.8203613341300007, "nonlinear": 13.79216999601, "action": 1.196086330138, "situat": 1.45133658003, "someth": 3.56492136819, "gradient": 33.61524847938, "have": 0.32527005150640004, "appropri": 1.4618957827399999, "tri": 1.23518305832, "case": 0.790812537778, "day": 0.16865870631700003, "architectur": 1.63469757919, "arguments\u00b9\u00b3": 7.369978720910001, "fast\u2076": 7.369978720910001, "call": 0.1309255488976, "cycl": 1.68810108164, "object": 7.685402263227, "more": 0.25537397399999995, "and": 0.0045982803706428, "correspond": 1.20141456099, "interfac": 6.081124073019999, "discuss": 1.57396904524, "torch": 3.40726260117, "these": 1.5022060074168, "benefit": 1.12116245116, "gain": 0.6142097989249999, "artifici": 2.11822899018, "websit": 0.924894023806, "faster": 4.06007935934, "winograd": 7.369978720910001, "softwaretheanotutorialgradientshtml": 7.369978720910001, "befor": 0.382551087518, "comment": 2.23653506908, "wwwscipylecturesorgintronumpyoperationshtml": 7.369978720910001, "matrix": 21.83041286916, "standard": 0.63741050982, "speedup": 26.512165504720002, "introduc": 1.0914275048520001, "zurich": 8.27834865036, "comput": 31.465585066619997, "fundament": 3.34644172238, "gentlest": 7.369978720910001, "depend": 2.420909445, "onli": 0.050648536658199995, "each": 0.521225067912, "introduct": 1.02276465794, "size": 0.9138372060609999, "santhanam": 14.739957441820001, "few": 0.275577913653, "tensor": 120.67614995424, "deeplearningnetsoftwaretheanoextendingextendingtheanochtml": 7.369978720910001, "necessari": 2.0890901347999997, "skip": 6.52477786388, "rise": 0.707740422218, "read": 2.51817804264, "packag": 14.404309144330004, "link": 1.5323408136899999, "common": 1.353303221084, "chain": 8.22054324895, "solv": 3.9673009540800006, "block": 1.16400781588, "wwwinfethzchpersonalmarkuspteachingnumethspringnumcoursehtml": 7.369978720910001, "oper": 12.798945966063, "composit": 3.0650797228799997, "phase": 1.4589111108700001, "convolut": 9.2326360171, "bio": 3.7456377879300002, "come": 1.41954953265, "abstract": 13.79513702394, "might": 2.3050232296020003, "timeconsum": 7.369978720910001, "that": 0.11133215462992, "githubcomparambharatcarndhelpersblobmasterimageprocessingimageprocessingtutorialipynb": 7.369978720910001, "headach": 3.9889840465599997, "right": 0.34035985417, "gpus": 6.964513612799999, "differenti": 12.293531204340002, "sem": 5.76054080847, "direct": 0.200705689496, "interspers": 3.85545265394, "first": 0.02276186943648, "game": 0.9477062580210001, "input": 7.50502600617, "cover": 0.526975319156, "inher": 2.37682874115, "show": 0.236682766013, "mention": 0.931747186336, "deeplearningnetsoftwaretheanolibraryconfightml": 7.369978720910001, "write": 2.886049759508, "fret": 4.9812159316699995, "tech": 2.9535506595200003, "down": 0.306673741186, "learn": 15.169537165409999, "same": 0.336178948812, "dens": 2.3434700776599997, "effici": 4.88381260242}, "logidf": {"after": 0.020490694648099998, "lowlevel": 7.369978720910001, "pro": 2.5416649836099996, "toolkit": 5.24174701506, "limit": 0.41782385463, "extern": 1.6565760028799998, "tensorflow": 7.369978720910001, "onc": 0.403765872355, "this": 0.0037864490525, "real": 0.824629060574, "play": 0.38110439064199997, "form": 0.120053184191, "red": 0.798535691347, "lstms": 7.369978720910001, "new": 0.0177299468511, "assign": 1.3445959556, "space": 0.874713164972, "with": 0.00119749171339, "addit": 0.220218882972, "way": 0.19809150993500002, "would": 0.0796176279647, "thank": 1.7928938993, "previous": 0.356602960063, "multipli": 3.01583728972, "the": 0.0, "dataset": 5.26584456664, "cudnn\u00b9\u2077": 7.369978720910001, "function": 0.914465741594, "class": 0.7497721899330001, "pictur": 1.25144109124, "python": 4.03065674296, "demystifi": 5.822416212189999, "hear": 1.43007066072, "well": 0.0635144383156, "onehot": 7.369978720910001, "done": 0.845975983129, "four": 0.190213538869, "assembl": 1.09899028905, "complet": 0.215285242047, "approach": 0.7302336145810001, "rllberkeleyeducgt": 7.369978720910001, "toward": 0.48877277716000006, "know": 0.952919694398, "featureset": 7.369978720910001, "their": 0.015360505122700001, "dev": 4.99973497944, "automat": 1.9150850473199998, "modul": 2.82988053166, "calculus": 4.1313002687400004, "run": 0.442714975539, "origin": 0.128612437587, "instead": 0.46663315041500003, "creat": 0.222576818514, "misunderstand": 3.4781584227999995, "express": 0.648191639641, "extend": 0.673191417311, "how": 0.47156695693000006, "programm": 1.6450872830399998, "instruct": 1.42770441799, "repost": 6.83935046985, "make": 0.07349765782289999, "spars": 3.04452243772, "quit": 1.05951513684, "multilay": 5.78074351579, "initi": 0.30010459245, "distribut": 1.00781305813, "krishnan": 6.14620328929, "cubla": 7.369978720910001, "found": 0.107841124048, "for": 0.00031499039539700004, "word": 0.585861082385, "interest": 0.47207177798199995, "deep": 1.2886734698, "difficult": 0.912110767588, "will": 0.202786534915, "exploit": 1.7568506145200002, "specif": 0.626980167541, "consid": 0.214894723824, "someon": 1.59635928666, "work": 0.109034567273, "unit": 0.143188061817, "alloc": 2.35668030939, "second": 0.10713976337999999, "incorpor": 0.9664045229739999, "phase\u00b9\u00b2": 7.369978720910001, "next": 0.402163685499, "below": 0.813626591936, "clear": 0.617474727198, "perform": 0.42618085058, "but": 0.0161923720719, "threedimension": 7.369978720910001, "need": 0.362740163442, "nonoptim": 7.369978720910001, "all": 0.011402632097799998, "final": 0.292733863948, "overload": 4.1118821828900005, "expect": 0.78850775216, "point": 0.23103235903299998, "quest": 2.54006626224, "caff": 6.89997509166, "applic": 1.23160392849, "releas": 0.608521699544, "numpyimread": 7.369978720910001, "level": 0.503462189943, "various": 0.28692650007, "has": 0.0427239448548, "enough": 0.802884439169, "replac": 0.444874803592, "polish": 1.88642737612, "differentiation\u2079": 7.369978720910001, "famous": 0.825060187979, "saniti": 4.64212589251, "sound": 1.13556799519, "green": 0.9672326803710001, "out": 0.0584263909193, "enjoy": 1.2020430306899998, "theano": 7.369978720910001, "good": 0.418589404907, "daunt": 4.62913869698, "slow": 1.39820680715, "concept": 0.977224437103, "manner": 1.36905901503, "channel": 1.30247948752, "pool": 1.953433973, "algebra": 3.7245288247199992, "spare": 2.3023331721, "optim": 2.4456277954099996, "field": 0.5760642583510001, "sourc": 0.529218310751, "togeth": 0.458032237308, "view": 0.49515994217299997, "wikiautomaticdifferenti": 7.369978720910001, "not": 0.0155524130075, "far": 0.536623764503, "henc": 1.68469971782, "tree": 1.41777488775, "autodifferenti": 7.369978720910001, "compon": 1.40974687623, "respect": 0.49733261904, "mehir": 7.369978720910001, "store": 1.2374487335200002, "worker": 1.3041026762899999, "happen": 1.08640441802, "unifi": 2.02908090683, "pleas": 2.21149829955, "easi": 1.6665296351499999, "width": 2.85036642328, "test": 0.977224437103, "notic": 1.47474978168, "asic": 5.801362803, "better": 0.6964279406, "than": 0.0322608622182, "problem": 0.569140724273, "complic": 1.7312682430000002, "they": 0.0297269947676, "design": 0.377239118022, "travers": 3.1272141535699998, "deploy": 2.00400270589, "core": 1.53108277245, "blue": 1.1217424415100001, "last": 0.19204364461100001, "let": 1.2488025672799998, "world": 0.107420248621, "task": 1.35748680661, "textbook": 2.9063720992400004, "rectifi": 3.9958100116300006, "eye": 1.22193786676, "blog": 2.65237310559, "away": 0.615957541869, "node": 3.7920308275, "featur": 0.423387418142, "feed": 2.05136865109, "practic": 0.533182530867, "neural": 4.0853151555, "alreadi": 0.670478380747, "normal": 0.959639378783, "itself": 0.5570837229510001, "algorithm": 3.33044239518, "array": 2.31692271093, "offer": 0.431112446902, "coordin": 1.73269223754, "either": 0.459327638815, "startup": 4.225826442240001, "effort\u00b2": 7.369978720910001, "moreov": 2.02287119019, "compil": 1.7398427864200001, "such": 0.059695977806, "signific": 0.373571744332, "sever": 0.06991112039689999, "colahgithubiopostsnumbackprop": 7.369978720910001, "explor": 1.22257937218, "therefor": 0.847591848336, "lie": 1.16805067564, "network": 0.9530830530519999, "deriv": 1.02381618275, "fast": 1.5836950247400001, "some": 0.0395735090645, "rule\u2077": 7.369978720910001, "drop": 0.8999535106219999, "both": 0.050842533389300004, "aris": 2.0214436382, "result": 0.136378908381, "thought": 0.685867118283, "off": 0.41352852038800003, "configmod": 7.369978720910001, "becom": 0.11771217648900001, "use": 0.0292080197316, "devic": 1.6110769470299997, "stop": 0.778579374963, "languag": 0.8306818244059999, "set": 0.171496011289, "ubprogram": 7.369978720910001, "ani": 0.125608358366, "outright": 2.9029218370499996, "from": 0.000567054168866, "tensors\u00b9": 7.369978720910001, "allow": 0.24028061118900002, "behavior": 1.71014813378, "whether": 0.791561189647, "detail": 0.816187777173, "num": 0.00031499039539700004, "pioneer": 1.55614674111, "instanc": 1.18089357972, "redund": 3.3940423897400005, "relu": 7.369978720910001, "els": 1.6945957207700002, "idea": 0.73863592212, "environ": 1.2341974030299998, "nondl": 7.369978720910001, "invok": 2.9029218370499996, "color": 1.3417002006799998, "functions\u2075": 7.369978720910001, "wwwtensorfloworgapidocspythonmathop": 7.369978720910001, "output": 2.03822657827, "correct": 1.29831763181, "height": 1.4115540278799998, "fulli": 1.02609828678, "roll": 1.4529683597299998, "unlik": 0.885954358842, "written": 0.671587369833, "plethora": 4.513508514690001, "much": 0.17749572930100002, "assum": 1.08435313525, "fortran\u00b9\u2075": 7.369978720910001, "are": 0.0294674735827, "topic": 1.6969991554100001, "explan": 1.87322041569, "intel": 3.9992405467300003, "analog": 2.20290964097, "power": 0.292396282715, "look": 0.6463866936, "option": 1.39846181161, "video": 1.19307248967, "backend": 6.83935046985, "tutorialnetlayerblobhtml": 7.369978720910001, "numpi": 7.369978720910001, "refer": 0.262553246798, "intern": 0.265095377816, "exercis": 1.5552513523, "think": 1.06717661175, "said": 0.436653165815, "combin": 0.529218310751, "enwikipediaorgwikichainrul": 7.369978720910001, "later": 0.0829654259878, "net": 1.9406330919499999, "general": 0.114952578063, "column": 1.95699427938, "methodsthecopneedstodefin": 7.369978720910001, "iter": 3.62283035867, "sigmoid": 6.964513612799999, "then": 0.08303386523089999, "goldrush": 6.7821920560099995, "drastic": 2.64347624975, "even": 0.152388564834, "index": 1.94546932912, "upper": 1.22686662419, "could": 0.18595627229000003, "simpler": 2.8858468633, "bug": 3.30953571036, "check": 1.87281049562, "train": 0.660918312839, "great": 0.235805258079, "master": 1.14779935699, "avoid": 0.900108441291, "goal": 1.18830712273, "anatomi": 3.22051485947, "framework": 2.10418454607, "downloadtensorfloworgpaperwhitepapernumpdf": 7.369978720910001, "ask": 0.776797209847, "librari": 0.986809980943, "taken": 0.470759772949, "anoth": 0.127896361652, "heart": 1.09974671874, "adjac": 1.83107088944, "pythonista": 7.369978720910001, "probabl": 0.972882412913, "machin": 1.39235958062, "into": 0.0149128632287, "paper": 0.979402539665, "bias": 2.61984276467, "howtosaddinganop": 7.369978720910001, "lua": 5.958991747200001, "abov": 0.643865229816, "inear": 7.369978720910001, "justifi": 2.02525498155, "symbol": 1.22901716266, "cumbersom": 4.37424644735, "effect": 0.333830227158, "transfer": 1.00264953547, "earlier": 0.624742371425, "cite": 1.00522796406, "choos": 1.43007066072, "wonder": 1.98319270637, "trialanderror": 7.369978720910001, "interpret": 1.1678481440000001, "flexibl": 2.2707222351599996, "help": 0.336207721344, "main": 0.225571540588, "type": 0.707101485387, "believ": 0.497746997996, "find": 0.547781330288, "bird": 1.86627452464, "abil": 0.996488297427, "conclus": 1.57818536893, "doe": 0.5340417297169999, "pros": 4.95406494261, "own": 0.164195077421, "approxim": 0.7944845577770001, "relat": 0.21310030165399999, "label": 1.49898832727, "fact": 0.5502899207949999, "hour": 0.815190981077, "who": 0.0609002329859, "decid": 0.655322871893, "univers": 0.222262105686, "permiss": 1.8373800586400002, "bigger": 2.58248697813, "output\u00b3": 7.369978720910001, "post": 0.8057001527009999, "here": 0.8850381883700001, "bitmap": 6.009002167769999, "veri": 0.230159793238, "softwar": 2.32849096333, "multipl": 1.01092401812, "were": 0.024291143681099997, "wrapper": 5.52942908751, "give": 0.311392552224, "long": 0.235645793878, "layer": 2.0969791623500003, "encod": 3.36811501148, "deeplearningnetsoftwaretheanolibrarytensorbasichtml": 7.369978720910001, "contain": 0.468845318236, "parallel": 1.52151886822, "about": 0.0628434774746, "evid": 0.8103634834160001, "intuit": 3.3216780971900004, "job": 1.1798682540899998, "scene": 1.23853486375, "brave": 2.7598209934099995, "thing": 0.8781935346799999, "catchup": 7.369978720910001, "when": 0.0205549888584, "just": 0.289531434109, "entir": 0.46603068026999994, "adapt": 1.2007864860200002, "twodimension": 7.369978720910001, "heard": 1.4933640154799999, "inputs\u2078": 7.369978720910001, "ensur": 1.22751130026, "what": 0.225887296827, "simpli": 0.923941491586, "dure": 0.0491209066894, "code\u00b9\u2074": 7.369978720910001, "them": 0.0941833269093, "under": 0.07526180538319999, "defin": 1.00368010925, "enforc": 1.59698117723, "sincer": 3.3410619640099997, "across": 0.549198455941, "newcom": 3.31472154739, "complex": 0.8502416364309999, "materi": 0.7561918990209999, "tool": 1.60887117963, "start": 0.236443369291, "whenev": 2.45292177377, "among": 0.228496097073, "articl": 0.702131739574, "love": 1.08958288195, "sketch": 2.39393487158, "guid": 0.912738218589, "softwaretheanooptimizationshtml": 7.369978720910001, "seri": 0.38193461069799994, "simpl": 1.2232212893899999, "semipro": 7.369978720910001, "systemat": 2.12085159855, "execut": 0.804854605864, "can": 0.162341096394, "whi": 1.18068843047, "central": 0.477540146039, "support": 0.237880610037, "abl": 0.599303982475, "mani": 0.0433157581221, "other": 0.00987474791976, "appli": 0.8316941898119999, "question": 0.790310929014, "step": 1.03954505698, "generat": 0.719182341736, "build": 0.491137452091, "provid": 0.19517784432500002, "calcul": 1.8131506592099997, "error": 1.7985854343, "linear": 2.63027764196, "unfortun": 2.29918950399, "repres": 0.38507723275, "too": 0.5965551547219999, "order": 0.22014038079300002, "rnns": 7.369978720910001, "there": 0.0400978929255, "exist": 0.38165779408699996, "valu": 0.823193310148, "inform": 0.454453704662, "move": 0.255615859253, "end": 0.101476798618, "vector": 3.25419887797, "con": 3.12434471114, "hardwar": 2.93441131931, "gokula": 7.369978720910001, "say": 0.562154280552, "best": 0.459227932947, "exampl": 0.40868267499899996, "continu": 0.13040487398700001, "now": 0.149092945021, "prefer": 1.10581884366, "again": 0.411340231612, "follow": 0.045356911094199995, "data": 1.2168205848, "themselv": 0.7225497843690001, "further": 0.308815895297, "white": 0.625566240123, "code": 1.35601909597, "lgebra": 7.369978720910001, "usual": 0.545279017064, "interact": 1.4858210267899998, "represent": 1.7797382876499999, "basic": 1.00436774895, "haven": 2.54086530344, "possibl": 0.348805474891, "ndarray": 7.369978720910001, "leverag": 3.5767392514699994, "reduc": 0.686617775143, "issu": 0.364099043934, "also": 0.0146571578, "person": 0.34018281601800004, "simplest": 3.3339697356999998, "around": 0.19387710578200001, "mean": 0.37092128352, "novic": 4.225826442240001, "seem": 0.829093032276, "which": 0.00517841384543, "modular": 3.91051243112, "pycuda": 7.369978720910001, "masteri": 3.73502760882, "method": 0.944461608841, "straightforward": 3.3234248225200003, "user": 2.04258810688, "research": 0.663727818138, "cudnn": 7.369978720910001, "student": 0.904923236645, "scenario": 2.73427932989, "may": 0.050709995284400004, "doc": 3.55007100439, "take": 0.130691962197, "wwwtensorfloworgtutorialsmandelbrot": 7.369978720910001, "understand": 1.0880858756799998, "peopl": 0.193265578473, "deeplearningnetsoftwaretheanoextendinggraphstructureshtml": 7.369978720910001, "zero": 2.1692741832299998, "challeng": 0.9392919688950001, "highlevel": 7.369978720910001, "intermedi": 2.43694467284, "want": 0.6916366062549999, "wast": 2.01480854277, "convert": 1.1860360368, "concret": 2.3035934117099996, "system": 0.327430345585, "cublas\u00b9\u2076": 7.369978720910001, "behind": 0.7345572374320001, "wherev": 3.08501379908, "extens": 0.6889985794750001, "howev": 0.0903151173475, "essenti": 1.07434378384, "regist": 1.3767657032700003, "actual": 0.628514181648, "gpgpus": 7.369978720910001, "plug": 3.41107212958, "cython": 7.369978720910001, "contributor": 2.67222935363, "saw": 0.667036036556, "sinc": 0.0803681994577, "stay": 0.9927416990379999, "ndimens": 7.369978720910001, "insan": 3.1637946769300003, "three": 0.06411868822490001, "class\u00b9\u2070": 7.369978720910001, "one": 0.0062553516455, "graph": 3.6299309802199997, "lot": 1.4835969502500002, "hope": 0.919824304455, "see": 0.240921585492, "becaus": 0.139343158825, "imag": 0.99376210729, "cach": 3.89182029811, "ineffici": 3.2640350228400004, "scikitlearn": 7.369978720910001, "get": 0.579769005782, "easier": 2.05923883436, "like": 0.139053576545, "two": 0.0136988443582, "implement": 1.27437940907, "text": 1.14048200999, "select": 0.704804687133, "collect": 0.49536666052, "alon": 1.09766791236, "blas": 5.561689949730001, "part": 0.04239531098280001, "add": 1.52875583713, "analyt": 2.8481901438599997, "memori": 0.9454338986599999, "import": 0.292818277066, "includ": 0.0188846813905, "cool": 1.9253988473800001, "anyon": 1.68164835081, "kera": 6.72812483474, "pretti": 2.75684036527, "choic": 1.14166497543, "principl": 1.2389696463600002, "should": 0.509419876758, "time": 0.0112115188626, "process": 0.527829199025, "back": 0.23166743089699998, "hood": 2.9523436587700003, "similar": 0.318556092114, "row": 1.71363732085, "differ": 0.212321121312, "been": 0.023645982368400004, "wwwdatasciencecentralcomprofilesblogsmatrixmultiplicationinneuralnetwork": 7.369978720910001, "decis": 0.7701082216959999, "numpyimsav": 7.369978720910001, "most": 0.020747896295599998, "between": 0.033953681165299995, "backpropag": 7.369978720910001, "return": 0.333126868592, "everi": 0.391485427421, "programmat": 5.3684987207, "postsnumbackprop": 7.369978720910001, "shape": 1.16420957115, "java": 3.45396369421, "along": 0.260344385917, "footnot": 3.2316172732700004, "syntax": 3.8203613341300007, "nonlinear": 4.59738999867, "action": 0.598043165069, "situat": 0.725668290015, "someth": 1.18830712273, "gradient": 3.73502760882, "have": 0.0147850023412, "appropri": 1.4618957827399999, "tri": 0.61759152916, "case": 0.395406268889, "day": 0.16865870631700003, "architectur": 1.63469757919, "arguments\u00b9\u00b3": 7.369978720910001, "fast\u2076": 7.369978720910001, "call": 0.0654627744488, "cycl": 1.68810108164, "object": 0.853933584803, "more": 0.017024931599999998, "and": 6.29901420636e-05, "correspond": 1.20141456099, "interfac": 3.0405620365099995, "discuss": 0.78698452262, "torch": 3.40726260117, "these": 0.0715336194008, "benefit": 1.12116245116, "gain": 0.6142097989249999, "artifici": 2.11822899018, "websit": 0.924894023806, "faster": 2.03003967967, "winograd": 7.369978720910001, "softwaretheanotutorialgradientshtml": 7.369978720910001, "befor": 0.0956377718795, "comment": 1.11826753454, "wwwscipylecturesorgintronumpyoperationshtml": 7.369978720910001, "matrix": 3.1186304098799997, "standard": 0.63741050982, "speedup": 6.6280413761800006, "introduc": 0.5457137524260001, "zurich": 4.13917432518, "comput": 1.36806891594, "fundament": 1.67322086119, "gentlest": 7.369978720910001, "depend": 0.806969815, "onli": 0.025324268329099998, "each": 0.173741689304, "introduct": 1.02276465794, "size": 0.9138372060609999, "santhanam": 7.369978720910001, "few": 0.275577913653, "tensor": 5.02817291476, "deeplearningnetsoftwaretheanoextendingextendingtheanochtml": 7.369978720910001, "necessari": 1.0445450673999999, "skip": 3.26238893194, "rise": 0.707740422218, "read": 0.83939268088, "packag": 2.0577584491900005, "link": 0.7661704068449999, "common": 0.338325805271, "chain": 1.64410864979, "solv": 1.9836504770400003, "block": 1.16400781588, "wwwinfethzchpersonalmarkuspteachingnumethspringnumcoursehtml": 7.369978720910001, "oper": 0.441342964347, "composit": 1.5325398614399999, "phase": 1.4589111108700001, "convolut": 4.61631800855, "bio": 3.7456377879300002, "come": 0.28390990653000003, "abstract": 2.29918950399, "might": 0.7683410765340001, "timeconsum": 7.369978720910001, "that": 0.00397614837964, "githubcomparambharatcarndhelpersblobmasterimageprocessingimageprocessingtutorialipynb": 7.369978720910001, "headach": 3.9889840465599997, "right": 0.34035985417, "gpus": 6.964513612799999, "differenti": 2.0489218673900003, "sem": 5.76054080847, "direct": 0.200705689496, "interspers": 3.85545265394, "first": 0.0075872898121599995, "game": 0.9477062580210001, "input": 2.50167533539, "cover": 0.526975319156, "inher": 2.37682874115, "show": 0.236682766013, "mention": 0.931747186336, "deeplearningnetsoftwaretheanolibraryconfightml": 7.369978720910001, "write": 0.721512439877, "fret": 4.9812159316699995, "tech": 2.9535506595200003, "down": 0.306673741186, "learn": 0.842752064745, "same": 0.112059649604, "dens": 2.3434700776599997, "effici": 1.62793753414}, "freq": {"after": 4, "lowlevel": 6, "pro": 1, "toolkit": 4, "limit": 1, "extern": 1, "tensorflow": 5, "onc": 1, "this": 32, "real": 1, "play": 1, "form": 4, "red": 1, "lstms": 1, "new": 2, "assign": 1, "space": 3, "with": 11, "addit": 2, "way": 13, "would": 9, "thank": 2, "previous": 2, "multipli": 1, "the": 141, "dataset": 1, "cudnn\u00b9\u2077": 1, "function": 6, "class": 8, "pictur": 2, "python": 2, "demystifi": 1, "hear": 1, "well": 7, "onehot": 1, "done": 5, "four": 2, "assembl": 1, "complet": 1, "approach": 4, "rllberkeleyeducgt": 1, "toward": 2, "know": 4, "featureset": 1, "their": 2, "dev": 1, "automat": 1, "modul": 3, "calculus": 1, "run": 4, "origin": 1, "instead": 6, "creat": 2, "misunderstand": 1, "express": 1, "extend": 1, "how": 12, "programm": 1, "instruct": 1, "repost": 1, "make": 7, "spars": 1, "quit": 1, "multilay": 1, "initi": 2, "distribut": 1, "krishnan": 2, "cubla": 4, "found": 1, "for": 23, "word": 3, "interest": 5, "deep": 12, "difficult": 1, "will": 10, "exploit": 2, "specif": 3, "consid": 2, "someon": 1, "work": 3, "unit": 1, "alloc": 2, "second": 2, "incorpor": 1, "phase\u00b9\u00b2": 1, "next": 1, "below": 3, "clear": 2, "perform": 5, "but": 6, "threedimension": 1, "need": 7, "nonoptim": 1, "all": 6, "final": 3, "overload": 1, "expect": 1, "point": 1, "quest": 1, "caff": 2, "applic": 2, "releas": 1, "numpyimread": 1, "level": 3, "various": 3, "has": 4, "enough": 3, "replac": 1, "polish": 1, "differentiation\u2079": 1, "famous": 1, "saniti": 1, "sound": 1, "green": 1, "out": 6, "enjoy": 1, "theano": 8, "good": 3, "daunt": 1, "slow": 2, "concept": 2, "manner": 1, "channel": 3, "pool": 1, "algebra": 1, "spare": 1, "optim": 13, "field": 1, "sourc": 1, "togeth": 3, "view": 1, "wikiautomaticdifferenti": 1, "not": 15, "far": 1, "henc": 2, "tree": 1, "autodifferenti": 6, "compon": 2, "respect": 2, "mehir": 1, "store": 5, "worker": 1, "happen": 1, "unifi": 1, "pleas": 1, "easi": 3, "width": 3, "test": 1, "notic": 2, "asic": 1, "better": 3, "than": 1, "problem": 2, "complic": 1, "they": 7, "design": 2, "travers": 1, "deploy": 1, "core": 1, "blue": 1, "last": 1, "let": 3, "world": 1, "task": 1, "textbook": 1, "rectifi": 1, "eye": 1, "blog": 1, "away": 1, "node": 2, "featur": 1, "feed": 2, "practic": 1, "neural": 7, "alreadi": 1, "normal": 1, "itself": 1, "algorithm": 2, "array": 1, "offer": 1, "coordin": 2, "either": 1, "startup": 1, "effort\u00b2": 1, "moreov": 1, "compil": 9, "such": 2, "signific": 2, "sever": 1, "colahgithubiopostsnumbackprop": 1, "explor": 1, "therefor": 1, "lie": 2, "network": 8, "deriv": 7, "fast": 4, "some": 3, "rule\u2077": 1, "drop": 1, "both": 3, "aris": 1, "result": 3, "thought": 3, "off": 2, "configmod": 1, "becom": 3, "use": 20, "devic": 1, "stop": 1, "languag": 8, "set": 1, "ubprogram": 1, "ani": 2, "outright": 1, "from": 9, "tensors\u00b9": 1, "allow": 1, "behavior": 1, "whether": 3, "detail": 1, "num": 24, "pioneer": 1, "instanc": 1, "redund": 1, "relu": 1, "els": 1, "idea": 9, "environ": 1, "nondl": 1, "invok": 1, "color": 1, "functions\u2075": 1, "wwwtensorfloworgapidocspythonmathop": 1, "output": 5, "correct": 1, "height": 3, "fulli": 2, "roll": 1, "unlik": 1, "written": 2, "plethora": 1, "much": 1, "assum": 1, "fortran\u00b9\u2075": 1, "are": 24, "topic": 1, "explan": 1, "intel": 1, "analog": 1, "power": 2, "look": 2, "option": 1, "video": 1, "backend": 1, "tutorialnetlayerblobhtml": 1, "numpi": 6, "refer": 3, "intern": 2, "exercis": 1, "think": 3, "said": 1, "combin": 1, "enwikipediaorgwikichainrul": 1, "later": 3, "net": 4, "general": 1, "column": 1, "methodsthecopneedstodefin": 1, "iter": 1, "sigmoid": 2, "then": 1, "goldrush": 1, "drastic": 1, "even": 5, "index": 1, "upper": 1, "could": 13, "simpler": 1, "bug": 1, "check": 4, "train": 1, "great": 1, "master": 2, "avoid": 1, "goal": 1, "anatomi": 1, "framework": 21, "downloadtensorfloworgpaperwhitepapernumpdf": 1, "ask": 2, "librari": 3, "taken": 2, "anoth": 4, "heart": 2, "adjac": 1, "pythonista": 1, "probabl": 2, "machin": 1, "into": 5, "paper": 1, "bias": 1, "howtosaddinganop": 1, "lua": 1, "abov": 3, "inear": 1, "justifi": 1, "symbol": 3, "cumbersom": 1, "effect": 1, "transfer": 1, "earlier": 1, "cite": 1, "choos": 1, "wonder": 1, "trialanderror": 1, "interpret": 1, "flexibl": 1, "help": 4, "main": 1, "type": 2, "believ": 3, "find": 2, "bird": 1, "abil": 1, "conclus": 1, "doe": 2, "pros": 1, "own": 6, "approxim": 1, "relat": 2, "label": 1, "fact": 1, "hour": 1, "who": 2, "decid": 2, "univers": 1, "permiss": 1, "bigger": 2, "output\u00b3": 1, "post": 5, "here": 2, "bitmap": 1, "veri": 5, "softwar": 1, "multipl": 4, "were": 1, "wrapper": 1, "give": 3, "long": 1, "layer": 1, "encod": 1, "deeplearningnetsoftwaretheanolibrarytensorbasichtml": 1, "contain": 1, "parallel": 3, "about": 5, "evid": 1, "intuit": 1, "job": 2, "scene": 1, "brave": 1, "thing": 6, "catchup": 1, "when": 3, "just": 5, "entir": 1, "adapt": 1, "twodimension": 1, "heard": 1, "inputs\u2078": 1, "ensur": 1, "what": 2, "simpli": 2, "dure": 1, "code\u00b9\u2074": 1, "them": 9, "under": 1, "defin": 2, "enforc": 1, "sincer": 1, "across": 2, "newcom": 1, "complex": 1, "materi": 1, "tool": 4, "start": 2, "whenev": 1, "among": 1, "articl": 2, "love": 1, "sketch": 1, "guid": 1, "softwaretheanooptimizationshtml": 1, "seri": 2, "simpl": 5, "semipro": 1, "systemat": 1, "execut": 2, "can": 25, "whi": 4, "central": 1, "support": 4, "abl": 7, "mani": 5, "other": 4, "appli": 1, "question": 2, "step": 3, "generat": 3, "build": 2, "provid": 7, "calcul": 6, "error": 1, "linear": 2, "unfortun": 2, "repres": 7, "too": 1, "order": 2, "rnns": 1, "there": 6, "exist": 1, "valu": 3, "inform": 4, "move": 2, "end": 1, "vector": 1, "con": 1, "hardwar": 1, "gokula": 2, "say": 2, "best": 1, "exampl": 2, "continu": 1, "now": 2, "prefer": 1, "again": 2, "follow": 2, "data": 11, "themselv": 2, "further": 1, "white": 1, "code": 7, "lgebra": 1, "usual": 4, "interact": 1, "represent": 2, "basic": 2, "haven": 1, "possibl": 1, "ndarray": 3, "leverag": 1, "reduc": 1, "issu": 5, "also": 8, "person": 1, "simplest": 1, "around": 1, "mean": 3, "novic": 1, "seem": 1, "which": 12, "modular": 1, "pycuda": 1, "masteri": 1, "method": 5, "straightforward": 3, "user": 2, "research": 1, "cudnn": 3, "student": 1, "scenario": 1, "may": 1, "doc": 1, "take": 8, "wwwtensorfloworgtutorialsmandelbrot": 1, "understand": 10, "peopl": 3, "deeplearningnetsoftwaretheanoextendinggraphstructureshtml": 1, "zero": 1, "challeng": 1, "highlevel": 6, "intermedi": 2, "want": 4, "wast": 2, "convert": 2, "concret": 1, "system": 1, "cublas\u00b9\u2076": 1, "behind": 1, "wherev": 1, "extens": 4, "howev": 1, "essenti": 1, "regist": 1, "actual": 1, "gpgpus": 1, "plug": 1, "cython": 1, "contributor": 1, "saw": 1, "sinc": 4, "stay": 1, "ndimens": 1, "insan": 1, "three": 1, "class\u00b9\u2070": 1, "one": 9, "graph": 16, "lot": 4, "hope": 2, "see": 3, "becaus": 4, "imag": 11, "cach": 1, "ineffici": 1, "scikitlearn": 1, "get": 7, "easier": 3, "like": 15, "two": 1, "implement": 13, "text": 2, "select": 1, "collect": 1, "alon": 1, "blas": 1, "part": 7, "add": 2, "analyt": 1, "memori": 1, "import": 3, "includ": 2, "cool": 1, "anyon": 1, "kera": 1, "pretti": 3, "choic": 5, "principl": 2, "should": 2, "time": 4, "process": 1, "back": 5, "hood": 1, "similar": 5, "row": 1, "differ": 7, "been": 3, "wwwdatasciencecentralcomprofilesblogsmatrixmultiplicationinneuralnetwork": 1, "decis": 1, "numpyimsav": 1, "most": 8, "between": 6, "backpropag": 1, "return": 2, "everi": 1, "programmat": 1, "postsnumbackprop": 1, "shape": 1, "java": 1, "along": 1, "footnot": 1, "syntax": 1, "nonlinear": 3, "action": 2, "situat": 2, "someth": 3, "gradient": 9, "have": 22, "appropri": 1, "tri": 2, "case": 2, "day": 1, "architectur": 1, "arguments\u00b9\u00b3": 1, "fast\u2076": 1, "call": 2, "cycl": 1, "object": 9, "more": 15, "and": 73, "correspond": 1, "interfac": 2, "discuss": 2, "torch": 1, "these": 21, "benefit": 1, "gain": 1, "artifici": 1, "websit": 1, "faster": 2, "winograd": 1, "softwaretheanotutorialgradientshtml": 1, "befor": 4, "comment": 2, "wwwscipylecturesorgintronumpyoperationshtml": 1, "matrix": 7, "standard": 1, "speedup": 4, "introduc": 2, "zurich": 2, "comput": 23, "fundament": 2, "gentlest": 1, "depend": 3, "onli": 2, "each": 3, "introduct": 1, "size": 1, "santhanam": 2, "few": 1, "tensor": 24, "deeplearningnetsoftwaretheanoextendingextendingtheanochtml": 1, "necessari": 2, "skip": 2, "rise": 1, "read": 3, "packag": 7, "link": 2, "common": 4, "chain": 5, "solv": 2, "block": 1, "wwwinfethzchpersonalmarkuspteachingnumethspringnumcoursehtml": 1, "oper": 29, "composit": 2, "phase": 1, "convolut": 2, "bio": 1, "come": 5, "abstract": 6, "might": 3, "timeconsum": 1, "that": 28, "githubcomparambharatcarndhelpersblobmasterimageprocessingimageprocessingtutorialipynb": 1, "headach": 1, "right": 1, "gpus": 1, "differenti": 6, "sem": 1, "direct": 1, "interspers": 1, "first": 3, "game": 1, "input": 3, "cover": 1, "inher": 1, "show": 1, "mention": 1, "deeplearningnetsoftwaretheanolibraryconfightml": 1, "write": 4, "fret": 1, "tech": 1, "down": 1, "learn": 18, "same": 3, "dens": 1, "effici": 3}, "idf": {"after": 1.02070207021, "lowlevel": 1587.6, "pro": 12.7008, "toolkit": 189.0, "limit": 1.5186531471200002, "extern": 5.24133377352, "tensorflow": 1587.6, "onc": 1.4974533106999999, "this": 1.00379362671, "real": 2.28103448276, "play": 1.46390041494, "form": 1.12755681818, "red": 2.22228443449, "lstms": 1587.6, "new": 1.0178880554, "assign": 3.83663605607, "space": 2.39818731118, "with": 1.0011982089899998, "addit": 1.24634950542, "way": 1.2190739461, "would": 1.0828729281799998, "thank": 6.00681044268, "previous": 1.42846859816, "multipli": 20.4061696658, "the": 1.0, "dataset": 193.609756098, "cudnn\u00b9\u2077": 1587.6, "function": 2.495441685, "class": 2.11651779763, "pictur": 3.4953764861300005, "python": 56.2978723404, "demystifi": 337.787234043, "hear": 4.17899447223, "well": 1.0655748708, "onehot": 1587.6, "done": 2.3302509907499998, "four": 1.20950784702, "assembl": 3.0011342155, "complet": 1.24021560816, "approach": 2.07556543339, "rllberkeleyeducgt": 1587.6, "toward": 1.6303142329, "know": 2.59327017315, "featureset": 1587.6, "their": 1.01547908405, "dev": 148.373831776, "automat": 6.787516032490001, "modul": 16.9434364995, "calculus": 62.2588235294, "run": 1.55692850838, "origin": 1.13724928367, "instead": 1.59461631177, "creat": 1.2492917847, "misunderstand": 32.4, "express": 1.9120799710900003, "extend": 1.9604840701400004, "how": 1.60250328051, "programm": 5.181462140990001, "instruct": 4.169117647059999, "repost": 933.882352941, "make": 1.0762660158600001, "spars": 21.0, "quit": 2.8849718335500003, "multilay": 324.0, "initi": 1.35, "distribut": 2.7396031061299997, "krishnan": 466.941176471, "cubla": 1587.6, "found": 1.11387076405, "for": 1.00031504001, "word": 1.7965372864099998, "interest": 1.60331246213, "deep": 3.6279707495399998, "difficult": 2.48957189901, "will": 1.22481098596, "exploit": 5.79416058394, "specif": 1.8719490626099997, "consid": 1.2397313759200002, "someon": 4.9350326391, "work": 1.11520089913, "unit": 1.15394679459, "alloc": 10.5558510638, "second": 1.1130898128, "incorpor": 2.62847682119, "phase\u00b9\u00b2": 1587.6, "next": 1.4950560316400001, "below": 2.25607503197, "clear": 1.85423966363, "perform": 1.5313977042500002, "but": 1.01632417899, "threedimension": 1587.6, "need": 1.4372623574099999, "nonoptim": 1587.6, "all": 1.01146788991, "final": 1.34008609775, "overload": 61.0615384615, "expect": 2.20011086475, "point": 1.25990000794, "quest": 12.6805111821, "caff": 992.25, "applic": 3.42672134686, "releas": 1.8377126982299998, "numpyimread": 1587.6, "level": 1.6544393497299998, "various": 1.3323262839899999, "has": 1.0436497502, "enough": 2.2319696330700003, "replac": 1.5602948402899999, "polish": 6.59576235978, "differentiation\u2079": 1587.6, "famous": 2.28201811125, "saniti": 103.764705882, "sound": 3.11294117647, "green": 2.63065451533, "out": 1.06016694491, "enjoy": 3.3269069572500003, "theano": 1587.6, "good": 1.51981619759, "daunt": 102.425806452, "slow": 4.04793472718, "concept": 2.65707112971, "manner": 3.93164933135, "channel": 3.6784059314199995, "pool": 7.052865393160001, "algebra": 41.4516971279, "spare": 9.997481108310001, "optim": 11.5377906977, "field": 1.7790228597, "sourc": 1.69760479042, "togeth": 1.58095996813, "view": 1.6407606448899998, "wikiautomaticdifferenti": 1587.6, "not": 1.01567398119, "far": 1.71022298826, "henc": 5.390831918509999, "tree": 4.127925117, "autodifferenti": 1587.6, "compon": 4.09491875161, "respect": 1.6443293630200002, "mehir": 1587.6, "store": 3.44680851064, "worker": 3.6843815270399998, "happen": 2.96359902931, "unifi": 7.60709151893, "pleas": 9.12938470385, "easi": 5.2937645882, "width": 17.294117647100002, "test": 2.65707112971, "notic": 4.36994219653, "asic": 330.75, "better": 2.0065722952500002, "than": 1.03278688525, "problem": 1.76674827509, "complic": 5.6478121664900005, "they": 1.03017325287, "design": 1.45825296225, "travers": 22.8103448276, "deploy": 7.41869158879, "core": 4.623179965059999, "blue": 3.07019918778, "last": 1.2117234010100002, "let": 3.48616600791, "world": 1.11340206186, "task": 3.88641370869, "textbook": 18.290322580599998, "rectifi": 54.3698630137, "eye": 3.39375801625, "blog": 14.1876675603, "away": 1.85142857143, "node": 44.3463687151, "featur": 1.52712581762, "feed": 7.77853993141, "practic": 1.70434782609, "neural": 59.4606741573, "alreadi": 1.9551724137900002, "normal": 2.61075481006, "itself": 1.74557449148, "algorithm": 27.9507042254, "array": 10.1444089457, "offer": 1.53896859248, "coordin": 5.65586034913, "either": 1.5830092731099998, "startup": 68.4310344828, "effort\u00b2": 1587.6, "moreov": 7.56, "compil": 5.696447793330001, "such": 1.06151377374, "signific": 1.4529147982100001, "sever": 1.07241286139, "colahgithubiopostsnumbackprop": 1587.6, "explor": 3.39593582888, "therefor": 2.33401940606, "lie": 3.2157180474000002, "network": 2.59369384088, "deriv": 2.78379800105, "fast": 4.8729281768, "some": 1.04036697248, "rule\u2077": 1587.6, "drop": 2.4594887684, "both": 1.05215720061, "aris": 7.54921540656, "result": 1.14611608432, "thought": 1.9854927463699998, "off": 1.5121440137200002, "configmod": 1587.6, "becom": 1.12492028626, "use": 1.0296387573799999, "devic": 5.00820189274, "stop": 2.1783754116400003, "languag": 2.29488291414, "set": 1.18707940781, "ubprogram": 1587.6, "ani": 1.13383802314, "outright": 18.227324913900002, "from": 1.00056721497, "tensors\u00b9": 1587.6, "allow": 1.2716059271100002, "behavior": 5.52978056426, "whether": 2.20683903253, "detail": 2.26186066391, "num": 1.00031504001, "pioneer": 4.74051955808, "instanc": 3.2572835453400004, "redund": 29.7861163227, "relu": 1587.6, "els": 5.44444444444, "idea": 2.0930784443, "environ": 3.43561999567, "nondl": 1587.6, "invok": 18.227324913900002, "color": 3.8255421686699997, "functions\u2075": 1587.6, "wwwtensorfloworgapidocspythonmathop": 1587.6, "output": 7.676982591880001, "correct": 3.6631287494199998, "height": 4.1023255814, "fulli": 2.79015817223, "roll": 4.27578777269, "unlik": 2.42529789184, "written": 1.9573418813999999, "plethora": 91.2413793103, "much": 1.1942229577299999, "assum": 2.9575260804799997, "fortran\u00b9\u2075": 1587.6, "are": 1.02990593578, "topic": 5.457545548300001, "explan": 6.50922509225, "intel": 54.5567010309, "analog": 9.05131128848, "power": 1.3396337861799998, "look": 1.9086318826599997, "option": 4.04896710023, "video": 3.29719626168, "backend": 933.882352941, "tutorialnetlayerblobhtml": 1587.6, "numpi": 1587.6, "refer": 1.30024570025, "intern": 1.30355530011, "exercis": 4.73627684964, "think": 2.90715986083, "said": 1.54751925139, "combin": 1.69760479042, "enwikipediaorgwikichainrul": 1587.6, "later": 1.08650424309, "net": 6.96315789474, "general": 1.1218202374200001, "column": 7.078020508250001, "methodsthecopneedstodefin": 1587.6, "iter": 37.4433962264, "sigmoid": 1058.4, "then": 1.08657860516, "goldrush": 882.0, "drastic": 14.0620017715, "even": 1.16461267606, "index": 6.9969149405, "upper": 3.41052631579, "could": 1.2043695949, "simpler": 17.9187358916, "bug": 27.372413793099998, "check": 6.50655737705, "train": 1.9365698950999999, "great": 1.26592775696, "master": 3.15125049623, "avoid": 2.45986984816, "goal": 3.28152128979, "anatomi": 25.0410094637, "framework": 8.200413223139998, "downloadtensorfloworgpaperwhitepapernumpdf": 1587.6, "ask": 2.1744966443, "librari": 2.68266306185, "taken": 1.6012102874399998, "anoth": 1.13643521832, "heart": 3.00340522134, "adjac": 6.240566037740001, "pythonista": 1587.6, "probabl": 2.64555907349, "machin": 4.02433460076, "into": 1.01502461479, "paper": 2.6628648104700003, "bias": 13.7335640138, "howtosaddinganop": 1587.6, "lua": 387.219512195, "abov": 1.90382539873, "inear": 1587.6, "justifi": 7.578042959430001, "symbol": 3.4178686760000003, "cumbersom": 79.38, "effect": 1.3963060686000002, "transfer": 2.72549356223, "earlier": 1.86776470588, "cite": 2.73253012048, "choos": 4.17899447223, "wonder": 7.265903890160001, "trialanderror": 1587.6, "interpret": 3.2150668286799995, "flexibl": 9.68639414277, "help": 1.39962972759, "main": 1.25303867403, "type": 2.0281042411900003, "believ": 1.6450108797, "find": 1.7294117647099998, "bird": 6.46416938111, "abil": 2.70875277256, "conclus": 4.84615384615, "doe": 1.70581282905, "pros": 141.75, "own": 1.17844418052, "approxim": 2.2132998745299997, "relat": 1.23750876919, "label": 4.47715736041, "fact": 1.73375559681, "hour": 2.25960717336, "who": 1.06279287723, "decid": 1.9257641921400002, "univers": 1.24889867841, "permiss": 6.280063291139999, "bigger": 13.23, "output\u00b3": 1587.6, "post": 2.23826307627, "here": 2.42307692308, "bitmap": 407.07692307699995, "veri": 1.25880114177, "softwar": 10.2624434389, "multipl": 2.74813917258, "were": 1.02458857696, "wrapper": 252.0, "give": 1.3653250774, "long": 1.2657259028899999, "layer": 8.14153846154, "encod": 29.0237659963, "deeplearningnetsoftwaretheanolibrarytensorbasichtml": 1587.6, "contain": 1.59814777532, "parallel": 4.57917507932, "about": 1.06486015159, "evid": 2.24872521246, "intuit": 27.7068062827, "job": 3.2539454806299997, "scene": 3.45055422734, "brave": 15.7970149254, "thing": 2.4065484311099996, "catchup": 1587.6, "when": 1.02076769755, "just": 1.33580143037, "entir": 1.59365589239, "adapt": 3.32272917539, "twodimension": 1587.6, "heard": 4.45204711161, "inputs\u2078": 1587.6, "ensur": 3.4127257093700005, "what": 1.25343439128, "simpli": 2.5192002538900002, "dure": 1.0503473370799998, "code\u00b9\u2074": 1587.6, "them": 1.09876115994, "under": 1.0781663837, "defin": 2.72830383227, "enforc": 4.93810264386, "sincer": 28.2491103203, "across": 1.7318642958400001, "newcom": 27.5147313692, "complex": 2.34021226415, "materi": 2.13014893332, "tool": 4.99716713881, "start": 1.26673581744, "whenev": 11.622254758399999, "among": 1.25670862028, "articl": 2.01805008262, "love": 2.97303370787, "sketch": 10.956521739100001, "guid": 2.49113447356, "softwaretheanooptimizationshtml": 1587.6, "seri": 1.46511627907, "simpl": 3.3981164383599998, "semipro": 1587.6, "systemat": 8.338235294119999, "execut": 2.2363713199, "can": 1.17626139142, "whi": 3.2566153846200003, "central": 1.6121039805000001, "support": 1.2685577307200002, "abl": 1.8208510150200001, "mani": 1.04426757877, "other": 1.00992366412, "appli": 2.2972073506, "question": 2.20408163265, "step": 2.8279301745599996, "generat": 2.05275407292, "build": 1.6341739578, "provid": 1.21552714187, "calcul": 6.12972972973, "error": 6.04109589041, "linear": 13.8776223776, "unfortun": 9.966101694919999, "repres": 1.46972782818, "too": 1.81585268215, "order": 1.24625166811, "rnns": 1587.6, "there": 1.04091266719, "exist": 1.4647107666799999, "valu": 2.2777618364400003, "inform": 1.5753125620200001, "move": 1.29125660838, "end": 1.10680423871, "vector": 25.898858075, "con": 22.744985673400002, "hardwar": 18.8104265403, "gokula": 1587.6, "say": 1.7544480053, "best": 1.5828514456600002, "exampl": 1.50483412322, "continu": 1.13928955867, "now": 1.160780873, "prefer": 3.0216977540900003, "again": 1.50883862384, "follow": 1.04640126549, "data": 3.37643555934, "themselv": 2.05967825636, "further": 1.3618116315, "white": 1.86930413282, "code": 3.8807137619199996, "lgebra": 1587.6, "usual": 1.72508964468, "interact": 4.4185917061, "represent": 5.928304705, "basic": 2.7301805675, "haven": 12.690647482000001, "possibl": 1.4173734488, "ndarray": 1587.6, "leverag": 35.7567567568, "reduc": 1.98698372966, "issu": 1.43921675279, "also": 1.01476510067, "person": 1.40520446097, "simplest": 28.0494699647, "around": 1.21394708671, "mean": 1.44906900329, "novic": 68.4310344828, "seem": 2.29123971713, "which": 1.005191845, "modular": 49.9245283019, "pycuda": 1587.6, "masteri": 41.889182058, "method": 2.5714285714300003, "straightforward": 27.7552447552, "user": 7.71053909665, "research": 1.9420183486200002, "cudnn": 1587.6, "student": 2.47174217655, "scenario": 15.3986420951, "may": 1.05201775893, "doc": 34.8157894737, "take": 1.13961668222, "wwwtensorfloworgtutorialsmandelbrot": 1587.6, "understand": 2.96858638743, "peopl": 1.21320495186, "deeplearningnetsoftwaretheanoextendinggraphstructureshtml": 1587.6, "zero": 8.75192943771, "challeng": 2.55816951337, "highlevel": 1587.6, "intermedi": 11.4380403458, "want": 1.99698113208, "wast": 7.499291450169999, "convert": 3.2740771293099997, "concret": 10.0100882724, "system": 1.38739840951, "cublas\u00b9\u2076": 1587.6, "behind": 2.0845588235299997, "wherev": 21.867768595, "extens": 1.99171998495, "howev": 1.0945191313299998, "essenti": 2.9280708225700005, "regist": 3.9620663838300003, "actual": 1.87482286254, "gpgpus": 1587.6, "plug": 30.297709923699998, "cython": 1587.6, "contributor": 14.4721969006, "saw": 1.94845360825, "sinc": 1.08368600683, "stay": 2.6986231514499996, "ndimens": 1587.6, "insan": 23.6602086438, "three": 1.06621893889, "class\u00b9\u2070": 1587.6, "one": 1.00627495722, "graph": 37.7102137767, "lot": 4.40877534018, "hope": 2.50884955752, "see": 1.27242125511, "becaus": 1.1495184997499999, "imag": 2.70137825421, "cach": 49.0, "ineffici": 26.154859967100002, "scikitlearn": 1587.6, "get": 1.78562591385, "easier": 7.84, "like": 1.14918566775, "two": 1.01379310345, "implement": 3.57648118946, "text": 3.12827586207, "select": 2.02345144022, "collect": 1.64109985528, "alon": 2.99716820842, "blas": 260.262295082, "part": 1.04330682789, "add": 4.61243463103, "analyt": 17.256521739100002, "memori": 2.57392996109, "import": 1.3401992233700002, "includ": 1.0190641247799999, "cool": 6.8578833693300005, "anyon": 5.37440758294, "kera": 835.5789473680001, "pretti": 15.75, "choic": 3.1319786940200003, "principl": 3.4520547945199995, "should": 1.6643254009900001, "time": 1.01127460348, "process": 1.69524826482, "back": 1.26070038911, "hood": 19.1507840772, "similar": 1.37514075357, "row": 5.549108703250001, "differ": 1.23654490225, "been": 1.0239277652399998, "wwwdatasciencecentralcomprofilesblogsmatrixmultiplicationinneuralnetwork": 1587.6, "decis": 2.16, "numpyimsav": 1587.6, "most": 1.02096463023, "between": 1.03453668708, "backpropag": 1587.6, "return": 1.39532431007, "everi": 1.47917637194, "programmat": 214.54054054099998, "postsnumbackprop": 1587.6, "shape": 3.20338983051, "java": 31.625498008, "along": 1.2973768080399999, "footnot": 25.320574162699998, "syntax": 45.6206896552, "nonlinear": 99.225, "action": 1.81855670103, "situat": 2.06611140031, "someth": 3.28152128979, "gradient": 41.889182058, "have": 1.0148948411399998, "appropri": 4.31413043478, "tri": 1.8544562551099997, "case": 1.48498737256, "day": 1.18371607516, "architectur": 5.12790697674, "arguments\u00b9\u00b3": 1587.6, "fast\u2076": 1587.6, "call": 1.0676529926, "cycl": 5.40919931857, "object": 2.3488681757700003, "more": 1.0171706817, "and": 1.00006299213, "correspond": 3.32481675393, "interfac": 20.9169960474, "discuss": 2.19676214197, "torch": 30.1825095057, "these": 1.07415426252, "benefit": 3.06841901817, "gain": 1.84819557625, "artifici": 8.31639601886, "websit": 2.52160101652, "faster": 7.61438848921, "winograd": 1587.6, "softwaretheanotutorialgradientshtml": 1587.6, "befor": 1.10036041031, "comment": 3.05954904606, "wwwscipylecturesorgintronumpyoperationshtml": 1587.6, "matrix": 22.6153846154, "standard": 1.8915763135900003, "speedup": 756.0, "introduc": 1.7258397651900002, "zurich": 62.750988142299995, "comput": 3.9277585353800006, "fundament": 5.32930513595, "gentlest": 1587.6, "depend": 2.2411067193700003, "onli": 1.0256476516600002, "each": 1.18974820144, "introduct": 2.7808723068799996, "size": 2.49387370405, "santhanam": 1587.6, "few": 1.31729173581, "tensor": 152.653846154, "deeplearningnetsoftwaretheanoextendingextendingtheanochtml": 1587.6, "necessari": 2.8421052631599997, "skip": 26.111842105300003, "rise": 2.02940048575, "read": 2.3149606299200003, "packag": 7.828402366860001, "link": 2.15151104486, "common": 1.4025974025999999, "chain": 5.17639387023, "solv": 7.26923076923, "block": 3.20274359492, "wwwinfethzchpersonalmarkuspteachingnumethspringnumcoursehtml": 1587.6, "oper": 1.55479384977, "composit": 4.629921259840001, "phase": 4.3012733676499995, "convolut": 101.121019108, "bio": 42.336000000000006, "come": 1.32831325301, "abstract": 9.966101694919999, "might": 2.1561863370900003, "timeconsum": 1587.6, "that": 1.00398406375, "githubcomparambharatcarndhelpersblobmasterimageprocessingimageprocessingtutorialipynb": 1587.6, "headach": 54.0, "right": 1.4054532577899999, "gpus": 1058.4, "differenti": 7.759530791789999, "sem": 317.52, "direct": 1.22226499346, "interspers": 47.25, "first": 1.00761614623, "game": 2.57978550536, "input": 12.2029208301, "cover": 1.69380134429, "inher": 10.7706919946, "show": 1.26703910615, "mention": 2.53894130817, "deeplearningnetsoftwaretheanolibraryconfightml": 1587.6, "write": 2.0575427682700003, "fret": 145.651376147, "tech": 19.1739130435, "down": 1.35889754344, "learn": 2.32275054865, "same": 1.11857958148, "dens": 10.4173228346, "effici": 5.09335899904}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  The Anatomy of Deep Learning Frameworks</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/02/anatomy-deep-learning-frameworks.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb The Anatomy of Deep Learning Frameworks Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2017/02/continuum-deliver-business-value-open-data-science.html\" rel=\"prev\" title=\"Continuum Analytics Webinar, Mar 8: Deliver New Business Value with Open Data Science\"/>\n<link href=\"https://www.kdnuggets.com/jobs/17/02-24-aetna-principal-data-scientist.html\" rel=\"next\" title=\"Aetna: Principal Data Scientist\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2017/02/anatomy-deep-learning-frameworks.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=63157\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2017/02/anatomy-deep-learning-frameworks.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-63157 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 24-Feb, 2017  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/02/index.html\">Feb</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/02/software.html\">Software</a> \u00bb The Anatomy of Deep Learning Frameworks (\u00a0<a href=\"/2017/n08.html\">17:n08</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\"><img align=\"right\" alt=\"Silver Blog, Feb 2017\" src=\"/images/top-kdnuggets-blog-2017-feb-silver.png\" width=\"100\"/>The Anatomy of Deep Learning Frameworks</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/02/continuum-deliver-business-value-open-data-science.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/jobs/17/02-24-aetna-principal-data-scientist.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <span class=\"http-likes\" style=\"float: left; font-size:14px\">http likes 849</span> <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a></div>\n<br/>\n<p class=\"excerpt\">\n     This post sketches out some common principles which would help you better understand deep learning frameworks,  and provides a guide on how to implement your own deep learning framework as well.<br>\u00a0\n  </br></p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<p><b>Gokula Krishnan Santhanam, ETH Zurich.</b></p>\n<p>Deep Learning, whether you like it or not is here to stay, and with any tech gold-rush comes a plethora of options that can seem daunting to newcomers.</p>\n<p>If you were to start off with deep learning, one of the first questions to ask is, which framework to learn? I\u2019d say instead of a simple trial-and-error, if you try to understand the building blocks of all these frameworks, it would help you make an informed decision. Common choices include <a href=\"http://deeplearning.net/software/theano/\" target=\"_blank\">Theano</a>, <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a>, <a href=\"http://torch.ch/\" target=\"_blank\">Torch</a>, and <a href=\"https://keras.io/\" target=\"_blank\">Keras</a>. All of these choices have their own pros and cons and have their own way of doing things. After exploring the white papers and the dev docs, I could understand the design choices and was able to abstract the fundamental concepts that are common to all of these.</p>\n<p>In this post, I have tried to sketch out these common principles which would help you better understand the frameworks and for the brave hearts among you, provide a guide on how to implement your own deep learning framework. The interesting thing about these principles is that they are not specific to DL alone, they are applicable whenever you want to do a series of computations on data. Hence, most DL frameworks can be used for non-DL tasks as well (see <a href=\"https://www.tensorflow.org/tutorials/mandelbrot/\" target=\"_blank\">www.tensorflow.org/tutorials/mandelbrot/</a>)</p>\n<p>There are a few core components of a DL framework, these are:</p>\n<ol class=\"three_ul\">\n<li>A Tensor Object\n<li>Operations on the Tensor Object\n<li>A Computation Graph and Optimizations\n<li>Auto-differentiation tools\n<li>BLAS / cuBLAS and cuDNN extensions\n</li></li></li></li></li></ol>\n<p>These should make your framework complete, but you would need to polish it to make it more easy to use. I will be using references to the Python NumPy package in this article to make it easier to understand. If you haven\u2019t used NumPy before, fret not, this article should be easy to understand even if you skip the numpy parts. I\u2019m a believer of understanding a system at multiple levels of abstractions so you will be seeing discussions of low-level optimizations interspersed with high-level Calculus and Linear Algebra. Drop a comment below if something needs more explanation!</p>\n<p><strong>NOTE</strong>: I have been a contributor to Theano and hence might be biased towards it in citing references. Having said that, theano also has one of the most informative websites of all the frameworks I\u2019ve come across.</p>\n<h3>A Tensor Object</h3>\n<p>\u00a0<br>\nAt the heart of the framework is the tensor object. A tensor is a generalization of a matrix to n-dimensions (think numpy\u2019s ndarrays). In other words, a matrix is a two-dimensional tensor with (rows, columns). An easy way to understand tensors is to consider them as N-D Arrays.</br></p>\n<p>As an example, take a color image. Let\u2019s say it\u2019s an RGB Bitmap image of size 258 x 320 (Height x Width). This is a three-dimensional tensor (height, width, channels). Take a look at the following images to understand this better (taken from: <a href=\"https://github.com/parambharat/CarND-helpers/blob/master/image_processing/Image_processing_tutorial.ipynb\" target=\"_blank\">github.com/parambharat/CarND-helpers/blob/master/image_processing/Image_processing_tutorial.ipynb</a>)</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/0*sEsx4_AW715qbGj_.\" width=\"80%\"/><br>\nA Normal RGB\u00a0Image</br></center></p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*ive85-ts9v9Di0zdCXHVIA.png\" width=\"80%\"/><br>\nRed, Green and Blue Channels of the same\u00a0image</br></center></p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/0*fLd66pq-hGr4sjoS.\" width=\"80%\"/><br/>\nThe same image represented as a 3D\u00a0Tensor</center></p>\n<p>As an extension, a set of 100 images can be represented as a 4D tensor (ID of image, height, width, channels).</p>\n<p>Similarly, we represent all input data as tensors\u00b9 before feeding them into the neural net. This is an abstraction necessary before we can feed data into a net, else we would have to define operations that work on each type and that\u2019s a lot of wasted effort\u00b2. We would also need to be able to get back data in a form we want.</p>\n<p>So, we need a Tensor Object that supports storing the data in form of tensors. Not just that, we would like the object to be able to convert other data types (images, text, video) into tensors and back. Think of something like numpy.imread and numpy.imsave, they read images as ndarrays and store ndarrays as images respectively.</p>\n<p>The basic Tensor object needs to support representing data in form of a tensor. This means supporting indexing, overloading operators, having a space efficient way to store the data and so on. Depending on further design choices, you might have to add more features as well.</p>\n<h3>Operations on the Tensor Object</h3>\n<p>\u00a0<br/>\nA neural network can be considered as a series of Operations performed on an input tensor to give an output. Learning is done by correcting the errors between the output created by the net and the expected output\u00b3. These operations could be something simple like matrix multiplication (in sigmoids) or more complicated like convolutions, pooling or LSTMs.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/0*oTsCkyvpRJVybDOr.\" width=\"99%\"/><br/>\nSigmoid layer represented as Matrix Operations, from <a href=\"http://www.datasciencecentral.com/profiles/blogs/matrix-multiplication-in-neural-networks\" target=\"_blank\">www.datasciencecentral.com/profiles/blogs/matrix-multiplication-in-neural-networks</a></center></p>\n<p>Check out the following links to get an idea of how extensive these operations could be:</p>\n<ol class=\"three_ul\">\n<li>NumPy: <a href=\"http://www.scipy-lectures.org/intro/numpy/operations.html\" target=\"_blank\">www.scipy-lectures.org/intro/numpy/operations.html</a>\n<li>Theano: <a href=\"http://deeplearning.net/software/theano/library/tensor/basic.html\" target=\"_blank\">deeplearning.net/software/theano/library/tensor/basic.html</a>\n<li>TensorFlow: <a href=\"https://www.tensorflow.org/api_docs/python/math_ops/\" target=\"_blank\">www.tensorflow.org/api_docs/python/math_ops/</a>\n</li></li></li></ol>\n<p>You could just skip this part and ask the user to implement these operations themselves, but it is too cumbersome and outright inefficient. Moreover, most operations are common enough that one could justify making them a part of the framework to spare headaches for the users. NumPy does a pretty good job of having a lot of operations already implemented (it\u2019s insanely fast as well) and there is a running theano <a href=\"https://github.com/Theano/Theano/issues/1916\" target=\"_blank\">issue</a> about incorporating more operations which show how important it is to have more operation supported by the framework.</p>\n<p>Instead of implementing operations as functions, they are usually implemented as classes. This allows us to store more information about the operation like calculated shape of the output (useful for sanity checks), how to compute the gradient or the gradient itself (for the auto-differentiation), have ways to be able to decide whether to compute the op on GPU or CPU\u2074 and so on. Again, this idea is similar to classes used for various algorithms implemented by scikit-learn. You could define a method called <em>compute</em> that does the actual computation and returns the tensor after the computation is done.</p>\n<p>These classes are usually derived from an abstract class (in theano, it\u2019s the <em>Op</em> class). This enforces a unified interface across the Ops and also provide a way to add new ops later on. This makes the framework very flexible and ensures people can use it even as new network architectures and nonlinearities come along.</p>\n<h3>Computation Graph and Optimizations</h3>\n<p>\u00a0<br/>\nSo far, we have classes for representing tensors and operations on them. The power of neural networks lies in the ability to chain multiple such operations to form a powerful approximator.</p>\n<p>Therefore, the standard use case is that you can initialize a tensor, perform actions after actions on them and finally interpret the resulting tensor as labels or real values. Sounds simple enough?</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/0*8YJSNIElgsCe8GRR.\" width=\"60%\"/><br/>\nChaining together different operations, taken from <a href=\"https://colah.github.io/posts/2015-08-Backprop/\" target=\"_blank\">colah.github.io/posts/2015-08-Backprop/</a></center></p>\n<p>Unfortunately, as you chain more and more operations together, several issues arise that can drastically slow down your code and introduce bugs as well.</p>\n<ol class=\"three_ul\">\n<li>Start later ops only after the previous one is done or do them in parallel?\n<li>How to assign to different devices and coordinate between them?\n<li>How do you avoid redundant operations (multiplying with ones, adding zeros), cache useful intermediate values, and reduce multiple operations into one ( replace <em>mul(mul(mul(Tensor,2),2),2)</em> with one <em>mul(Tensor, 8)</em> )\n</li></li></li></ol>\n<p>There are more such issues and it becomes necessary to be able to get a bigger picture to even notice that these issues exist. We need a way to optimize the resultant chain of operations for both space and time.</p>\n<p>In order to get a bigger picture, we introduce a Computation Graph which is basically an object that contains links to the instances of various Ops and the relations between which operation takes the output of which operation as well as additional information. Depending on the framework in question, this can be implemented in different ways.</p>\n<p>For example,</p>\n<ol class=\"three_ul\">\n<li>Theano <a href=\"http://deeplearning.net/software/theano/extending/graphstructures.html\" target=\"_blank\">deeplearning.net/software/theano/extending/graphstructures.html</a>\n<li>TensorFlow <a href=\"http://download.tensorflow.org/paper/whitepaper2015.pdf\" target=\"_blank\">download.tensorflow.org/paper/whitepaper2015.pdf</a>\n<li>Caffe<a href=\"http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html\" target=\"_blank\"> http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html</a>\n</li></li></li></ol>\n<p>Like many ideas in Deep Learning, the idea of computation graphs has been around for quite some time. Take a look at any Compilers textbook and you can find similar ideas in Abstract Syntax Trees and Intermediate Representations used for Optimization. These ideas have been extended and adapted to the Deep Learning scenario to give us the computational graph. The idea of optimizing the graph before code generation (will be covered later) is straightforward. The optimizations themselves could again be implemented as classes or functions\u2075 and could be selectively applied depending on whether you want the code to compile fast or run fast\u2076.</p>\n<p>Additionally, since you get a bird\u2019s eye view of what will be happening in the network, the graph class can then decide on how to allocate GPU memory (like Register Allocation in Compilers) and coordinate between various machines when deployed in a distributed environment. This helps us to effectively solve the three problems mentioned above.</p>\n<h3>Auto-differentiation tools</h3>\n<p>\u00a0<br/>\nAnother benefit of having the computational graph is that calculating gradients used in the learning phase becomes modular and straightforward to compute. This is thanks to the chain rule\u2077 that lets you calculate derivatives of composition of functions in a systematic way. As we saw earlier, neural nets can be thought of composition of simple nonlinearities giving rise to more complex functions. Differentiating these functions is simply traversing the graph from the outputs back to the inputs\u2078. Symbolic Differentiation or Automatic Differentiation\u2079 is a programmatic way by which gradients can be computed in a computation graph.</p>\n<p>Symbolic differentiation refers to calculating the derivatives analytically i.e., you get an expression of what the gradient is. To use it, you simply plug in the values into the derivative and use it. Unfortunately, some nonlinearities like ReLU (Rectified Linear Units) are not differentiable at some points. So, we instead calculate the gradient in an iterative manner. Since the second method could be used universally, most computational graph packages like Computation Graph Toolkit (<a href=\"http://rll.berkeley.edu/cgt/\" target=\"_blank\">rll.berkeley.edu/cgt/</a>) implement auto-differentiation but you can use symbolic differentiation if you are creating your own.</p>\n<p>It is usually not a good idea to roll out your own gradient computation module because it is easier and faster for the toolkit to provide it as part of the package. So, either have your own Computation Graph toolkit and auto-differentiation module or use an external package for both.</p>\n<p>Since the derivative at each node has to computed with respect to only its adjacent nodes, the method to compute gradients can be added to the class\u00b9\u2070 \u00b9\u00b9 and can be invoked by the differentiation module.</p>\n<h3>BLAS / cuBLAS and cuDNN extensions</h3>\n<p>\u00a0<br/>\nWith all the above components, you can stop right now and have a fully functional Deep Learning framework. It would be able to take data as input and convert to tensors, perform operations on them in an efficient way, compute gradients to learn and return back results for the test dataset. The problem, however, lies in the fact that since you most likely implemented it in a high-level language (Java / Python / Lua), there is an inherent upper limit to the speedups you can get. This is because even the simplest operations in a high-level language take more time (CPU cycles) than when done in a low-level language.</p>\n<p>In these situations, there are two different approaches we could take.</p>\n<p>The first one is an another analogy from compilers. The last step of a compilation process is hardware specific code generation in Assembly. Similarly, instead of running the graph written in the high-level language, the corresponding code for the network is generated in C and this is compiled and executed. The code for this is stored in each of the Ops and can be combined together in the compilation phase\u00b9\u00b2. Transferring data to and from the low-level to high-level code is done by wrappers like pyCUDA and Cython.</p>\n<p>The second approach is to have a backend implemented in a low-level language like C++, this means that the low-level language\u200a\u2014\u200ahigh-level language interaction is internal to the framework unlike the previous approach and could be faster because we don\u2019t need to compile the entire graph every time. Instead, we could just call the compiled methods with the appropriate arguments\u00b9\u00b3.</p>\n<p>Another source of non-optimal behavior comes from slow implementations at the low-level language. It is difficult to write efficient code\u00b9\u2074 and we will be better off using libraries that have optimized implementations of these methods. <strong>BLAS</strong> or <strong>B</strong>asic <strong>L</strong>inear <strong>A</strong>lgebra<strong> S</strong>ubprograms are a collection of optimized matrix operations, initially written in Fortran\u00b9\u2075. These can be leveraged to do very fast matrix (tensor) operations and can provide significant speedups. There are many other software packages like Intel MKL, ATLAS which also perform similar functions. Which one to choose is a personal preference.</p>\n<p>BLAS packages are usually optimized assuming that the instructions will be run on a CPU. In the deep learning situation, this is not the case and BLAS may not be able to fully exploit the parallelism offered by GPGPUs. To solve this issue, NVIDIA has released cuBLAS\u00b9\u2076 which is optimized for GPUs. This is now included with the CUDA toolkit and is probably why not many people have heard of it. Finally, cuDNN\u00b9\u2077 is a library that builds on the featureset of cuBLAS and provides optimized Neural Network specific operations like Winograd Convolution and RNNs.</p>\n<p>So, by using these packages, you could gain significant speed-ups in your framework. Speed-ups are important in DL because it is the difference between training a neural network in four hours instead of four days. In the fast moving world of AI startups, that\u2019s the difference between being a pioneer and playing a game of catch-up. So exploit parallelism and optimized libraries wherever you can!</p>\n<h3>Conclusion</h3>\n<p>\u00a0<br/>\nWe have finally come to the end of a pretty long post, thanks a lot for reading it. I hope I have demystified the anatomy of deep learning frameworks for many of you. My main goal in writing this post was to make concrete my understanding of how different frameworks are doing essentially the same thing. This will be a very helpful exercise for anyone who\u2019s above a novice level but below a pro level (a semi-pro, if you will). Once you can understand how things work behind the scenes, they become easier to approach and master. Frameworks do a great job in abstracting away most of these ideas in order to provide a simple interface for the programmers. No wonder that most of these concepts are not very evident when you are learning a framework.</p>\n<p>As someone who\u2019s interested in not just the applications of Deep Learning but also in the fundamental challenges of the field, I believe that knowing how things work under the hood is an important step towards mastery of the topic as it clears out many of the misunderstandings and provides a simpler way to think about why things are the way they are. I sincerely believe that a good worker knows not just which tool to use but also why that tool is the best choice. This blog is a step in that direction.<br/>\nHope you enjoyed reading this post as much as I did writing it. Please do let me know your thoughts in the comments below!</p>\n<p>If you found this interesting, and want to know more about me/hire me as an intern, I\u2019d love to hear from you! You can find my CV <a href=\"https://goo.gl/wHPF9y\" target=\"_blank\">here.</a></p>\n<h3>Footnotes</h3>\n<ul>\n<li>[1] It\u2019s not straightforward how you would represent text as tensors. The first way is to use a one-hot encoding, which is a very sparse matrix and wastes a lot of space. A more dense representation is word vectors. These are pretty cool and I probably might write another post on them if enough people are interested!\n<li>[2] Also, as you will see in the Auto-differentiation part, it\u2019s not clear how you would calculate the derivatives of words. They\u2019re not even continuous!\n<li>[3] This is the (in)famous backpropagation algorithm and is central to learning in Multilayered neural networks.\n<li>[4] This also means moving the data to GPU or back. I have noticed that in Theano (possibly other frameworks as well), this is the most time-consuming part during execution.\n<li>[5]<a href=\"http://www.deeplearning.net/software/theano/optimizations.html\" target=\"_blank\"> http://www.deeplearning.net/software/theano/optimizations.html</a>\n<li>[6]<a href=\"http://deeplearning.net/software/theano/library/config.html#config.mode\" target=\"_blank\">deeplearning.net/software/theano/library/config.html#config.mode</a>\n<li>[7] d(f(g(x)) / dx = (df / dg) * (dg / dx). cf.<a href=\"https://en.wikipedia.org/wiki/Chain_rule\" target=\"_blank\">en.wikipedia.org/wiki/Chain_rule</a>\n<li>[8] Check out<a href=\"https://colah.github.io/posts/2015-08-Backprop/\" target=\"_blank\"> https://colah.github.io/posts/2015-08-Backprop/</a> for a more detailed discussion of derivatives on computational graphs\n<li>[9]<a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\" target=\"_blank\"> https://en.wikipedia.org/wiki/Automatic_differentiation</a>\n<li>[10]<a href=\"https://www.tensorflow.org/how_tos/adding_an_op/\" target=\"_blank\"> https://www.tensorflow.org/how_tos/adding_an_op/</a>\n<li>[11]<a href=\"http://deeplearning.net/software/theano/tutorial/gradients.html\" target=\"_blank\"> http://deeplearning.net/software/theano/tutorial/gradients.html</a>\n<li>[12]<a href=\"http://deeplearning.net/software/theano/extending/extending_theano_c.html#methods-the-c-op-needs-to-define\" target=\"_blank\">deeplearning.net/software/theano/extending/extending_theano_c.html#methods-the-c-op-needs-to-define</a>\n<li>[13] Both TensorFlow and Caffe do this.\n<li>[14] If you are interested, you can check out the materials of<a href=\"https://www.inf.ethz.ch/personal/markusp/teaching/263-2300-ETH-spring11/course.html,\" target=\"_blank\">www.inf.ethz.ch/personal/markusp/teaching/263-2300-ETH-spring11/course.html,</a> will be taking this next sem\u00a0:)\n<li>[15]<a href=\"http://www.netlib.org/blas/\" target=\"_blank\"> http://www.netlib.org/blas/</a>\n<li>[16]<a href=\"https://developer.nvidia.com/cublas\" target=\"_blank\"> https://developer.nvidia.com/cublas</a>\n<li>[17]<a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\"> https://developer.nvidia.com/cudnn</a>\n</li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n<p><b>Bio: <a href=\"https://www.linkedin.com/in/sgokula/\" target=\"_blank\">Gokula Krishnan Santhanam</a></b> is a Masters Student in CS at ETH Zurich, a Deep Learning Researcher, and a Pythonista.</p>\n<p><a href=\"https://medium.com/@gokul_uf/the-anatomy-of-deep-learning-frameworks-46e2a7af5e47#.v1q8n48pu\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/02/deep-learning-artificial-intelligence-quest-agi.html\">Deep Learning, Artificial Intuition and the Quest for AGI</a>\n<li><a href=\"/2016/08/gentlest-introduction-tensorflow-part-1.html\">The Gentlest Introduction to Tensorflow \u2013 Part 1</a>\n<li><a href=\"/2017/02/why-deep-learning-performs-so-well.html\">3 practical thoughts on why deep learning performs so well</a>\n</li></li></li></ul>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2017/02/continuum-deliver-business-value-open-data-science.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/jobs/17/02-24-aetna-principal-data-scientist.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2017/index.html\">2017</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/02/index.html\">Feb</a> \u00bb <a href=\"https://www.kdnuggets.com/2017/02/software.html\">Software</a> \u00bb The Anatomy of Deep Learning Frameworks (\u00a0<a href=\"/2017/n08.html\">17:n08</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556342650\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.727 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-27 01:24:10 -->\n<!-- Compression = gzip -->", "content_tokenized": ["gokula", "krishnan", "santhanam", "zurich", "deep", "learn", "whether", "like", "not", "here", "stay", "and", "with", "ani", "tech", "goldrush", "come", "plethora", "option", "that", "can", "seem", "daunt", "newcom", "were", "start", "off", "with", "deep", "learn", "one", "the", "first", "question", "ask", "which", "framework", "learn", "say", "instead", "simpl", "trialanderror", "tri", "understand", "the", "build", "block", "all", "these", "framework", "would", "help", "make", "inform", "decis", "common", "choic", "includ", "theano", "tensorflow", "torch", "and", "kera", "all", "these", "choic", "have", "their", "own", "pros", "and", "con", "and", "have", "their", "own", "way", "thing", "after", "explor", "the", "white", "paper", "and", "the", "dev", "doc", "could", "understand", "the", "design", "choic", "and", "abl", "abstract", "the", "fundament", "concept", "that", "are", "common", "all", "these", "this", "post", "have", "tri", "sketch", "out", "these", "common", "principl", "which", "would", "help", "better", "understand", "the", "framework", "and", "for", "the", "brave", "heart", "among", "provid", "guid", "how", "implement", "own", "deep", "learn", "framework", "the", "interest", "thing", "about", "these", "principl", "that", "they", "are", "not", "specif", "alon", "they", "are", "applic", "whenev", "want", "seri", "comput", "data", "henc", "most", "framework", "can", "use", "for", "nondl", "task", "well", "see", "wwwtensorfloworgtutorialsmandelbrot", "there", "are", "few", "core", "compon", "framework", "these", "are", "tensor", "object", "oper", "the", "tensor", "object", "comput", "graph", "and", "optim", "autodifferenti", "tool", "cubla", "and", "cudnn", "extens", "these", "should", "make", "framework", "complet", "but", "would", "need", "polish", "make", "more", "easi", "use", "will", "use", "refer", "the", "python", "numpi", "packag", "this", "articl", "make", "easier", "understand", "haven", "use", "numpi", "befor", "fret", "not", "this", "articl", "should", "easi", "understand", "even", "skip", "the", "numpi", "part", "believ", "understand", "system", "multipl", "level", "abstract", "will", "see", "discuss", "lowlevel", "optim", "interspers", "with", "highlevel", "calculus", "and", "linear", "algebra", "drop", "comment", "below", "someth", "need", "more", "explan", "have", "been", "contributor", "theano", "and", "henc", "might", "bias", "toward", "cite", "refer", "have", "said", "that", "theano", "also", "has", "one", "the", "most", "inform", "websit", "all", "the", "framework", "come", "across", "tensor", "object", "the", "heart", "the", "framework", "the", "tensor", "object", "tensor", "general", "matrix", "ndimens", "think", "numpi", "ndarray", "other", "word", "matrix", "twodimension", "tensor", "with", "row", "column", "easi", "way", "understand", "tensor", "consid", "them", "array", "exampl", "take", "color", "imag", "let", "say", "bitmap", "imag", "size", "num", "num", "height", "width", "this", "threedimension", "tensor", "height", "width", "channel", "take", "look", "the", "follow", "imag", "understand", "this", "better", "taken", "from", "githubcomparambharatcarndhelpersblobmasterimageprocessingimageprocessingtutorialipynb", "normal", "imag", "red", "green", "and", "blue", "channel", "the", "same", "imag", "the", "same", "imag", "repres", "tensor", "extens", "set", "num", "imag", "can", "repres", "tensor", "imag", "height", "width", "channel", "similar", "repres", "all", "input", "data", "tensors\u00b9", "befor", "feed", "them", "into", "the", "neural", "net", "this", "abstract", "necessari", "befor", "can", "feed", "data", "into", "net", "els", "would", "have", "defin", "oper", "that", "work", "each", "type", "and", "that", "lot", "wast", "effort\u00b2", "would", "also", "need", "abl", "get", "back", "data", "form", "want", "need", "tensor", "object", "that", "support", "store", "the", "data", "form", "tensor", "not", "just", "that", "would", "like", "the", "object", "abl", "convert", "other", "data", "type", "imag", "text", "video", "into", "tensor", "and", "back", "think", "someth", "like", "numpyimread", "and", "numpyimsav", "they", "read", "imag", "ndarray", "and", "store", "ndarray", "imag", "respect", "the", "basic", "tensor", "object", "need", "support", "repres", "data", "form", "tensor", "this", "mean", "support", "index", "overload", "oper", "have", "space", "effici", "way", "store", "the", "data", "and", "depend", "further", "design", "choic", "might", "have", "add", "more", "featur", "well", "oper", "the", "tensor", "object", "neural", "network", "can", "consid", "seri", "oper", "perform", "input", "tensor", "give", "output", "learn", "done", "correct", "the", "error", "between", "the", "output", "creat", "the", "net", "and", "the", "expect", "output\u00b3", "these", "oper", "could", "someth", "simpl", "like", "matrix", "multipl", "sigmoid", "more", "complic", "like", "convolut", "pool", "lstms", "sigmoid", "layer", "repres", "matrix", "oper", "from", "wwwdatasciencecentralcomprofilesblogsmatrixmultiplicationinneuralnetwork", "check", "out", "the", "follow", "link", "get", "idea", "how", "extens", "these", "oper", "could", "numpi", "wwwscipylecturesorgintronumpyoperationshtml", "theano", "deeplearningnetsoftwaretheanolibrarytensorbasichtml", "tensorflow", "wwwtensorfloworgapidocspythonmathop", "could", "just", "skip", "this", "part", "and", "ask", "the", "user", "implement", "these", "oper", "themselv", "but", "too", "cumbersom", "and", "outright", "ineffici", "moreov", "most", "oper", "are", "common", "enough", "that", "one", "could", "justifi", "make", "them", "part", "the", "framework", "spare", "headach", "for", "the", "user", "numpi", "doe", "pretti", "good", "job", "have", "lot", "oper", "alreadi", "implement", "insan", "fast", "well", "and", "there", "run", "theano", "issu", "about", "incorpor", "more", "oper", "which", "show", "how", "import", "have", "more", "oper", "support", "the", "framework", "instead", "implement", "oper", "function", "they", "are", "usual", "implement", "class", "this", "allow", "store", "more", "inform", "about", "the", "oper", "like", "calcul", "shape", "the", "output", "use", "for", "saniti", "check", "how", "comput", "the", "gradient", "the", "gradient", "itself", "for", "the", "autodifferenti", "have", "way", "abl", "decid", "whether", "comput", "the", "and", "again", "this", "idea", "similar", "class", "use", "for", "various", "algorithm", "implement", "scikitlearn", "could", "defin", "method", "call", "comput", "that", "doe", "the", "actual", "comput", "and", "return", "the", "tensor", "after", "the", "comput", "done", "these", "class", "are", "usual", "deriv", "from", "abstract", "class", "theano", "the", "class", "this", "enforc", "unifi", "interfac", "across", "the", "and", "also", "provid", "way", "add", "new", "later", "this", "make", "the", "framework", "veri", "flexibl", "and", "ensur", "peopl", "can", "use", "even", "new", "network", "architectur", "and", "nonlinear", "come", "along", "comput", "graph", "and", "optim", "far", "have", "class", "for", "repres", "tensor", "and", "oper", "them", "the", "power", "neural", "network", "lie", "the", "abil", "chain", "multipl", "such", "oper", "form", "power", "approxim", "therefor", "the", "standard", "use", "case", "that", "can", "initi", "tensor", "perform", "action", "after", "action", "them", "and", "final", "interpret", "the", "result", "tensor", "label", "real", "valu", "sound", "simpl", "enough", "chain", "togeth", "differ", "oper", "taken", "from", "colahgithubiopostsnumbackprop", "unfortun", "chain", "more", "and", "more", "oper", "togeth", "sever", "issu", "aris", "that", "can", "drastic", "slow", "down", "code", "and", "introduc", "bug", "well", "start", "later", "onli", "after", "the", "previous", "one", "done", "them", "parallel", "how", "assign", "differ", "devic", "and", "coordin", "between", "them", "how", "avoid", "redund", "oper", "multipli", "with", "one", "zero", "cach", "use", "intermedi", "valu", "and", "reduc", "multipl", "oper", "into", "one", "replac", "num", "num", "with", "one", "there", "are", "more", "such", "issu", "and", "becom", "necessari", "abl", "get", "bigger", "pictur", "even", "notic", "that", "these", "issu", "exist", "need", "way", "optim", "the", "result", "chain", "oper", "for", "both", "space", "and", "time", "order", "get", "bigger", "pictur", "introduc", "comput", "graph", "which", "basic", "object", "that", "contain", "link", "the", "instanc", "various", "and", "the", "relat", "between", "which", "oper", "take", "the", "output", "which", "oper", "well", "addit", "inform", "depend", "the", "framework", "question", "this", "can", "implement", "differ", "way", "for", "exampl", "theano", "deeplearningnetsoftwaretheanoextendinggraphstructureshtml", "tensorflow", "downloadtensorfloworgpaperwhitepapernumpdf", "caff", "tutorialnetlayerblobhtml", "like", "mani", "idea", "deep", "learn", "the", "idea", "comput", "graph", "has", "been", "around", "for", "quit", "some", "time", "take", "look", "ani", "compil", "textbook", "and", "can", "find", "similar", "idea", "abstract", "syntax", "tree", "and", "intermedi", "represent", "use", "for", "optim", "these", "idea", "have", "been", "extend", "and", "adapt", "the", "deep", "learn", "scenario", "give", "the", "comput", "graph", "the", "idea", "optim", "the", "graph", "befor", "code", "generat", "will", "cover", "later", "straightforward", "the", "optim", "themselv", "could", "again", "implement", "class", "functions\u2075", "and", "could", "select", "appli", "depend", "whether", "want", "the", "code", "compil", "fast", "run", "fast\u2076", "addit", "sinc", "get", "bird", "eye", "view", "what", "will", "happen", "the", "network", "the", "graph", "class", "can", "then", "decid", "how", "alloc", "memori", "like", "regist", "alloc", "compil", "and", "coordin", "between", "various", "machin", "when", "deploy", "distribut", "environ", "this", "help", "effect", "solv", "the", "three", "problem", "mention", "abov", "autodifferenti", "tool", "anoth", "benefit", "have", "the", "comput", "graph", "that", "calcul", "gradient", "use", "the", "learn", "phase", "becom", "modular", "and", "straightforward", "comput", "this", "thank", "the", "chain", "rule\u2077", "that", "let", "calcul", "deriv", "composit", "function", "systemat", "way", "saw", "earlier", "neural", "net", "can", "thought", "composit", "simpl", "nonlinear", "give", "rise", "more", "complex", "function", "differenti", "these", "function", "simpli", "travers", "the", "graph", "from", "the", "output", "back", "the", "inputs\u2078", "symbol", "differenti", "automat", "differentiation\u2079", "programmat", "way", "which", "gradient", "can", "comput", "comput", "graph", "symbol", "differenti", "refer", "calcul", "the", "deriv", "analyt", "get", "express", "what", "the", "gradient", "use", "simpli", "plug", "the", "valu", "into", "the", "deriv", "and", "use", "unfortun", "some", "nonlinear", "like", "relu", "rectifi", "linear", "unit", "are", "not", "differenti", "some", "point", "instead", "calcul", "the", "gradient", "iter", "manner", "sinc", "the", "second", "method", "could", "use", "univers", "most", "comput", "graph", "packag", "like", "comput", "graph", "toolkit", "rllberkeleyeducgt", "implement", "autodifferenti", "but", "can", "use", "symbol", "differenti", "are", "creat", "own", "usual", "not", "good", "idea", "roll", "out", "own", "gradient", "comput", "modul", "becaus", "easier", "and", "faster", "for", "the", "toolkit", "provid", "part", "the", "packag", "either", "have", "own", "comput", "graph", "toolkit", "and", "autodifferenti", "modul", "use", "extern", "packag", "for", "both", "sinc", "the", "deriv", "each", "node", "has", "comput", "with", "respect", "onli", "adjac", "node", "the", "method", "comput", "gradient", "can", "the", "class\u00b9\u2070", "and", "can", "invok", "the", "differenti", "modul", "cubla", "and", "cudnn", "extens", "with", "all", "the", "abov", "compon", "can", "stop", "right", "now", "and", "have", "fulli", "function", "deep", "learn", "framework", "would", "abl", "take", "data", "input", "and", "convert", "tensor", "perform", "oper", "them", "effici", "way", "comput", "gradient", "learn", "and", "return", "back", "result", "for", "the", "test", "dataset", "the", "problem", "howev", "lie", "the", "fact", "that", "sinc", "most", "like", "implement", "highlevel", "languag", "java", "python", "lua", "there", "inher", "upper", "limit", "the", "speedup", "can", "get", "this", "becaus", "even", "the", "simplest", "oper", "highlevel", "languag", "take", "more", "time", "cycl", "than", "when", "done", "lowlevel", "languag", "these", "situat", "there", "are", "two", "differ", "approach", "could", "take", "the", "first", "one", "anoth", "analog", "from", "compil", "the", "last", "step", "compil", "process", "hardwar", "specif", "code", "generat", "assembl", "similar", "instead", "run", "the", "graph", "written", "the", "highlevel", "languag", "the", "correspond", "code", "for", "the", "network", "generat", "and", "this", "compil", "and", "execut", "the", "code", "for", "this", "store", "each", "the", "and", "can", "combin", "togeth", "the", "compil", "phase\u00b9\u00b2", "transfer", "data", "and", "from", "the", "lowlevel", "highlevel", "code", "done", "wrapper", "like", "pycuda", "and", "cython", "the", "second", "approach", "have", "backend", "implement", "lowlevel", "languag", "like", "this", "mean", "that", "the", "lowlevel", "languag", "highlevel", "languag", "interact", "intern", "the", "framework", "unlik", "the", "previous", "approach", "and", "could", "faster", "becaus", "need", "compil", "the", "entir", "graph", "everi", "time", "instead", "could", "just", "call", "the", "compil", "method", "with", "the", "appropri", "arguments\u00b9\u00b3", "anoth", "sourc", "nonoptim", "behavior", "come", "from", "slow", "implement", "the", "lowlevel", "languag", "difficult", "write", "effici", "code\u00b9\u2074", "and", "will", "better", "off", "use", "librari", "that", "have", "optim", "implement", "these", "method", "asic", "inear", "lgebra", "ubprogram", "are", "collect", "optim", "matrix", "oper", "initi", "written", "fortran\u00b9\u2075", "these", "can", "leverag", "veri", "fast", "matrix", "tensor", "oper", "and", "can", "provid", "signific", "speedup", "there", "are", "mani", "other", "softwar", "packag", "like", "intel", "which", "also", "perform", "similar", "function", "which", "one", "choos", "person", "prefer", "packag", "are", "usual", "optim", "assum", "that", "the", "instruct", "will", "run", "the", "deep", "learn", "situat", "this", "not", "the", "case", "and", "may", "not", "abl", "fulli", "exploit", "the", "parallel", "offer", "gpgpus", "solv", "this", "issu", "has", "releas", "cublas\u00b9\u2076", "which", "optim", "for", "gpus", "this", "now", "includ", "with", "the", "toolkit", "and", "probabl", "whi", "not", "mani", "peopl", "have", "heard", "final", "cudnn\u00b9\u2077", "librari", "that", "build", "the", "featureset", "cubla", "and", "provid", "optim", "neural", "network", "specif", "oper", "like", "winograd", "convolut", "and", "rnns", "use", "these", "packag", "could", "gain", "signific", "speedup", "framework", "speedup", "are", "import", "becaus", "the", "differ", "between", "train", "neural", "network", "four", "hour", "instead", "four", "day", "the", "fast", "move", "world", "startup", "that", "the", "differ", "between", "pioneer", "and", "play", "game", "catchup", "exploit", "parallel", "and", "optim", "librari", "wherev", "can", "conclus", "have", "final", "come", "the", "end", "pretti", "long", "post", "thank", "lot", "for", "read", "hope", "have", "demystifi", "the", "anatomi", "deep", "learn", "framework", "for", "mani", "main", "goal", "write", "this", "post", "make", "concret", "understand", "how", "differ", "framework", "are", "essenti", "the", "same", "thing", "this", "will", "veri", "help", "exercis", "for", "anyon", "who", "abov", "novic", "level", "but", "below", "pro", "level", "semipro", "will", "onc", "can", "understand", "how", "thing", "work", "behind", "the", "scene", "they", "becom", "easier", "approach", "and", "master", "framework", "great", "job", "abstract", "away", "most", "these", "idea", "order", "provid", "simpl", "interfac", "for", "the", "programm", "wonder", "that", "most", "these", "concept", "are", "not", "veri", "evid", "when", "are", "learn", "framework", "someon", "who", "interest", "not", "just", "the", "applic", "deep", "learn", "but", "also", "the", "fundament", "challeng", "the", "field", "believ", "that", "know", "how", "thing", "work", "under", "the", "hood", "import", "step", "toward", "masteri", "the", "topic", "clear", "out", "mani", "the", "misunderstand", "and", "provid", "simpler", "way", "think", "about", "whi", "thing", "are", "the", "way", "they", "are", "sincer", "believ", "that", "good", "worker", "know", "not", "just", "which", "tool", "use", "but", "also", "whi", "that", "tool", "the", "best", "choic", "this", "blog", "step", "that", "direct", "hope", "enjoy", "read", "this", "post", "much", "write", "pleas", "let", "know", "thought", "the", "comment", "below", "found", "this", "interest", "and", "want", "know", "more", "about", "mehir", "intern", "love", "hear", "from", "can", "find", "here", "footnot", "num", "not", "straightforward", "how", "would", "repres", "text", "tensor", "the", "first", "way", "use", "onehot", "encod", "which", "veri", "spars", "matrix", "and", "wast", "lot", "space", "more", "dens", "represent", "word", "vector", "these", "are", "pretti", "cool", "and", "probabl", "might", "write", "anoth", "post", "them", "enough", "peopl", "are", "interest", "num", "also", "will", "see", "the", "autodifferenti", "part", "not", "clear", "how", "would", "calcul", "the", "deriv", "word", "they", "not", "even", "continu", "num", "this", "the", "famous", "backpropag", "algorithm", "and", "central", "learn", "multilay", "neural", "network", "num", "this", "also", "mean", "move", "the", "data", "back", "have", "notic", "that", "theano", "possibl", "other", "framework", "well", "this", "the", "most", "timeconsum", "part", "dure", "execut", "num", "softwaretheanooptimizationshtml", "num", "deeplearningnetsoftwaretheanolibraryconfightml", "configmod", "num", "enwikipediaorgwikichainrul", "num", "check", "out", "postsnumbackprop", "for", "more", "detail", "discuss", "deriv", "comput", "graph", "num", "wikiautomaticdifferenti", "num", "howtosaddinganop", "num", "softwaretheanotutorialgradientshtml", "num", "deeplearningnetsoftwaretheanoextendingextendingtheanochtml", "methodsthecopneedstodefin", "num", "both", "tensorflow", "and", "caff", "this", "num", "are", "interest", "can", "check", "out", "the", "materi", "wwwinfethzchpersonalmarkuspteachingnumethspringnumcoursehtml", "will", "take", "this", "next", "sem", "num", "blas", "num", "cubla", "num", "cudnn", "bio", "gokula", "krishnan", "santhanam", "master", "student", "zurich", "deep", "learn", "research", "and", "pythonista", "origin", "repost", "with", "permiss", "relat", "deep", "learn", "artifici", "intuit", "and", "the", "quest", "for", "the", "gentlest", "introduct", "tensorflow", "part", "num", "num", "practic", "thought", "whi", "deep", "learn", "perform", "well"], "timestamp_scraper": 1556377982.765535, "title": "The Anatomy of Deep Learning Frameworks", "read_time": 835.8, "content_html": "<div class=\"post\" id=\"post-\">\n<p><b>Gokula Krishnan Santhanam, ETH Zurich.</b></p>\n<p>Deep Learning, whether you like it or not is here to stay, and with any tech gold-rush comes a plethora of options that can seem daunting to newcomers.</p>\n<p>If you were to start off with deep learning, one of the first questions to ask is, which framework to learn? I\u2019d say instead of a simple trial-and-error, if you try to understand the building blocks of all these frameworks, it would help you make an informed decision. Common choices include <a href=\"http://deeplearning.net/software/theano/\" target=\"_blank\">Theano</a>, <a href=\"https://www.tensorflow.org/\" target=\"_blank\">TensorFlow</a>, <a href=\"http://torch.ch/\" target=\"_blank\">Torch</a>, and <a href=\"https://keras.io/\" target=\"_blank\">Keras</a>. All of these choices have their own pros and cons and have their own way of doing things. After exploring the white papers and the dev docs, I could understand the design choices and was able to abstract the fundamental concepts that are common to all of these.</p>\n<p>In this post, I have tried to sketch out these common principles which would help you better understand the frameworks and for the brave hearts among you, provide a guide on how to implement your own deep learning framework. The interesting thing about these principles is that they are not specific to DL alone, they are applicable whenever you want to do a series of computations on data. Hence, most DL frameworks can be used for non-DL tasks as well (see <a href=\"https://www.tensorflow.org/tutorials/mandelbrot/\" target=\"_blank\">www.tensorflow.org/tutorials/mandelbrot/</a>)</p>\n<p>There are a few core components of a DL framework, these are:</p>\n<ol class=\"three_ul\">\n<li>A Tensor Object\n<li>Operations on the Tensor Object\n<li>A Computation Graph and Optimizations\n<li>Auto-differentiation tools\n<li>BLAS / cuBLAS and cuDNN extensions\n</li></li></li></li></li></ol>\n<p>These should make your framework complete, but you would need to polish it to make it more easy to use. I will be using references to the Python NumPy package in this article to make it easier to understand. If you haven\u2019t used NumPy before, fret not, this article should be easy to understand even if you skip the numpy parts. I\u2019m a believer of understanding a system at multiple levels of abstractions so you will be seeing discussions of low-level optimizations interspersed with high-level Calculus and Linear Algebra. Drop a comment below if something needs more explanation!</p>\n<p><strong>NOTE</strong>: I have been a contributor to Theano and hence might be biased towards it in citing references. Having said that, theano also has one of the most informative websites of all the frameworks I\u2019ve come across.</p>\n<h3>A Tensor Object</h3>\n<p>\u00a0<br>\nAt the heart of the framework is the tensor object. A tensor is a generalization of a matrix to n-dimensions (think numpy\u2019s ndarrays). In other words, a matrix is a two-dimensional tensor with (rows, columns). An easy way to understand tensors is to consider them as N-D Arrays.</br></p>\n<p>As an example, take a color image. Let\u2019s say it\u2019s an RGB Bitmap image of size 258 x 320 (Height x Width). This is a three-dimensional tensor (height, width, channels). Take a look at the following images to understand this better (taken from: <a href=\"https://github.com/parambharat/CarND-helpers/blob/master/image_processing/Image_processing_tutorial.ipynb\" target=\"_blank\">github.com/parambharat/CarND-helpers/blob/master/image_processing/Image_processing_tutorial.ipynb</a>)</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/0*sEsx4_AW715qbGj_.\" width=\"80%\"/><br>\nA Normal RGB\u00a0Image</br></center></p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*ive85-ts9v9Di0zdCXHVIA.png\" width=\"80%\"/><br>\nRed, Green and Blue Channels of the same\u00a0image</br></center></p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/0*fLd66pq-hGr4sjoS.\" width=\"80%\"/><br/>\nThe same image represented as a 3D\u00a0Tensor</center></p>\n<p>As an extension, a set of 100 images can be represented as a 4D tensor (ID of image, height, width, channels).</p>\n<p>Similarly, we represent all input data as tensors\u00b9 before feeding them into the neural net. This is an abstraction necessary before we can feed data into a net, else we would have to define operations that work on each type and that\u2019s a lot of wasted effort\u00b2. We would also need to be able to get back data in a form we want.</p>\n<p>So, we need a Tensor Object that supports storing the data in form of tensors. Not just that, we would like the object to be able to convert other data types (images, text, video) into tensors and back. Think of something like numpy.imread and numpy.imsave, they read images as ndarrays and store ndarrays as images respectively.</p>\n<p>The basic Tensor object needs to support representing data in form of a tensor. This means supporting indexing, overloading operators, having a space efficient way to store the data and so on. Depending on further design choices, you might have to add more features as well.</p>\n<h3>Operations on the Tensor Object</h3>\n<p>\u00a0<br/>\nA neural network can be considered as a series of Operations performed on an input tensor to give an output. Learning is done by correcting the errors between the output created by the net and the expected output\u00b3. These operations could be something simple like matrix multiplication (in sigmoids) or more complicated like convolutions, pooling or LSTMs.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/0*oTsCkyvpRJVybDOr.\" width=\"99%\"/><br/>\nSigmoid layer represented as Matrix Operations, from <a href=\"http://www.datasciencecentral.com/profiles/blogs/matrix-multiplication-in-neural-networks\" target=\"_blank\">www.datasciencecentral.com/profiles/blogs/matrix-multiplication-in-neural-networks</a></center></p>\n<p>Check out the following links to get an idea of how extensive these operations could be:</p>\n<ol class=\"three_ul\">\n<li>NumPy: <a href=\"http://www.scipy-lectures.org/intro/numpy/operations.html\" target=\"_blank\">www.scipy-lectures.org/intro/numpy/operations.html</a>\n<li>Theano: <a href=\"http://deeplearning.net/software/theano/library/tensor/basic.html\" target=\"_blank\">deeplearning.net/software/theano/library/tensor/basic.html</a>\n<li>TensorFlow: <a href=\"https://www.tensorflow.org/api_docs/python/math_ops/\" target=\"_blank\">www.tensorflow.org/api_docs/python/math_ops/</a>\n</li></li></li></ol>\n<p>You could just skip this part and ask the user to implement these operations themselves, but it is too cumbersome and outright inefficient. Moreover, most operations are common enough that one could justify making them a part of the framework to spare headaches for the users. NumPy does a pretty good job of having a lot of operations already implemented (it\u2019s insanely fast as well) and there is a running theano <a href=\"https://github.com/Theano/Theano/issues/1916\" target=\"_blank\">issue</a> about incorporating more operations which show how important it is to have more operation supported by the framework.</p>\n<p>Instead of implementing operations as functions, they are usually implemented as classes. This allows us to store more information about the operation like calculated shape of the output (useful for sanity checks), how to compute the gradient or the gradient itself (for the auto-differentiation), have ways to be able to decide whether to compute the op on GPU or CPU\u2074 and so on. Again, this idea is similar to classes used for various algorithms implemented by scikit-learn. You could define a method called <em>compute</em> that does the actual computation and returns the tensor after the computation is done.</p>\n<p>These classes are usually derived from an abstract class (in theano, it\u2019s the <em>Op</em> class). This enforces a unified interface across the Ops and also provide a way to add new ops later on. This makes the framework very flexible and ensures people can use it even as new network architectures and nonlinearities come along.</p>\n<h3>Computation Graph and Optimizations</h3>\n<p>\u00a0<br/>\nSo far, we have classes for representing tensors and operations on them. The power of neural networks lies in the ability to chain multiple such operations to form a powerful approximator.</p>\n<p>Therefore, the standard use case is that you can initialize a tensor, perform actions after actions on them and finally interpret the resulting tensor as labels or real values. Sounds simple enough?</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/0*8YJSNIElgsCe8GRR.\" width=\"60%\"/><br/>\nChaining together different operations, taken from <a href=\"https://colah.github.io/posts/2015-08-Backprop/\" target=\"_blank\">colah.github.io/posts/2015-08-Backprop/</a></center></p>\n<p>Unfortunately, as you chain more and more operations together, several issues arise that can drastically slow down your code and introduce bugs as well.</p>\n<ol class=\"three_ul\">\n<li>Start later ops only after the previous one is done or do them in parallel?\n<li>How to assign to different devices and coordinate between them?\n<li>How do you avoid redundant operations (multiplying with ones, adding zeros), cache useful intermediate values, and reduce multiple operations into one ( replace <em>mul(mul(mul(Tensor,2),2),2)</em> with one <em>mul(Tensor, 8)</em> )\n</li></li></li></ol>\n<p>There are more such issues and it becomes necessary to be able to get a bigger picture to even notice that these issues exist. We need a way to optimize the resultant chain of operations for both space and time.</p>\n<p>In order to get a bigger picture, we introduce a Computation Graph which is basically an object that contains links to the instances of various Ops and the relations between which operation takes the output of which operation as well as additional information. Depending on the framework in question, this can be implemented in different ways.</p>\n<p>For example,</p>\n<ol class=\"three_ul\">\n<li>Theano <a href=\"http://deeplearning.net/software/theano/extending/graphstructures.html\" target=\"_blank\">deeplearning.net/software/theano/extending/graphstructures.html</a>\n<li>TensorFlow <a href=\"http://download.tensorflow.org/paper/whitepaper2015.pdf\" target=\"_blank\">download.tensorflow.org/paper/whitepaper2015.pdf</a>\n<li>Caffe<a href=\"http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html\" target=\"_blank\"> http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html</a>\n</li></li></li></ol>\n<p>Like many ideas in Deep Learning, the idea of computation graphs has been around for quite some time. Take a look at any Compilers textbook and you can find similar ideas in Abstract Syntax Trees and Intermediate Representations used for Optimization. These ideas have been extended and adapted to the Deep Learning scenario to give us the computational graph. The idea of optimizing the graph before code generation (will be covered later) is straightforward. The optimizations themselves could again be implemented as classes or functions\u2075 and could be selectively applied depending on whether you want the code to compile fast or run fast\u2076.</p>\n<p>Additionally, since you get a bird\u2019s eye view of what will be happening in the network, the graph class can then decide on how to allocate GPU memory (like Register Allocation in Compilers) and coordinate between various machines when deployed in a distributed environment. This helps us to effectively solve the three problems mentioned above.</p>\n<h3>Auto-differentiation tools</h3>\n<p>\u00a0<br/>\nAnother benefit of having the computational graph is that calculating gradients used in the learning phase becomes modular and straightforward to compute. This is thanks to the chain rule\u2077 that lets you calculate derivatives of composition of functions in a systematic way. As we saw earlier, neural nets can be thought of composition of simple nonlinearities giving rise to more complex functions. Differentiating these functions is simply traversing the graph from the outputs back to the inputs\u2078. Symbolic Differentiation or Automatic Differentiation\u2079 is a programmatic way by which gradients can be computed in a computation graph.</p>\n<p>Symbolic differentiation refers to calculating the derivatives analytically i.e., you get an expression of what the gradient is. To use it, you simply plug in the values into the derivative and use it. Unfortunately, some nonlinearities like ReLU (Rectified Linear Units) are not differentiable at some points. So, we instead calculate the gradient in an iterative manner. Since the second method could be used universally, most computational graph packages like Computation Graph Toolkit (<a href=\"http://rll.berkeley.edu/cgt/\" target=\"_blank\">rll.berkeley.edu/cgt/</a>) implement auto-differentiation but you can use symbolic differentiation if you are creating your own.</p>\n<p>It is usually not a good idea to roll out your own gradient computation module because it is easier and faster for the toolkit to provide it as part of the package. So, either have your own Computation Graph toolkit and auto-differentiation module or use an external package for both.</p>\n<p>Since the derivative at each node has to computed with respect to only its adjacent nodes, the method to compute gradients can be added to the class\u00b9\u2070 \u00b9\u00b9 and can be invoked by the differentiation module.</p>\n<h3>BLAS / cuBLAS and cuDNN extensions</h3>\n<p>\u00a0<br/>\nWith all the above components, you can stop right now and have a fully functional Deep Learning framework. It would be able to take data as input and convert to tensors, perform operations on them in an efficient way, compute gradients to learn and return back results for the test dataset. The problem, however, lies in the fact that since you most likely implemented it in a high-level language (Java / Python / Lua), there is an inherent upper limit to the speedups you can get. This is because even the simplest operations in a high-level language take more time (CPU cycles) than when done in a low-level language.</p>\n<p>In these situations, there are two different approaches we could take.</p>\n<p>The first one is an another analogy from compilers. The last step of a compilation process is hardware specific code generation in Assembly. Similarly, instead of running the graph written in the high-level language, the corresponding code for the network is generated in C and this is compiled and executed. The code for this is stored in each of the Ops and can be combined together in the compilation phase\u00b9\u00b2. Transferring data to and from the low-level to high-level code is done by wrappers like pyCUDA and Cython.</p>\n<p>The second approach is to have a backend implemented in a low-level language like C++, this means that the low-level language\u200a\u2014\u200ahigh-level language interaction is internal to the framework unlike the previous approach and could be faster because we don\u2019t need to compile the entire graph every time. Instead, we could just call the compiled methods with the appropriate arguments\u00b9\u00b3.</p>\n<p>Another source of non-optimal behavior comes from slow implementations at the low-level language. It is difficult to write efficient code\u00b9\u2074 and we will be better off using libraries that have optimized implementations of these methods. <strong>BLAS</strong> or <strong>B</strong>asic <strong>L</strong>inear <strong>A</strong>lgebra<strong> S</strong>ubprograms are a collection of optimized matrix operations, initially written in Fortran\u00b9\u2075. These can be leveraged to do very fast matrix (tensor) operations and can provide significant speedups. There are many other software packages like Intel MKL, ATLAS which also perform similar functions. Which one to choose is a personal preference.</p>\n<p>BLAS packages are usually optimized assuming that the instructions will be run on a CPU. In the deep learning situation, this is not the case and BLAS may not be able to fully exploit the parallelism offered by GPGPUs. To solve this issue, NVIDIA has released cuBLAS\u00b9\u2076 which is optimized for GPUs. This is now included with the CUDA toolkit and is probably why not many people have heard of it. Finally, cuDNN\u00b9\u2077 is a library that builds on the featureset of cuBLAS and provides optimized Neural Network specific operations like Winograd Convolution and RNNs.</p>\n<p>So, by using these packages, you could gain significant speed-ups in your framework. Speed-ups are important in DL because it is the difference between training a neural network in four hours instead of four days. In the fast moving world of AI startups, that\u2019s the difference between being a pioneer and playing a game of catch-up. So exploit parallelism and optimized libraries wherever you can!</p>\n<h3>Conclusion</h3>\n<p>\u00a0<br/>\nWe have finally come to the end of a pretty long post, thanks a lot for reading it. I hope I have demystified the anatomy of deep learning frameworks for many of you. My main goal in writing this post was to make concrete my understanding of how different frameworks are doing essentially the same thing. This will be a very helpful exercise for anyone who\u2019s above a novice level but below a pro level (a semi-pro, if you will). Once you can understand how things work behind the scenes, they become easier to approach and master. Frameworks do a great job in abstracting away most of these ideas in order to provide a simple interface for the programmers. No wonder that most of these concepts are not very evident when you are learning a framework.</p>\n<p>As someone who\u2019s interested in not just the applications of Deep Learning but also in the fundamental challenges of the field, I believe that knowing how things work under the hood is an important step towards mastery of the topic as it clears out many of the misunderstandings and provides a simpler way to think about why things are the way they are. I sincerely believe that a good worker knows not just which tool to use but also why that tool is the best choice. This blog is a step in that direction.<br/>\nHope you enjoyed reading this post as much as I did writing it. Please do let me know your thoughts in the comments below!</p>\n<p>If you found this interesting, and want to know more about me/hire me as an intern, I\u2019d love to hear from you! You can find my CV <a href=\"https://goo.gl/wHPF9y\" target=\"_blank\">here.</a></p>\n<h3>Footnotes</h3>\n<ul>\n<li>[1] It\u2019s not straightforward how you would represent text as tensors. The first way is to use a one-hot encoding, which is a very sparse matrix and wastes a lot of space. A more dense representation is word vectors. These are pretty cool and I probably might write another post on them if enough people are interested!\n<li>[2] Also, as you will see in the Auto-differentiation part, it\u2019s not clear how you would calculate the derivatives of words. They\u2019re not even continuous!\n<li>[3] This is the (in)famous backpropagation algorithm and is central to learning in Multilayered neural networks.\n<li>[4] This also means moving the data to GPU or back. I have noticed that in Theano (possibly other frameworks as well), this is the most time-consuming part during execution.\n<li>[5]<a href=\"http://www.deeplearning.net/software/theano/optimizations.html\" target=\"_blank\"> http://www.deeplearning.net/software/theano/optimizations.html</a>\n<li>[6]<a href=\"http://deeplearning.net/software/theano/library/config.html#config.mode\" target=\"_blank\">deeplearning.net/software/theano/library/config.html#config.mode</a>\n<li>[7] d(f(g(x)) / dx = (df / dg) * (dg / dx). cf.<a href=\"https://en.wikipedia.org/wiki/Chain_rule\" target=\"_blank\">en.wikipedia.org/wiki/Chain_rule</a>\n<li>[8] Check out<a href=\"https://colah.github.io/posts/2015-08-Backprop/\" target=\"_blank\"> https://colah.github.io/posts/2015-08-Backprop/</a> for a more detailed discussion of derivatives on computational graphs\n<li>[9]<a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\" target=\"_blank\"> https://en.wikipedia.org/wiki/Automatic_differentiation</a>\n<li>[10]<a href=\"https://www.tensorflow.org/how_tos/adding_an_op/\" target=\"_blank\"> https://www.tensorflow.org/how_tos/adding_an_op/</a>\n<li>[11]<a href=\"http://deeplearning.net/software/theano/tutorial/gradients.html\" target=\"_blank\"> http://deeplearning.net/software/theano/tutorial/gradients.html</a>\n<li>[12]<a href=\"http://deeplearning.net/software/theano/extending/extending_theano_c.html#methods-the-c-op-needs-to-define\" target=\"_blank\">deeplearning.net/software/theano/extending/extending_theano_c.html#methods-the-c-op-needs-to-define</a>\n<li>[13] Both TensorFlow and Caffe do this.\n<li>[14] If you are interested, you can check out the materials of<a href=\"https://www.inf.ethz.ch/personal/markusp/teaching/263-2300-ETH-spring11/course.html,\" target=\"_blank\">www.inf.ethz.ch/personal/markusp/teaching/263-2300-ETH-spring11/course.html,</a> will be taking this next sem\u00a0:)\n<li>[15]<a href=\"http://www.netlib.org/blas/\" target=\"_blank\"> http://www.netlib.org/blas/</a>\n<li>[16]<a href=\"https://developer.nvidia.com/cublas\" target=\"_blank\"> https://developer.nvidia.com/cublas</a>\n<li>[17]<a href=\"https://developer.nvidia.com/cudnn\" target=\"_blank\"> https://developer.nvidia.com/cudnn</a>\n</li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n<p><b>Bio: <a href=\"https://www.linkedin.com/in/sgokula/\" target=\"_blank\">Gokula Krishnan Santhanam</a></b> is a Masters Student in CS at ETH Zurich, a Deep Learning Researcher, and a Pythonista.</p>\n<p><a href=\"https://medium.com/@gokul_uf/the-anatomy-of-deep-learning-frameworks-46e2a7af5e47#.v1q8n48pu\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2017/02/deep-learning-artificial-intelligence-quest-agi.html\">Deep Learning, Artificial Intuition and the Quest for AGI</a>\n<li><a href=\"/2016/08/gentlest-introduction-tensorflow-part-1.html\">The Gentlest Introduction to Tensorflow \u2013 Part 1</a>\n<li><a href=\"/2017/02/why-deep-learning-performs-so-well.html\">3 practical thoughts on why deep learning performs so well</a>\n</li></li></li></ul>\n</div> ", "website": "kdnuggets"}