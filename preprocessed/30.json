{"content": "comments By Rohith Gandhi G Neural networks are machine learning algorithms that provide state of the accuracy on many use cases. But, a lot of times the accuracy of the network we are building might not be satisfactory or might not take us to the top positions on the leaderboard in data science competitions. Therefore, we are always looking for better ways to improve the performance of our models. There are many techniques available that could help us achieve that. Follow along to get to know them and to build your own accurate neural network. \u00a0 Check for Overfitting \u00a0 The first step in ensuring your neural network performs well on the testing data is to verify that your neural network does not overfit. Ok, stop, what is overfitting? overfitting happens when your model starts to memorise values from the training data instead of learning from them. Therefore, when your model encounters a data it hasn\u2019t seen before, it is unable to perform well on them. To give you a better understanding, let\u2019s look at an analogy. We all would have a classmate who is good at memorising, and suppose a test on maths is coming up. You and your friend, who is good at memorising start studying from the text book. Your friend goes on memorising each formula, question and answer from the textbook but you, on the other hand, are smarter than him, so you decide to build on intuition and work out problems and learn how these formulas come into play. Test day arrives, if the problems in the test paper are taken straight out of the textbooks, then you can expect your memorising friend to do better on it but, if the problems are new ones that involve applying intuition, you do better on the test and your memorising friend fails miserably. How to identify if your model is overfitting? you can just cross check the training accuracy and testing accuracy. If training accuracy is much higher than testing accuracy then you can posit that your model has overfitted. You can also plot the predicted points on a graph to verify. There are some techniques to avoid overfitting: Regularisation of data (L1 or L2). Dropouts\u200a\u2014\u200aRandomly dropping connections between neurons, forcing the network to find new paths and generalise. Early Stopping\u200a\u2014\u200aPrecipitates the training of the neural network, leading to reduction in error in the test set. \u00a0 Hyperparameter Tuning \u00a0 Hyperparameters are values that you must initialise to the network, these values can\u2019t be learned by the network while training. E.x: In a convolutional neural network, some of the hyperparameters are kernel size, the number of layers in the neural network, activation function, loss function, optimizer , batch size, number of epochs to train etc. Each neural network will have its best set of hyperparameters which will lead to maximum accuracy. You might ask, \u201cthere are so many hyperparameters, how do I choose what to use for each?\u201d, Unfortunately, there is no direct method to identify the best set of hyperparameter for each neural network so it is mostly obtained through trial and error. But, there are some best practices for some hyperparameters which are mentioned below, Learning Rate\u200a\u2014\u200aChoosing an optimum learning rate is important as it decides whether your network converges to the global minima or not. Selecting a high learning rate almost never gets you to the global minima as you have a very good chance of overshooting it. Therefore, you are always around the global minima but never converge to it. Selecting a small learning rate can help a neural network converge to the global minima but it takes a huge amount of time. Therefore, you have to train the network for a longer period of time. A small learning rate also makes the network susceptible to getting stuck in local minimum. i.e the network will converge onto a local minima and unable to come out of it due to the small learning rate. Therefore, you must be careful while setting the learning rate. Network Architecture\u200a\u2014\u200aThere is no standard architecture that gives you high accuracy in all test cases. You have to experiment, try out different architectures, obtain inference from the result and try again. One idea that I would suggest is to use proven architectures instead of building one of your own. E.x: for image recognition task, you have VGG net, Resnet, Google\u2019s Inception network etc. These are all open sourced and have proven to be highly accurate, therefore, you could just copy their architecture and tweak them for your purpose. Optimizers and Loss function\u200a\u2014\u200aThere is a myriad of options available for you to choose from. In fact, you could even define your custom loss function if necessary. But the commonly used optimizers are RMSprop, Stochastic Gradient Descent and Adam. These optimizers seem to work for most of the use cases. Commonly used loss functions are categorical cross entropy if your use case is a classification task. If you are performing a regression task, mean squared error is the commonly used loss function. Feel free to experiment with the hyperparameters of these optimizers and also with different optimizers and loss functions. Batch Size & Number of Epochs\u200a\u2014\u200aAgain, there is no standard value for batch size and epochs that works for all use cases. You have to experiment and try out different ones. In general practice, batch size values are set as either 8, 16, 32\u2026 The number of epochs depends on the developer\u2019s preference and the computing power he/she has. ReLU Activation Function Activation Function\u200a\u2014\u200aActivation functions map the non-linear functional inputs to the outputs. Activation functions are highly important and choosing the right activation function helps your model to learn better. Nowadays, Rectified Linear  is the most widely used activation function as it solves the problem of vanishing gradients. Earlier Sigmoid and Tanh were the most widely used activation function. But, they suffered from the problem of vanishing gradients, i.e during backpropagation, the gradients diminish in value when they reach the beginning layers. This stopped the neural network from scaling to bigger sizes with more layers. ReLU was able to overcome this problem and hence allowed neural networks to be of large sizes. \u00a0 Ensemble of Algorithms \u00a0 If individual neural networks are not as accurate as you would like them to be, you can create an ensemble of neural networks and combine their predictive power. You can choose different neural network architectures and train them on different parts of the data and ensemble them and use their collective predictive power to get high accuracy on test data. Suppose, you are building a cats vs dogs classifier, 0-cat and 1-dog. When combining different cats vs dogs classifiers, the accuracy of the ensemble algorithm increases based on the Pearson Correlation between the individual classifiers. Let us look at an example, take 3 models and measure their individual accuracy. Ground Truth: 1111111111\r Classifier 1: 1111111100 = 80% accuracy\r Classifier 2: 1111111100 = 80% accuracy\r Classifier 3: 1011111100 = 70% accuracy The Pearson Correlation of the three models is high. Therefore, ensembling them does not improve the accuracy. If we ensemble the above three models using a majority vote, we get the following result. Ensemble Result: 1111111100 = 80% accuracy Now, let us look at three models having a very low Pearson Correlation between their outputs. Ground Truth: 1111111111\r Classifier 1: 1111111100 = 80% accuracy\r Classifier 2: 0111011101 = 70% accuracy\r Classifier 3: 1000101111 = 60% accuracy When we ensemble these three weak learners, we get the following result. Ensemble Result: 1111111101 = 90% accuracy As you can see above, an ensemble of weak learners with low Pearson Correlation is able to outperform an ensemble with high Pearson Correlation between them. \u00a0 Dearth of\u00a0Data \u00a0 After performing all of the techniques above, if your model still doesn\u2019t perform better in your test dataset, it could be ascribed to the lack of training data. There are many use cases where the amount of training data available is restricted. If you are not able to collect more data then you could resort to data augmentation techniques. Data Augmentation Techniques If you are working on a dataset of images, you can augment new images to the training data by shearing the image, flipping the image, randomly cropping the image etc. This could provide different examples for the neural network to train on. \u00a0 Conclusion \u00a0 These techniques are considered as best practices and often seem to be effective in increasing the model\u2019s ability to learn features. This might seem like a long post, thank you for reading through it and let me know if any of these techniques did work for you\u00a0:) \u00a0 Bio: Rohith Gandhi G : \"What I cannot create, I do not understand\" - Richard Feynman. Original . Reposted with permission. Related: Is Learning Rate Useful in Artificial Neural Networks? Regularization in Machine Learning Top 8 Free Must-Read Books on Deep Learning", "title_html": "<h1 id=\"title\">Improving the Performance of a Neural Network</h1> ", "url": "https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html", "tfidf": {"tfidf": {"after": 1.02070207021, "hand": 1.6152202665600002, "myriad": 46.285714285699996, "googl": 11.388809182200001, "plot": 5.383519837230001, "pearson": 117.9494799405, "data": 47.27009783076, "wide": 3.1196698762, "neuron": 64.2753036437, "way": 1.2190739461, "would": 3.2486187845399996, "thank": 6.00681044268, "number": 4.40571666436, "miser": 38.8166259169, "taken": 1.6012102874399998, "etc": 12.6200317965, "dataset": 387.219512196, "function": 37.431625274999995, "onto": 4.47589512264, "dog": 12.52544378698, "descent": 8.494382022469999, "learner": 150.4834123222, "crop": 9.71009174312, "resnet": 1443.27272727, "well": 2.1311497416, "whether": 2.20683903253, "never": 3.11538461538, "path": 4.6421052631599995, "higher": 2.1218925421, "know": 5.1865403463, "their": 5.07739542025, "scale": 3.7469907953699995, "measur": 2.41093394077, "instead": 3.18923262354, "creat": 2.4985835694, "how": 4.80750984153, "test": 29.22778242681, "repost": 933.882352941, "truth": 11.491856677519998, "than": 2.0655737705, "increas": 2.6404989605, "due": 1.23789473684, "correl": 65.930232558, "deep": 3.6279707495399998, "will": 3.67443295788, "consid": 1.2397313759200002, "purpos": 2.23416830847, "goe": 4.251740760580001, "new": 3.0536641662, "below": 2.25607503197, "perform": 9.1883862255, "but": 8.13059343192, "were": 1.02458857696, "obtain": 5.37258883248, "our": 2.35758835759, "classif": 8.067073170730001, "output": 15.353965183760002, "expect": 2.20011086475, "point": 1.25990000794, "hyperparamet": 11546.18181816, "has": 2.0872995004, "have": 9.134053570259999, "arriv": 2.03173790632, "stop": 6.535126234920001, "vote": 3.0011342155, "reduct": 6.320063694269999, "use": 15.444581360699999, "experi": 5.61187698834, "out": 5.30083472455, "model": 25.0871740848, "optimum": 58.1538461538, "suffer": 2.16117615029, "good": 4.55944859277, "alway": 4.13491340018, "much": 1.1942229577299999, "forc": 1.32399299475, "huge": 4.38927287808, "optim": 69.2267441862, "classmat": 41.023255814, "global": 13.22448979592, "loss": 14.551787351040002, "suscept": 20.889473684200002, "not": 9.14106583071, "involv": 1.4498630137000001, "day": 1.18371607516, "shear": 56.2978723404, "happen": 2.96359902931, "tweak": 113.4, "almost": 1.53584212054, "then": 3.25973581548, "they": 2.06034650574, "choos": 20.894972361150003, "reach": 1.49801849406, "effect": 1.3963060686000002, "let": 13.94466403164, "scienc": 2.31969608416, "batch": 142.7056179776, "featur": 1.52712581762, "practic": 5.11304347827, "major": 1.14852058164, "stuck": 18.945107398599998, "rmsprop": 1443.27272727, "provid": 2.43105428374, "valu": 13.666571018640003, "either": 1.5830092731099998, "develop": 1.1955719557200002, "accuraci": 255.24115755600002, "with": 6.007189253939998, "network": 70.02973370376, "math": 22.0806675939, "lack": 1.9271667880599999, "lot": 4.40877534018, "some": 4.16146788992, "sourc": 1.69760479042, "small": 4.0784380887, "collect": 3.28219971056, "result": 5.7305804216, "set": 5.93539703905, "ani": 1.13383802314, "question": 2.20408163265, "and": 32.00201574816, "from": 8.00453771976, "answer": 4.64890190337, "allow": 1.2716059271100002, "richard": 2.20041580042, "work": 5.57600449565, "num": 28.00882112028, "veri": 2.51760228354, "relu": 2886.54545454, "idea": 2.0930784443, "free": 3.43636363636, "for": 15.004725600150001, "depend": 2.2411067193700003, "predict": 15.554539516650001, "regular": 2.09418282548, "precipit": 10.08, "seen": 1.61079545455, "book": 2.86829268292, "input": 12.2029208301, "competit": 3.06960556845, "are": 26.77755433028, "better": 12.0394337715, "rectifi": 54.3698630137, "minima": 1725.6521739150003, "look": 7.634527530639999, "option": 4.04896710023, "chanc": 4.2449197861000005, "combin": 3.39520958084, "posit": 2.74505057492, "net": 6.96315789474, "general": 1.1218202374200001, "sigmoid": 1058.4, "map": 4.0728578758300005, "doe": 3.4116256581, "even": 1.16461267606, "straight": 6.203985932, "give": 2.7306501548, "three": 4.26487575556, "defin": 2.72830383227, "train": 23.2388387412, "avoid": 2.45986984816, "step": 2.8279301745599996, "longer": 2.02319357716, "error": 18.123287671230003, "ask": 2.1744966443, "lead": 2.5328653478, "overcom": 8.38668779715, "period": 1.3430335843, "machin": 8.04866920152, "again": 3.01767724768, "paper": 2.6628648104700003, "abov": 5.7114761961900005, "earlier": 1.86776470588, "local": 3.03440366972, "suppos": 8.46043165468, "overfit": 10102.90909089, "help": 4.19888918277, "larg": 1.18574949585, "minimum": 6.02962400304, "cat": 21.0696748506, "abil": 2.70875277256, "conclus": 4.84615384615, "own": 2.35688836104, "base": 1.14628158845, "relat": 1.23750876919, "fact": 1.73375559681, "who": 2.12558575446, "decid": 3.8515283842800003, "squar": 3.26666666667, "permiss": 6.280063291139999, "bigger": 13.23, "post": 2.23826307627, "maximum": 4.80072573329, "infer": 21.1398135819, "long": 1.2657259028899999, "gandhi": 77.82352941180001, "layer": 24.424615384619997, "henc": 5.390831918509999, "care": 2.49426551453, "feynman": 172.56521739099998, "epoch": 153.391304348, "neural": 1010.8314606741, "vanish": 37.3113983548, "just": 2.67160286074, "adam": 4.43092380687, "ensur": 3.4127257093700005, "him": 1.63434218653, "dure": 1.0503473370799998, "classifi": 47.6438812938, "weak": 9.41078838174, "check": 13.0131147541, "where": 1.06715063521, "ensembl": 184.21518987299999, "flip": 31.3136094675, "categor": 15.0198675497, "start": 2.53347163488, "intuit": 55.4136125654, "them": 9.88885043946, "studi": 1.53184098804, "standard": 3.7831526271800007, "kernel": 70.56, "abl": 5.46255304506, "tune": 10.4173228346, "mani": 4.17707031508, "entropi": 107.27027027, "the": 76.0, "accur": 17.306686046520003, "individu": 5.4012247675200005, "seem": 6.87371915139, "linear": 13.8776223776, "unfortun": 9.966101694919999, "earli": 1.12468121281, "graph": 37.7102137767, "leaderboard": 1323.0, "there": 9.36821400471, "converg": 61.1791907516, "connect": 1.8843916913900003, "cross": 4.66255506608, "formula": 17.28470332064, "best": 6.331405782640001, "origin": 1.13724928367, "right": 1.4054532577899999, "prefer": 3.0216977540900003, "take": 3.4188500466600003, "follow": 3.1392037964699995, "now": 1.160780873, "might": 8.624745348360001, "tri": 5.563368765329999, "high": 8.03441295545, "satisfactori": 31.6886227545, "regress": 51.2129032258, "power": 4.018901358539999, "feel": 3.1356903021900004, "incept": 16.8535031847, "also": 3.04429530201, "around": 1.21394708671, "mean": 1.44906900329, "task": 11.65924112607, "stochast": 128.032258065, "encount": 4.13976531943, "therefor": 16.338135842419998, "smarter": 170.709677419, "open": 1.24556723678, "ground": 3.95220313666, "into": 1.01502461479, "build": 8.170869789000001, "hasn": 1443.27272727, "trial": 4.04175152749, "which": 2.01038369, "like": 2.2983713355, "activ": 11.71228329032, "other": 1.00992366412, "appli": 2.2972073506, "techniqu": 26.105708245260004, "one": 4.02509982888, "restrict": 3.1062414400300002, "custom": 3.6346153846199996, "begin": 1.3305397251100002, "see": 1.27242125511, "algorithm": 83.8521126762, "imag": 16.20826952526, "problem": 10.60048965054, "get": 10.7137554831, "still": 1.1866357724799999, "random": 14.3804347826, "through": 2.14149861738, "tanh": 1443.27272727, "select": 4.04690288044, "drop": 2.4594887684, "part": 1.04330682789, "backpropag": 1443.27272727, "num\u2026": 1443.27272727, "play": 1.46390041494, "understand": 5.93717277486, "this": 4.01517450684, "find": 1.7294117647099998, "regularis": 481.09090909099996, "differ": 8.65581431575, "recognit": 4.40022172949, "most": 4.08385852092, "between": 4.13814674832, "low": 4.26144141726, "what": 3.7603031738399997, "diminish": 9.83643122677, "fail": 1.9281029876099998, "along": 1.2973768080399999, "all": 5.05733944955, "top": 3.6775538568400004, "nonlinear": 99.225, "augment": 49.56087408959999, "verifi": 28.4516129032, "gradient": 167.556728232, "solv": 7.26923076923, "case": 8.90992423536, "dearth": 168.893617021, "that": 10.0398406375, "architectur": 30.76744186044, "memoris": 2026.723404258, "dropout": 167.115789474, "rate": 17.1239045436, "more": 2.0343413634, "improv": 4.08753861998, "achiev": 1.87216981132, "these": 8.59323410016, "proven": 19.636363636360002, "numcat": 1443.27272727, "amount": 4.54054054054, "artifici": 8.31639601886, "could": 7.226217569399999, "befor": 1.10036041031, "comment": 3.05954904606, "state": 1.0477133240899998, "friend": 8.80776699028, "can": 12.93887530562, "avail": 5.1865403463, "comput": 3.9277585353800006, "make": 1.0762660158600001, "textbook": 36.580645161199996, "analog": 9.05131128848, "unabl": 6.6163784121599996, "each": 4.75899280576, "size": 17.45711592835, "mustread": 1443.27272727, "necessari": 2.8421052631599997, "numdog": 1443.27272727, "read": 2.3149606299200003, "time": 3.03382381044, "common": 4.2077922078, "ascrib": 23.9096385542, "method": 2.5714285714300003, "convolut": 101.121019108, "bio": 42.336000000000006, "come": 3.9849397590299995, "must": 3.844067796619999, "exampl": 3.00966824644, "outperform": 82.2590673575, "text": 3.12827586207, "identifi": 4.60374075686, "direct": 1.22226499346, "import": 2.6803984467400004, "first": 1.00761614623, "initialis": 835.5789473680001, "copi": 3.8375634517800004, "rohith": 2886.54545454, "hesh": 1443.27272727, "generalis": 114.215827338, "overshoot": 305.307692308, "while": 2.0883977900599997, "resort": 8.16246786632, "nowaday": 21.454054054100002, "often": 1.29452054795, "suggest": 1.7571665744299998, "learn": 37.1640087784, "when": 5.10383848775, "mention": 2.53894130817}, "logtfidf": {"after": 0.020490694648099998, "hand": 0.479471335336, "myriad": 3.8348333667400003, "googl": 2.43263122258, "plot": 1.68334240509, "pearson": 15.8040924213, "data": 17.0354881872, "wide": 0.8891600135079999, "neuron": 4.16317547727, "way": 0.19809150993500002, "would": 0.23885288389409998, "thank": 1.7928938993, "number": 0.3864343136744, "miser": 3.65884865786, "taken": 0.470759772949, "etc": 4.310019263910001, "dataset": 10.53168913328, "function": 13.716986123909999, "onto": 1.4987063591299998, "dog": 3.6692297957599997, "descent": 2.13940500645, "learner": 8.641411360860001, "crop": 2.27316573057, "resnet": 7.2746685411000005, "well": 0.1270288766312, "whether": 0.791561189647, "never": 0.886410872182, "path": 1.5351679838499999, "higher": 0.752308398995, "know": 1.905839388796, "their": 0.07680252561350001, "scale": 1.32095306328, "measur": 0.880014199726, "instead": 0.9332663008300001, "creat": 0.445153637028, "how": 1.4147008707900002, "test": 10.749468808133, "repost": 6.83935046985, "truth": 3.49698297796, "than": 0.0645217244364, "increas": 0.555641437858, "due": 0.21341214386399998, "correl": 12.895795940150002, "deep": 1.2886734698, "will": 0.6083596047450001, "consid": 0.214894723824, "purpos": 0.803869037322, "goe": 1.4473284897999998, "new": 0.0531898405533, "below": 0.813626591936, "perform": 2.55708510348, "but": 0.1295389765752, "were": 0.024291143681099997, "obtain": 1.976325407006, "our": 0.8576392141820001, "classif": 2.08779073629, "output": 4.07645315654, "expect": 0.78850775216, "point": 0.23103235903299998, "hyperparamet": 58.197348328800004, "has": 0.0854478897096, "have": 0.1330650210708, "arriv": 0.7088915382879999, "stop": 2.335738124889, "vote": 1.09899028905, "reduct": 1.8437292863099999, "use": 0.43812029597400004, "experi": 1.878818861799, "out": 0.2921319545965, "model": 8.849400877332002, "optimum": 4.06309201872, "suffer": 0.7706525875229999, "good": 1.2557682147209999, "alway": 1.452638409144, "much": 0.17749572930100002, "forc": 0.280652166524, "huge": 1.47916358195, "optim": 14.673766772459999, "classmat": 3.7141391208699996, "global": 4.7831041484800005, "loss": 5.315726153052, "suscept": 3.03924538062, "not": 0.1399717170675, "involv": 0.371469078658, "day": 0.16865870631700003, "shear": 4.03065674296, "happen": 1.08640441802, "tweak": 4.73092139129, "almost": 0.42907884333400004, "then": 0.24910159569269996, "they": 0.0594539895352, "choos": 7.1503533036, "reach": 0.40414323085000003, "effect": 0.333830227158, "let": 4.995210269119999, "scienc": 0.841436178891, "batch": 14.297958126960001, "featur": 0.423387418142, "practic": 1.5995475926009999, "major": 0.138474663439, "stuck": 2.94154571342, "rmsprop": 7.2746685411000005, "provid": 0.39035568865000003, "valu": 4.939159860888, "either": 0.459327638815, "develop": 0.178624694913, "accuraci": 50.929530812, "with": 0.00718495028034, "network": 25.733242432403998, "math": 3.09470245618, "lack": 0.656050938907, "lot": 1.4835969502500002, "some": 0.158294036258, "sourc": 0.529218310751, "small": 0.9213054151769999, "collect": 0.99073332104, "result": 0.681894541905, "set": 0.857480056445, "ani": 0.125608358366, "question": 0.790310929014, "and": 0.0020156845460352, "from": 0.004536433350928, "answer": 1.5366310419, "allow": 0.24028061118900002, "richard": 0.788646342695, "work": 0.545172836365, "num": 0.008819731071116001, "veri": 0.460319586476, "relu": 14.549337082200001, "idea": 0.73863592212, "free": 1.0825332985340002, "for": 0.004724855930955001, "depend": 0.806969815, "predict": 4.937220713069999, "regular": 0.739163417847, "precipit": 2.3105532626400005, "seen": 0.47672812813, "book": 0.7211395764, "input": 2.50167533539, "competit": 1.12154907401, "are": 0.7661543131502, "better": 4.1785676436, "rectifi": 3.9958100116300006, "minima": 29.219612087049995, "look": 2.5855467744, "option": 1.39846181161, "chanc": 1.44572292349, "combin": 1.058436621502, "posit": 0.633304637216, "net": 1.9406330919499999, "general": 0.114952578063, "sigmoid": 6.964513612799999, "map": 1.40434493384, "doe": 1.0680834594339998, "even": 0.152388564834, "straight": 1.82519197774, "give": 0.622785104448, "three": 0.25647475289960003, "defin": 1.00368010925, "train": 7.931019754068, "avoid": 0.900108441291, "step": 1.03954505698, "longer": 0.7046772417749999, "error": 5.395756302900001, "ask": 0.776797209847, "lead": 0.47240805973399996, "overcom": 2.12664566269, "period": 0.294930924153, "machin": 2.78471916124, "again": 0.822680463224, "paper": 0.979402539665, "abov": 1.9315956894480002, "earlier": 0.624742371425, "local": 0.833735480412, "suppos": 2.88450602954, "overfit": 50.922679787700005, "help": 1.008623164032, "larg": 0.17037506060600002, "minimum": 1.79668465441, "cat": 4.70937523056, "abil": 0.996488297427, "conclus": 1.57818536893, "own": 0.328390154842, "base": 0.13652330228700002, "relat": 0.21310030165399999, "fact": 0.5502899207949999, "who": 0.1218004659718, "decid": 1.310645743786, "squar": 1.18377009701, "permiss": 1.8373800586400002, "bigger": 2.58248697813, "post": 0.8057001527009999, "maximum": 1.5687671009200002, "infer": 3.0511581621399997, "long": 0.235645793878, "gandhi": 7.3225932789999995, "layer": 6.290937487050001, "henc": 1.68469971782, "care": 0.9139943029109999, "feynman": 5.15077523685, "epoch": 14.586791360320001, "neural": 69.4503576435, "vanish": 5.85230337066, "just": 0.579062868218, "adam": 1.4886080966, "ensur": 1.22751130026, "him": 0.49124039099699995, "dure": 0.0491209066894, "classifi": 14.998766716349998, "weak": 3.0974191016000003, "check": 3.74562099124, "where": 0.0649921387457, "ensembl": 31.00030242815, "flip": 3.4440528103099997, "categor": 2.70937382803, "start": 0.472886738582, "intuit": 6.643356194380001, "them": 0.8476499421836999, "studi": 0.426470272221, "standard": 1.27482101964, "kernel": 4.2564634117, "abl": 1.7979119474250003, "tune": 2.3434700776599997, "mani": 0.1732630324884, "entropi": 4.67535154014, "the": 0.0, "accur": 5.25744184455, "individu": 1.764040343955, "seem": 2.487279096828, "linear": 2.63027764196, "unfortun": 2.29918950399, "earli": 0.117499629108, "graph": 3.6299309802199997, "leaderboard": 7.18765716411, "there": 0.36088103632949997, "converg": 10.910051000720001, "connect": 0.633605058682, "cross": 1.692832829518, "formula": 4.31334945738, "best": 1.836911731788, "origin": 0.128612437587, "right": 0.34035985417, "prefer": 1.10581884366, "take": 0.392075886591, "follow": 0.1360707332826, "now": 0.149092945021, "might": 3.0733643061360003, "tri": 1.8527745874800001, "high": 0.9647665057800001, "satisfactori": 3.4559577128199996, "regress": 3.9359915164199997, "power": 0.8771888481450001, "feel": 1.1428493419299999, "incept": 2.82455853933, "also": 0.0439714734, "around": 0.19387710578200001, "mean": 0.37092128352, "task": 4.0724604198300005, "stochast": 4.8522822483, "encount": 1.4206391000999998, "therefor": 5.933142938352, "smarter": 5.13996432075, "open": 0.219591038029, "ground": 1.362251997968, "into": 0.0149128632287, "build": 2.455687260455, "hasn": 7.2746685411000005, "trial": 1.3966781444299998, "which": 0.01035682769086, "like": 0.27810715309, "activ": 3.049572826272, "other": 0.00987474791976, "appli": 0.8316941898119999, "techniqu": 9.21370693649, "one": 0.025021406582, "restrict": 1.13341345513, "custom": 1.2905032964799998, "begin": 0.285584668268, "see": 0.240921585492, "algorithm": 9.99132718554, "imag": 5.96257264374, "problem": 3.414844345638, "get": 3.478614034692, "still": 0.17112222142900002, "random": 3.9454428130199997, "through": 0.1367173837698, "tanh": 7.2746685411000005, "select": 1.409609374266, "drop": 0.8999535106219999, "part": 0.04239531098280001, "backpropag": 7.2746685411000005, "num\u2026": 7.2746685411000005, "play": 0.38110439064199997, "understand": 2.1761717513599996, "this": 0.01514579621, "find": 0.547781330288, "regularis": 6.17605625244, "differ": 1.486247849184, "recognit": 1.4816549327200002, "most": 0.08299158518239999, "between": 0.13581472466119998, "low": 1.5129205666980001, "what": 0.677661890481, "diminish": 2.28609296507, "fail": 0.656536611573, "along": 0.260344385917, "all": 0.057013160488999994, "top": 1.218201275576, "nonlinear": 4.59738999867, "augment": 8.41376821479, "verifi": 5.31011534192, "gradient": 14.94011043528, "solv": 1.9836504770400003, "case": 2.372437613334, "dearth": 5.129269031630001, "that": 0.039761483796399995, "architectur": 9.80818547514, "memoris": 34.934497273139996, "dropout": 5.1186869223, "rate": 6.088270977328, "more": 0.034049863199999995, "improv": 1.4295916078639999, "achiev": 0.6270980851169999, "these": 0.5722689552064, "proven": 4.56847190866, "numcat": 7.2746685411000005, "amount": 1.639797772398, "artifici": 2.11822899018, "could": 1.1157376337400002, "befor": 0.0956377718795, "comment": 1.11826753454, "state": 0.0466100027668, "friend": 3.1573583344959997, "can": 1.7857520603339998, "avail": 1.642363758867, "comput": 1.36806891594, "make": 0.07349765782289999, "textbook": 5.812744198480001, "analog": 2.20290964097, "unabl": 2.39280194408, "each": 0.694966757216, "size": 6.396860442426999, "mustread": 7.2746685411000005, "necessari": 1.0445450673999999, "numdog": 7.2746685411000005, "read": 0.83939268088, "time": 0.0336345565878, "common": 1.014977415813, "ascrib": 3.17428166443, "method": 0.944461608841, "convolut": 4.61631800855, "bio": 3.7456377879300002, "come": 0.8517297195900001, "must": 1.306767894776, "exampl": 0.8173653499979999, "outperform": 4.409873625, "text": 1.14048200999, "identifi": 1.667444000944, "direct": 0.200705689496, "import": 0.585636554132, "first": 0.0075872898121599995, "initialis": 6.72812483474, "copi": 1.34483764744, "rohith": 14.549337082200001, "hesh": 7.2746685411000005, "generalis": 4.73808988077, "overshoot": 5.721320095319999, "while": 0.08649996758760002, "resort": 2.09954655785, "nowaday": 3.0659136276999996, "often": 0.258140393351, "suggest": 0.563702610877, "learn": 13.48403303592, "when": 0.102774944292, "mention": 0.931747186336}, "logidf": {"after": 0.020490694648099998, "hand": 0.479471335336, "myriad": 3.8348333667400003, "googl": 2.43263122258, "plot": 1.68334240509, "pearson": 3.16081848426, "data": 1.2168205848, "wide": 0.44458000675399995, "neuron": 4.16317547727, "way": 0.19809150993500002, "would": 0.0796176279647, "thank": 1.7928938993, "number": 0.0966085784186, "miser": 3.65884865786, "taken": 0.470759772949, "etc": 1.4366730879700003, "dataset": 5.26584456664, "function": 0.914465741594, "onto": 1.4987063591299998, "dog": 1.8346148978799999, "descent": 2.13940500645, "learner": 4.320705680430001, "crop": 2.27316573057, "resnet": 7.2746685411000005, "well": 0.0635144383156, "whether": 0.791561189647, "never": 0.443205436091, "path": 1.5351679838499999, "higher": 0.752308398995, "know": 0.952919694398, "their": 0.015360505122700001, "scale": 1.32095306328, "measur": 0.880014199726, "instead": 0.46663315041500003, "creat": 0.222576818514, "how": 0.47156695693000006, "test": 0.977224437103, "repost": 6.83935046985, "truth": 1.74849148898, "than": 0.0322608622182, "increas": 0.277820718929, "due": 0.21341214386399998, "correl": 2.57915918803, "deep": 1.2886734698, "will": 0.202786534915, "consid": 0.214894723824, "purpos": 0.803869037322, "goe": 1.4473284897999998, "new": 0.0177299468511, "below": 0.813626591936, "perform": 0.42618085058, "but": 0.0161923720719, "were": 0.024291143681099997, "obtain": 0.988162703503, "our": 0.8576392141820001, "classif": 2.08779073629, "output": 2.03822657827, "expect": 0.78850775216, "point": 0.23103235903299998, "hyperparamet": 7.2746685411000005, "has": 0.0427239448548, "have": 0.0147850023412, "arriv": 0.7088915382879999, "stop": 0.778579374963, "vote": 1.09899028905, "reduct": 1.8437292863099999, "use": 0.0292080197316, "experi": 0.626272953933, "out": 0.0584263909193, "model": 0.7374500731110001, "optimum": 4.06309201872, "suffer": 0.7706525875229999, "good": 0.418589404907, "alway": 0.726319204572, "much": 0.17749572930100002, "forc": 0.280652166524, "huge": 1.47916358195, "optim": 2.4456277954099996, "classmat": 3.7141391208699996, "global": 1.1957760371200001, "loss": 0.885954358842, "suscept": 3.03924538062, "not": 0.0155524130075, "involv": 0.371469078658, "day": 0.16865870631700003, "shear": 4.03065674296, "happen": 1.08640441802, "tweak": 4.73092139129, "almost": 0.42907884333400004, "then": 0.08303386523089999, "they": 0.0297269947676, "choos": 1.43007066072, "reach": 0.40414323085000003, "effect": 0.333830227158, "let": 1.2488025672799998, "scienc": 0.841436178891, "batch": 3.5744895317400003, "featur": 0.423387418142, "practic": 0.533182530867, "major": 0.138474663439, "stuck": 2.94154571342, "rmsprop": 7.2746685411000005, "provid": 0.19517784432500002, "valu": 0.823193310148, "either": 0.459327638815, "develop": 0.178624694913, "accuraci": 2.5464765406, "with": 0.00119749171339, "network": 0.9530830530519999, "math": 3.09470245618, "lack": 0.656050938907, "lot": 1.4835969502500002, "some": 0.0395735090645, "sourc": 0.529218310751, "small": 0.307101805059, "collect": 0.49536666052, "result": 0.136378908381, "set": 0.171496011289, "ani": 0.125608358366, "question": 0.790310929014, "and": 6.29901420636e-05, "from": 0.000567054168866, "answer": 1.5366310419, "allow": 0.24028061118900002, "richard": 0.788646342695, "work": 0.109034567273, "num": 0.00031499039539700004, "veri": 0.230159793238, "relu": 7.2746685411000005, "idea": 0.73863592212, "free": 0.5412666492670001, "for": 0.00031499039539700004, "depend": 0.806969815, "predict": 1.6457402376899999, "regular": 0.739163417847, "precipit": 2.3105532626400005, "seen": 0.47672812813, "book": 0.3605697882, "input": 2.50167533539, "competit": 1.12154907401, "are": 0.0294674735827, "better": 0.6964279406, "rectifi": 3.9958100116300006, "minima": 5.843922417409999, "look": 0.6463866936, "option": 1.39846181161, "chanc": 1.44572292349, "combin": 0.529218310751, "posit": 0.316652318608, "net": 1.9406330919499999, "general": 0.114952578063, "sigmoid": 6.964513612799999, "map": 1.40434493384, "doe": 0.5340417297169999, "even": 0.152388564834, "straight": 1.82519197774, "give": 0.311392552224, "three": 0.06411868822490001, "defin": 1.00368010925, "train": 0.660918312839, "avoid": 0.900108441291, "step": 1.03954505698, "longer": 0.7046772417749999, "error": 1.7985854343, "ask": 0.776797209847, "lead": 0.23620402986699998, "overcom": 2.12664566269, "period": 0.294930924153, "machin": 1.39235958062, "again": 0.411340231612, "paper": 0.979402539665, "abov": 0.643865229816, "earlier": 0.624742371425, "local": 0.416867740206, "suppos": 1.44225301477, "overfit": 7.2746685411000005, "help": 0.336207721344, "larg": 0.17037506060600002, "minimum": 1.79668465441, "cat": 2.35468761528, "abil": 0.996488297427, "conclus": 1.57818536893, "own": 0.164195077421, "base": 0.13652330228700002, "relat": 0.21310030165399999, "fact": 0.5502899207949999, "who": 0.0609002329859, "decid": 0.655322871893, "squar": 1.18377009701, "permiss": 1.8373800586400002, "bigger": 2.58248697813, "post": 0.8057001527009999, "maximum": 1.5687671009200002, "infer": 3.0511581621399997, "long": 0.235645793878, "gandhi": 3.6612966394999997, "layer": 2.0969791623500003, "henc": 1.68469971782, "care": 0.9139943029109999, "feynman": 5.15077523685, "epoch": 3.6466978400800003, "neural": 4.0853151555, "vanish": 2.92615168533, "just": 0.289531434109, "adam": 1.4886080966, "ensur": 1.22751130026, "him": 0.49124039099699995, "dure": 0.0491209066894, "classifi": 1.6665296351499999, "weak": 1.5487095508000002, "check": 1.87281049562, "where": 0.0649921387457, "ensembl": 2.81820931165, "flip": 3.4440528103099997, "categor": 2.70937382803, "start": 0.236443369291, "intuit": 3.3216780971900004, "them": 0.0941833269093, "studi": 0.426470272221, "standard": 0.63741050982, "kernel": 4.2564634117, "abl": 0.599303982475, "tune": 2.3434700776599997, "mani": 0.0433157581221, "entropi": 4.67535154014, "the": 0.0, "accur": 1.75248061485, "individu": 0.588013447985, "seem": 0.829093032276, "linear": 2.63027764196, "unfortun": 2.29918950399, "earli": 0.117499629108, "graph": 3.6299309802199997, "leaderboard": 7.18765716411, "there": 0.0400978929255, "converg": 2.7275127501800003, "connect": 0.633605058682, "cross": 0.846416414759, "formula": 2.15667472869, "best": 0.459227932947, "origin": 0.128612437587, "right": 0.34035985417, "prefer": 1.10581884366, "take": 0.130691962197, "follow": 0.045356911094199995, "now": 0.149092945021, "might": 0.7683410765340001, "tri": 0.61759152916, "high": 0.13782378654000002, "satisfactori": 3.4559577128199996, "regress": 3.9359915164199997, "power": 0.292396282715, "feel": 1.1428493419299999, "incept": 2.82455853933, "also": 0.0146571578, "around": 0.19387710578200001, "mean": 0.37092128352, "task": 1.35748680661, "stochast": 4.8522822483, "encount": 1.4206391000999998, "therefor": 0.847591848336, "smarter": 5.13996432075, "open": 0.219591038029, "ground": 0.681125998984, "into": 0.0149128632287, "build": 0.491137452091, "hasn": 7.2746685411000005, "trial": 1.3966781444299998, "which": 0.00517841384543, "like": 0.139053576545, "activ": 0.381196603284, "other": 0.00987474791976, "appli": 0.8316941898119999, "techniqu": 1.31624384807, "one": 0.0062553516455, "restrict": 1.13341345513, "custom": 1.2905032964799998, "begin": 0.285584668268, "see": 0.240921585492, "algorithm": 3.33044239518, "imag": 0.99376210729, "problem": 0.569140724273, "get": 0.579769005782, "still": 0.17112222142900002, "random": 1.9727214065099998, "through": 0.0683586918849, "tanh": 7.2746685411000005, "select": 0.704804687133, "drop": 0.8999535106219999, "part": 0.04239531098280001, "backpropag": 7.2746685411000005, "num\u2026": 7.2746685411000005, "play": 0.38110439064199997, "understand": 1.0880858756799998, "this": 0.0037864490525, "find": 0.547781330288, "regularis": 6.17605625244, "differ": 0.212321121312, "recognit": 1.4816549327200002, "most": 0.020747896295599998, "between": 0.033953681165299995, "low": 0.7564602833490001, "what": 0.225887296827, "diminish": 2.28609296507, "fail": 0.656536611573, "along": 0.260344385917, "all": 0.011402632097799998, "top": 0.609100637788, "nonlinear": 4.59738999867, "augment": 2.8045894049299998, "verifi": 2.65505767096, "gradient": 3.73502760882, "solv": 1.9836504770400003, "case": 0.395406268889, "dearth": 5.129269031630001, "that": 0.00397614837964, "architectur": 1.63469757919, "memoris": 5.822416212189999, "dropout": 5.1186869223, "rate": 0.761033872166, "more": 0.017024931599999998, "improv": 0.7147958039319999, "achiev": 0.6270980851169999, "these": 0.0715336194008, "proven": 2.28423595433, "numcat": 7.2746685411000005, "amount": 0.819898886199, "artifici": 2.11822899018, "could": 0.18595627229000003, "befor": 0.0956377718795, "comment": 1.11826753454, "state": 0.0466100027668, "friend": 0.7893395836239999, "can": 0.162341096394, "avail": 0.547454586289, "comput": 1.36806891594, "make": 0.07349765782289999, "textbook": 2.9063720992400004, "analog": 2.20290964097, "unabl": 1.19640097204, "each": 0.173741689304, "size": 0.9138372060609999, "mustread": 7.2746685411000005, "necessari": 1.0445450673999999, "numdog": 7.2746685411000005, "read": 0.83939268088, "time": 0.0112115188626, "common": 0.338325805271, "ascrib": 3.17428166443, "method": 0.944461608841, "convolut": 4.61631800855, "bio": 3.7456377879300002, "come": 0.28390990653000003, "must": 0.653383947388, "exampl": 0.40868267499899996, "outperform": 4.409873625, "text": 1.14048200999, "identifi": 0.833722000472, "direct": 0.200705689496, "import": 0.292818277066, "first": 0.0075872898121599995, "initialis": 6.72812483474, "copi": 1.34483764744, "rohith": 7.2746685411000005, "hesh": 7.2746685411000005, "generalis": 4.73808988077, "overshoot": 5.721320095319999, "while": 0.04324998379380001, "resort": 2.09954655785, "nowaday": 3.0659136276999996, "often": 0.258140393351, "suggest": 0.563702610877, "learn": 0.842752064745, "when": 0.0205549888584, "mention": 0.931747186336}, "freq": {"after": 1, "hand": 1, "myriad": 1, "googl": 1, "plot": 1, "pearson": 5, "data": 14, "wide": 2, "neuron": 1, "way": 1, "would": 3, "thank": 1, "number": 4, "miser": 1, "taken": 1, "etc": 3, "dataset": 2, "function": 15, "onto": 1, "dog": 2, "descent": 1, "learner": 2, "crop": 1, "resnet": 1, "well": 2, "whether": 1, "never": 2, "path": 1, "higher": 1, "know": 2, "their": 5, "scale": 1, "measur": 1, "instead": 2, "creat": 2, "how": 3, "test": 11, "repost": 1, "truth": 2, "than": 2, "increas": 2, "due": 1, "correl": 5, "deep": 1, "will": 3, "consid": 1, "purpos": 1, "goe": 1, "new": 3, "below": 1, "perform": 6, "but": 8, "were": 1, "obtain": 2, "our": 1, "classif": 1, "output": 2, "expect": 1, "point": 1, "hyperparamet": 8, "has": 2, "have": 9, "arriv": 1, "stop": 3, "vote": 1, "reduct": 1, "use": 15, "experi": 3, "out": 5, "model": 12, "optimum": 1, "suffer": 1, "good": 3, "alway": 2, "much": 1, "forc": 1, "huge": 1, "optim": 6, "classmat": 1, "global": 4, "loss": 6, "suscept": 1, "not": 9, "involv": 1, "day": 1, "shear": 1, "happen": 1, "tweak": 1, "almost": 1, "then": 3, "they": 2, "choos": 5, "reach": 1, "effect": 1, "let": 4, "scienc": 1, "batch": 4, "featur": 1, "practic": 3, "major": 1, "stuck": 1, "rmsprop": 1, "provid": 2, "valu": 6, "either": 1, "develop": 1, "accuraci": 20, "with": 6, "network": 27, "math": 1, "lack": 1, "lot": 1, "some": 4, "sourc": 1, "small": 3, "collect": 2, "result": 5, "set": 5, "ani": 1, "question": 1, "and": 32, "from": 8, "answer": 1, "allow": 1, "richard": 1, "work": 5, "num": 28, "veri": 2, "relu": 2, "idea": 1, "free": 2, "for": 15, "depend": 1, "predict": 3, "regular": 1, "precipit": 1, "seen": 1, "book": 2, "input": 1, "competit": 1, "are": 26, "better": 6, "rectifi": 1, "minima": 5, "look": 4, "option": 1, "chanc": 1, "combin": 2, "posit": 2, "net": 1, "general": 1, "sigmoid": 1, "map": 1, "doe": 2, "even": 1, "straight": 1, "give": 2, "three": 4, "defin": 1, "train": 12, "avoid": 1, "step": 1, "longer": 1, "error": 3, "ask": 1, "lead": 2, "overcom": 1, "period": 1, "machin": 2, "again": 2, "paper": 1, "abov": 3, "earlier": 1, "local": 2, "suppos": 2, "overfit": 7, "help": 3, "larg": 1, "minimum": 1, "cat": 2, "abil": 1, "conclus": 1, "own": 2, "base": 1, "relat": 1, "fact": 1, "who": 2, "decid": 2, "squar": 1, "permiss": 1, "bigger": 1, "post": 1, "maximum": 1, "infer": 1, "long": 1, "gandhi": 2, "layer": 3, "henc": 1, "care": 1, "feynman": 1, "epoch": 4, "neural": 17, "vanish": 2, "just": 2, "adam": 1, "ensur": 1, "him": 1, "dure": 1, "classifi": 9, "weak": 2, "check": 2, "where": 1, "ensembl": 11, "flip": 1, "categor": 1, "start": 2, "intuit": 2, "them": 9, "studi": 1, "standard": 2, "kernel": 1, "abl": 3, "tune": 1, "mani": 4, "entropi": 1, "the": 76, "accur": 3, "individu": 3, "seem": 3, "linear": 1, "unfortun": 1, "earli": 1, "graph": 1, "leaderboard": 1, "there": 9, "converg": 4, "connect": 1, "cross": 2, "formula": 2, "best": 4, "origin": 1, "right": 1, "prefer": 1, "take": 3, "follow": 3, "now": 1, "might": 4, "tri": 3, "high": 7, "satisfactori": 1, "regress": 1, "power": 3, "feel": 1, "incept": 1, "also": 3, "around": 1, "mean": 1, "task": 3, "stochast": 1, "encount": 1, "therefor": 7, "smarter": 1, "open": 1, "ground": 2, "into": 1, "build": 5, "hasn": 1, "trial": 1, "which": 2, "like": 2, "activ": 8, "other": 1, "appli": 1, "techniqu": 7, "one": 4, "restrict": 1, "custom": 1, "begin": 1, "see": 1, "algorithm": 3, "imag": 6, "problem": 6, "get": 6, "still": 1, "random": 2, "through": 2, "tanh": 1, "select": 2, "drop": 1, "part": 1, "backpropag": 1, "num\u2026": 1, "play": 1, "understand": 2, "this": 4, "find": 1, "regularis": 1, "differ": 7, "recognit": 1, "most": 4, "between": 4, "low": 2, "what": 3, "diminish": 1, "fail": 1, "along": 1, "all": 5, "top": 2, "nonlinear": 1, "augment": 3, "verifi": 2, "gradient": 4, "solv": 1, "case": 6, "dearth": 1, "that": 10, "architectur": 6, "memoris": 6, "dropout": 1, "rate": 8, "more": 2, "improv": 2, "achiev": 1, "these": 8, "proven": 2, "numcat": 1, "amount": 2, "artifici": 1, "could": 6, "befor": 1, "comment": 1, "state": 1, "friend": 4, "can": 11, "avail": 3, "comput": 1, "make": 1, "textbook": 2, "analog": 1, "unabl": 2, "each": 4, "size": 7, "mustread": 1, "necessari": 1, "numdog": 1, "read": 1, "time": 3, "common": 3, "ascrib": 1, "method": 1, "convolut": 1, "bio": 1, "come": 3, "must": 2, "exampl": 2, "outperform": 1, "text": 1, "identifi": 2, "direct": 1, "import": 2, "first": 1, "initialis": 1, "copi": 1, "rohith": 2, "hesh": 1, "generalis": 1, "overshoot": 1, "while": 2, "resort": 1, "nowaday": 1, "often": 1, "suggest": 1, "learn": 16, "when": 5, "mention": 1}, "idf": {"after": 1.02070207021, "hand": 1.6152202665600002, "myriad": 46.285714285699996, "googl": 11.388809182200001, "plot": 5.383519837230001, "pearson": 23.5898959881, "data": 3.37643555934, "wide": 1.5598349381, "neuron": 64.2753036437, "way": 1.2190739461, "would": 1.0828729281799998, "thank": 6.00681044268, "number": 1.10142916609, "miser": 38.8166259169, "taken": 1.6012102874399998, "etc": 4.2066772655, "dataset": 193.609756098, "function": 2.495441685, "onto": 4.47589512264, "dog": 6.26272189349, "descent": 8.494382022469999, "learner": 75.2417061611, "crop": 9.71009174312, "resnet": 1443.27272727, "well": 1.0655748708, "whether": 2.20683903253, "never": 1.55769230769, "path": 4.6421052631599995, "higher": 2.1218925421, "know": 2.59327017315, "their": 1.01547908405, "scale": 3.7469907953699995, "measur": 2.41093394077, "instead": 1.59461631177, "creat": 1.2492917847, "how": 1.60250328051, "test": 2.65707112971, "repost": 933.882352941, "truth": 5.745928338759999, "than": 1.03278688525, "increas": 1.32024948025, "due": 1.23789473684, "correl": 13.1860465116, "deep": 3.6279707495399998, "will": 1.22481098596, "consid": 1.2397313759200002, "purpos": 2.23416830847, "goe": 4.251740760580001, "new": 1.0178880554, "below": 2.25607503197, "perform": 1.5313977042500002, "but": 1.01632417899, "were": 1.02458857696, "obtain": 2.68629441624, "our": 2.35758835759, "classif": 8.067073170730001, "output": 7.676982591880001, "expect": 2.20011086475, "point": 1.25990000794, "hyperparamet": 1443.27272727, "has": 1.0436497502, "have": 1.0148948411399998, "arriv": 2.03173790632, "stop": 2.1783754116400003, "vote": 3.0011342155, "reduct": 6.320063694269999, "use": 1.0296387573799999, "experi": 1.87062566278, "out": 1.06016694491, "model": 2.0905978404, "optimum": 58.1538461538, "suffer": 2.16117615029, "good": 1.51981619759, "alway": 2.06745670009, "much": 1.1942229577299999, "forc": 1.32399299475, "huge": 4.38927287808, "optim": 11.5377906977, "classmat": 41.023255814, "global": 3.30612244898, "loss": 2.42529789184, "suscept": 20.889473684200002, "not": 1.01567398119, "involv": 1.4498630137000001, "day": 1.18371607516, "shear": 56.2978723404, "happen": 2.96359902931, "tweak": 113.4, "almost": 1.53584212054, "then": 1.08657860516, "they": 1.03017325287, "choos": 4.17899447223, "reach": 1.49801849406, "effect": 1.3963060686000002, "let": 3.48616600791, "scienc": 2.31969608416, "batch": 35.6764044944, "featur": 1.52712581762, "practic": 1.70434782609, "major": 1.14852058164, "stuck": 18.945107398599998, "rmsprop": 1443.27272727, "provid": 1.21552714187, "valu": 2.2777618364400003, "either": 1.5830092731099998, "develop": 1.1955719557200002, "accuraci": 12.7620578778, "with": 1.0011982089899998, "network": 2.59369384088, "math": 22.0806675939, "lack": 1.9271667880599999, "lot": 4.40877534018, "some": 1.04036697248, "sourc": 1.69760479042, "small": 1.3594793629, "collect": 1.64109985528, "result": 1.14611608432, "set": 1.18707940781, "ani": 1.13383802314, "question": 2.20408163265, "and": 1.00006299213, "from": 1.00056721497, "answer": 4.64890190337, "allow": 1.2716059271100002, "richard": 2.20041580042, "work": 1.11520089913, "num": 1.00031504001, "veri": 1.25880114177, "relu": 1443.27272727, "idea": 2.0930784443, "free": 1.71818181818, "for": 1.00031504001, "depend": 2.2411067193700003, "predict": 5.18484650555, "regular": 2.09418282548, "precipit": 10.08, "seen": 1.61079545455, "book": 1.43414634146, "input": 12.2029208301, "competit": 3.06960556845, "are": 1.02990593578, "better": 2.0065722952500002, "rectifi": 54.3698630137, "minima": 345.13043478300006, "look": 1.9086318826599997, "option": 4.04896710023, "chanc": 4.2449197861000005, "combin": 1.69760479042, "posit": 1.37252528746, "net": 6.96315789474, "general": 1.1218202374200001, "sigmoid": 1058.4, "map": 4.0728578758300005, "doe": 1.70581282905, "even": 1.16461267606, "straight": 6.203985932, "give": 1.3653250774, "three": 1.06621893889, "defin": 2.72830383227, "train": 1.9365698950999999, "avoid": 2.45986984816, "step": 2.8279301745599996, "longer": 2.02319357716, "error": 6.04109589041, "ask": 2.1744966443, "lead": 1.2664326739, "overcom": 8.38668779715, "period": 1.3430335843, "machin": 4.02433460076, "again": 1.50883862384, "paper": 2.6628648104700003, "abov": 1.90382539873, "earlier": 1.86776470588, "local": 1.51720183486, "suppos": 4.23021582734, "overfit": 1443.27272727, "help": 1.39962972759, "larg": 1.18574949585, "minimum": 6.02962400304, "cat": 10.5348374253, "abil": 2.70875277256, "conclus": 4.84615384615, "own": 1.17844418052, "base": 1.14628158845, "relat": 1.23750876919, "fact": 1.73375559681, "who": 1.06279287723, "decid": 1.9257641921400002, "squar": 3.26666666667, "permiss": 6.280063291139999, "bigger": 13.23, "post": 2.23826307627, "maximum": 4.80072573329, "infer": 21.1398135819, "long": 1.2657259028899999, "gandhi": 38.911764705900005, "layer": 8.14153846154, "henc": 5.390831918509999, "care": 2.49426551453, "feynman": 172.56521739099998, "epoch": 38.347826087, "neural": 59.4606741573, "vanish": 18.6556991774, "just": 1.33580143037, "adam": 4.43092380687, "ensur": 3.4127257093700005, "him": 1.63434218653, "dure": 1.0503473370799998, "classifi": 5.2937645882, "weak": 4.70539419087, "check": 6.50655737705, "where": 1.06715063521, "ensembl": 16.746835443, "flip": 31.3136094675, "categor": 15.0198675497, "start": 1.26673581744, "intuit": 27.7068062827, "them": 1.09876115994, "studi": 1.53184098804, "standard": 1.8915763135900003, "kernel": 70.56, "abl": 1.8208510150200001, "tune": 10.4173228346, "mani": 1.04426757877, "entropi": 107.27027027, "the": 1.0, "accur": 5.768895348840001, "individu": 1.8004082558400003, "seem": 2.29123971713, "linear": 13.8776223776, "unfortun": 9.966101694919999, "earli": 1.12468121281, "graph": 37.7102137767, "leaderboard": 1323.0, "there": 1.04091266719, "converg": 15.2947976879, "connect": 1.8843916913900003, "cross": 2.33127753304, "formula": 8.64235166032, "best": 1.5828514456600002, "origin": 1.13724928367, "right": 1.4054532577899999, "prefer": 3.0216977540900003, "take": 1.13961668222, "follow": 1.04640126549, "now": 1.160780873, "might": 2.1561863370900003, "tri": 1.8544562551099997, "high": 1.14777327935, "satisfactori": 31.6886227545, "regress": 51.2129032258, "power": 1.3396337861799998, "feel": 3.1356903021900004, "incept": 16.8535031847, "also": 1.01476510067, "around": 1.21394708671, "mean": 1.44906900329, "task": 3.88641370869, "stochast": 128.032258065, "encount": 4.13976531943, "therefor": 2.33401940606, "smarter": 170.709677419, "open": 1.24556723678, "ground": 1.97610156833, "into": 1.01502461479, "build": 1.6341739578, "hasn": 1443.27272727, "trial": 4.04175152749, "which": 1.005191845, "like": 1.14918566775, "activ": 1.46403541129, "other": 1.00992366412, "appli": 2.2972073506, "techniqu": 3.7293868921800004, "one": 1.00627495722, "restrict": 3.1062414400300002, "custom": 3.6346153846199996, "begin": 1.3305397251100002, "see": 1.27242125511, "algorithm": 27.9507042254, "imag": 2.70137825421, "problem": 1.76674827509, "get": 1.78562591385, "still": 1.1866357724799999, "random": 7.1902173913, "through": 1.07074930869, "tanh": 1443.27272727, "select": 2.02345144022, "drop": 2.4594887684, "part": 1.04330682789, "backpropag": 1443.27272727, "num\u2026": 1443.27272727, "play": 1.46390041494, "understand": 2.96858638743, "this": 1.00379362671, "find": 1.7294117647099998, "regularis": 481.09090909099996, "differ": 1.23654490225, "recognit": 4.40022172949, "most": 1.02096463023, "between": 1.03453668708, "low": 2.13072070863, "what": 1.25343439128, "diminish": 9.83643122677, "fail": 1.9281029876099998, "along": 1.2973768080399999, "all": 1.01146788991, "top": 1.8387769284200002, "nonlinear": 99.225, "augment": 16.5202913632, "verifi": 14.2258064516, "gradient": 41.889182058, "solv": 7.26923076923, "case": 1.48498737256, "dearth": 168.893617021, "that": 1.00398406375, "architectur": 5.12790697674, "memoris": 337.787234043, "dropout": 167.115789474, "rate": 2.14048806795, "more": 1.0171706817, "improv": 2.04376930999, "achiev": 1.87216981132, "these": 1.07415426252, "proven": 9.818181818180001, "numcat": 1443.27272727, "amount": 2.27027027027, "artifici": 8.31639601886, "could": 1.2043695949, "befor": 1.10036041031, "comment": 3.05954904606, "state": 1.0477133240899998, "friend": 2.20194174757, "can": 1.17626139142, "avail": 1.7288467821, "comput": 3.9277585353800006, "make": 1.0762660158600001, "textbook": 18.290322580599998, "analog": 9.05131128848, "unabl": 3.3081892060799998, "each": 1.18974820144, "size": 2.49387370405, "mustread": 1443.27272727, "necessari": 2.8421052631599997, "numdog": 1443.27272727, "read": 2.3149606299200003, "time": 1.01127460348, "common": 1.4025974025999999, "ascrib": 23.9096385542, "method": 2.5714285714300003, "convolut": 101.121019108, "bio": 42.336000000000006, "come": 1.32831325301, "must": 1.9220338983099996, "exampl": 1.50483412322, "outperform": 82.2590673575, "text": 3.12827586207, "identifi": 2.30187037843, "direct": 1.22226499346, "import": 1.3401992233700002, "first": 1.00761614623, "initialis": 835.5789473680001, "copi": 3.8375634517800004, "rohith": 1443.27272727, "hesh": 1443.27272727, "generalis": 114.215827338, "overshoot": 305.307692308, "while": 1.0441988950299999, "resort": 8.16246786632, "nowaday": 21.454054054100002, "often": 1.29452054795, "suggest": 1.7571665744299998, "learn": 2.32275054865, "when": 1.02076769755, "mention": 2.53894130817}}, "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Improving the Performance of a Neural Network</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Improving the Performance of a Neural Network Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/05/deep-learning-summit-toronto-geoff-hinton.html\" rel=\"prev\" title=\"Deep Learning Summit, Toronto featuring Geoff Hinton \u2013 save with KDnuggets\"/>\n<link href=\"https://www.kdnuggets.com/2018/05/introduction-content-personalization.html\" rel=\"next\" title=\"Introduction to Content Personalization\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=81525\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2018/05/improving-performance-neural-network.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-81525 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 30-May, 2018  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/05/index.html\">May</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/05/tutorials.html\">Tutorials, Overviews</a> \u00bb Improving the Performance of a Neural Network (\u00a0<a href=\"/2018/n22.html\">18:n22</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Improving the Performance of a Neural Network</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/05/deep-learning-summit-toronto-geoff-hinton.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/05/introduction-content-personalization.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/ensemble-methods\" rel=\"tag\">Ensemble Methods</a>, <a href=\"https://www.kdnuggets.com/tag/hyperparameter\" rel=\"tag\">Hyperparameter</a>, <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/overfitting\" rel=\"tag\">Overfitting</a>, <a href=\"https://www.kdnuggets.com/tag/tips\" rel=\"tag\">Tips</a></div>\n<br/>\n<p class=\"excerpt\">\n     There are many techniques available that could help us achieve that. Follow along to get to know them and to build your own accurate neural network.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"www.linkedin.com//in/grohith327\" rel=\"noopener noreferrer\" target=\"_blank\">Rohith Gandhi G</a></b></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/1000/1*QQbYkOWFlJiE75LKG0FKbg.jpeg\" width=\"65%\"/></p>\n<p>Neural networks are machine learning algorithms that provide state of the accuracy on many use cases. But, a lot of times the accuracy of the network we are building might not be satisfactory or might not take us to the top positions on the leaderboard in data science competitions. Therefore, we are always looking for better ways to improve the performance of our models. There are many techniques available that could help us achieve that. Follow along to get to know them and to build your own accurate neural network.</p>\n<p>\u00a0</p>\n<h3><b>Check for Overfitting</b></h3>\n<p>\u00a0<br>\n<img alt=\"Image result for overfitting\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*_7OPgojau8hkiPUiHoGK_w.png\" width=\"99%\"/></br></p>\n<p>The first step in ensuring your neural network performs well on the testing data is to verify that your neural network does not overfit. Ok, stop, what is overfitting? overfitting happens when your model starts to memorise values from the training data instead of learning from them. Therefore, when your model encounters a data it hasn\u2019t seen before, it is unable to perform well on them. To give you a better understanding, let\u2019s look at an analogy. We all would have a classmate who is good at memorising, and suppose a test on maths is coming up. You and your friend, who is good at memorising start studying from the text book. Your friend goes on memorising each formula, question and answer from the textbook but you, on the other hand, are smarter than him, so you decide to build on intuition and work out problems and learn how these formulas come into play. Test day arrives, if the problems in the test paper are taken straight out of the textbooks, then you can expect your memorising friend to do better on it but, if the problems are new ones that involve applying intuition, you do better on the test and your memorising friend fails miserably.</p>\n<p>How to identify if your model is overfitting? you can just cross check the training accuracy and testing accuracy. If training accuracy is much higher than testing accuracy then you can posit that your model has overfitted. You can also plot the predicted points on a graph to verify. There are some techniques to avoid overfitting:</p>\n<ul>\n<li>Regularisation of data (L1 or L2).\n<li>Dropouts\u200a\u2014\u200aRandomly dropping connections between neurons, forcing the network to find new paths and generalise.\n<li>Early Stopping\u200a\u2014\u200aPrecipitates the training of the neural network, leading to reduction in error in the test set.\n</li></li></li></ul>\n<p>\u00a0</p>\n<h3><b>Hyperparameter Tuning</b></h3>\n<p>\u00a0<br>\n<img alt=\"Image result for hyperparameter tuning\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/600/1*0215Gzmw56XvORtB7-Torw.png\" width=\"65%\"/></br></p>\n<p>Hyperparameters are values that you must initialise to the network, these values can\u2019t be learned by the network while training. E.x: In a convolutional neural network, some of the hyperparameters are kernel size, the number of layers in the neural network, activation function, loss function, optimizer used(gradient descent, RMSprop), batch size, number of epochs to train etc.</p>\n<p>Each neural network will have its best set of hyperparameters which will lead to maximum accuracy. You might ask, \u201cthere are so many hyperparameters, how do I choose what to use for each?\u201d, Unfortunately, there is no direct method to identify the best set of hyperparameter for each neural network so it is mostly obtained through trial and error. But, there are some best practices for some hyperparameters which are mentioned below,</p>\n<ul>\n<li>Learning Rate\u200a\u2014\u200aChoosing an optimum learning rate is important as it decides whether your network converges to the global minima or not. Selecting a high learning rate almost never gets you to the global minima as you have a very good chance of overshooting it. Therefore, you are always around the global minima but never converge to it. Selecting a small learning rate can help a neural network converge to the global minima but it takes a huge amount of time. Therefore, you have to train the network for a longer period of time. A small learning rate also makes the network susceptible to getting stuck in local minimum. i.e the network will converge onto a local minima and unable to come out of it due to the small learning rate. Therefore, you must be careful while setting the learning rate.\n<li>Network Architecture\u200a\u2014\u200aThere is no standard architecture that gives you high accuracy in all test cases. You have to experiment, try out different architectures, obtain inference from the result and try again. One idea that I would suggest is to use proven architectures instead of building one of your own. E.x: for image recognition task, you have VGG net, Resnet, Google\u2019s Inception network etc. These are all open sourced and have proven to be highly accurate, therefore, you could just copy their architecture and tweak them for your purpose.\n<li>Optimizers and Loss function\u200a\u2014\u200aThere is a myriad of options available for you to choose from. In fact, you could even define your custom loss function if necessary. But the commonly used optimizers are RMSprop, Stochastic Gradient Descent and Adam. These optimizers seem to work for most of the use cases. Commonly used loss functions are categorical cross entropy if your use case is a classification task. If you are performing a regression task, mean squared error is the commonly used loss function. Feel free to experiment with the hyperparameters of these optimizers and also with different optimizers and loss functions.\n<li>Batch Size &amp; Number of Epochs\u200a\u2014\u200aAgain, there is no standard value for batch size and epochs that works for all use cases. You have to experiment and try out different ones. In general practice, batch size values are set as either 8, 16, 32\u2026 The number of epochs depends on the developer\u2019s preference and the computing power he/she has.\n</li></li></li></li></ul>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/600/0*XFIo9zwmGsOJ9q6q.png\" width=\"65%\"/><br>\n<font size=\"-1\">ReLU Activation Function</font></br></center></p>\n<ul>\n<li>Activation Function\u200a\u2014\u200aActivation functions map the non-linear functional inputs to the outputs. Activation functions are highly important and choosing the right activation function helps your model to learn better. Nowadays, Rectified Linear Unit(ReLU) is the most widely used activation function as it solves the problem of vanishing gradients. Earlier Sigmoid and Tanh were the most widely used activation function. But, they suffered from the problem of vanishing gradients, i.e during backpropagation, the gradients diminish in value when they reach the beginning layers. This stopped the neural network from scaling to bigger sizes with more layers. ReLU was able to overcome this problem and hence allowed neural networks to be of large sizes.\n</li></ul>\n<p>\u00a0</p>\n<h3><b>Ensemble of Algorithms</b></h3>\n<p>\u00a0<br>\n<img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/600/0*PkEAXYxPnY4SgpqD.\" width=\"75%\"/></br></p>\n<p>If individual neural networks are not as accurate as you would like them to be, you can create an ensemble of neural networks and combine their predictive power. You can choose different neural network architectures and train them on different parts of the data and ensemble them and use their collective predictive power to get high accuracy on test data. Suppose, you are building a cats vs dogs classifier, 0-cat and 1-dog. When combining different cats vs dogs classifiers, the accuracy of the ensemble algorithm increases based on the Pearson Correlation between the individual classifiers. Let us look at an example, take 3 models and measure their individual accuracy.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Ground Truth: 1111111111\r\nClassifier 1: 1111111100 = 80% accuracy\r\nClassifier 2: 1111111100 = 80% accuracy\r\nClassifier 3: 1011111100 = 70% accuracy</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The Pearson Correlation of the three models is high. Therefore, ensembling them does not improve the accuracy. If we ensemble the above three models using a majority vote, we get the following result.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Ensemble Result: 1111111100 = 80% accuracy</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Now, let us look at three models having a very low Pearson Correlation between their outputs.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Ground Truth: 1111111111\r\nClassifier 1: 1111111100 = 80% accuracy\r\nClassifier 2: 0111011101 = 70% accuracy\r\nClassifier 3: 1000101111 = 60% accuracy</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>When we ensemble these three weak learners, we get the following result.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Ensemble Result: 1111111101 = 90% accuracy</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>As you can see above, an ensemble of weak learners with low Pearson Correlation is able to outperform an ensemble with high Pearson Correlation between them.</p>\n<p>\u00a0</p>\n<h3><b>Dearth of\u00a0Data</b></h3>\n<p>\u00a0<br/>\n<img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*95eOokKGzqLUA621yq3WZA.jpeg\" width=\"75%\"/></p>\n<p>After performing all of the techniques above, if your model still doesn\u2019t perform better in your test dataset, it could be ascribed to the lack of training data. There are many use cases where the amount of training data available is restricted. If you are not able to collect more data then you could resort to data augmentation techniques.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*N884-sR3X5r1titKRS9lxQ.jpeg\" width=\"99%\"/><br/>\n<font size=\"-1\">Data Augmentation Techniques</font></center></p>\n<p>If you are working on a dataset of images, you can augment new images to the training data by shearing the image, flipping the image, randomly cropping the image etc. This could provide different examples for the neural network to train on.</p>\n<p>\u00a0</p>\n<h3><b>Conclusion</b></h3>\n<p>\u00a0<br/>\nThese techniques are considered as best practices and often seem to be effective in increasing the model\u2019s ability to learn features. This might seem like a long post, thank you for reading through it and let me know if any of these techniques did work for you\u00a0:)</p>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"ww.linkedin.com//in/grohith327\" rel=\"noopener noreferrer\" target=\"_blank\">Rohith Gandhi G</a></b>: \"What I cannot create, I do not understand\" - Richard Feynman.</p>\n<p><a href=\"https://towardsdatascience.com/how-to-increase-the-accuracy-of-a-neural-network-9f5d1c6f407d\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/01/learning-rate-useful-neural-network.html\">Is Learning Rate Useful in Artificial Neural Networks?</a>\n<li><a href=\"/2018/01/regularization-machine-learning.html\">Regularization in Machine Learning</a>\n<li><a href=\"/2018/04/top-free-books-deep-learning.html\">Top 8 Free Must-Read Books on Deep Learning</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/05/deep-learning-summit-toronto-geoff-hinton.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/05/introduction-content-personalization.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-2-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-3-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-4-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-5-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-6-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/best-data-visualization-techniques.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-mp-7-best-data-viz');\"><b>Best Data Visualization Techniques for small and large data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-1-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-2-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-3-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-4-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/04/data-visualization-python-matplotlib-seaborn.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-5-plt-sea-viz');\"><b>Data Visualization in Python: Matplotlib vs Seaborn</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-6-ts-intro');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-23-ms-7-recognize');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-delivering-trusted-ai-microsoft.html\">Delivering Trusted AI with DataRobot and Microsoft</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-ai-data-production-landscape.html\">AI and the data production landscape</a><li> <a href=\"https://www.kdnuggets.com/2019/04/most-desired-skill-data-science.html\">The most desired skill in data science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/projects-include-data-science-portfolio.html\">Projects to Include in a Data Science Portfolio</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rework-meet-worlds-leading-ai-deep-learning-experts.html\">Meet the World\u2019s Leading AI &amp; Deep Learning ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/problem-data-science-job-postings.html\">The problem with data science job postings</a><li> <a href=\"https://www.kdnuggets.com/2019/04/graduating-gans-understanding-generative-adversarial-networks.html\">Graduating in GANs: Going From Understanding Generative Advers...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-join-new-generation-ai-technologists.html\">Join the new generation of AI technologists</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr17-apr23.html\">Top tweets, Apr 17\u201323: The History of Artificial #NeuralN...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/activestate-top-10-python-use-cases.html\">Top 10 Python Use Cases</a><li> <a href=\"https://www.kdnuggets.com/2019/04/future-generative-adversarial-networks.html\">Generative Adversarial Networks \u2013 Key Milestones and Sta...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/attention-craving-rnn-building-transformer-networks.html\">Attention Craving RNNS: Building Up To Transformer Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-24-fors-marsh-group-lead-data-scientist.html\">Fors Marsh Group: Lead Data Scientist [Arlington, VA]</a><li> <a href=\"https://www.kdnuggets.com/2019/n16.html\">KDnuggets 19:n16, Apr 24: Data Visualization in Python with...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-mega-paw-las-vegas-lower-rates-end-friday.html\">Lower Rates End Friday for Mega-PAW Vegas \u2013 the Largest Pred...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0415-0421.html\">Top Stories, Apr 15-21: Data Visualization in Python: Matplotl...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/wharton-customer-analytics-initiative-conference.html\">Wharton Customer Analytics Initiative Annual Conference in Phi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/machine-learning-graph-analytics.html\">Machine Learning and Deep Link Graph Analytics: A Powerful Com...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-online.html\">2019 Best Masters in Data Science and Analytics \u2013 Online</a><li> <a href=\"https://www.kdnuggets.com/2019/04/worth-studying-data-science-masters.html\">Was it Worth Studying a Data Science Masters?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/approach-pre-trained-deep-learning-models-caution.html\">Approach pre-trained deep learning models with caution</a><li> <a href=\"https://www.kdnuggets.com/2019/04/coursera-earn-deep-learning-certificate.html\">Earn a Deep Learning Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/04/octoparse-scrape-data-website.html\">Easy Way to Scrape Data from Website By Yourself</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-environment.html\">AI Supporting The Earth</a><li> <a href=\"https://www.kdnuggets.com/2019/04/mueller-report-word-cloud-brief-tutorial-r.html\">The Mueller Report Word Cloud: A brief tutorial in R</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/05/index.html\">May</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/05/tutorials.html\">Tutorials, Overviews</a> \u00bb Improving the Performance of a Neural Network (\u00a0<a href=\"/2018/n22.html\">18:n22</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1556327703\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.691 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-26 21:15:03 -->\n<!-- Compression = gzip -->", "content_tokenized": ["comment", "rohith", "gandhi", "neural", "network", "are", "machin", "learn", "algorithm", "that", "provid", "state", "the", "accuraci", "mani", "use", "case", "but", "lot", "time", "the", "accuraci", "the", "network", "are", "build", "might", "not", "satisfactori", "might", "not", "take", "the", "top", "posit", "the", "leaderboard", "data", "scienc", "competit", "therefor", "are", "alway", "look", "for", "better", "way", "improv", "the", "perform", "our", "model", "there", "are", "mani", "techniqu", "avail", "that", "could", "help", "achiev", "that", "follow", "along", "get", "know", "them", "and", "build", "own", "accur", "neural", "network", "check", "for", "overfit", "the", "first", "step", "ensur", "neural", "network", "perform", "well", "the", "test", "data", "verifi", "that", "neural", "network", "doe", "not", "overfit", "stop", "what", "overfit", "overfit", "happen", "when", "model", "start", "memoris", "valu", "from", "the", "train", "data", "instead", "learn", "from", "them", "therefor", "when", "model", "encount", "data", "hasn", "seen", "befor", "unabl", "perform", "well", "them", "give", "better", "understand", "let", "look", "analog", "all", "would", "have", "classmat", "who", "good", "memoris", "and", "suppos", "test", "math", "come", "and", "friend", "who", "good", "memoris", "start", "studi", "from", "the", "text", "book", "friend", "goe", "memoris", "each", "formula", "question", "and", "answer", "from", "the", "textbook", "but", "the", "other", "hand", "are", "smarter", "than", "him", "decid", "build", "intuit", "and", "work", "out", "problem", "and", "learn", "how", "these", "formula", "come", "into", "play", "test", "day", "arriv", "the", "problem", "the", "test", "paper", "are", "taken", "straight", "out", "the", "textbook", "then", "can", "expect", "memoris", "friend", "better", "but", "the", "problem", "are", "new", "one", "that", "involv", "appli", "intuit", "better", "the", "test", "and", "memoris", "friend", "fail", "miser", "how", "identifi", "model", "overfit", "can", "just", "cross", "check", "the", "train", "accuraci", "and", "test", "accuraci", "train", "accuraci", "much", "higher", "than", "test", "accuraci", "then", "can", "posit", "that", "model", "has", "overfit", "can", "also", "plot", "the", "predict", "point", "graph", "verifi", "there", "are", "some", "techniqu", "avoid", "overfit", "regularis", "data", "dropout", "random", "drop", "connect", "between", "neuron", "forc", "the", "network", "find", "new", "path", "and", "generalis", "earli", "stop", "precipit", "the", "train", "the", "neural", "network", "lead", "reduct", "error", "the", "test", "set", "hyperparamet", "tune", "hyperparamet", "are", "valu", "that", "must", "initialis", "the", "network", "these", "valu", "can", "learn", "the", "network", "while", "train", "convolut", "neural", "network", "some", "the", "hyperparamet", "are", "kernel", "size", "the", "number", "layer", "the", "neural", "network", "activ", "function", "loss", "function", "optim", "batch", "size", "number", "epoch", "train", "etc", "each", "neural", "network", "will", "have", "best", "set", "hyperparamet", "which", "will", "lead", "maximum", "accuraci", "might", "ask", "there", "are", "mani", "hyperparamet", "how", "choos", "what", "use", "for", "each", "unfortun", "there", "direct", "method", "identifi", "the", "best", "set", "hyperparamet", "for", "each", "neural", "network", "most", "obtain", "through", "trial", "and", "error", "but", "there", "are", "some", "best", "practic", "for", "some", "hyperparamet", "which", "are", "mention", "below", "learn", "rate", "choos", "optimum", "learn", "rate", "import", "decid", "whether", "network", "converg", "the", "global", "minima", "not", "select", "high", "learn", "rate", "almost", "never", "get", "the", "global", "minima", "have", "veri", "good", "chanc", "overshoot", "therefor", "are", "alway", "around", "the", "global", "minima", "but", "never", "converg", "select", "small", "learn", "rate", "can", "help", "neural", "network", "converg", "the", "global", "minima", "but", "take", "huge", "amount", "time", "therefor", "have", "train", "the", "network", "for", "longer", "period", "time", "small", "learn", "rate", "also", "make", "the", "network", "suscept", "get", "stuck", "local", "minimum", "the", "network", "will", "converg", "onto", "local", "minima", "and", "unabl", "come", "out", "due", "the", "small", "learn", "rate", "therefor", "must", "care", "while", "set", "the", "learn", "rate", "network", "architectur", "there", "standard", "architectur", "that", "give", "high", "accuraci", "all", "test", "case", "have", "experi", "tri", "out", "differ", "architectur", "obtain", "infer", "from", "the", "result", "and", "tri", "again", "one", "idea", "that", "would", "suggest", "use", "proven", "architectur", "instead", "build", "one", "own", "for", "imag", "recognit", "task", "have", "net", "resnet", "googl", "incept", "network", "etc", "these", "are", "all", "open", "sourc", "and", "have", "proven", "high", "accur", "therefor", "could", "just", "copi", "their", "architectur", "and", "tweak", "them", "for", "purpos", "optim", "and", "loss", "function", "there", "myriad", "option", "avail", "for", "choos", "from", "fact", "could", "even", "defin", "custom", "loss", "function", "necessari", "but", "the", "common", "use", "optim", "are", "rmsprop", "stochast", "gradient", "descent", "and", "adam", "these", "optim", "seem", "work", "for", "most", "the", "use", "case", "common", "use", "loss", "function", "are", "categor", "cross", "entropi", "use", "case", "classif", "task", "are", "perform", "regress", "task", "mean", "squar", "error", "the", "common", "use", "loss", "function", "feel", "free", "experi", "with", "the", "hyperparamet", "these", "optim", "and", "also", "with", "differ", "optim", "and", "loss", "function", "batch", "size", "number", "epoch", "again", "there", "standard", "valu", "for", "batch", "size", "and", "epoch", "that", "work", "for", "all", "use", "case", "have", "experi", "and", "tri", "out", "differ", "one", "general", "practic", "batch", "size", "valu", "are", "set", "either", "num", "num", "num\u2026", "the", "number", "epoch", "depend", "the", "develop", "prefer", "and", "the", "comput", "power", "hesh", "has", "relu", "activ", "function", "activ", "function", "activ", "function", "map", "the", "nonlinear", "function", "input", "the", "output", "activ", "function", "are", "high", "import", "and", "choos", "the", "right", "activ", "function", "help", "model", "learn", "better", "nowaday", "rectifi", "linear", "the", "most", "wide", "use", "activ", "function", "solv", "the", "problem", "vanish", "gradient", "earlier", "sigmoid", "and", "tanh", "were", "the", "most", "wide", "use", "activ", "function", "but", "they", "suffer", "from", "the", "problem", "vanish", "gradient", "dure", "backpropag", "the", "gradient", "diminish", "valu", "when", "they", "reach", "the", "begin", "layer", "this", "stop", "the", "neural", "network", "from", "scale", "bigger", "size", "with", "more", "layer", "relu", "abl", "overcom", "this", "problem", "and", "henc", "allow", "neural", "network", "larg", "size", "ensembl", "algorithm", "individu", "neural", "network", "are", "not", "accur", "would", "like", "them", "can", "creat", "ensembl", "neural", "network", "and", "combin", "their", "predict", "power", "can", "choos", "differ", "neural", "network", "architectur", "and", "train", "them", "differ", "part", "the", "data", "and", "ensembl", "them", "and", "use", "their", "collect", "predict", "power", "get", "high", "accuraci", "test", "data", "suppos", "are", "build", "cat", "dog", "classifi", "numcat", "and", "numdog", "when", "combin", "differ", "cat", "dog", "classifi", "the", "accuraci", "the", "ensembl", "algorithm", "increas", "base", "the", "pearson", "correl", "between", "the", "individu", "classifi", "let", "look", "exampl", "take", "num", "model", "and", "measur", "their", "individu", "accuraci", "ground", "truth", "num", "classifi", "num", "num", "num", "accuraci", "classifi", "num", "num", "num", "accuraci", "classifi", "num", "num", "num", "accuraci", "the", "pearson", "correl", "the", "three", "model", "high", "therefor", "ensembl", "them", "doe", "not", "improv", "the", "accuraci", "ensembl", "the", "abov", "three", "model", "use", "major", "vote", "get", "the", "follow", "result", "ensembl", "result", "num", "num", "accuraci", "now", "let", "look", "three", "model", "have", "veri", "low", "pearson", "correl", "between", "their", "output", "ground", "truth", "num", "classifi", "num", "num", "num", "accuraci", "classifi", "num", "num", "num", "accuraci", "classifi", "num", "num", "num", "accuraci", "when", "ensembl", "these", "three", "weak", "learner", "get", "the", "follow", "result", "ensembl", "result", "num", "num", "accuraci", "can", "see", "abov", "ensembl", "weak", "learner", "with", "low", "pearson", "correl", "abl", "outperform", "ensembl", "with", "high", "pearson", "correl", "between", "them", "dearth", "data", "after", "perform", "all", "the", "techniqu", "abov", "model", "still", "perform", "better", "test", "dataset", "could", "ascrib", "the", "lack", "train", "data", "there", "are", "mani", "use", "case", "where", "the", "amount", "train", "data", "avail", "restrict", "are", "not", "abl", "collect", "more", "data", "then", "could", "resort", "data", "augment", "techniqu", "data", "augment", "techniqu", "are", "work", "dataset", "imag", "can", "augment", "new", "imag", "the", "train", "data", "shear", "the", "imag", "flip", "the", "imag", "random", "crop", "the", "imag", "etc", "this", "could", "provid", "differ", "exampl", "for", "the", "neural", "network", "train", "conclus", "these", "techniqu", "are", "consid", "best", "practic", "and", "often", "seem", "effect", "increas", "the", "model", "abil", "learn", "featur", "this", "might", "seem", "like", "long", "post", "thank", "for", "read", "through", "and", "let", "know", "ani", "these", "techniqu", "work", "for", "bio", "rohith", "gandhi", "what", "can", "not", "creat", "not", "understand", "richard", "feynman", "origin", "repost", "with", "permiss", "relat", "learn", "rate", "use", "artifici", "neural", "network", "regular", "machin", "learn", "top", "num", "free", "mustread", "book", "deep", "learn"], "timestamp_scraper": 1556370945.342955, "title": "Improving the Performance of a Neural Network", "read_time": 446.09999999999997, "content_html": "<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"www.linkedin.com//in/grohith327\" rel=\"noopener noreferrer\" target=\"_blank\">Rohith Gandhi G</a></b></p>\n<p><img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/1000/1*QQbYkOWFlJiE75LKG0FKbg.jpeg\" width=\"65%\"/></p>\n<p>Neural networks are machine learning algorithms that provide state of the accuracy on many use cases. But, a lot of times the accuracy of the network we are building might not be satisfactory or might not take us to the top positions on the leaderboard in data science competitions. Therefore, we are always looking for better ways to improve the performance of our models. There are many techniques available that could help us achieve that. Follow along to get to know them and to build your own accurate neural network.</p>\n<p>\u00a0</p>\n<h3><b>Check for Overfitting</b></h3>\n<p>\u00a0<br>\n<img alt=\"Image result for overfitting\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*_7OPgojau8hkiPUiHoGK_w.png\" width=\"99%\"/></br></p>\n<p>The first step in ensuring your neural network performs well on the testing data is to verify that your neural network does not overfit. Ok, stop, what is overfitting? overfitting happens when your model starts to memorise values from the training data instead of learning from them. Therefore, when your model encounters a data it hasn\u2019t seen before, it is unable to perform well on them. To give you a better understanding, let\u2019s look at an analogy. We all would have a classmate who is good at memorising, and suppose a test on maths is coming up. You and your friend, who is good at memorising start studying from the text book. Your friend goes on memorising each formula, question and answer from the textbook but you, on the other hand, are smarter than him, so you decide to build on intuition and work out problems and learn how these formulas come into play. Test day arrives, if the problems in the test paper are taken straight out of the textbooks, then you can expect your memorising friend to do better on it but, if the problems are new ones that involve applying intuition, you do better on the test and your memorising friend fails miserably.</p>\n<p>How to identify if your model is overfitting? you can just cross check the training accuracy and testing accuracy. If training accuracy is much higher than testing accuracy then you can posit that your model has overfitted. You can also plot the predicted points on a graph to verify. There are some techniques to avoid overfitting:</p>\n<ul>\n<li>Regularisation of data (L1 or L2).\n<li>Dropouts\u200a\u2014\u200aRandomly dropping connections between neurons, forcing the network to find new paths and generalise.\n<li>Early Stopping\u200a\u2014\u200aPrecipitates the training of the neural network, leading to reduction in error in the test set.\n</li></li></li></ul>\n<p>\u00a0</p>\n<h3><b>Hyperparameter Tuning</b></h3>\n<p>\u00a0<br>\n<img alt=\"Image result for hyperparameter tuning\" class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/600/1*0215Gzmw56XvORtB7-Torw.png\" width=\"65%\"/></br></p>\n<p>Hyperparameters are values that you must initialise to the network, these values can\u2019t be learned by the network while training. E.x: In a convolutional neural network, some of the hyperparameters are kernel size, the number of layers in the neural network, activation function, loss function, optimizer used(gradient descent, RMSprop), batch size, number of epochs to train etc.</p>\n<p>Each neural network will have its best set of hyperparameters which will lead to maximum accuracy. You might ask, \u201cthere are so many hyperparameters, how do I choose what to use for each?\u201d, Unfortunately, there is no direct method to identify the best set of hyperparameter for each neural network so it is mostly obtained through trial and error. But, there are some best practices for some hyperparameters which are mentioned below,</p>\n<ul>\n<li>Learning Rate\u200a\u2014\u200aChoosing an optimum learning rate is important as it decides whether your network converges to the global minima or not. Selecting a high learning rate almost never gets you to the global minima as you have a very good chance of overshooting it. Therefore, you are always around the global minima but never converge to it. Selecting a small learning rate can help a neural network converge to the global minima but it takes a huge amount of time. Therefore, you have to train the network for a longer period of time. A small learning rate also makes the network susceptible to getting stuck in local minimum. i.e the network will converge onto a local minima and unable to come out of it due to the small learning rate. Therefore, you must be careful while setting the learning rate.\n<li>Network Architecture\u200a\u2014\u200aThere is no standard architecture that gives you high accuracy in all test cases. You have to experiment, try out different architectures, obtain inference from the result and try again. One idea that I would suggest is to use proven architectures instead of building one of your own. E.x: for image recognition task, you have VGG net, Resnet, Google\u2019s Inception network etc. These are all open sourced and have proven to be highly accurate, therefore, you could just copy their architecture and tweak them for your purpose.\n<li>Optimizers and Loss function\u200a\u2014\u200aThere is a myriad of options available for you to choose from. In fact, you could even define your custom loss function if necessary. But the commonly used optimizers are RMSprop, Stochastic Gradient Descent and Adam. These optimizers seem to work for most of the use cases. Commonly used loss functions are categorical cross entropy if your use case is a classification task. If you are performing a regression task, mean squared error is the commonly used loss function. Feel free to experiment with the hyperparameters of these optimizers and also with different optimizers and loss functions.\n<li>Batch Size &amp; Number of Epochs\u200a\u2014\u200aAgain, there is no standard value for batch size and epochs that works for all use cases. You have to experiment and try out different ones. In general practice, batch size values are set as either 8, 16, 32\u2026 The number of epochs depends on the developer\u2019s preference and the computing power he/she has.\n</li></li></li></li></ul>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/600/0*XFIo9zwmGsOJ9q6q.png\" width=\"65%\"/><br>\n<font size=\"-1\">ReLU Activation Function</font></br></center></p>\n<ul>\n<li>Activation Function\u200a\u2014\u200aActivation functions map the non-linear functional inputs to the outputs. Activation functions are highly important and choosing the right activation function helps your model to learn better. Nowadays, Rectified Linear Unit(ReLU) is the most widely used activation function as it solves the problem of vanishing gradients. Earlier Sigmoid and Tanh were the most widely used activation function. But, they suffered from the problem of vanishing gradients, i.e during backpropagation, the gradients diminish in value when they reach the beginning layers. This stopped the neural network from scaling to bigger sizes with more layers. ReLU was able to overcome this problem and hence allowed neural networks to be of large sizes.\n</li></ul>\n<p>\u00a0</p>\n<h3><b>Ensemble of Algorithms</b></h3>\n<p>\u00a0<br>\n<img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/600/0*PkEAXYxPnY4SgpqD.\" width=\"75%\"/></br></p>\n<p>If individual neural networks are not as accurate as you would like them to be, you can create an ensemble of neural networks and combine their predictive power. You can choose different neural network architectures and train them on different parts of the data and ensemble them and use their collective predictive power to get high accuracy on test data. Suppose, you are building a cats vs dogs classifier, 0-cat and 1-dog. When combining different cats vs dogs classifiers, the accuracy of the ensemble algorithm increases based on the Pearson Correlation between the individual classifiers. Let us look at an example, take 3 models and measure their individual accuracy.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Ground Truth: 1111111111\r\nClassifier 1: 1111111100 = 80% accuracy\r\nClassifier 2: 1111111100 = 80% accuracy\r\nClassifier 3: 1011111100 = 70% accuracy</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>The Pearson Correlation of the three models is high. Therefore, ensembling them does not improve the accuracy. If we ensemble the above three models using a majority vote, we get the following result.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Ensemble Result: 1111111100 = 80% accuracy</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>Now, let us look at three models having a very low Pearson Correlation between their outputs.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Ground Truth: 1111111111\r\nClassifier 1: 1111111100 = 80% accuracy\r\nClassifier 2: 0111011101 = 70% accuracy\r\nClassifier 3: 1000101111 = 60% accuracy</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>When we ensemble these three weak learners, we get the following result.</p>\n<div style=\"width:98%;border:1px solid #ccc; overflow:auto; padding-left:10px; padding-bottom:10px; padding-top:10px\">\n<pre>Ensemble Result: 1111111101 = 90% accuracy</pre>\n</div>\n<p><br class=\"blank\"/></p>\n<p>As you can see above, an ensemble of weak learners with low Pearson Correlation is able to outperform an ensemble with high Pearson Correlation between them.</p>\n<p>\u00a0</p>\n<h3><b>Dearth of\u00a0Data</b></h3>\n<p>\u00a0<br/>\n<img class=\"aligncenter\" src=\"https://cdn-images-1.medium.com/max/800/1*95eOokKGzqLUA621yq3WZA.jpeg\" width=\"75%\"/></p>\n<p>After performing all of the techniques above, if your model still doesn\u2019t perform better in your test dataset, it could be ascribed to the lack of training data. There are many use cases where the amount of training data available is restricted. If you are not able to collect more data then you could resort to data augmentation techniques.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*N884-sR3X5r1titKRS9lxQ.jpeg\" width=\"99%\"/><br/>\n<font size=\"-1\">Data Augmentation Techniques</font></center></p>\n<p>If you are working on a dataset of images, you can augment new images to the training data by shearing the image, flipping the image, randomly cropping the image etc. This could provide different examples for the neural network to train on.</p>\n<p>\u00a0</p>\n<h3><b>Conclusion</b></h3>\n<p>\u00a0<br/>\nThese techniques are considered as best practices and often seem to be effective in increasing the model\u2019s ability to learn features. This might seem like a long post, thank you for reading through it and let me know if any of these techniques did work for you\u00a0:)</p>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"ww.linkedin.com//in/grohith327\" rel=\"noopener noreferrer\" target=\"_blank\">Rohith Gandhi G</a></b>: \"What I cannot create, I do not understand\" - Richard Feynman.</p>\n<p><a href=\"https://towardsdatascience.com/how-to-increase-the-accuracy-of-a-neural-network-9f5d1c6f407d\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/01/learning-rate-useful-neural-network.html\">Is Learning Rate Useful in Artificial Neural Networks?</a>\n<li><a href=\"/2018/01/regularization-machine-learning.html\">Regularization in Machine Learning</a>\n<li><a href=\"/2018/04/top-free-books-deep-learning.html\">Top 8 Free Must-Read Books on Deep Learning</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "website": "kdnuggets"}