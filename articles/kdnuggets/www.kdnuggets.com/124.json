{"title_html": "<h1 id=\"title\">Data Science Project Flow for Startups</h1> ", "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Data Science Project Flow for Startups</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/01/data-science-project-flow-startups.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Data Science Project Flow for Startups Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2019/01/top-tweets-jan16-jan22.html\" rel=\"prev\" title=\"Top KDnuggets tweets, Jan 16-22: How to build an API for a machine learning model in 5 minutes using Flask; Great Introduction to Deep #ReinforcementLearning\"/>\n<link href=\"https://www.kdnuggets.com/2019/01/top-jobs-data-science.html\" rel=\"next\" title=\"The Data Science Gold Rush: Top Jobs in Data Science and How to Secure Them\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2019/01/data-science-project-flow-startups.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=89743\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2019/01/data-science-project-flow-startups.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-89743 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 24-Jan, 2019  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/01/index.html\">Jan</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/01/tutorials.html\">Tutorials, Overviews</a> \u00bb Data Science Project Flow for Startups (\u00a0<a href=\"/2019/n05.html\">19:n05</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Data Science Project Flow for Startups</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2019/01/top-tweets-jan16-jan22.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/01/top-jobs-data-science.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/data-science\" rel=\"tag\">Data Science</a>, <a href=\"https://www.kdnuggets.com/tag/startups\" rel=\"tag\">startups</a>, <a href=\"https://www.kdnuggets.com/tag/workflow\" rel=\"tag\">Workflow</a></div>\n<br/>\n<p class=\"excerpt\">\n     The aim of this post, then, is to present the characteristic project flow that I have identified in the working process of both my colleagues and myself in recent years. Hopefully, this can help both data scientists and the people working with them to structure data science projects in a way that reflects their uniqueness.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"http://www.shaypalachy.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Shay Palachy</a>, Data Scientist &amp; Consultant</b></p>\n<p>I was recently asked by a startup I\u2019m consulting (<a href=\"https://www.bigpanda.io/\" rel=\"noopener noreferrer\" target=\"_blank\">BigPanda</a>) to give my opinion about the structure and flow of data science projects, which made me think about what makes them unique. Both managers and the different teams in a startup might find the differences between a data science project and a software development one unintuitive and confusing. If not stated and accounted for explicitly, these fundamental differences might cause misunderstanding and clashes between the data scientist and his peers.</p>\n<p>Respectively, researchers coming from academia (or highly research-oriented industry research groups) might have their own challenges when arriving at a startup or a smaller company. They might find it challenging to incorporate new types of inputs, such as product and business needs, tighter infrastructure and compute constraints and costumer feedback, into their research and development process.</p>\n<p>The aim of this post, then, is to present the characteristic project flow that I have identified in the working process of both my colleagues and myself in recent years. Hopefully, this can help both data scientists and the people working with them to structure data science projects in a way that reflects their uniqueness.</p>\n<p>The flow was built with small startups in mind, where a small team of data scientists (usually one to four) run short and mid-sized projects led by a single person at a time. Bigger teams or those in machine-learning-first, deep-tech startups might still find this a useful structure, but processes there are longer and structured differently in many cases.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*fBNC1lqYSdM6bjMxxr56Tw.png\" width=\"99%\"><br>\n<font size=\"-1\">Figure 1: Data Science Project Flow for\u00a0Startups</font></br></img></center></p>\n<p>I have divided the process into three aspects that run in parallel: product, data science and data engineering. In many cases (including most of the places I worked for), there might not be a data engineer to perform these duties. In this case the data scientist is usually in charge of working with developers to help with these aspects. Alternatively, the data scientist might do these preparations, if they happen to be the rarest of all of God\u2019s beasts:\u00a0<em>the Full Stack Data Scientist!\u00a0</em>\u2728\ud83e\udd84\u2728. You can thus replace\u00a0<em>data engineer\u00a0</em>with\u00a0<em>data scientist\u00a0</em>whenever it is mentioned<em>,</em>\u00a0depending on your environment.<br>\nOn the time axis, I broke the process down into four distinct phases:</br></p>\n<ol>\n<li>Scoping\n<li>Research\n<li>(Model) Development\n<li>Deployment\n</li></li></li></li></ol>\n<p>I\u2019ll try and walk you through each of these, in order.</p>\n<p>\u00a0</p>\n<h3>1. The Scoping\u00a0Phase</h3>\n<p>\u00a0<br>\nDefining the scope of a data science project is crucial more than in any other type of project.</br></p>\n<p><b>1.1. Product\u00a0need</b></p>\n<p>A project should always start with a product need\u00a0(even if the original idea was technical or theoretical), a need validated to some degree by product/business/customer success people. The product person should have an idea of how this feature should (roughly) end up looking, and that either existing or new customers will be willing to pay for it (or that it will prevent churn / drive subscriptions / drive sales of other products / etc.).</p>\n<p>A product need is not a full project definition, but should rather be stated as a problem or challenge; e.g.\u00a0<em>\u201cour customers need a way to understand how they spend their budgets\u201d</em>\u00a0or\u00a0<em>\u201cwe do not manage to get our older users to keep taking their medicine; this increases churn\u201d</em>\u00a0or\u00a0<em>\u201ccustomers will pay more for a product that can also predict rush hours at the airports they run\u201d</em>.</p>\n<p><b>1.2. Initial solution\u00a0ideation</b></p>\n<p>This is where the data scientist, together with the product person in charge, the data engineer and any other stakeholder, comes up with different rough sketches for possible solutions. This means both the general approach (e.g. unsupervised clustering vs boosted-tree-based classification vs probabilistic inference) <strong>and</strong> the data to be used (e.g. this specific table from our database, or some specific user behavior that we do not yet monitor or save, or an external data source).</p>\n<p>This usually also involves some level of data exploration. You can\u2019t go real deep here, but any promising \u201clow-hanging fruits\u201d can help guide ideation.</p>\n<p>The data scientist should lead this process and is usually in charge of providing most of the solution ideas, but I would urge you to use all those taking part in the process for solution ideation; I have had the good fortune to get the best solution ideas for a project handed to me by a back-end developer, the CTO or the product person in charge. Don\u2019t assume that different, and less theory-oriented backgrounds, invalidate people from taking part in this phase; the additional minds and viewpoints are always valuable.</p>\n<p><b>1.3. Data preparation and accessibility</b></p>\n<p>The team should now have a good idea of the data that would hopefully be used to explore possible solutions (or at least the first such data set or source). Thus, the process of providing data access and preparing it for exploration and use should already start, in parallel with the next phases.</p>\n<p>This can sometime entail dumping large data sets from production databases into their staging/exploration counterparts, or to colder storage (for example, object storage) if its time availability is not critical in the research phase. Conversely, it can mean pulling large data dumps from very cold storage back into table or document form to enable fast querying and complex computations. Whatever the case, this phase is required for the research phase to start and frequently ends up taking more time than expected, and so that\u2019s the right time to initiate it.</p>\n<p><b>1.4. Scope &amp;\u00a0KPIs</b></p>\n<p>This phase is about deciding together on the scope and the KPIs of the project.</p>\n<p>KPIs should be defined first in product terms, but in much more detail than before; e.g. with respect to the three aforementioned product needs, they might become\u00a0<em>\u201ccustomers could now use a dashboard with CTR stats and projection per category\u201d</em>, or\u00a0<em>\u201cmissed medicine days by users over 65 will be reduced by at least 10% over the next two quarters\u201d</em>, or\u00a0<em>\u201ccustomers will receive weekly predictions of rush hours in their airports with granularity of at least an hour, and approximation of at least \u00b150%\u201d</em>.</p>\n<p>These KPIS should be then translated to measurable model metrics. With luck, these will be very hard metrics, such as\u00a0<em>\u201cpredicting the expected CTR of an ad with approximation of at least X% in at least Y% of the cases, for any ad that runs for at least a week, and for any client with more than two months of historic data\u201d</em>. In some cases, however, softer metrics will have to be used, such as\u00a0<em>\u201ctime required for topic exploration using the generated expanded queries will be shortened, and/or result quality will improve, when compared to the original queries\u201d</em>. This is especially true when the model is meant to assist some complex human function.</p>\n<p>Technically even these metrics can be defined very strictly (and in academic research, they usually are), but depending on resources and time constraints we might settle with approximating them using human feedback. In this case each feedback iteration might take longer, and so we will usually try to find additional hard metrics to guide us through most of the upcoming research iterations, with the costlier feedback being elicited only once every few iterations, or on significant changes.</p>\n<p>Finally, scope is especially important here because research projects have a tendency to drag on, and to naturally expand in size and scope as new possibilities arise while researching or when an examined approach answers the demands only partially.</p>\n<p><strong>Scope limitation 1:</strong>\u00a0I find it more productive to limit scope explicitly; for example, if you\u2019ve decided that a Multi-Armed Bandit based model is the most promising approach to start with, you might define the project scope to a single two/three weeks iteration of model development, deploying the model regardless of its accuracy (as long as it\u2019s over 60%, for example). Then, if improvement in accuracy is valuable (in some cases it might turn out to be less so), developing a second model might be thought of as a separate project.</p>\n<p><strong>Scope limitation 2:</strong>\u00a0Another variation on scope limitation is using increasing degrees of complexity; for example, the first project might aim to deploy a model that only needs to provide a rather large set of candidates of ad wording and color variations for your own customer success people to work with; the second might attempt to build a model that gives a smaller set of suggestions that the customer can see herself; and a final project might try for a model that highlights a single option, ranking below it a couple more, and adding CTR projections and demographic reach for each variation.</p>\n<blockquote><p>This is already a huge departure from software engineering, where usually components are iterated over for increased scale rather than complexity.</p></blockquote>\n<p>Nevertheless, the metric-to-product-value function might be a step function, meaning that any model performing under some X value has no use for the customer; in these cases, we will prefer iterating until that threshold is suppressed. However, while this X might be very high in some cases, I believe that both product/business people and data scientists tend to overestimate the height of this step; it\u2019s very easy to state that anything under 95% accuracy (for example) provides no value and can\u2019t be sold. In many cases, however, careful examination and challenging of product assumptions can lead to very valuable products that might not be as demanding technically (at least for the first iteration of the product).</p>\n<p><b>1.5. Scope &amp; KPIs\u00a0approval</b></p>\n<p>Finally, the product person in charge needs to approve the scope and KPIs defined. It is the data scientist\u2019s job to make sure everybody understand the implications of the scope\u200a\u2014\u200awhat was included and what was prioritized\u200a\u2014\u200aand the relation between the product KPIs and the harder metrics that will guide her during model development, including the extent to which the letter approximate the former. Stating this explicitly can prevent cases where the consumers of the models being developed\u200a\u2014\u200aproduct and business people\u200a\u2014\u200aunderstand only during or after model development that the wrong metric was optimized.</p>\n<p><b>General remarks on\u00a0scoping</b></p>\n<p>In many places this phase is skipped, with the data scientist eager to start digging at the data and explore cool papers about possible solutions; in my experience, this is almost always for the worst.\u00a0Skipping this phase can result in long weeks or months spent in developing cool models that end up not answering a real need, or failing in a very specific KPI that could have been explicitly defined with some premeditation.</p>\n<p>\u00a0</p>\n<h3>2. The Research\u00a0Phase</h3>\n<p>\u00a0<br>\n<b>2.1. Data exploration</b></br></p>\n<p>This is where the fun starts! The main advantage of having this phase commence after scoping is that our exploration can now be guided by the actual hard KPIs and model metrics we have decided on.</p>\n<p>As always, there is a balance to be struck here between exploration and exploitation; even when having clear KPIs in mind, it is valuable to explore some seemingly unrelated avenues to a certain degree.</p>\n<p>By now the initial set of required data should have been made available by data engineering. However, some deficiencies in the explored data will often be discovered during this phase, and additional data sources might be added to the working set. The data engineer should be prepared for this.</p>\n<p>Finally, although separated here from the literature and solution review phase, they are usually either done in parallel or alternated between.</p>\n<p><b>2.2. Literature &amp; solutions review</b></p>\n<p>Both academic literature and existing code and tools are reviewed in this phase. Balance is again important; both between exploration and exploitation, and between diving into the intricacies of the material and extracting takeaways and possible uses quickly.</p>\n<p>In the case of academic literature, the choice of how deep to go into aspects like formal proofs and preceding literature depends heavily on both the time constraints and the context of the project: Are we building a strong basis for a core capability of the company or devising a solution to a one-off problem? Do we plan to publish our work on the subject in an academic paper? Are you planing to become the team\u2019s expert on the topic?</p>\n<blockquote><p>For example, take the case where a data scientist embarking on a project to help the sales department better predict lead generation yield or churn feels she has only a shallow understanding of stochastic process theory, on which many common solutions to these problems are built. The appropriate response to this feeling can be very different; if she works for an algo-trading company she should definitely be diving into said theory, probably even taking an online course on the topic, as it is very relevant to his work; if, on the other hand, she works for a medical imaging company focused on automatic tumor detection in liver x-ray scans, I\u2019d say she should find an applicable solution quickly and move on.</p></blockquote>\n<p>In the case of code and implementations, the depth of understanding to aim for depends on technical aspects, some of which might be discovered only later in the process, but many of which can also be predicted ahead of time.</p>\n<blockquote><p>For example, if the production environment only supports deploying Java and Scala code for backend uses and the solution is thus expected to be provided in a JVM language, the data scientist will have to go deeper into Python-based implementations she finds even during this research phase, as going forward with them into the model development phase entails translating them to a JVM language.</p></blockquote>\n<p>Finally, while reviewing literature, keep in mind that not only the chosen research direction (or couple of directions) should to be presented to the rest of the team. Rather, a brief review of the field and all examined solutions should accompany the choice made, explaining the upsides and downsides of each direction and the justifications for that choice.</p>\n<p><b>2.3. Technical validity\u00a0check</b></p>\n<p>With a suggestion for a possible solution, the data engineer and any involved developers need to estimate, with the help of the data scientist, the form and complexity of this solution in production. Both the product needs and the structure and characteristics of the suggested solution should help determine the adequate data storage, processing (stream vs batch), ability to scale (horizontally and vertically) and a rough estimate of cost.</p>\n<p>This is an important check to perform at this stage because some data and software engineering can begin in parallel to model development. Additionally, a suggested solution might turn out to be inadequate or too costly in engineering terms, in which case this should be identified and dealt with as soon as possible. When technical issues are considered before model development starts, the knowledge gained during the research phase can then be used to suggest an alternate solution that might better fit technical constraints. This is another reason why the research phase must also result in some overview of the solution landscape, and not just in a single solution direction.</p>\n<p><b>2.4. Scope &amp; KPIs validation</b></p>\n<p>Again, the product manager needs to approve that the suggested solution, now stated in more technical terms, meets the scope and KPIs defined. Possible technical criteria that usually have easily detectable product implications are response time (and its relation to computation time), the freshness of data and sometimes cached mid-calculations (which are related to querying and batch computation frequency), difficulty and cost (including data cost) of domain adaptation for domain-specific models (domains are most often clients, but can be industries, languages, countries and so on) and solution composability (e.g. do data and model structures allow to easily break a country-wise model down to a per-region model, or to compose several such models into a per-continent model), though many more exist.</p>\n<p>\u00a0</p>\n<h3>3. The Development Phase</h3>\n<p>\u00a0<br/>\n<b>3.1. Model development &amp; experiments framework setup</b></p>\n<p>The amount and complexity of setup required for model development to begin depends heavily on the infrastructure and amount of technical support available to the data scientist. In smaller places, and in places not yet used to supporting data science research projects, setup might sum up to the data scientist opening a new code repository and firing up a local Jupyter Notebook server, or requesting a stronger cloud machine to run computations on.</p>\n<p>In other cases it might entail writing custom code for more complex functionalities such as data and model versioning or experiment tracking and management. When this functionality is instead provided by some external product or service (and more and more of these are popping up these days), some setup in the form of linking data sources, allocating resources and setting up custom packages might follow.</p>\n<p><b>3.2. Model development</b></p>\n<p>With the required infrastructure in place, actual model development can begin in earnest. The extent of what is considered the model to be developed here varies by company, and depends on the relation, and the divide, between the model to be delivered by the data scientist and the service or feature to be deployed in production. The various type of approaches to this divide can perhaps be captured somewhat by considering a spectrum.</p>\n<p>On one end of spectrum lies the case where everything is\u00a0<em>the model</em>: from data aggregation and preprocessing, through model training (possibly periodically), model deployment, serving (possibly with scaling) and continuous monitoring. On the other end lies the case where just the choice of model type and hyperparameters, and commonly also advanced data preprocessing and feature generation, is thought of as\u00a0<em>the model</em>.</p>\n<p>A company\u2019s location on the spectrum depends on numerous factors: the data scientists\u2019 preferable research language; relevant libraries and open source availability; supported production languages in the company; the existence of a data engineer and devs dedicated solely to data science related code; and the technical capabilities and work methodology of the data scientists.</p>\n<p>In case of a very full-stack-y data scientist, combined with enough support from a dedicated data engineer and devs\u200a\u2014\u200a<em>or, alternatively, with enough existing infrastructure dedicated to the operation and automation of data lake-ing and aggregation, model serving, scaling and monitoring (and possibly also versioning)\u200a</em>\u2014\u200athe wider definition for a model can be taken, and an end-to-end solution can be used throughout most of the iterations on model development.</p>\n<p>This usually means building the complete pipeline first, from data sources all the way to scaleable served models, with simple placeholders for data preprocessing, feature generation and the model itself. Iterations are then made on the data-science-y parts, while keeping limiting the scope to what is available and deployable on the existing infrastructure.</p>\n<p>This end-to-end approach can take more time to setup, and each iteration on model types and parameters make take longer to test, but it saves time later paid for in the productization phase.</p>\n<p>I personally love it, but it\u2019s complex to implement and maintain, and its not always appropriate. In that case, some parts of the start and the end of the pipeline are left to the productization phase.</p>\n<p><b>3.3. Model\u00a0test</b></p>\n<p>While developing the model, different versions of it (and the data processing pipeline accompanying it) should be continuously tested against the predetermined hard metric(s). This gives a rough estimate of progress and also allows the data scientist to decide when the model seems to be working well enough to warrant the overall KPI check. Do note that this can be misleading, as getting from 50% to 70% accuracy, for example, is in many cases much easier than getting from 70% to 90% accuracy.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*oP0Jg_POKFt9mlVYQrz7Tg.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Figure 2: Model failure entails iteration, but an approach failure might send you back to\u00a0research</font></center></p>\n<p>When tests show that a model is off the mark, we usually investigate it and its output to guide improvements. Sometimes, however, the gap in performance is very large, with different variations of the chosen research directions all falling short\u200a\u2014\u200aan\u00a0<em>approach failure</em>. This might warrant a change in the research direction, sending the project back into the research phase.\u00a0This is the aspect of data science projects that is hardest to accept: the very real possibility of backtracking.</p>\n<p>Another possible result of\u00a0<em>approach failure</em>\u00a0is a change to the goal. With luck, it can be minor product-wise but restate the goal technically in a simpler way.</p>\n<blockquote><p>For example, instead of trying to generate a one-sentence summary of an article, choose the sentence in the article that best summarizes it.</p></blockquote>\n<p>The final possible result is of course project cancellation; if the data scientist is sure all research avenues have been explored, and the product manager is sure that a valid product cannot be built around existing performance, it might be time to move on to another project.\u00a0Do not underestimate the ability to identify an unsalvageable project and the courage to make the decision to end it; it is a crucial part of the\u00a0<em>fail-fast</em>\u00a0methodology.</p>\n<p><b>3.4. KPIs\u00a0check</b></p>\n<p>If the predetermined hard metric is the only KPI and captures all product needs exactly, then this phase can be more of a formality, when the final model is presented and the development phase is declared over.\u00a0<strong>This is usually not the case.</strong></p>\n<p>In the more common case, the hard metric is a good approximation of the actual product needs, but not a perfect one. This phase is thus an opportunity to make sure that the softer metrics, that cannot be checked automatically, are also satisfied. This is done together with product and customer success. If you can additionally check the actual value to a customer directly\u2014 e.g. when working with a design partner\u200a\u2014\u200athen it\u2019s the best guide you could find for your iterations.</p>\n<blockquote><p>For example, let\u2019s say that we\u2019re dealing with a complex task such extracting relevant documents, given a query, from a huge corpus. The team might have decided that to try and increase the quality of the result set, focusing on variance in content and topics of the returned documents, as clients feel the systems tends to cluster quite similar documents in top results.</p>\n<p>Model development might have progressed with some measurable metric for content variance in the results set\u200a\u2014\u200aeach model is scored by how varied are the top 20 documents it returns, given a set of test queries; perhaps you measure overall distance between document topics in some topic vector space, or just the number of unique topics or flatness of significant word distributions.</p>\n<p>Even when the data scientist settles on a model which improves this metric significantly, product and customer success people should definitely take a look at the actual results for a significant sample of the test queries; they might find problems hard to quantify, but possible to solve, such as a model increasing result variance by pushing up some recurring non-relevant topic, or by including results on similar topics but from different sources (e.g. news article vs tweets, which use a very different language).</p></blockquote>\n<p>When the product person is convinced the model answers the stated goals of the project (to a satisfactory degree), the team can move forward to productizing it.</p>\n<p>\u00a0</p>\n<h3>4. The Deployment Phase</h3>\n<p>\u00a0<br/>\n<b>4.1. Solution Productization &amp; Monitoring Setup</b></p>\n<p>This phase, as mentioned earlier, depends on the approach to both data science research and model serving in the company, as well as several key technical factors.</p>\n<p><strong>Productization:</strong>\u00a0In cases where research language can be used in production, this phase might entail adapting the model code to work in a scalable manner; how simple or complex this process is depends both on distributive computing support for the model language, and the specific libraries and custom code used.</p>\n<p>When research and production language are different, this might also involve wrapping the model code in a production language wrapper, compiling it to a low level binary or implementing the same logic in production language (or finding such an implementation).</p>\n<p>Scaleable data ingestion and processing also need to be set up, in the (quite common) case where this was not part of the model. This can mean, for example, turning Python functions that ran on a single core to a pipeline streaming data goes through, or into batch jobs running periodically. In the case of significant data re-use, a caching layer is sometimes set up.</p>\n<p><strong>Monitoring:</strong>\u00a0Finally, a way to continuously monitor the performance of the model is set up; in rare cases, when the source of production data is constant, this can perhaps be safely skipped, but I\u2019d say that in most cases you can\u2019t be sure of the stability of the source data distribution. Setting up such a performance check, then, can help us to not only detect problems in the model that we might have missed during development and productization, but more importantly changes in the source data distribution above which the model operates\u200a\u2014\u200acommonly referred to as a\u00a0<em>covariate shift</em>\u200a\u2014\u200athat can degrade, in time, the performance of a perfectly good model.</p>\n<blockquote><p>Take, for example, the case where<strong>\u00a0</strong>our product is an app that detects skin marks and evaluate whether to recommend the user to go see a skin doctor. A covariate shift might happen in our data when a popular new phone goes to market, equipped with a camera significantly different from those present in our data.</p></blockquote>\n<p><b>4.2. Solution Deployment</b></p>\n<p>If everything is set up correctly, then this stage can sum up to, hopefully, pushing a button to deploy the new model\u200a\u2014\u200aand any code serving it\u200a\u2014\u200ato the company\u2019s production environment.</p>\n<p><strong>Partial Deployment:</strong>\u00a0It is possible, however, that in order to test the effectiveness of the model (for example, in reducing churn, or increasing average monthly spending per user), the model will be deployed to in a manner that only part of the user/customer base is exposed to it. This enables a direct comparison of the effect on any measurable KPIs between the two (or more) groups in the user base.</p>\n<p>Another reason you might not want to deploy the model to everyone is if it was developed to answer the needs of a specific customer or a group of customers, or if it\u2019s a premium feature or part of a specific plan. Alternatively, the model might have some element of personalization per user or customer; this is can sometimes be achieved by actually having a single model which take customer characteristics into account, but sometimes entails actually training and deploying a different model for each customer.</p>\n<p>Whatever the case, all these scenarios increase the complexity of deploying the model, and depending on existing infrastructure in the company (e.g. if you\u2019re already deploying some of the product features to subsets of your customers) they might require a significant amount of additional development by your back-end team.</p>\n<p>This phase is even more complex when the model is to be deployed on end-products, like user phones or wearables, in which case model deployment might only happen as part of the next app or firmware update deployed.</p>\n<p><strong>Generating Bias:</strong>\u00a0Finally, all cases of partial deployment are actually a pressing issue to the data science team for another reason: this naturally introduces bias into the future data the model will start accumulating\u200a\u2014\u200athe model will start operating on data by a subset of users with possibly unique characteristics. Depending on the product and the specific biased characteristics, this can have a big impact on the performance of the model in the wild, and possibly on future models trained on data accumulated during this period.</p>\n<blockquote><p>For example, in the case of device update, users who updated their apps/firmware earlier tend to fall into certain demographics (younger, more tech-savvy, higher income, etc.).</p></blockquote>\n<p><b>4.3. KPIs\u00a0check</b></p>\n<p>I\u2019ve added another KPIs check here because I think a solution cannot be marked as delivered before its performance and successful answering of product and customer needs has been validated after deployment and actual use.</p>\n<p>This might mean sifting through and running analysis on the resulting data a couple of weeks after deployment. When actual customers are involved, however, this must also involve product or customers success people sitting with the customers and trying to understand the actual impact the model has on their use of the product.</p>\n<p><b>4.4. Solution delivered</b></p>\n<p>Users and customers are happy. Product people have managed to build or adapt the product they wanted around the model. We\u2019re done. Toasts are toasted, cheers are cheered, and all is well.</p>\n<p>The solution is delivered, and I would call the project done at this point. It does, however, keeps on living in a specific way\u200a\u2014\u200amaintenance.</p>\n<p><b>Maintenance</b></p>\n<p>Having set up health checks and continuous performance monitoring for the model, these can trigger up short bursts of working on the project.</p>\n<p>When something seems to be suspicious, we usually start by looking at the data (e.g. for covariate shifts), and perhaps simulating the response of the model to various cases that we suspect cause the problem. The results of these checks can send us into anything between a few hours of small code changes and re-training of the model and full model development iterations (as reflected in the figure opening this post), with severe cases sometimes entailing going back to the research phase to try completely different directions.</p>\n<p>\u00a0</p>\n<h3>Final Words</h3>\n<p>\u00a0<br/>\nThis is a suggestion for the flow of data science projects. It is also very specific, limited in scope\u200a\u2014\u200afor the sake of simplicity and visibility\u200a\u2014\u200aand obviously cannot cover the many variations on this flow that exist in practice. It also represents my experience.</p>\n<p>For all of these reasons, I\u2019d love to hear your feedback, insights and experience from running, leading or managing data science projects, whatever their size, and whatever the size of the data science team you are part of.</p>\n<p>For another great take on this topic, I recommend reading\u00a0<a href=\"https://towardsdatascience.com/data-science-agile-cycles-my-method-for-managing-data-science-projects-in-the-hi-tech-industry-b289e8a72818\" rel=\"noopener noreferrer\" target=\"_blank\">my friend Ori\u2019s post on agile development for data science</a>. I would also like to thank\u00a0<a href=\"https://medium.com/@inbarnaor\" rel=\"noopener noreferrer\" target=\"_blank\">Inbar Naor</a>, Shir Meir Lador (<a href=\"http://twitter.com/DataLady\" rel=\"noopener noreferrer\" target=\"_blank\">@DataLady</a>) and\u00a0<a href=\"http://twitter.com/seffi\" rel=\"noopener noreferrer\" target=\"_blank\">@seffi</a>.cohen for their feedback.</p>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"http://www.shaypalachy.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Shay Palachy</a></b> is a data scientist and a data science consultant. He also loves working on community and open source projects.</p>\n<p><a href=\"https://towardsdatascience.com/data-science-project-flow-for-startups-282a93d4508d\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2019/01/end-to-end-guide-machine-learning-project.html\">End To End Guide For Machine Learning Projects</a>\n<li><a href=\"/2018/12/machine-learning-project-checklist.html\">The Machine Learning Project Checklist</a>\n<li><a href=\"/2018/07/manage-machine-learning-lifecycle-mlflow.html\">Manage your Machine Learning Lifecycle with MLflow\u200a \u2013 \u200aPart 1</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2019/01/top-tweets-jan16-jan22.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2019/01/top-jobs-data-science.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-mp-1-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-mp-2-another-10-books');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/recognize-good-data-scientist-job-from-bad.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-mp-3-good-from-bad');\"><b>How to Recognize a Good Data Scientist Job From a Bad One</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-mp-4-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/04/introduction-time-series-forecasting-simple-neural-networks-lstm.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-mp-5-intro-ts');\"><b>An Introduction on Time Series Forecasting with Simple Neural Networks &amp; LSTM</b></a>\n<li> <a href=\"/2019/03/random-forest-python.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-mp-6-explain-rf');\"><b>Explaining Random Forest (with Python Implementation)</b></a>\n<li> <a href=\"/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-mp-7-age-gender');\"><b>Predict Age and Gender Using Convolutional Neural Network and OpenCV</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-ms-1-optimization-ga');\"><b>Artificial Neural Networks Optimization using Genetic Algorithm with Python</b></a>\n<li> <a href=\"/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-ms-2-another-10-courses');\"><b>Another 10 Free Must-See Courses for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2019/04/top-10-coding-mistakes-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-ms-3-mistakes');\"><b>Top 10 Coding Mistakes Made by Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-ms-4-r-py-viz');\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-ms-5-azure-cert');\"><b>8 Reasons Why You Should Get a Microsoft Azure Certification</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-ms-6-dl-toolset');\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n<li> <a href=\"/2019/03/work-data-science-ai-big-data.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-16-ms-7-how-work');\"><b>How To Work In Data Science, AI, Big Data</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/paw-data-driven-government-workshops-announced.html\">Data Driven Government Workshops Announced!</a><li> <a href=\"https://www.kdnuggets.com/2019/04/rise-generative-adversarial-networks.html\">The Rise of Generative Adversarial Networks</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-19-nasdaq100-leader-data-science.html\">NASDAQ 100: Leader of Data Science [East Coast]</a><li> <a href=\"https://www.kdnuggets.com/2019/04/data-visualization-python-matplotlib-seaborn.html\">Data Visualization in Python: Matplotlib vs Seaborn</a><li> <a href=\"https://www.kdnuggets.com/2019/04/intel-unleash-faster-python-data.html\">Unleash a faster Python on your data</a><li> <a href=\"https://www.kdnuggets.com/2019/04/sisense-blox-beyond-dashboards.html\">Sisense BloX \u2013 Go Beyond Dashboards</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/sisense-blox-beyond-dashboards.html\">Sisense BloX \u2013 Go Beyond Dashboards</a><li> <a href=\"https://www.kdnuggets.com/2019/04/3-big-problems-big-data.html\">3 Big Problems with Big Data and How to Solve Them</a><li> <a href=\"https://www.kdnuggets.com/2019/04/distributed-artificial-intelligence-multi-agent-systems-agent-based-modeling-swarm-intelligence.html\">Distributed Artificial Intelligence: A primer on Multi-Agent S...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/how-optimization-works.html\">How Optimization Works</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-apr10-apr16.html\">Top tweets, Apr 10\u201316: Math for Programmers teaches you t...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-san-francisco-over-500-data-professionals.html\">DATAx San Francisco | 14-15 May | Over 500 Data Professionals</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-17-bottomline-technologies-data-scientist.html\">Bottomline Technologies, Inc: Data Scientist [McLean, VA or Po...</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-17-ten-x-data-scientist.html\">Ten-X: Data Scientist [San Mateo, CA]</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-17-ten-x-data-engineer.html\">Ten-X: Sr Data Engineer [San Mateo, CA]</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-data-visualization-techniques.html\">Best Data Visualization Techniques for small and large data</a><li> <a href=\"https://www.kdnuggets.com/2019/04/building-flask-api-automatically-extract-named-entities-spacy.html\">Building a Flask API to Automatically Extract Named Entities U...</a><li> <a href=\"https://www.kdnuggets.com/2019/n15.html\">KDnuggets 19:n15, Apr 17: Time Series Forecasting with Neur...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/northwestern-ms-data-science.html\">Northwestern\u2019s MS in Data Science</a><li> <a href=\"https://www.kdnuggets.com/2019/04/penn-online-ms-data-analytics.html\">Penn State Online MS in Data Analytics.</a><li> <a href=\"https://www.kdnuggets.com/2019/04/introduction-clustering-algorithms.html\">How Machines Make Sense of Big Data: an Introduction to Cluste...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/best-masters-data-science-analytics-europe.html\">2019 Best Masters in Data Science and Analytics \u2013 Europe Edi...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/k-means-clustering-unsupervised-learning-recommender-systems.html\">K-Means Clustering: Unsupervised Learning for Recommender Systems</a><li> <a href=\"https://www.kdnuggets.com/2019/04/data-science-with-optimus-part-2-setting-dataops-environment.html\">Data Science with Optimus Part 2: Setting your DataOps Environ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0408-0414.html\">Top Stories, Apr 8-14: How to Recognize a Good Data Scientist ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-hot-deep-learning-applications-las-vegas.html\">Hot Deep Learning Applications at Deep Learning World \u2013 Las ...</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2019/index.html\">2019</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/01/index.html\">Jan</a> \u00bb <a href=\"https://www.kdnuggets.com/2019/01/tutorials.html\">Tutorials, Overviews</a> \u00bb Data Science Project Flow for Startups (\u00a0<a href=\"/2019/n05.html\">19:n05</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1555894888\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.787 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-21 21:01:28 -->\n<!-- Compression = gzip -->", "content": "\n    comments   \n By  Shay Palachy  , Data Scientist & Consultant   \n I was recently asked by a startup I\u2019m consulting ( BigPanda  ) to give my opinion about the structure and flow of data science projects, which made me think about what makes them unique. Both managers and the different teams in a startup might find the differences between a data science project and a software development one unintuitive and confusing. If not stated and accounted for explicitly, these fundamental differences might cause misunderstanding and clashes between the data scientist and his peers.  \n Respectively, researchers coming from academia (or highly research-oriented industry research groups) might have their own challenges when arriving at a startup or a smaller company. They might find it challenging to incorporate new types of inputs, such as product and business needs, tighter infrastructure and compute constraints and costumer feedback, into their research and development process.  \n The aim of this post, then, is to present the characteristic project flow that I have identified in the working process of both my colleagues and myself in recent years. Hopefully, this can help both data scientists and the people working with them to structure data science projects in a way that reflects their uniqueness.  \n The flow was built with small startups in mind, where a small team of data scientists (usually one to four) run short and mid-sized projects led by a single person at a time. Bigger teams or those in machine-learning-first, deep-tech startups might still find this a useful structure, but processes there are longer and structured differently in many cases.  \n \n Figure 1: Data Science Project Flow for\u00a0Startups      \n I have divided the process into three aspects that run in parallel: product, data science and data engineering. In many cases (including most of the places I worked for), there might not be a data engineer to perform these duties. In this case the data scientist is usually in charge of working with developers to help with these aspects. Alternatively, the data scientist might do these preparations, if they happen to be the rarest of all of God\u2019s beasts:\u00a0 the Full Stack Data Scientist!\u00a0  \u2728\ud83e\udd84\u2728. You can thus replace\u00a0 data engineer\u00a0  with\u00a0 data scientist\u00a0  whenever it is mentioned ,  \u00a0depending on your environment. \nOn the time axis, I broke the process down into four distinct phases:   \n \n Scoping\n Research\n (Model) Development\n Deployment\n      \n I\u2019ll try and walk you through each of these, in order.  \n \u00a0  \n 1. The Scoping\u00a0Phase  \n \u00a0 \nDefining the scope of a data science project is crucial more than in any other type of project.   \n 1.1. Product\u00a0need   \n A project should always start with a product need\u00a0(even if the original idea was technical or theoretical), a need validated to some degree by product/business/customer success people. The product person should have an idea of how this feature should (roughly) end up looking, and that either existing or new customers will be willing to pay for it (or that it will prevent churn / drive subscriptions / drive sales of other products / etc.).  \n A product need is not a full project definition, but should rather be stated as a problem or challenge; e.g.\u00a0 \u201cour customers need a way to understand how they spend their budgets\u201d  \u00a0or\u00a0 \u201cwe do not manage to get our older users to keep taking their medicine; this increases churn\u201d  \u00a0or\u00a0 \u201ccustomers will pay more for a product that can also predict rush hours at the airports they run\u201d  .  \n 1.2. Initial solution\u00a0ideation   \n This is where the data scientist, together with the product person in charge, the data engineer and any other stakeholder, comes up with different rough sketches for possible solutions. This means both the general approach (e.g. unsupervised clustering vs boosted-tree-based classification vs probabilistic inference)  and   the data to be used (e.g. this specific table from our database, or some specific user behavior that we do not yet monitor or save, or an external data source).  \n This usually also involves some level of data exploration. You can\u2019t go real deep here, but any promising \u201clow-hanging fruits\u201d can help guide ideation.  \n The data scientist should lead this process and is usually in charge of providing most of the solution ideas, but I would urge you to use all those taking part in the process for solution ideation; I have had the good fortune to get the best solution ideas for a project handed to me by a back-end developer, the CTO or the product person in charge. Don\u2019t assume that different, and less theory-oriented backgrounds, invalidate people from taking part in this phase; the additional minds and viewpoints are always valuable.  \n 1.3. Data preparation and accessibility   \n The team should now have a good idea of the data that would hopefully be used to explore possible solutions (or at least the first such data set or source). Thus, the process of providing data access and preparing it for exploration and use should already start, in parallel with the next phases.  \n This can sometime entail dumping large data sets from production databases into their staging/exploration counterparts, or to colder storage (for example, object storage) if its time availability is not critical in the research phase. Conversely, it can mean pulling large data dumps from very cold storage back into table or document form to enable fast querying and complex computations. Whatever the case, this phase is required for the research phase to start and frequently ends up taking more time than expected, and so that\u2019s the right time to initiate it.  \n 1.4. Scope &\u00a0KPIs   \n This phase is about deciding together on the scope and the KPIs of the project.  \n KPIs should be defined first in product terms, but in much more detail than before; e.g. with respect to the three aforementioned product needs, they might become\u00a0 \u201ccustomers could now use a dashboard with CTR stats and projection per category\u201d  , or\u00a0 \u201cmissed medicine days by users over 65 will be reduced by at least 10% over the next two quarters\u201d  , or\u00a0 \u201ccustomers will receive weekly predictions of rush hours in their airports with granularity of at least an hour, and approximation of at least \u00b150%\u201d  .  \n These KPIS should be then translated to measurable model metrics. With luck, these will be very hard metrics, such as\u00a0 \u201cpredicting the expected CTR of an ad with approximation of at least X% in at least Y% of the cases, for any ad that runs for at least a week, and for any client with more than two months of historic data\u201d  . In some cases, however, softer metrics will have to be used, such as\u00a0 \u201ctime required for topic exploration using the generated expanded queries will be shortened, and/or result quality will improve, when compared to the original queries\u201d  . This is especially true when the model is meant to assist some complex human function.  \n Technically even these metrics can be defined very strictly (and in academic research, they usually are), but depending on resources and time constraints we might settle with approximating them using human feedback. In this case each feedback iteration might take longer, and so we will usually try to find additional hard metrics to guide us through most of the upcoming research iterations, with the costlier feedback being elicited only once every few iterations, or on significant changes.  \n Finally, scope is especially important here because research projects have a tendency to drag on, and to naturally expand in size and scope as new possibilities arise while researching or when an examined approach answers the demands only partially.  \n Scope limitation 1:  \u00a0I find it more productive to limit scope explicitly; for example, if you\u2019ve decided that a Multi-Armed Bandit based model is the most promising approach to start with, you might define the project scope to a single two/three weeks iteration of model development, deploying the model regardless of its accuracy (as long as it\u2019s over 60%, for example). Then, if improvement in accuracy is valuable (in some cases it might turn out to be less so), developing a second model might be thought of as a separate project.  \n Scope limitation 2:  \u00a0Another variation on scope limitation is using increasing degrees of complexity; for example, the first project might aim to deploy a model that only needs to provide a rather large set of candidates of ad wording and color variations for your own customer success people to work with; the second might attempt to build a model that gives a smaller set of suggestions that the customer can see herself; and a final project might try for a model that highlights a single option, ranking below it a couple more, and adding CTR projections and demographic reach for each variation.  \n This is already a huge departure from software engineering, where usually components are iterated over for increased scale rather than complexity.   \n Nevertheless, the metric-to-product-value function might be a step function, meaning that any model performing under some X value has no use for the customer; in these cases, we will prefer iterating until that threshold is suppressed. However, while this X might be very high in some cases, I believe that both product/business people and data scientists tend to overestimate the height of this step; it\u2019s very easy to state that anything under 95% accuracy (for example) provides no value and can\u2019t be sold. In many cases, however, careful examination and challenging of product assumptions can lead to very valuable products that might not be as demanding technically (at least for the first iteration of the product).  \n 1.5. Scope & KPIs\u00a0approval   \n Finally, the product person in charge needs to approve the scope and KPIs defined. It is the data scientist\u2019s job to make sure everybody understand the implications of the scope\u200a\u2014\u200awhat was included and what was prioritized\u200a\u2014\u200aand the relation between the product KPIs and the harder metrics that will guide her during model development, including the extent to which the letter approximate the former. Stating this explicitly can prevent cases where the consumers of the models being developed\u200a\u2014\u200aproduct and business people\u200a\u2014\u200aunderstand only during or after model development that the wrong metric was optimized.  \n General remarks on\u00a0scoping   \n In many places this phase is skipped, with the data scientist eager to start digging at the data and explore cool papers about possible solutions; in my experience, this is almost always for the worst.\u00a0Skipping this phase can result in long weeks or months spent in developing cool models that end up not answering a real need, or failing in a very specific KPI that could have been explicitly defined with some premeditation.  \n \u00a0  \n 2. The Research\u00a0Phase  \n \u00a0 \n 2.1. Data exploration    \n This is where the fun starts! The main advantage of having this phase commence after scoping is that our exploration can now be guided by the actual hard KPIs and model metrics we have decided on.  \n As always, there is a balance to be struck here between exploration and exploitation; even when having clear KPIs in mind, it is valuable to explore some seemingly unrelated avenues to a certain degree.  \n By now the initial set of required data should have been made available by data engineering. However, some deficiencies in the explored data will often be discovered during this phase, and additional data sources might be added to the working set. The data engineer should be prepared for this.  \n Finally, although separated here from the literature and solution review phase, they are usually either done in parallel or alternated between.  \n 2.2. Literature & solutions review   \n Both academic literature and existing code and tools are reviewed in this phase. Balance is again important; both between exploration and exploitation, and between diving into the intricacies of the material and extracting takeaways and possible uses quickly.  \n In the case of academic literature, the choice of how deep to go into aspects like formal proofs and preceding literature depends heavily on both the time constraints and the context of the project: Are we building a strong basis for a core capability of the company or devising a solution to a one-off problem? Do we plan to publish our work on the subject in an academic paper? Are you planing to become the team\u2019s expert on the topic?  \n For example, take the case where a data scientist embarking on a project to help the sales department better predict lead generation yield or churn feels she has only a shallow understanding of stochastic process theory, on which many common solutions to these problems are built. The appropriate response to this feeling can be very different; if she works for an algo-trading company she should definitely be diving into said theory, probably even taking an online course on the topic, as it is very relevant to his work; if, on the other hand, she works for a medical imaging company focused on automatic tumor detection in liver x-ray scans, I\u2019d say she should find an applicable solution quickly and move on.   \n In the case of code and implementations, the depth of understanding to aim for depends on technical aspects, some of which might be discovered only later in the process, but many of which can also be predicted ahead of time.  \n For example, if the production environment only supports deploying Java and Scala code for backend uses and the solution is thus expected to be provided in a JVM language, the data scientist will have to go deeper into Python-based implementations she finds even during this research phase, as going forward with them into the model development phase entails translating them to a JVM language.   \n Finally, while reviewing literature, keep in mind that not only the chosen research direction (or couple of directions) should to be presented to the rest of the team. Rather, a brief review of the field and all examined solutions should accompany the choice made, explaining the upsides and downsides of each direction and the justifications for that choice.  \n 2.3. Technical validity\u00a0check   \n With a suggestion for a possible solution, the data engineer and any involved developers need to estimate, with the help of the data scientist, the form and complexity of this solution in production. Both the product needs and the structure and characteristics of the suggested solution should help determine the adequate data storage, processing (stream vs batch), ability to scale (horizontally and vertically) and a rough estimate of cost.  \n This is an important check to perform at this stage because some data and software engineering can begin in parallel to model development. Additionally, a suggested solution might turn out to be inadequate or too costly in engineering terms, in which case this should be identified and dealt with as soon as possible. When technical issues are considered before model development starts, the knowledge gained during the research phase can then be used to suggest an alternate solution that might better fit technical constraints. This is another reason why the research phase must also result in some overview of the solution landscape, and not just in a single solution direction.  \n 2.4. Scope & KPIs validation   \n Again, the product manager needs to approve that the suggested solution, now stated in more technical terms, meets the scope and KPIs defined. Possible technical criteria that usually have easily detectable product implications are response time (and its relation to computation time), the freshness of data and sometimes cached mid-calculations (which are related to querying and batch computation frequency), difficulty and cost (including data cost) of domain adaptation for domain-specific models (domains are most often clients, but can be industries, languages, countries and so on) and solution composability (e.g. do data and model structures allow to easily break a country-wise model down to a per-region model, or to compose several such models into a per-continent model), though many more exist.  \n \u00a0  \n 3. The Development Phase  \n \u00a0  \n 3.1. Model development & experiments framework setup   \n The amount and complexity of setup required for model development to begin depends heavily on the infrastructure and amount of technical support available to the data scientist. In smaller places, and in places not yet used to supporting data science research projects, setup might sum up to the data scientist opening a new code repository and firing up a local Jupyter Notebook server, or requesting a stronger cloud machine to run computations on.  \n In other cases it might entail writing custom code for more complex functionalities such as data and model versioning or experiment tracking and management. When this functionality is instead provided by some external product or service (and more and more of these are popping up these days), some setup in the form of linking data sources, allocating resources and setting up custom packages might follow.  \n 3.2. Model development   \n With the required infrastructure in place, actual model development can begin in earnest. The extent of what is considered the model to be developed here varies by company, and depends on the relation, and the divide, between the model to be delivered by the data scientist and the service or feature to be deployed in production. The various type of approaches to this divide can perhaps be captured somewhat by considering a spectrum.  \n On one end of spectrum lies the case where everything is\u00a0 the model  : from data aggregation and preprocessing, through model training (possibly periodically), model deployment, serving (possibly with scaling) and continuous monitoring. On the other end lies the case where just the choice of model type and hyperparameters, and commonly also advanced data preprocessing and feature generation, is thought of as\u00a0 the model  .  \n A company\u2019s location on the spectrum depends on numerous factors: the data scientists\u2019 preferable research language; relevant libraries and open source availability; supported production languages in the company; the existence of a data engineer and devs dedicated solely to data science related code; and the technical capabilities and work methodology of the data scientists.  \n In case of a very full-stack-y data scientist, combined with enough support from a dedicated data engineer and devs\u200a\u2014\u200a or, alternatively, with enough existing infrastructure dedicated to the operation and automation of data lake-ing and aggregation, model serving, scaling and monitoring (and possibly also versioning)\u200a  \u2014\u200athe wider definition for a model can be taken, and an end-to-end solution can be used throughout most of the iterations on model development.  \n This usually means building the complete pipeline first, from data sources all the way to scaleable served models, with simple placeholders for data preprocessing, feature generation and the model itself. Iterations are then made on the data-science-y parts, while keeping limiting the scope to what is available and deployable on the existing infrastructure.  \n This end-to-end approach can take more time to setup, and each iteration on model types and parameters make take longer to test, but it saves time later paid for in the productization phase.  \n I personally love it, but it\u2019s complex to implement and maintain, and its not always appropriate. In that case, some parts of the start and the end of the pipeline are left to the productization phase.  \n 3.3. Model\u00a0test   \n While developing the model, different versions of it (and the data processing pipeline accompanying it) should be continuously tested against the predetermined hard metric(s). This gives a rough estimate of progress and also allows the data scientist to decide when the model seems to be working well enough to warrant the overall KPI check. Do note that this can be misleading, as getting from 50% to 70% accuracy, for example, is in many cases much easier than getting from 70% to 90% accuracy.  \n   \n Figure 2: Model failure entails iteration, but an approach failure might send you back to\u00a0research    \n When tests show that a model is off the mark, we usually investigate it and its output to guide improvements. Sometimes, however, the gap in performance is very large, with different variations of the chosen research directions all falling short\u200a\u2014\u200aan\u00a0 approach failure  . This might warrant a change in the research direction, sending the project back into the research phase.\u00a0This is the aspect of data science projects that is hardest to accept: the very real possibility of backtracking.  \n Another possible result of\u00a0 approach failure  \u00a0is a change to the goal. With luck, it can be minor product-wise but restate the goal technically in a simpler way.  \n For example, instead of trying to generate a one-sentence summary of an article, choose the sentence in the article that best summarizes it.   \n The final possible result is of course project cancellation; if the data scientist is sure all research avenues have been explored, and the product manager is sure that a valid product cannot be built around existing performance, it might be time to move on to another project.\u00a0Do not underestimate the ability to identify an unsalvageable project and the courage to make the decision to end it; it is a crucial part of the\u00a0 fail-fast  \u00a0methodology.  \n 3.4. KPIs\u00a0check   \n If the predetermined hard metric is the only KPI and captures all product needs exactly, then this phase can be more of a formality, when the final model is presented and the development phase is declared over.\u00a0 This is usually not the case.   \n In the more common case, the hard metric is a good approximation of the actual product needs, but not a perfect one. This phase is thus an opportunity to make sure that the softer metrics, that cannot be checked automatically, are also satisfied. This is done together with product and customer success. If you can additionally check the actual value to a customer directly\u2014 e.g. when working with a design partner\u200a\u2014\u200athen it\u2019s the best guide you could find for your iterations.  \n For example, let\u2019s say that we\u2019re dealing with a complex task such extracting relevant documents, given a query, from a huge corpus. The team might have decided that to try and increase the quality of the result set, focusing on variance in content and topics of the returned documents, as clients feel the systems tends to cluster quite similar documents in top results.  \n Model development might have progressed with some measurable metric for content variance in the results set\u200a\u2014\u200aeach model is scored by how varied are the top 20 documents it returns, given a set of test queries; perhaps you measure overall distance between document topics in some topic vector space, or just the number of unique topics or flatness of significant word distributions.  \n Even when the data scientist settles on a model which improves this metric significantly, product and customer success people should definitely take a look at the actual results for a significant sample of the test queries; they might find problems hard to quantify, but possible to solve, such as a model increasing result variance by pushing up some recurring non-relevant topic, or by including results on similar topics but from different sources (e.g. news article vs tweets, which use a very different language).   \n When the product person is convinced the model answers the stated goals of the project (to a satisfactory degree), the team can move forward to productizing it.  \n \u00a0  \n 4. The Deployment Phase  \n \u00a0  \n 4.1. Solution Productization & Monitoring Setup   \n This phase, as mentioned earlier, depends on the approach to both data science research and model serving in the company, as well as several key technical factors.  \n Productization:  \u00a0In cases where research language can be used in production, this phase might entail adapting the model code to work in a scalable manner; how simple or complex this process is depends both on distributive computing support for the model language, and the specific libraries and custom code used.  \n When research and production language are different, this might also involve wrapping the model code in a production language wrapper, compiling it to a low level binary or implementing the same logic in production language (or finding such an implementation).  \n Scaleable data ingestion and processing also need to be set up, in the (quite common) case where this was not part of the model. This can mean, for example, turning Python functions that ran on a single core to a pipeline streaming data goes through, or into batch jobs running periodically. In the case of significant data re-use, a caching layer is sometimes set up.  \n Monitoring:  \u00a0Finally, a way to continuously monitor the performance of the model is set up; in rare cases, when the source of production data is constant, this can perhaps be safely skipped, but I\u2019d say that in most cases you can\u2019t be sure of the stability of the source data distribution. Setting up such a performance check, then, can help us to not only detect problems in the model that we might have missed during development and productization, but more importantly changes in the source data distribution above which the model operates\u200a\u2014\u200acommonly referred to as a\u00a0 covariate shift  \u200a\u2014\u200athat can degrade, in time, the performance of a perfectly good model.  \n Take, for example, the case where \u00a0  our product is an app that detects skin marks and evaluate whether to recommend the user to go see a skin doctor. A covariate shift might happen in our data when a popular new phone goes to market, equipped with a camera significantly different from those present in our data.   \n 4.2. Solution Deployment   \n If everything is set up correctly, then this stage can sum up to, hopefully, pushing a button to deploy the new model\u200a\u2014\u200aand any code serving it\u200a\u2014\u200ato the company\u2019s production environment.  \n Partial Deployment:  \u00a0It is possible, however, that in order to test the effectiveness of the model (for example, in reducing churn, or increasing average monthly spending per user), the model will be deployed to in a manner that only part of the user/customer base is exposed to it. This enables a direct comparison of the effect on any measurable KPIs between the two (or more) groups in the user base.  \n Another reason you might not want to deploy the model to everyone is if it was developed to answer the needs of a specific customer or a group of customers, or if it\u2019s a premium feature or part of a specific plan. Alternatively, the model might have some element of personalization per user or customer; this is can sometimes be achieved by actually having a single model which take customer characteristics into account, but sometimes entails actually training and deploying a different model for each customer.  \n Whatever the case, all these scenarios increase the complexity of deploying the model, and depending on existing infrastructure in the company (e.g. if you\u2019re already deploying some of the product features to subsets of your customers) they might require a significant amount of additional development by your back-end team.  \n This phase is even more complex when the model is to be deployed on end-products, like user phones or wearables, in which case model deployment might only happen as part of the next app or firmware update deployed.  \n Generating Bias:  \u00a0Finally, all cases of partial deployment are actually a pressing issue to the data science team for another reason: this naturally introduces bias into the future data the model will start accumulating\u200a\u2014\u200athe model will start operating on data by a subset of users with possibly unique characteristics. Depending on the product and the specific biased characteristics, this can have a big impact on the performance of the model in the wild, and possibly on future models trained on data accumulated during this period.  \n For example, in the case of device update, users who updated their apps/firmware earlier tend to fall into certain demographics (younger, more tech-savvy, higher income, etc.).   \n 4.3. KPIs\u00a0check   \n I\u2019ve added another KPIs check here because I think a solution cannot be marked as delivered before its performance and successful answering of product and customer needs has been validated after deployment and actual use.  \n This might mean sifting through and running analysis on the resulting data a couple of weeks after deployment. When actual customers are involved, however, this must also involve product or customers success people sitting with the customers and trying to understand the actual impact the model has on their use of the product.  \n 4.4. Solution delivered   \n Users and customers are happy. Product people have managed to build or adapt the product they wanted around the model. We\u2019re done. Toasts are toasted, cheers are cheered, and all is well.  \n The solution is delivered, and I would call the project done at this point. It does, however, keeps on living in a specific way\u200a\u2014\u200amaintenance.  \n Maintenance   \n Having set up health checks and continuous performance monitoring for the model, these can trigger up short bursts of working on the project.  \n When something seems to be suspicious, we usually start by looking at the data (e.g. for covariate shifts), and perhaps simulating the response of the model to various cases that we suspect cause the problem. The results of these checks can send us into anything between a few hours of small code changes and re-training of the model and full model development iterations (as reflected in the figure opening this post), with severe cases sometimes entailing going back to the research phase to try completely different directions.  \n \u00a0  \n Final Words  \n \u00a0  \nThis is a suggestion for the flow of data science projects. It is also very specific, limited in scope\u200a\u2014\u200afor the sake of simplicity and visibility\u200a\u2014\u200aand obviously cannot cover the many variations on this flow that exist in practice. It also represents my experience.  \n For all of these reasons, I\u2019d love to hear your feedback, insights and experience from running, leading or managing data science projects, whatever their size, and whatever the size of the data science team you are part of.  \n For another great take on this topic, I recommend reading\u00a0 my friend Ori\u2019s post on agile development for data science  . I would also like to thank\u00a0 Inbar Naor  , Shir Meir Lador ( @DataLady  ) and\u00a0 @seffi  .cohen for their feedback.  \n \u00a0  \n Bio:  Shay Palachy    is a data scientist and a data science consultant. He also loves working on community and open source projects.  \n Original  . Reposted with permission.  \n Related:   \n \n End To End Guide For Machine Learning Projects  \n The Machine Learning Project Checklist  \n Manage your Machine Learning Lifecycle with MLflow\u200a \u2013 \u200aPart 1  \n     \n   \n  \n     \n  ", "title": "Data Science Project Flow for Startups  ", "content_html": "<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"http://www.shaypalachy.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Shay Palachy</a>, Data Scientist &amp; Consultant</b></p>\n<p>I was recently asked by a startup I\u2019m consulting (<a href=\"https://www.bigpanda.io/\" rel=\"noopener noreferrer\" target=\"_blank\">BigPanda</a>) to give my opinion about the structure and flow of data science projects, which made me think about what makes them unique. Both managers and the different teams in a startup might find the differences between a data science project and a software development one unintuitive and confusing. If not stated and accounted for explicitly, these fundamental differences might cause misunderstanding and clashes between the data scientist and his peers.</p>\n<p>Respectively, researchers coming from academia (or highly research-oriented industry research groups) might have their own challenges when arriving at a startup or a smaller company. They might find it challenging to incorporate new types of inputs, such as product and business needs, tighter infrastructure and compute constraints and costumer feedback, into their research and development process.</p>\n<p>The aim of this post, then, is to present the characteristic project flow that I have identified in the working process of both my colleagues and myself in recent years. Hopefully, this can help both data scientists and the people working with them to structure data science projects in a way that reflects their uniqueness.</p>\n<p>The flow was built with small startups in mind, where a small team of data scientists (usually one to four) run short and mid-sized projects led by a single person at a time. Bigger teams or those in machine-learning-first, deep-tech startups might still find this a useful structure, but processes there are longer and structured differently in many cases.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*fBNC1lqYSdM6bjMxxr56Tw.png\" width=\"99%\"><br>\n<font size=\"-1\">Figure 1: Data Science Project Flow for\u00a0Startups</font></br></img></center></p>\n<p>I have divided the process into three aspects that run in parallel: product, data science and data engineering. In many cases (including most of the places I worked for), there might not be a data engineer to perform these duties. In this case the data scientist is usually in charge of working with developers to help with these aspects. Alternatively, the data scientist might do these preparations, if they happen to be the rarest of all of God\u2019s beasts:\u00a0<em>the Full Stack Data Scientist!\u00a0</em>\u2728\ud83e\udd84\u2728. You can thus replace\u00a0<em>data engineer\u00a0</em>with\u00a0<em>data scientist\u00a0</em>whenever it is mentioned<em>,</em>\u00a0depending on your environment.<br>\nOn the time axis, I broke the process down into four distinct phases:</br></p>\n<ol>\n<li>Scoping\n<li>Research\n<li>(Model) Development\n<li>Deployment\n</li></li></li></li></ol>\n<p>I\u2019ll try and walk you through each of these, in order.</p>\n<p>\u00a0</p>\n<h3>1. The Scoping\u00a0Phase</h3>\n<p>\u00a0<br>\nDefining the scope of a data science project is crucial more than in any other type of project.</br></p>\n<p><b>1.1. Product\u00a0need</b></p>\n<p>A project should always start with a product need\u00a0(even if the original idea was technical or theoretical), a need validated to some degree by product/business/customer success people. The product person should have an idea of how this feature should (roughly) end up looking, and that either existing or new customers will be willing to pay for it (or that it will prevent churn / drive subscriptions / drive sales of other products / etc.).</p>\n<p>A product need is not a full project definition, but should rather be stated as a problem or challenge; e.g.\u00a0<em>\u201cour customers need a way to understand how they spend their budgets\u201d</em>\u00a0or\u00a0<em>\u201cwe do not manage to get our older users to keep taking their medicine; this increases churn\u201d</em>\u00a0or\u00a0<em>\u201ccustomers will pay more for a product that can also predict rush hours at the airports they run\u201d</em>.</p>\n<p><b>1.2. Initial solution\u00a0ideation</b></p>\n<p>This is where the data scientist, together with the product person in charge, the data engineer and any other stakeholder, comes up with different rough sketches for possible solutions. This means both the general approach (e.g. unsupervised clustering vs boosted-tree-based classification vs probabilistic inference) <strong>and</strong> the data to be used (e.g. this specific table from our database, or some specific user behavior that we do not yet monitor or save, or an external data source).</p>\n<p>This usually also involves some level of data exploration. You can\u2019t go real deep here, but any promising \u201clow-hanging fruits\u201d can help guide ideation.</p>\n<p>The data scientist should lead this process and is usually in charge of providing most of the solution ideas, but I would urge you to use all those taking part in the process for solution ideation; I have had the good fortune to get the best solution ideas for a project handed to me by a back-end developer, the CTO or the product person in charge. Don\u2019t assume that different, and less theory-oriented backgrounds, invalidate people from taking part in this phase; the additional minds and viewpoints are always valuable.</p>\n<p><b>1.3. Data preparation and accessibility</b></p>\n<p>The team should now have a good idea of the data that would hopefully be used to explore possible solutions (or at least the first such data set or source). Thus, the process of providing data access and preparing it for exploration and use should already start, in parallel with the next phases.</p>\n<p>This can sometime entail dumping large data sets from production databases into their staging/exploration counterparts, or to colder storage (for example, object storage) if its time availability is not critical in the research phase. Conversely, it can mean pulling large data dumps from very cold storage back into table or document form to enable fast querying and complex computations. Whatever the case, this phase is required for the research phase to start and frequently ends up taking more time than expected, and so that\u2019s the right time to initiate it.</p>\n<p><b>1.4. Scope &amp;\u00a0KPIs</b></p>\n<p>This phase is about deciding together on the scope and the KPIs of the project.</p>\n<p>KPIs should be defined first in product terms, but in much more detail than before; e.g. with respect to the three aforementioned product needs, they might become\u00a0<em>\u201ccustomers could now use a dashboard with CTR stats and projection per category\u201d</em>, or\u00a0<em>\u201cmissed medicine days by users over 65 will be reduced by at least 10% over the next two quarters\u201d</em>, or\u00a0<em>\u201ccustomers will receive weekly predictions of rush hours in their airports with granularity of at least an hour, and approximation of at least \u00b150%\u201d</em>.</p>\n<p>These KPIS should be then translated to measurable model metrics. With luck, these will be very hard metrics, such as\u00a0<em>\u201cpredicting the expected CTR of an ad with approximation of at least X% in at least Y% of the cases, for any ad that runs for at least a week, and for any client with more than two months of historic data\u201d</em>. In some cases, however, softer metrics will have to be used, such as\u00a0<em>\u201ctime required for topic exploration using the generated expanded queries will be shortened, and/or result quality will improve, when compared to the original queries\u201d</em>. This is especially true when the model is meant to assist some complex human function.</p>\n<p>Technically even these metrics can be defined very strictly (and in academic research, they usually are), but depending on resources and time constraints we might settle with approximating them using human feedback. In this case each feedback iteration might take longer, and so we will usually try to find additional hard metrics to guide us through most of the upcoming research iterations, with the costlier feedback being elicited only once every few iterations, or on significant changes.</p>\n<p>Finally, scope is especially important here because research projects have a tendency to drag on, and to naturally expand in size and scope as new possibilities arise while researching or when an examined approach answers the demands only partially.</p>\n<p><strong>Scope limitation 1:</strong>\u00a0I find it more productive to limit scope explicitly; for example, if you\u2019ve decided that a Multi-Armed Bandit based model is the most promising approach to start with, you might define the project scope to a single two/three weeks iteration of model development, deploying the model regardless of its accuracy (as long as it\u2019s over 60%, for example). Then, if improvement in accuracy is valuable (in some cases it might turn out to be less so), developing a second model might be thought of as a separate project.</p>\n<p><strong>Scope limitation 2:</strong>\u00a0Another variation on scope limitation is using increasing degrees of complexity; for example, the first project might aim to deploy a model that only needs to provide a rather large set of candidates of ad wording and color variations for your own customer success people to work with; the second might attempt to build a model that gives a smaller set of suggestions that the customer can see herself; and a final project might try for a model that highlights a single option, ranking below it a couple more, and adding CTR projections and demographic reach for each variation.</p>\n<blockquote><p>This is already a huge departure from software engineering, where usually components are iterated over for increased scale rather than complexity.</p></blockquote>\n<p>Nevertheless, the metric-to-product-value function might be a step function, meaning that any model performing under some X value has no use for the customer; in these cases, we will prefer iterating until that threshold is suppressed. However, while this X might be very high in some cases, I believe that both product/business people and data scientists tend to overestimate the height of this step; it\u2019s very easy to state that anything under 95% accuracy (for example) provides no value and can\u2019t be sold. In many cases, however, careful examination and challenging of product assumptions can lead to very valuable products that might not be as demanding technically (at least for the first iteration of the product).</p>\n<p><b>1.5. Scope &amp; KPIs\u00a0approval</b></p>\n<p>Finally, the product person in charge needs to approve the scope and KPIs defined. It is the data scientist\u2019s job to make sure everybody understand the implications of the scope\u200a\u2014\u200awhat was included and what was prioritized\u200a\u2014\u200aand the relation between the product KPIs and the harder metrics that will guide her during model development, including the extent to which the letter approximate the former. Stating this explicitly can prevent cases where the consumers of the models being developed\u200a\u2014\u200aproduct and business people\u200a\u2014\u200aunderstand only during or after model development that the wrong metric was optimized.</p>\n<p><b>General remarks on\u00a0scoping</b></p>\n<p>In many places this phase is skipped, with the data scientist eager to start digging at the data and explore cool papers about possible solutions; in my experience, this is almost always for the worst.\u00a0Skipping this phase can result in long weeks or months spent in developing cool models that end up not answering a real need, or failing in a very specific KPI that could have been explicitly defined with some premeditation.</p>\n<p>\u00a0</p>\n<h3>2. The Research\u00a0Phase</h3>\n<p>\u00a0<br>\n<b>2.1. Data exploration</b></br></p>\n<p>This is where the fun starts! The main advantage of having this phase commence after scoping is that our exploration can now be guided by the actual hard KPIs and model metrics we have decided on.</p>\n<p>As always, there is a balance to be struck here between exploration and exploitation; even when having clear KPIs in mind, it is valuable to explore some seemingly unrelated avenues to a certain degree.</p>\n<p>By now the initial set of required data should have been made available by data engineering. However, some deficiencies in the explored data will often be discovered during this phase, and additional data sources might be added to the working set. The data engineer should be prepared for this.</p>\n<p>Finally, although separated here from the literature and solution review phase, they are usually either done in parallel or alternated between.</p>\n<p><b>2.2. Literature &amp; solutions review</b></p>\n<p>Both academic literature and existing code and tools are reviewed in this phase. Balance is again important; both between exploration and exploitation, and between diving into the intricacies of the material and extracting takeaways and possible uses quickly.</p>\n<p>In the case of academic literature, the choice of how deep to go into aspects like formal proofs and preceding literature depends heavily on both the time constraints and the context of the project: Are we building a strong basis for a core capability of the company or devising a solution to a one-off problem? Do we plan to publish our work on the subject in an academic paper? Are you planing to become the team\u2019s expert on the topic?</p>\n<blockquote><p>For example, take the case where a data scientist embarking on a project to help the sales department better predict lead generation yield or churn feels she has only a shallow understanding of stochastic process theory, on which many common solutions to these problems are built. The appropriate response to this feeling can be very different; if she works for an algo-trading company she should definitely be diving into said theory, probably even taking an online course on the topic, as it is very relevant to his work; if, on the other hand, she works for a medical imaging company focused on automatic tumor detection in liver x-ray scans, I\u2019d say she should find an applicable solution quickly and move on.</p></blockquote>\n<p>In the case of code and implementations, the depth of understanding to aim for depends on technical aspects, some of which might be discovered only later in the process, but many of which can also be predicted ahead of time.</p>\n<blockquote><p>For example, if the production environment only supports deploying Java and Scala code for backend uses and the solution is thus expected to be provided in a JVM language, the data scientist will have to go deeper into Python-based implementations she finds even during this research phase, as going forward with them into the model development phase entails translating them to a JVM language.</p></blockquote>\n<p>Finally, while reviewing literature, keep in mind that not only the chosen research direction (or couple of directions) should to be presented to the rest of the team. Rather, a brief review of the field and all examined solutions should accompany the choice made, explaining the upsides and downsides of each direction and the justifications for that choice.</p>\n<p><b>2.3. Technical validity\u00a0check</b></p>\n<p>With a suggestion for a possible solution, the data engineer and any involved developers need to estimate, with the help of the data scientist, the form and complexity of this solution in production. Both the product needs and the structure and characteristics of the suggested solution should help determine the adequate data storage, processing (stream vs batch), ability to scale (horizontally and vertically) and a rough estimate of cost.</p>\n<p>This is an important check to perform at this stage because some data and software engineering can begin in parallel to model development. Additionally, a suggested solution might turn out to be inadequate or too costly in engineering terms, in which case this should be identified and dealt with as soon as possible. When technical issues are considered before model development starts, the knowledge gained during the research phase can then be used to suggest an alternate solution that might better fit technical constraints. This is another reason why the research phase must also result in some overview of the solution landscape, and not just in a single solution direction.</p>\n<p><b>2.4. Scope &amp; KPIs validation</b></p>\n<p>Again, the product manager needs to approve that the suggested solution, now stated in more technical terms, meets the scope and KPIs defined. Possible technical criteria that usually have easily detectable product implications are response time (and its relation to computation time), the freshness of data and sometimes cached mid-calculations (which are related to querying and batch computation frequency), difficulty and cost (including data cost) of domain adaptation for domain-specific models (domains are most often clients, but can be industries, languages, countries and so on) and solution composability (e.g. do data and model structures allow to easily break a country-wise model down to a per-region model, or to compose several such models into a per-continent model), though many more exist.</p>\n<p>\u00a0</p>\n<h3>3. The Development Phase</h3>\n<p>\u00a0<br/>\n<b>3.1. Model development &amp; experiments framework setup</b></p>\n<p>The amount and complexity of setup required for model development to begin depends heavily on the infrastructure and amount of technical support available to the data scientist. In smaller places, and in places not yet used to supporting data science research projects, setup might sum up to the data scientist opening a new code repository and firing up a local Jupyter Notebook server, or requesting a stronger cloud machine to run computations on.</p>\n<p>In other cases it might entail writing custom code for more complex functionalities such as data and model versioning or experiment tracking and management. When this functionality is instead provided by some external product or service (and more and more of these are popping up these days), some setup in the form of linking data sources, allocating resources and setting up custom packages might follow.</p>\n<p><b>3.2. Model development</b></p>\n<p>With the required infrastructure in place, actual model development can begin in earnest. The extent of what is considered the model to be developed here varies by company, and depends on the relation, and the divide, between the model to be delivered by the data scientist and the service or feature to be deployed in production. The various type of approaches to this divide can perhaps be captured somewhat by considering a spectrum.</p>\n<p>On one end of spectrum lies the case where everything is\u00a0<em>the model</em>: from data aggregation and preprocessing, through model training (possibly periodically), model deployment, serving (possibly with scaling) and continuous monitoring. On the other end lies the case where just the choice of model type and hyperparameters, and commonly also advanced data preprocessing and feature generation, is thought of as\u00a0<em>the model</em>.</p>\n<p>A company\u2019s location on the spectrum depends on numerous factors: the data scientists\u2019 preferable research language; relevant libraries and open source availability; supported production languages in the company; the existence of a data engineer and devs dedicated solely to data science related code; and the technical capabilities and work methodology of the data scientists.</p>\n<p>In case of a very full-stack-y data scientist, combined with enough support from a dedicated data engineer and devs\u200a\u2014\u200a<em>or, alternatively, with enough existing infrastructure dedicated to the operation and automation of data lake-ing and aggregation, model serving, scaling and monitoring (and possibly also versioning)\u200a</em>\u2014\u200athe wider definition for a model can be taken, and an end-to-end solution can be used throughout most of the iterations on model development.</p>\n<p>This usually means building the complete pipeline first, from data sources all the way to scaleable served models, with simple placeholders for data preprocessing, feature generation and the model itself. Iterations are then made on the data-science-y parts, while keeping limiting the scope to what is available and deployable on the existing infrastructure.</p>\n<p>This end-to-end approach can take more time to setup, and each iteration on model types and parameters make take longer to test, but it saves time later paid for in the productization phase.</p>\n<p>I personally love it, but it\u2019s complex to implement and maintain, and its not always appropriate. In that case, some parts of the start and the end of the pipeline are left to the productization phase.</p>\n<p><b>3.3. Model\u00a0test</b></p>\n<p>While developing the model, different versions of it (and the data processing pipeline accompanying it) should be continuously tested against the predetermined hard metric(s). This gives a rough estimate of progress and also allows the data scientist to decide when the model seems to be working well enough to warrant the overall KPI check. Do note that this can be misleading, as getting from 50% to 70% accuracy, for example, is in many cases much easier than getting from 70% to 90% accuracy.</p>\n<p><center><img src=\"https://cdn-images-1.medium.com/max/800/1*oP0Jg_POKFt9mlVYQrz7Tg.png\" width=\"99%\"/><br/>\n<font size=\"-1\">Figure 2: Model failure entails iteration, but an approach failure might send you back to\u00a0research</font></center></p>\n<p>When tests show that a model is off the mark, we usually investigate it and its output to guide improvements. Sometimes, however, the gap in performance is very large, with different variations of the chosen research directions all falling short\u200a\u2014\u200aan\u00a0<em>approach failure</em>. This might warrant a change in the research direction, sending the project back into the research phase.\u00a0This is the aspect of data science projects that is hardest to accept: the very real possibility of backtracking.</p>\n<p>Another possible result of\u00a0<em>approach failure</em>\u00a0is a change to the goal. With luck, it can be minor product-wise but restate the goal technically in a simpler way.</p>\n<blockquote><p>For example, instead of trying to generate a one-sentence summary of an article, choose the sentence in the article that best summarizes it.</p></blockquote>\n<p>The final possible result is of course project cancellation; if the data scientist is sure all research avenues have been explored, and the product manager is sure that a valid product cannot be built around existing performance, it might be time to move on to another project.\u00a0Do not underestimate the ability to identify an unsalvageable project and the courage to make the decision to end it; it is a crucial part of the\u00a0<em>fail-fast</em>\u00a0methodology.</p>\n<p><b>3.4. KPIs\u00a0check</b></p>\n<p>If the predetermined hard metric is the only KPI and captures all product needs exactly, then this phase can be more of a formality, when the final model is presented and the development phase is declared over.\u00a0<strong>This is usually not the case.</strong></p>\n<p>In the more common case, the hard metric is a good approximation of the actual product needs, but not a perfect one. This phase is thus an opportunity to make sure that the softer metrics, that cannot be checked automatically, are also satisfied. This is done together with product and customer success. If you can additionally check the actual value to a customer directly\u2014 e.g. when working with a design partner\u200a\u2014\u200athen it\u2019s the best guide you could find for your iterations.</p>\n<blockquote><p>For example, let\u2019s say that we\u2019re dealing with a complex task such extracting relevant documents, given a query, from a huge corpus. The team might have decided that to try and increase the quality of the result set, focusing on variance in content and topics of the returned documents, as clients feel the systems tends to cluster quite similar documents in top results.</p>\n<p>Model development might have progressed with some measurable metric for content variance in the results set\u200a\u2014\u200aeach model is scored by how varied are the top 20 documents it returns, given a set of test queries; perhaps you measure overall distance between document topics in some topic vector space, or just the number of unique topics or flatness of significant word distributions.</p>\n<p>Even when the data scientist settles on a model which improves this metric significantly, product and customer success people should definitely take a look at the actual results for a significant sample of the test queries; they might find problems hard to quantify, but possible to solve, such as a model increasing result variance by pushing up some recurring non-relevant topic, or by including results on similar topics but from different sources (e.g. news article vs tweets, which use a very different language).</p></blockquote>\n<p>When the product person is convinced the model answers the stated goals of the project (to a satisfactory degree), the team can move forward to productizing it.</p>\n<p>\u00a0</p>\n<h3>4. The Deployment Phase</h3>\n<p>\u00a0<br/>\n<b>4.1. Solution Productization &amp; Monitoring Setup</b></p>\n<p>This phase, as mentioned earlier, depends on the approach to both data science research and model serving in the company, as well as several key technical factors.</p>\n<p><strong>Productization:</strong>\u00a0In cases where research language can be used in production, this phase might entail adapting the model code to work in a scalable manner; how simple or complex this process is depends both on distributive computing support for the model language, and the specific libraries and custom code used.</p>\n<p>When research and production language are different, this might also involve wrapping the model code in a production language wrapper, compiling it to a low level binary or implementing the same logic in production language (or finding such an implementation).</p>\n<p>Scaleable data ingestion and processing also need to be set up, in the (quite common) case where this was not part of the model. This can mean, for example, turning Python functions that ran on a single core to a pipeline streaming data goes through, or into batch jobs running periodically. In the case of significant data re-use, a caching layer is sometimes set up.</p>\n<p><strong>Monitoring:</strong>\u00a0Finally, a way to continuously monitor the performance of the model is set up; in rare cases, when the source of production data is constant, this can perhaps be safely skipped, but I\u2019d say that in most cases you can\u2019t be sure of the stability of the source data distribution. Setting up such a performance check, then, can help us to not only detect problems in the model that we might have missed during development and productization, but more importantly changes in the source data distribution above which the model operates\u200a\u2014\u200acommonly referred to as a\u00a0<em>covariate shift</em>\u200a\u2014\u200athat can degrade, in time, the performance of a perfectly good model.</p>\n<blockquote><p>Take, for example, the case where<strong>\u00a0</strong>our product is an app that detects skin marks and evaluate whether to recommend the user to go see a skin doctor. A covariate shift might happen in our data when a popular new phone goes to market, equipped with a camera significantly different from those present in our data.</p></blockquote>\n<p><b>4.2. Solution Deployment</b></p>\n<p>If everything is set up correctly, then this stage can sum up to, hopefully, pushing a button to deploy the new model\u200a\u2014\u200aand any code serving it\u200a\u2014\u200ato the company\u2019s production environment.</p>\n<p><strong>Partial Deployment:</strong>\u00a0It is possible, however, that in order to test the effectiveness of the model (for example, in reducing churn, or increasing average monthly spending per user), the model will be deployed to in a manner that only part of the user/customer base is exposed to it. This enables a direct comparison of the effect on any measurable KPIs between the two (or more) groups in the user base.</p>\n<p>Another reason you might not want to deploy the model to everyone is if it was developed to answer the needs of a specific customer or a group of customers, or if it\u2019s a premium feature or part of a specific plan. Alternatively, the model might have some element of personalization per user or customer; this is can sometimes be achieved by actually having a single model which take customer characteristics into account, but sometimes entails actually training and deploying a different model for each customer.</p>\n<p>Whatever the case, all these scenarios increase the complexity of deploying the model, and depending on existing infrastructure in the company (e.g. if you\u2019re already deploying some of the product features to subsets of your customers) they might require a significant amount of additional development by your back-end team.</p>\n<p>This phase is even more complex when the model is to be deployed on end-products, like user phones or wearables, in which case model deployment might only happen as part of the next app or firmware update deployed.</p>\n<p><strong>Generating Bias:</strong>\u00a0Finally, all cases of partial deployment are actually a pressing issue to the data science team for another reason: this naturally introduces bias into the future data the model will start accumulating\u200a\u2014\u200athe model will start operating on data by a subset of users with possibly unique characteristics. Depending on the product and the specific biased characteristics, this can have a big impact on the performance of the model in the wild, and possibly on future models trained on data accumulated during this period.</p>\n<blockquote><p>For example, in the case of device update, users who updated their apps/firmware earlier tend to fall into certain demographics (younger, more tech-savvy, higher income, etc.).</p></blockquote>\n<p><b>4.3. KPIs\u00a0check</b></p>\n<p>I\u2019ve added another KPIs check here because I think a solution cannot be marked as delivered before its performance and successful answering of product and customer needs has been validated after deployment and actual use.</p>\n<p>This might mean sifting through and running analysis on the resulting data a couple of weeks after deployment. When actual customers are involved, however, this must also involve product or customers success people sitting with the customers and trying to understand the actual impact the model has on their use of the product.</p>\n<p><b>4.4. Solution delivered</b></p>\n<p>Users and customers are happy. Product people have managed to build or adapt the product they wanted around the model. We\u2019re done. Toasts are toasted, cheers are cheered, and all is well.</p>\n<p>The solution is delivered, and I would call the project done at this point. It does, however, keeps on living in a specific way\u200a\u2014\u200amaintenance.</p>\n<p><b>Maintenance</b></p>\n<p>Having set up health checks and continuous performance monitoring for the model, these can trigger up short bursts of working on the project.</p>\n<p>When something seems to be suspicious, we usually start by looking at the data (e.g. for covariate shifts), and perhaps simulating the response of the model to various cases that we suspect cause the problem. The results of these checks can send us into anything between a few hours of small code changes and re-training of the model and full model development iterations (as reflected in the figure opening this post), with severe cases sometimes entailing going back to the research phase to try completely different directions.</p>\n<p>\u00a0</p>\n<h3>Final Words</h3>\n<p>\u00a0<br/>\nThis is a suggestion for the flow of data science projects. It is also very specific, limited in scope\u200a\u2014\u200afor the sake of simplicity and visibility\u200a\u2014\u200aand obviously cannot cover the many variations on this flow that exist in practice. It also represents my experience.</p>\n<p>For all of these reasons, I\u2019d love to hear your feedback, insights and experience from running, leading or managing data science projects, whatever their size, and whatever the size of the data science team you are part of.</p>\n<p>For another great take on this topic, I recommend reading\u00a0<a href=\"https://towardsdatascience.com/data-science-agile-cycles-my-method-for-managing-data-science-projects-in-the-hi-tech-industry-b289e8a72818\" rel=\"noopener noreferrer\" target=\"_blank\">my friend Ori\u2019s post on agile development for data science</a>. I would also like to thank\u00a0<a href=\"https://medium.com/@inbarnaor\" rel=\"noopener noreferrer\" target=\"_blank\">Inbar Naor</a>, Shir Meir Lador (<a href=\"http://twitter.com/DataLady\" rel=\"noopener noreferrer\" target=\"_blank\">@DataLady</a>) and\u00a0<a href=\"http://twitter.com/seffi\" rel=\"noopener noreferrer\" target=\"_blank\">@seffi</a>.cohen for their feedback.</p>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"http://www.shaypalachy.com/\" rel=\"noopener noreferrer\" target=\"_blank\">Shay Palachy</a></b> is a data scientist and a data science consultant. He also loves working on community and open source projects.</p>\n<p><a href=\"https://towardsdatascience.com/data-science-project-flow-for-startups-282a93d4508d\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2019/01/end-to-end-guide-machine-learning-project.html\">End To End Guide For Machine Learning Projects</a>\n<li><a href=\"/2018/12/machine-learning-project-checklist.html\">The Machine Learning Project Checklist</a>\n<li><a href=\"/2018/07/manage-machine-learning-lifecycle-mlflow.html\">Manage your Machine Learning Lifecycle with MLflow\u200a \u2013 \u200aPart 1</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div> ", "url": "https://www.kdnuggets.com/2019/01/data-science-project-flow-startups.html"}