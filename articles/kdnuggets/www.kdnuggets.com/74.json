{"content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/gregory-piatetsky\" rel=\"author\" title=\"Posts by Gregory Piatetsky\">Gregory Piatetsky</a>, KDnuggets.</b></div>\n<a href=\"http://yann.lecun.com/\" target=\"_blank\">\n<img align=\"right\" alt=\"Yann LeCun\" src=\"/images/yann-lecun-2006.jpg\"/>\n<b>Prof. Yann LeCun</b></a> \r\nhas been much in the news lately, as one of the leading experts in \r\n<a href=\"http://deeplearning.net/\" target=\"_blank\">\r\nDeep Learning</a> - a breakthrough advance in machine learning which has been achieving amazing successes, \r\nas a founding Director of \r\n<a href=\"http://cds.nyu.edu/\" target=\"_blank\">\r\nNYU Center for Data Science</a>, \r\nand as the \r\n<a href=\"http://www.wired.com/wiredenterprise/2013/12/facebook-yann-lecun/\" target=\"_blank\">\r\nnewly appointed </a>\r\nDirector of the AI Research Lab at Facebook.\r\nSee his bio at the end of this post and you can learn more about his work at <a href=\"http://yann.lecun.com/\" target=\"_blank\">yann.lecun.com/</a>.\r\n<br/><br/>\r\nHe is extremely busy, combining his new job at Facebook and his old job at NYU, so I am very pleased that he agreed to answer a few questions for KDnuggets readers.\r\n<br/><br/>\n<b>Gregory Piatetsky: 1. Artificial Neural networks have been studied for 50 years, but only recently they have achieved remarkable successes, in such difficult tasks as speech and image recognition, with Deep Learning Networks. What factors enabled this success - big data, algorithms, hardware?</b>\n<br/><br/>\n<b>Yann LeCun</b>: Despite a commonly-held belief, there have been numerous successful applications of neural nets since the late 80's.\r\n<br/><br/>\r\nDeep learning has come to designate any learning method that can train a system with more than 2 or 3 non-linear hidden layers.\r\n<br/><br/>\r\nAround 2003, Geoff Hinton, Yoshua Bengio and myself initiated a kind of \"conspiracy\" to revive the interest of the machine learning community in the problem of learning representations (as opposed to just learning simple classifiers). It took until 2006-2007 to get some traction, primarily through new results on unsupervised training (or unsupervised pre-training, followed by supervised fine-tuning), with work by Geoff Hinton, Yoshua Bengio, Andrew Ng and myself.\r\n<br/><br/>\r\nBut much of the recent practical applications of deep learning use purely supervised learning based on back-propagation, altogether not very different from the neural nets of the late 80's and early 90's. \r\n<br/><br/>\n<b>What's different is that we can run very large and very deep networks on fast GPUs</b> (sometimes with billions of connections, and 12 layers) and train them on large datasets with millions of examples. We also have \r\n<b>a few more tricks than in the past</b>, such as a <a href=\"http://cs.nyu.edu/~wanli/dropc/dropc.pdf\" target=\"_blank\">\r\nregularization method called \"drop out\"</a>, rectifying non-linearity for the units, different types of spatial pooling, etc.\r\n<br/><br/>\r\nMany successful applications, particularly for image recognition use the convolutional network architecture \r\n(<a href=\"http://code.google.com/p/cuda-convnet/\">ConvNet</a>), a concept I developed at Bell Labs in the late 80s and early 90s. At Bell Labs in the mid 1990s we commercially deployed a number of ConvNet-based systems for reading the amount on bank check automatically (printed or handwritten). \r\n<br/><br/>\r\nAt some point in the late 1990s, one of these systems was reading 10 to 20% of all the checks in the US. Interest in ConvNet was rekindled in the last 5 years or so, with nice work from my group, from Geoff Hinton, Andrew Ng, and Yoshua Bengio, as from Jurgen Schmidhuber's group at IDSIA in Switzerland, and from NEC Labs in California. ConvNets are now widely used by Facebook, Google, Microsoft, IBM, Baidu, NEC and others for image and speech recognition. \r\n[GP: A student of Yann Lecun recently \r\n<a href=\"/2014/02/deep-learning-wins-dogs-vs-cats-competition.html\" target=\"_blank\">won Dogs vs Cats competition on Kaggle</a> using a version of ConvNet, achieving 98.9% accuracy.]\r\n<br/><br/>\n<b>GP: 2. Deep learning is not an easy to use method. What tools, tutorials would you recommend to data scientists, who want to learn more and use it on their data? Your opinion of Pylearn2, Theano?</b>\n<br/><br/>\n<b>Yann LeCun</b>: There are two main packages: \r\n<ul class=\"three_ul\">\n<li> <a href=\"http://torch.ch/\" target=\"_blank\"><b>Torch7</b></a>, and \r\n</li><li> <a href=\"http://deeplearning.net/software/theano/\">\n<b>Theano</b></a> + <a href=\"https://github.com/lisa-lab/pylearn2\"><b>Pylearn2</b></a>. \r\n</li></ul>\n<br/>\u00a0<br/>\r\nThey have slightly different philosophies and relative advantages and disadvantages. Torch7 is an extension of the LuaJIT language that adds multi-dimensional arrays and numerical library. It also includes an object-oriented package for deep learning, computer vision, and such. The main advantage of Torch7 is that LuaJIT is extremely fast, in addition to being very flexible (it's a compiled version of the popular Lua language). \r\n<br/><br/>\r\nTheano+Pylearn2 has the advantage of using Python (it's widely used, and has lots of libraries for many things), and the disadvantage of using Python (it's slow).\r\n<br/><br/>\n<b>GP: 3. You and I have met a while ago at a scientific advisory meeting of KXEN, where \r\n<a href=\"http://en.wikipedia.org/wiki/Vladimir_Vapnik\">\r\nVapnik</a>'s  Statistical Learning Theory and SVM were a major topic.  What is the relationship between Deep Learning and \r\n<a href=\"http://en.wikipedia.org/wiki/Support_vector_machine\" target=\"_blank\">\r\nSupport Vector Machines</a> / <a href=\"http://en.wikipedia.org/wiki/Statistical_learning_theory\" target=\"_blank\">Statistical Learning Theory?</a></b>\n<br/><br/>\n<b>Yann LeCun</b>: \r\nVapnik and I were in nearby office at Bell Labs in the early 1990s, in Larry Jackel's Adaptive Systems Research Department. Convolutional nets, Support Vector Machines, Tangent Distance, and several other influential methods were invented within a few meters of each other, and within a few years of each other.  When AT&amp;T spun off Lucent In 1995, I became the head of that department which became the Image Processing Research Department at AT&amp;T Labs - Research. Machine Learning members included Yoshua Bengio, Leon Bottou, and Patrick Haffner, and Vladimir Vapnik. Visitors and interns included Bernhard Scholkopf, Jason Weston, Olivier Chapelle, and others.\r\n<br/><br/>\n<img align=\"right\" alt=\"Support Vectors\" src=\"/images/support-vectors.jpg\"/>\r\nVapnik and I often had lively discussions about the relative merits of (deep) neural nets and kernel machines. Basically, I have always been interested in solving the problem of learning features or learning representations. I had only a moderate interest in kernel methods because they did nothing to address this problem. Naturally, SVMs are wonderful as a generic classification method with beautiful math behind them. But in the end, they are nothing more than simple two-layer systems. The first layer can be seen as a set of units (one per support vector) that measure a kind of similarity between the input vector and each support vector using the kernel function. The second layer linearly combines these similarities. \r\n<br/><br/>\r\nIt's a two-layer system in which the first layer is trained with the simplest of all unsupervised learning method: simply store the training samples as prototypes in the units. Basically, varying the smoothness of the kernel function allows us to interpolate between two simple methods: linear classification, and template matching. I got in trouble about 10 years ago by saying that kernel methods were a form of glorified template matching. Vapnik, on the other hand, argued that SVMs had a very clear way of doing capacity control. An SVM with a \"narrow\" kernel function can always learn the training set perfectly, but its generalization error is controlled by the width of the kernel and the sparsity of the dual coefficients. Vapnik really believes in his bounds. He worried that neural nets didn't have similarly good ways to do capacity control (although neural nets do have generalization bounds, since they have finite VC dimension).\r\n<br/><br/>\r\nMy counter argument was that the ability to do capacity control was somewhat secondary to the ability to compute highly complex function with a limited amount of computation. Performing image recognition with invariance to shifts, scale, rotation, lighting conditions, and background clutter was impossible (or extremely inefficient) for a kernel machine operating at the pixel level. But it was quite easy for deep architectures such as convolutional nets.\r\n<br/><br/>\n<b>GP: 4. Congratulations on your recent appointment as the head of Facebook new AI Lab. What can you tell us about AI and Machine Learning advances we can expect from Facebook in the next couple of years?</b>\n<br/><br/>\n<img align=\"right\" alt=\"Facebook\" src=\"/images/facebook.jpg\"/>\n<b>Yann LeCun</b>: Thank you! it's a very exciting opportunity. Basically, Facebook's main objective is to enable communication between people. But people today are bombarded with information from friends, news organizations, websites, etc. Facebook helps people sift through this mass of information. But that requires to know what people are interested in, what motivates them, what entertains them, what makes them learn new things. This requires an understanding of people that only AI can provide. Progress in AI will allow us to understand content, such as text, images, video, speech, audio, music, among other things.\r\n<br/><br/>\r\nHere is \r\n<a href=\"/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab-part2.html\" target=\"_blank\">\r\npart 2 of the interview.</a>\n<br/><br/>\n<strong>Bio: </strong>\n<a href=\"http://en.wikipedia.org/wiki/Yann_LeCun\">\r\nYann LeCun</a> is Director of AI Research at Facebook, and the founding\r\ndirector of the Center for Data Science at New York University. He is\r\nSilver Professor of Computer Science, Neural Science, and Electrical\r\nEngineering and NYU, affiliated with the Courant Institute of\r\nMathematical Science, the Center for Neural Science, and the ECE\r\nDepartment.\r\n<br/><br/>\r\nHe received the Electrical Engineer Diploma from Ecole Sup\u00e9rieure\r\nd'Ing\u00e9nieurs en Electrotechnique et Electronique (ESIEE), Paris in\r\n1983, and a PhD in Computer Science from Universit\u00e9 Pierre et Marie\r\nCurie (Paris) in 1987. After a postdoc at the University of Toronto,\r\nhe joined AT&amp;T Bell Laboratories in Holmdel, NJ in 1988. He became\r\nhead of the Image Processing Research Department at AT&amp;T Labs-Research\r\nin 1996, and joined NYU as a professor in 2003, after a brief period\r\nas a Fellow of the NEC Research Institute in Princeton. He was named\r\nDirector of AI Research at Facebook in late 2013 and retains a\r\npart-time position on the NYU faculty.\r\n<br/><br/>\r\nHis current interests include machine learning, computer perception,\r\nmobile robotics, and computational neuroscience.  He has published\r\nover 180 technical papers and book chapters on these topics as well as\r\non neural networks, handwriting recognition, image processing and\r\ncompression, and on dedicated circuits and architectures for computer\r\nperception. The character recognition technology he developed at Bell\r\nLabs is used by several banks around the world to read checks and was\r\nreading between 10 and 20% of all the checks in the US in the early\r\n2000s.  His image compression technology, called DjVu, is used by\r\nhundreds of web sites and publishers and millions of users to access\r\nscanned documents on the Web. A pattern recognition method he\r\ndeveloped, called convolutional network, is the basis of products and\r\nservices deployed by companies such as AT&amp;T, Google, Microsoft, NEC,\r\nIBM, Baidu, and Facebook for document recognition, human-computer\r\ninteraction, image tagging, speech recognition, and video analytics.\r\n<br/><br/>\r\nLeCun has been on the editorial board of IJCV, IEEE PAMI, and IEEE\r\nTrans. Neural Networks, was program chair of CVPR'06, and is chair of\r\nICLR 2013 and 2014. He is on the science advisory board of Institute\r\nfor Pure and Applied Mathematics, and has advised many large and small\r\ncompanies about machine learning technology, including several\r\nstartups he co-founded. He is the recipient of the 2014 IEEE Neural\r\nNetwork Pioneer Award.\r\n  </div> ", "url": "https://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab.html", "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  KDnuggets Exclusive: Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb KDnuggets Exclusive: Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2014/02/data-mining-tool-gamers-pre-launch-secrets.html\" rel=\"prev\" title=\"Is data mining the new tool for gamers seeking pre-launch secrets?\"/>\n<link href=\"https://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab-part2.html\" rel=\"next\" title=\"KDnuggets Exclusive: Part 2 of the Interview with Yann LeCun\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=15256\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-15256 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 20-Feb, 2014  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2014/index.html\">2014</a> \u00bb <a href=\"https://www.kdnuggets.com/2014/02/index.html\">Feb</a> \u00bb <a href=\"https://www.kdnuggets.com/2014/02/opinions-interviews.html\">Opinions, Interviews</a> \u00bb KDnuggets Exclusive: Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab (\u00a0<a href=\"/2014/n05.html\">14:n05</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">KDnuggets Exclusive: Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2014/02/data-mining-tool-gamers-pre-launch-secrets.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab-part2.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <span class=\"http-likes\" style=\"float: left; font-size:14px\">http likes 264</span> <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/andrew-ng\" rel=\"tag\">Andrew Ng</a>, <a href=\"https://www.kdnuggets.com/tag/deep-learning\" rel=\"tag\">Deep Learning</a>, <a href=\"https://www.kdnuggets.com/tag/facebook\" rel=\"tag\">Facebook</a>, <a href=\"https://www.kdnuggets.com/tag/interview\" rel=\"tag\">Interview</a>, <a href=\"https://www.kdnuggets.com/tag/nyu\" rel=\"tag\">NYU</a>, <a href=\"https://www.kdnuggets.com/tag/support-vector-machines\" rel=\"tag\">Support Vector Machines</a>, <a href=\"https://www.kdnuggets.com/tag/vladimir-vapnik\" rel=\"tag\">Vladimir Vapnik</a>, <a href=\"https://www.kdnuggets.com/tag/yann-lecun\" rel=\"tag\">Yann LeCun</a></div>\n<br/>\n<p class=\"excerpt\">\n     We discuss what enabled Deep Learning to achieve remarkable successes recently, his argument with Vapnik about (deep) neural nets vs kernel (support vector) machines, and what kind of AI can we expect from Facebook.  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/gregory-piatetsky\" rel=\"author\" title=\"Posts by Gregory Piatetsky\">Gregory Piatetsky</a>, KDnuggets.</b></div>\n<a href=\"http://yann.lecun.com/\" target=\"_blank\">\n<img align=\"right\" alt=\"Yann LeCun\" src=\"/images/yann-lecun-2006.jpg\"/>\n<b>Prof. Yann LeCun</b></a> \r\nhas been much in the news lately, as one of the leading experts in \r\n<a href=\"http://deeplearning.net/\" target=\"_blank\">\r\nDeep Learning</a> - a breakthrough advance in machine learning which has been achieving amazing successes, \r\nas a founding Director of \r\n<a href=\"http://cds.nyu.edu/\" target=\"_blank\">\r\nNYU Center for Data Science</a>, \r\nand as the \r\n<a href=\"http://www.wired.com/wiredenterprise/2013/12/facebook-yann-lecun/\" target=\"_blank\">\r\nnewly appointed </a>\r\nDirector of the AI Research Lab at Facebook.\r\nSee his bio at the end of this post and you can learn more about his work at <a href=\"http://yann.lecun.com/\" target=\"_blank\">yann.lecun.com/</a>.\r\n<br/><br/>\r\nHe is extremely busy, combining his new job at Facebook and his old job at NYU, so I am very pleased that he agreed to answer a few questions for KDnuggets readers.\r\n<br/><br/>\n<b>Gregory Piatetsky: 1. Artificial Neural networks have been studied for 50 years, but only recently they have achieved remarkable successes, in such difficult tasks as speech and image recognition, with Deep Learning Networks. What factors enabled this success - big data, algorithms, hardware?</b>\n<br/><br/>\n<b>Yann LeCun</b>: Despite a commonly-held belief, there have been numerous successful applications of neural nets since the late 80's.\r\n<br/><br/>\r\nDeep learning has come to designate any learning method that can train a system with more than 2 or 3 non-linear hidden layers.\r\n<br/><br/>\r\nAround 2003, Geoff Hinton, Yoshua Bengio and myself initiated a kind of \"conspiracy\" to revive the interest of the machine learning community in the problem of learning representations (as opposed to just learning simple classifiers). It took until 2006-2007 to get some traction, primarily through new results on unsupervised training (or unsupervised pre-training, followed by supervised fine-tuning), with work by Geoff Hinton, Yoshua Bengio, Andrew Ng and myself.\r\n<br/><br/>\r\nBut much of the recent practical applications of deep learning use purely supervised learning based on back-propagation, altogether not very different from the neural nets of the late 80's and early 90's. \r\n<br/><br/>\n<b>What's different is that we can run very large and very deep networks on fast GPUs</b> (sometimes with billions of connections, and 12 layers) and train them on large datasets with millions of examples. We also have \r\n<b>a few more tricks than in the past</b>, such as a <a href=\"http://cs.nyu.edu/~wanli/dropc/dropc.pdf\" target=\"_blank\">\r\nregularization method called \"drop out\"</a>, rectifying non-linearity for the units, different types of spatial pooling, etc.\r\n<br/><br/>\r\nMany successful applications, particularly for image recognition use the convolutional network architecture \r\n(<a href=\"http://code.google.com/p/cuda-convnet/\">ConvNet</a>), a concept I developed at Bell Labs in the late 80s and early 90s. At Bell Labs in the mid 1990s we commercially deployed a number of ConvNet-based systems for reading the amount on bank check automatically (printed or handwritten). \r\n<br/><br/>\r\nAt some point in the late 1990s, one of these systems was reading 10 to 20% of all the checks in the US. Interest in ConvNet was rekindled in the last 5 years or so, with nice work from my group, from Geoff Hinton, Andrew Ng, and Yoshua Bengio, as from Jurgen Schmidhuber's group at IDSIA in Switzerland, and from NEC Labs in California. ConvNets are now widely used by Facebook, Google, Microsoft, IBM, Baidu, NEC and others for image and speech recognition. \r\n[GP: A student of Yann Lecun recently \r\n<a href=\"/2014/02/deep-learning-wins-dogs-vs-cats-competition.html\" target=\"_blank\">won Dogs vs Cats competition on Kaggle</a> using a version of ConvNet, achieving 98.9% accuracy.]\r\n<br/><br/>\n<b>GP: 2. Deep learning is not an easy to use method. What tools, tutorials would you recommend to data scientists, who want to learn more and use it on their data? Your opinion of Pylearn2, Theano?</b>\n<br/><br/>\n<b>Yann LeCun</b>: There are two main packages: \r\n<ul class=\"three_ul\">\n<li> <a href=\"http://torch.ch/\" target=\"_blank\"><b>Torch7</b></a>, and \r\n</li><li> <a href=\"http://deeplearning.net/software/theano/\">\n<b>Theano</b></a> + <a href=\"https://github.com/lisa-lab/pylearn2\"><b>Pylearn2</b></a>. \r\n</li></ul>\n<br/>\u00a0<br/>\r\nThey have slightly different philosophies and relative advantages and disadvantages. Torch7 is an extension of the LuaJIT language that adds multi-dimensional arrays and numerical library. It also includes an object-oriented package for deep learning, computer vision, and such. The main advantage of Torch7 is that LuaJIT is extremely fast, in addition to being very flexible (it's a compiled version of the popular Lua language). \r\n<br/><br/>\r\nTheano+Pylearn2 has the advantage of using Python (it's widely used, and has lots of libraries for many things), and the disadvantage of using Python (it's slow).\r\n<br/><br/>\n<b>GP: 3. You and I have met a while ago at a scientific advisory meeting of KXEN, where \r\n<a href=\"http://en.wikipedia.org/wiki/Vladimir_Vapnik\">\r\nVapnik</a>'s  Statistical Learning Theory and SVM were a major topic.  What is the relationship between Deep Learning and \r\n<a href=\"http://en.wikipedia.org/wiki/Support_vector_machine\" target=\"_blank\">\r\nSupport Vector Machines</a> / <a href=\"http://en.wikipedia.org/wiki/Statistical_learning_theory\" target=\"_blank\">Statistical Learning Theory?</a></b>\n<br/><br/>\n<b>Yann LeCun</b>: \r\nVapnik and I were in nearby office at Bell Labs in the early 1990s, in Larry Jackel's Adaptive Systems Research Department. Convolutional nets, Support Vector Machines, Tangent Distance, and several other influential methods were invented within a few meters of each other, and within a few years of each other.  When AT&amp;T spun off Lucent In 1995, I became the head of that department which became the Image Processing Research Department at AT&amp;T Labs - Research. Machine Learning members included Yoshua Bengio, Leon Bottou, and Patrick Haffner, and Vladimir Vapnik. Visitors and interns included Bernhard Scholkopf, Jason Weston, Olivier Chapelle, and others.\r\n<br/><br/>\n<img align=\"right\" alt=\"Support Vectors\" src=\"/images/support-vectors.jpg\"/>\r\nVapnik and I often had lively discussions about the relative merits of (deep) neural nets and kernel machines. Basically, I have always been interested in solving the problem of learning features or learning representations. I had only a moderate interest in kernel methods because they did nothing to address this problem. Naturally, SVMs are wonderful as a generic classification method with beautiful math behind them. But in the end, they are nothing more than simple two-layer systems. The first layer can be seen as a set of units (one per support vector) that measure a kind of similarity between the input vector and each support vector using the kernel function. The second layer linearly combines these similarities. \r\n<br/><br/>\r\nIt's a two-layer system in which the first layer is trained with the simplest of all unsupervised learning method: simply store the training samples as prototypes in the units. Basically, varying the smoothness of the kernel function allows us to interpolate between two simple methods: linear classification, and template matching. I got in trouble about 10 years ago by saying that kernel methods were a form of glorified template matching. Vapnik, on the other hand, argued that SVMs had a very clear way of doing capacity control. An SVM with a \"narrow\" kernel function can always learn the training set perfectly, but its generalization error is controlled by the width of the kernel and the sparsity of the dual coefficients. Vapnik really believes in his bounds. He worried that neural nets didn't have similarly good ways to do capacity control (although neural nets do have generalization bounds, since they have finite VC dimension).\r\n<br/><br/>\r\nMy counter argument was that the ability to do capacity control was somewhat secondary to the ability to compute highly complex function with a limited amount of computation. Performing image recognition with invariance to shifts, scale, rotation, lighting conditions, and background clutter was impossible (or extremely inefficient) for a kernel machine operating at the pixel level. But it was quite easy for deep architectures such as convolutional nets.\r\n<br/><br/>\n<b>GP: 4. Congratulations on your recent appointment as the head of Facebook new AI Lab. What can you tell us about AI and Machine Learning advances we can expect from Facebook in the next couple of years?</b>\n<br/><br/>\n<img align=\"right\" alt=\"Facebook\" src=\"/images/facebook.jpg\"/>\n<b>Yann LeCun</b>: Thank you! it's a very exciting opportunity. Basically, Facebook's main objective is to enable communication between people. But people today are bombarded with information from friends, news organizations, websites, etc. Facebook helps people sift through this mass of information. But that requires to know what people are interested in, what motivates them, what entertains them, what makes them learn new things. This requires an understanding of people that only AI can provide. Progress in AI will allow us to understand content, such as text, images, video, speech, audio, music, among other things.\r\n<br/><br/>\r\nHere is \r\n<a href=\"/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab-part2.html\" target=\"_blank\">\r\npart 2 of the interview.</a>\n<br/><br/>\n<strong>Bio: </strong>\n<a href=\"http://en.wikipedia.org/wiki/Yann_LeCun\">\r\nYann LeCun</a> is Director of AI Research at Facebook, and the founding\r\ndirector of the Center for Data Science at New York University. He is\r\nSilver Professor of Computer Science, Neural Science, and Electrical\r\nEngineering and NYU, affiliated with the Courant Institute of\r\nMathematical Science, the Center for Neural Science, and the ECE\r\nDepartment.\r\n<br/><br/>\r\nHe received the Electrical Engineer Diploma from Ecole Sup\u00e9rieure\r\nd'Ing\u00e9nieurs en Electrotechnique et Electronique (ESIEE), Paris in\r\n1983, and a PhD in Computer Science from Universit\u00e9 Pierre et Marie\r\nCurie (Paris) in 1987. After a postdoc at the University of Toronto,\r\nhe joined AT&amp;T Bell Laboratories in Holmdel, NJ in 1988. He became\r\nhead of the Image Processing Research Department at AT&amp;T Labs-Research\r\nin 1996, and joined NYU as a professor in 2003, after a brief period\r\nas a Fellow of the NEC Research Institute in Princeton. He was named\r\nDirector of AI Research at Facebook in late 2013 and retains a\r\npart-time position on the NYU faculty.\r\n<br/><br/>\r\nHis current interests include machine learning, computer perception,\r\nmobile robotics, and computational neuroscience.  He has published\r\nover 180 technical papers and book chapters on these topics as well as\r\non neural networks, handwriting recognition, image processing and\r\ncompression, and on dedicated circuits and architectures for computer\r\nperception. The character recognition technology he developed at Bell\r\nLabs is used by several banks around the world to read checks and was\r\nreading between 10 and 20% of all the checks in the US in the early\r\n2000s.  His image compression technology, called DjVu, is used by\r\nhundreds of web sites and publishers and millions of users to access\r\nscanned documents on the Web. A pattern recognition method he\r\ndeveloped, called convolutional network, is the basis of products and\r\nservices deployed by companies such as AT&amp;T, Google, Microsoft, NEC,\r\nIBM, Baidu, and Facebook for document recognition, human-computer\r\ninteraction, image tagging, speech recognition, and video analytics.\r\n<br/><br/>\r\nLeCun has been on the editorial board of IJCV, IEEE PAMI, and IEEE\r\nTrans. Neural Networks, was program chair of CVPR'06, and is chair of\r\nICLR 2013 and 2014. He is on the science advisory board of Institute\r\nfor Pure and Applied Mathematics, and has advised many large and small\r\ncompanies about machine learning technology, including several\r\nstartups he co-founded. He is the recipient of the 2014 IEEE Neural\r\nNetwork Pioneer Award.\r\n  </div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2014/02/data-mining-tool-gamers-pre-launch-secrets.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab-part2.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-mp-1-another-10']);\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-mp-2-simplilearn']);\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/03/typical-data-scientist-2019.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-mp-3-typical']);\"><b>Who is a typical Data Scientist in 2019?</b></a>\n<li> <a href=\"/2019/03/pareto-principle-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-mp-4-pareto']);\"><b>The Pareto Principle for Data Scientists</b></a>\n<li> <a href=\"/2019/03/data-science-job-applications.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-mp-5-tell-you']);\"><b>What no one will tell you about data science job applications</b></a>\n<li> <a href=\"/2019/03/women-ai-big-data-science-machine-learning.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-mp-6-inspiring-women']);\"><b>19 Inspiring Women in AI, Big Data, Data Science, Machine Learning</b></a>\n<li> <a href=\"/2019/03/favorite-ml-ai-breakthroughs.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-mp-7-breakthroughs']);\"><b>My favorite mind-blowing Machine Learning/AI breakthroughs</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-ms-1-ann-optim-ga']);\"><b>Artificial Neural Networks Optimization using Genetic Algorithm with Python</b></a>\n<li> <a href=\"/2019/03/typical-data-scientist-2019.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-ms-2-typical']);\"><b>Who is a typical Data Scientist in 2019?</b></a>\n<li> <a href=\"/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-ms-3-azure-cert']);\"><b>8 Reasons Why You Should Get a Microsoft Azure Certification</b></a>\n<li> <a href=\"/2019/03/pareto-principle-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-ms-4-pareto']);\"><b>The Pareto Principle for Data Scientists</b></a>\n<li> <a href=\"/2019/03/r-vs-python-data-visualization.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-ms-5-r-vs-py-viz']);\"><b>R vs Python for Data Visualization</b></a>\n<li> <a href=\"/2019/03/work-data-science-ai-big-data.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-ms-6-how-work']);\"><b>How To Work In Data Science, AI, Big Data</b></a>\n<li> <a href=\"/2019/03/deep-learning-toolset-overview.html\" onclick=\"ga('send','pageview','/x/pbc/2019/04-02-ms-7-dl-toolset']);\"><b>The Deep Learning Toolset \u2014 An Overview</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/jmp-statistical-thinking-free-online-course.html\">Statistical Thinking for Industrial Problem Solving (ST...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datarobot-from-business-intelligence-machine-intelligence.html\">From Business Intelligence to Machine Intelligence</a><li> <a href=\"https://www.kdnuggets.com/2019/04/ai-makes-decision.html\">What is missing when AI makes a decision?</a><li> <a href=\"https://www.kdnuggets.com/2019/04/spatio-temporal-statistics-primer.html\">Spatio-Temporal Statistics: A Primer</a><li> <a href=\"https://www.kdnuggets.com/2019/04/another-10-free-must-see-courses-machine-learning-data-science.html\">Another 10 Free Must-See Courses for Machine Learning a...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html\">Download your DATAx guide to AI in Marketing</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html\">Download your DATAx guide to AI in Marketing</a><li> <a href=\"https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html\">KDnuggets Offer: Save 20% on Strata in London</a><li> <a href=\"https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html\">Training a Champion: Building Deep Neural Nets for Big Data An...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/building-recommender-system.html\">Building a Recommender System</a><li> <a href=\"https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html\">Predict Age and Gender Using Convolutional Neural Network and ...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html\">Top tweets, Mar 27 \u2013 Apr 02: Here is a great explanat...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/odsc-east-selling-out-india-announced.html\">ODSC East is selling out; ODSC India announced</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-03-accelerate-ai-data-science-career-expo-2019.html\">Accelerate AI and Data Science Career Expo, May 4, 2019</a><li> <a href=\"https://www.kdnuggets.com/2019/04/formulated-data-career-datasciencego-san-diego.html\">Grow your data career at DataScienceGO, San Diego, Sep 27-29</a><li> <a href=\"https://www.kdnuggets.com/2019/04/nlp-pytorch.html\">Getting started with NLP using the PyTorch framework</a><li> <a href=\"https://www.kdnuggets.com/2019/04/diy-your-data-science-education.html\">How to DIY Your Data Science Education</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-8-data-science-use-cases-gaming.html\">Top 8 Data Science Use Cases in Gaming</a><li> <a href=\"https://www.kdnuggets.com/2019/n13.html\">KDnuggets 19:n13, Apr 3: Top 10 Data Scientist Coding Mista...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/psu-make-better-data-driven-business-decisions.html\">Make better data-driven business decisions</a><li> <a href=\"https://www.kdnuggets.com/2019/04/top-news-week-0325-0331.html\">Top Stories, Mar 25-31: R vs Python for Data Visualization; Th...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/paw-two-predictive-analytics-world-events-europe-fall.html\">Two Predictive Analytics World Events in Europe This Fall</a><li> <a href=\"https://www.kdnuggets.com/2019/04/7-qualities-big-data-visualization-tools.html\">7 Qualities Your Big Data Visualization Tools Absolutely Must ...</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-faculty-ai-machine-learning.html\">Yeshiva University: Tenure-track Faculty in AI and Machine Lea...</a><li> <a href=\"https://www.kdnuggets.com/2019/04/which-face-real-stylegan.html\">Which Face is Real?</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/04-02-yeshiva-university-program-director-faculty-artificial-intelligence-machine-learning.html\">Yeshiva University: Program Director / Tenure Track Faculty Me...</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2014/index.html\">2014</a> \u00bb <a href=\"https://www.kdnuggets.com/2014/02/index.html\">Feb</a> \u00bb <a href=\"https://www.kdnuggets.com/2014/02/opinions-interviews.html\">Opinions, Interviews</a> \u00bb KDnuggets Exclusive: Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab (\u00a0<a href=\"/2014/n05.html\">14:n05</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1554600744\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"center\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"center\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.764 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-04-06 21:32:24 -->\n<!-- Compression = gzip -->", "title": "KDnuggets Exclusive: Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab  ", "content": "\n By  Gregory Piatetsky  , KDnuggets.   \n \n  \n Prof. Yann LeCun    \r\nhas been much in the news lately, as one of the leading experts in \r\n \r\nDeep Learning   - a breakthrough advance in machine learning which has been achieving amazing successes, \r\nas a founding Director of \r\n \r\nNYU Center for Data Science  , \r\nand as the \r\n \r\nnewly appointed   \r\nDirector of the AI Research Lab at Facebook.\r\nSee his bio at the end of this post and you can learn more about his work at  yann.lecun.com/  .\r\n   \r\nHe is extremely busy, combining his new job at Facebook and his old job at NYU, so I am very pleased that he agreed to answer a few questions for KDnuggets readers.\r\n   \n Gregory Piatetsky: 1. Artificial Neural networks have been studied for 50 years, but only recently they have achieved remarkable successes, in such difficult tasks as speech and image recognition, with Deep Learning Networks. What factors enabled this success - big data, algorithms, hardware?  \n   \n Yann LeCun  : Despite a commonly-held belief, there have been numerous successful applications of neural nets since the late 80's.\r\n   \r\nDeep learning has come to designate any learning method that can train a system with more than 2 or 3 non-linear hidden layers.\r\n   \r\nAround 2003, Geoff Hinton, Yoshua Bengio and myself initiated a kind of \"conspiracy\" to revive the interest of the machine learning community in the problem of learning representations (as opposed to just learning simple classifiers). It took until 2006-2007 to get some traction, primarily through new results on unsupervised training (or unsupervised pre-training, followed by supervised fine-tuning), with work by Geoff Hinton, Yoshua Bengio, Andrew Ng and myself.\r\n   \r\nBut much of the recent practical applications of deep learning use purely supervised learning based on back-propagation, altogether not very different from the neural nets of the late 80's and early 90's. \r\n   \n What's different is that we can run very large and very deep networks on fast GPUs   (sometimes with billions of connections, and 12 layers) and train them on large datasets with millions of examples. We also have \r\n a few more tricks than in the past  , such as a  \r\nregularization method called \"drop out\"  , rectifying non-linearity for the units, different types of spatial pooling, etc.\r\n   \r\nMany successful applications, particularly for image recognition use the convolutional network architecture \r\n( ConvNet  ), a concept I developed at Bell Labs in the late 80s and early 90s. At Bell Labs in the mid 1990s we commercially deployed a number of ConvNet-based systems for reading the amount on bank check automatically (printed or handwritten). \r\n   \r\nAt some point in the late 1990s, one of these systems was reading 10 to 20% of all the checks in the US. Interest in ConvNet was rekindled in the last 5 years or so, with nice work from my group, from Geoff Hinton, Andrew Ng, and Yoshua Bengio, as from Jurgen Schmidhuber's group at IDSIA in Switzerland, and from NEC Labs in California. ConvNets are now widely used by Facebook, Google, Microsoft, IBM, Baidu, NEC and others for image and speech recognition. \r\n[GP: A student of Yann Lecun recently \r\n won Dogs vs Cats competition on Kaggle   using a version of ConvNet, achieving 98.9% accuracy.]\r\n   \n GP: 2. Deep learning is not an easy to use method. What tools, tutorials would you recommend to data scientists, who want to learn more and use it on their data? Your opinion of Pylearn2, Theano?  \n   \n Yann LeCun  : There are two main packages: \r\n \n   Torch7   , and \r\n    \n Theano    +  Pylearn2   . \r\n   \n  \u00a0  \r\nThey have slightly different philosophies and relative advantages and disadvantages. Torch7 is an extension of the LuaJIT language that adds multi-dimensional arrays and numerical library. It also includes an object-oriented package for deep learning, computer vision, and such. The main advantage of Torch7 is that LuaJIT is extremely fast, in addition to being very flexible (it's a compiled version of the popular Lua language). \r\n   \r\nTheano+Pylearn2 has the advantage of using Python (it's widely used, and has lots of libraries for many things), and the disadvantage of using Python (it's slow).\r\n   \n GP: 3. You and I have met a while ago at a scientific advisory meeting of KXEN, where \r\n \r\nVapnik  's  Statistical Learning Theory and SVM were a major topic.  What is the relationship between Deep Learning and \r\n \r\nSupport Vector Machines   /  Statistical Learning Theory?   \n   \n Yann LeCun  : \r\nVapnik and I were in nearby office at Bell Labs in the early 1990s, in Larry Jackel's Adaptive Systems Research Department. Convolutional nets, Support Vector Machines, Tangent Distance, and several other influential methods were invented within a few meters of each other, and within a few years of each other.  When AT&T spun off Lucent In 1995, I became the head of that department which became the Image Processing Research Department at AT&T Labs - Research. Machine Learning members included Yoshua Bengio, Leon Bottou, and Patrick Haffner, and Vladimir Vapnik. Visitors and interns included Bernhard Scholkopf, Jason Weston, Olivier Chapelle, and others.\r\n   \n  \r\nVapnik and I often had lively discussions about the relative merits of (deep) neural nets and kernel machines. Basically, I have always been interested in solving the problem of learning features or learning representations. I had only a moderate interest in kernel methods because they did nothing to address this problem. Naturally, SVMs are wonderful as a generic classification method with beautiful math behind them. But in the end, they are nothing more than simple two-layer systems. The first layer can be seen as a set of units (one per support vector) that measure a kind of similarity between the input vector and each support vector using the kernel function. The second layer linearly combines these similarities. \r\n   \r\nIt's a two-layer system in which the first layer is trained with the simplest of all unsupervised learning method: simply store the training samples as prototypes in the units. Basically, varying the smoothness of the kernel function allows us to interpolate between two simple methods: linear classification, and template matching. I got in trouble about 10 years ago by saying that kernel methods were a form of glorified template matching. Vapnik, on the other hand, argued that SVMs had a very clear way of doing capacity control. An SVM with a \"narrow\" kernel function can always learn the training set perfectly, but its generalization error is controlled by the width of the kernel and the sparsity of the dual coefficients. Vapnik really believes in his bounds. He worried that neural nets didn't have similarly good ways to do capacity control (although neural nets do have generalization bounds, since they have finite VC dimension).\r\n   \r\nMy counter argument was that the ability to do capacity control was somewhat secondary to the ability to compute highly complex function with a limited amount of computation. Performing image recognition with invariance to shifts, scale, rotation, lighting conditions, and background clutter was impossible (or extremely inefficient) for a kernel machine operating at the pixel level. But it was quite easy for deep architectures such as convolutional nets.\r\n   \n GP: 4. Congratulations on your recent appointment as the head of Facebook new AI Lab. What can you tell us about AI and Machine Learning advances we can expect from Facebook in the next couple of years?  \n   \n  \n Yann LeCun  : Thank you! it's a very exciting opportunity. Basically, Facebook's main objective is to enable communication between people. But people today are bombarded with information from friends, news organizations, websites, etc. Facebook helps people sift through this mass of information. But that requires to know what people are interested in, what motivates them, what entertains them, what makes them learn new things. This requires an understanding of people that only AI can provide. Progress in AI will allow us to understand content, such as text, images, video, speech, audio, music, among other things.\r\n   \r\nHere is \r\n \r\npart 2 of the interview.  \n   \n Bio:   \n \r\nYann LeCun   is Director of AI Research at Facebook, and the founding\r\ndirector of the Center for Data Science at New York University. He is\r\nSilver Professor of Computer Science, Neural Science, and Electrical\r\nEngineering and NYU, affiliated with the Courant Institute of\r\nMathematical Science, the Center for Neural Science, and the ECE\r\nDepartment.\r\n   \r\nHe received the Electrical Engineer Diploma from Ecole Sup\u00e9rieure\r\nd'Ing\u00e9nieurs en Electrotechnique et Electronique (ESIEE), Paris in\r\n1983, and a PhD in Computer Science from Universit\u00e9 Pierre et Marie\r\nCurie (Paris) in 1987. After a postdoc at the University of Toronto,\r\nhe joined AT&T Bell Laboratories in Holmdel, NJ in 1988. He became\r\nhead of the Image Processing Research Department at AT&T Labs-Research\r\nin 1996, and joined NYU as a professor in 2003, after a brief period\r\nas a Fellow of the NEC Research Institute in Princeton. He was named\r\nDirector of AI Research at Facebook in late 2013 and retains a\r\npart-time position on the NYU faculty.\r\n   \r\nHis current interests include machine learning, computer perception,\r\nmobile robotics, and computational neuroscience.  He has published\r\nover 180 technical papers and book chapters on these topics as well as\r\non neural networks, handwriting recognition, image processing and\r\ncompression, and on dedicated circuits and architectures for computer\r\nperception. The character recognition technology he developed at Bell\r\nLabs is used by several banks around the world to read checks and was\r\nreading between 10 and 20% of all the checks in the US in the early\r\n2000s.  His image compression technology, called DjVu, is used by\r\nhundreds of web sites and publishers and millions of users to access\r\nscanned documents on the Web. A pattern recognition method he\r\ndeveloped, called convolutional network, is the basis of products and\r\nservices deployed by companies such as AT&T, Google, Microsoft, NEC,\r\nIBM, Baidu, and Facebook for document recognition, human-computer\r\ninteraction, image tagging, speech recognition, and video analytics.\r\n   \r\nLeCun has been on the editorial board of IJCV, IEEE PAMI, and IEEE\r\nTrans. Neural Networks, was program chair of CVPR'06, and is chair of\r\nICLR 2013 and 2014. He is on the science advisory board of Institute\r\nfor Pure and Applied Mathematics, and has advised many large and small\r\ncompanies about machine learning technology, including several\r\nstartups he co-founded. He is the recipient of the 2014 IEEE Neural\r\nNetwork Pioneer Award.\r\n    ", "title_html": "<h1 id=\"title\">KDnuggets Exclusive: Interview with Yann LeCun, Deep Learning Expert, Director of Facebook AI Lab</h1> "}