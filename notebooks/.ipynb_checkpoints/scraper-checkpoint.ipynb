{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from scraper_config_reader.ipynb\n",
      "Importing Jupyter notebook from scraper_requests.ipynb\n",
      "Importing Jupyter notebook from scraper_data_reader.ipynb\n",
      "Importing Jupyter notebook from utils_os.ipynb\n",
      "Importing Jupyter notebook from model_article_url_discriminator.ipynb\n",
      "Importing Jupyter notebook from constants.ipynb\n",
      "Importing Jupyter notebook from utils_soup.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "\n",
    "from scraper_config_reader import ScraperConfigReader\n",
    "from scraper_requests import ScraperRequests\n",
    "from scraper_data_reader import ReaderScrapedData\n",
    "from model_article_url_discriminator import URLCleaner\n",
    "from utils_soup import UtilsSoup\n",
    "from utils_os import UtilsOS\n",
    "from constants import Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from alphabet_detector import AlphabetDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    \"\"\"\"\"\"\n",
    "    _extensions_banned = [\".jpg\", \".png\", \".zip\", \".xml\"]\n",
    "    \n",
    "    def _manage_article_directory(self, directory, start_from_zero):\n",
    "        exists_directory = UtilsOS.directory_exists(directory)\n",
    "        if start_from_zero and exists_directory:\n",
    "            UtilsOS.directory_remove(directory)\n",
    "        UtilsOS.directory_maybe_create(directory)\n",
    "    \n",
    "    def _check_link_extension(self, link):\n",
    "        for extension in self._extensions_banned:\n",
    "            if link[-len(extension):] == extension:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _normalize_url(self, url):\n",
    "        url = url.split(\"?\")[0]\n",
    "        url = url.split(\"#\")[0]\n",
    "        url = url.split(\"&\")[0]\n",
    "        url = url.split(\"@\")[0]\n",
    "        if url[-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "        return url\n",
    "            \n",
    "    def _clean_links(self, website, links, prefix_url):\n",
    "        links = [prefix_url + link if len(link) > 0 and link[0] == \"/\" else link for link in links] # fix \"/index.html\"\n",
    "        links = [prefix_url + \"/\" + link if \"//\" not in link else link for link in links] # fix \"index.html\"\n",
    "        links = [self._normalize_url(link) for link in links] # remove http parameters (after ?)\n",
    "        links = list(set(links)) # remove duplicates\n",
    "        return links\n",
    "    \n",
    "    def _ok_title(self, title, ad):\n",
    "        num_words = len(title.split(\" \"))\n",
    "        ok_num_words = num_words >= 2 and num_words <= 20\n",
    "        ok_alphabet = ad.only_alphabet_chars(title, \"LATIN\")\n",
    "        return ok_num_words and ok_alphabet\n",
    "\n",
    "    def _ok_content(self, content, ad):\n",
    "        num_words = len(content.split(\" \"))\n",
    "        ok_num_words = num_words >= 100\n",
    "        ok_alphabet = ad.only_alphabet_chars(content, \"LATIN\")\n",
    "        return ok_num_words and ok_alphabet\n",
    "    \n",
    "    def _get_random_url_by_domain(self, data):\n",
    "        # return one url for each domain\n",
    "        res = [] # res = []\n",
    "        domain_urls = [data[dom] for dom in data.keys()]\n",
    "        for urls in domain_urls:\n",
    "            res.append(random.choice(urls)[\"url\"])\n",
    "        return res\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def scrape_website_incremental(self, website, config, path_to_articles, path_to_url_not_article,\n",
    "                                   max_scraped=-1, max_tries=-1, start_from_zero=False):\n",
    "        \"\"\"Incremental scraping of a website, according to a site configuration\"\"\"\n",
    "        directory = path_to_articles + \"/\" + website\n",
    "\n",
    "        # eventually clean \"article\" directory\n",
    "        self._manage_article_directory(directory, start_from_zero)\n",
    "\n",
    "        # create request handler, url cleaner and alphabet detector\n",
    "        scraper_requests = ScraperRequests()\n",
    "        url_cleaner = URLCleaner()\n",
    "        ad = AlphabetDetector()\n",
    "\n",
    "        # parse already scraped data\n",
    "        scraped_data = ReaderScrapedData.read_data_of_website(website, path_to_articles)\n",
    "        titles = set(ReaderScrapedData.get_titles(scraped_data))\n",
    "        urls_by_domain = self._get_random_url_by_domain(scraped_data) # get one url for each domain of the selected website\n",
    "        urls = ReaderScrapedData.get_urls(scraped_data)\n",
    "        if not UtilsOS.file_exists(path_to_url_not_article):\n",
    "            UtilsOS.write_to_json([], path_to_url_not_article)\n",
    "        url_not_article = UtilsOS.read_json(path_to_url_not_article) # list of strings\n",
    "            \n",
    "\n",
    "        # decides from which url we start scraping\n",
    "        first_url = config[ScraperConfigReader.first_url_key][0]\n",
    "        if len(urls_by_domain) > 0:\n",
    "            queue = urls_by_domain\n",
    "        else:\n",
    "            queue = [first_url]\n",
    "\n",
    "        # extract prefix url\n",
    "        prefix_url = \"/\".join(first_url.split(\"/\")[:3])\n",
    "\n",
    "        # create the url black list\n",
    "        already_considered = set(urls)\n",
    "        already_considered.add(first_url)\n",
    "\n",
    "        counter = 0\n",
    "        counter_added = 0\n",
    "        counter_delta_incremental = len(urls)\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            # get url to visit\n",
    "            url = queue.pop(0)\n",
    "            \n",
    "            # if we already know that this link does not correspond to an article, we don't visit it\n",
    "            if url in url_not_article and not len(queue) == 0:\n",
    "                continue\n",
    "            \n",
    "            print(\"Visiting \" + url)\n",
    "            print(\"URLs in queue: {0}\".format(len(queue)))\n",
    "\n",
    "            # visit url\n",
    "            try:\n",
    "                # make request\n",
    "                response = scraper_requests.make_get(url)\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # get all outer links from url, clean them and add to queue\n",
    "                links = soup.find_all(\"a\")\n",
    "                print(len(links))\n",
    "                links = [tag[\"href\"] for tag in links if tag.has_attr(\"href\")]\n",
    "                links = self._clean_links(website, links, prefix_url)\n",
    "                links = url_cleaner.filter_urls(links, website)\n",
    "                print(len(links))\n",
    "                for link in links:\n",
    "                    if link not in already_considered:\n",
    "                        already_considered.add(link)\n",
    "                        queue.append(link)\n",
    "\n",
    "                # fill data\n",
    "                data = {\"url\": url, \"html\": str(soup)}\n",
    "                data[\"title_html\"], data[\"title\"] = UtilsSoup.get_with_selector(config[ScraperConfigReader.title_selector_key], soup)\n",
    "                data[\"content_html\"], data[\"content\"] = UtilsSoup.get_with_selector(config[ScraperConfigReader.content_selector_key], soup)\n",
    "                data[\"timestamp_scraper\"] = datetime.today().timestamp()\n",
    "                data[\"website\"] = website\n",
    "                \n",
    "                ok_title = self._ok_title(data[\"title\"], ad)\n",
    "                ok_content = self._ok_content(data[\"content\"], ad)\n",
    "\n",
    "                # eventually save article\n",
    "                if ok_title and ok_content: # we save only if we got the necessary info\n",
    "                    if data[\"title\"] not in titles:\n",
    "                        titles.add(data[\"title\"])\n",
    "                        counter_added += 1\n",
    "                        print(\"{0} - Extracted article: \".format(counter_delta_incremental + counter_added) + data[\"title\"])\n",
    "\n",
    "                        # eventually create domain directory\n",
    "                        sub_directory = directory + \"/\" + url.split(\"/\")[2]\n",
    "                        UtilsOS.directory_maybe_create(sub_directory)\n",
    "\n",
    "                        # write to file\n",
    "                        UtilsOS.write_to_json(data, sub_directory + \"/\" + str(counter_delta_incremental + counter_added) + '.json')\n",
    "\n",
    "                        # eventually end scraping\n",
    "                        if counter_added == max_scraped:\n",
    "                            break\n",
    "                    else:\n",
    "                        print(\"Article already extracted: {0}\".format(data[\"title\"]))\n",
    "                else:\n",
    "                    url_not_article.append(data[\"url\"])\n",
    "                    UtilsOS.write_to_json(url_not_article, path_to_url_not_article)\n",
    "                counter += 1\n",
    "                if counter == max_tries:\n",
    "                    break\n",
    "                \n",
    "                # sleep...\n",
    "                time.sleep(0.2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"------------\")\n",
    "                time.sleep(1) # time to escape by KeywordInterrupt\n",
    "                continue\n",
    "                \n",
    "            # shuffle queue\n",
    "            random.shuffle(queue)\n",
    "\n",
    "            print(\"------------\")\n",
    "            \n",
    "    def incremental_scraping_from_configs(self, configs, path_to_articles, path_to_url_not_article, max_scraped=10, max_tries=30):\n",
    "        items = list(configs.items())\n",
    "        random.shuffle(items)\n",
    "        while True:\n",
    "            for website, conf in items:\n",
    "                self.scrape_website_incremental(website, conf, path_to_articles, path_to_url_not_article, max_scraped=max_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = ScraperConfigReader.get_configs(Constants.path_to_scraper_config, need_javascript=False)\n",
    "    scraper = Scraper()\n",
    "    scraper.incremental_scraping_from_configs(configs, Constants.path_to_articles, Constants.path_to_url_not_article,\n",
    "                                              max_tries=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = ScraperConfigReader.get_configs(Constants.path_to_scraper_config, need_javascript=False)\n",
    "    scraper = Scraper()\n",
    "    website = \"techcrunch\"\n",
    "    scraper.scrape_website_incremental(website, configs[website], Constants.path_to_articles, Constants.path_to_url_not_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bill McKibben has been sounding the climate alarm for decades. Here’s his best advice.  '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.vox.com/2019/5/3/18307660/climate-change-green-new-deal-bill-mckibben-falter4\"\n",
    "\n",
    "configs = ScraperConfigReader.get_configs(Constants.path_to_scraper_config, need_javascript=False)\n",
    "website = \"vox\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "title = UtilsSoup.get_with_selector(configs[website][ScraperConfigReader.title_selector_key], soup)[1]\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n One of the first writers to sound the alarm on climate change was Bill McKibben.   \\n His 1989 book,  The End of Nature   , introduced a mainstream audience to the problem of rising greenhouse gas emissions, and propelled him to eventually form the international environmental group  350.org   in 2007.   \\n McKibben’s latest book,  Falter   ,   is a depressing vindication of his first one. Thirty years ago, he warned that human beings were altering the planet in such a way that we would imperil our own existence. Today, he says, “we are even deeper in the hole.”  \\n There is now no conceivable way to stop climate change; at best, it’s a battle to mitigate its impact — and we’re quickly running out of time. Even worse, McKibben writes, “because of the way power and wealth are currently distributed on our planet ... we’re uniquely ill-prepared to cope with the emerging challenges.”  \\n Despite the gloomy tone, McKibben’s book ends with an affirmation of the power of activism and nonviolent resistance, which, he insists, is our only way out of this mess.  \\n In February, I  interviewed David Wallace-Wells   about his book,  The Uninhabitable Earth   .   Wallace-Wells paints a similarly bleak picture of the problem, but McKibben’s book is written from the perspective of an activist; consequently, he frames climate change as a fight that can be won.  \\n I spoke to McKibben about what that fight looks like, and why he’s optimistic about the wave of resistance we’ve seen in recent months. We also discussed the stark challenges in front of us, and how ill-prepared to deal with them we really are. “One way or the other,” he told me, “we’re going to have to reexamine our relationship with the world around us.” How quickly we do this will determine just how dark our future is.   \\n A lightly edited transcript of our conversation follows.  \\n  \\n Sean Illing  \\n There’s an  interesting debate   in the climate world about the best way to mobilize people. Some think it’s counterproductive to be alarmist, mostly because it risks overstating the evidence or because it “feeds a paralyzing evidence of doom and hopelessness,” as climate researcher  Michael Mann   put it. Journalists like the aforementioned  Wallace-Wells   say it’s absolutely time to panic.   \\n Where do you come down on this?  \\n Bill McKibben  \\n We actually have some experience in what it takes to mobilize people, having built the first big planet-scale climate campaign called 350.org. And I think the correct answer to that is honesty. There’s no need to make things appear darker than they are. They’re plenty dark already; at the same time, there’s no need to insist that there aren’t any paths forward.   \\n We’ve been given a few cards to play. The politicians are continually failing us, but the engineers have done their jobs quite well. The fact that the price of a solar panel dropped 90 percent in the last decade is remarkable, and it offers us a playable hand if we can take full advantage of it.   \\n To get back to your question, I remember when we were starting 350.org and trying to pick the name. [The name 350 refers to the goal of keeping atmospheric CO2 concentrations at 350 parts per million]. People were saying, “Oh, it’s too depressing because we’re already past 350 parts per million.” But I remember thinking, “No, that’s not right.” If someone goes to the doctor and the doctor says your cholesterol is way too high, you might already have had a mild stroke. You don’t curse out the doctor for being depressing. You say, “Okay, what pill do I need to take? What do we do about this?”  \\n So I think we should simply be as honest as possible about our circumstances.   \\n Sean Illing  \\n You just said the engineers are doing their jobs and developing the sorts of green technologies we need to respond to this problem, so what’s preventing us from dramatically ramping them up?  \\n Bill McKibben  \\n Much of the book is devoted to this very question. The short answer is the fossil fuel industry. This is what’s keeping us from making any serious effort to do anything about this. This has always been the biggest roadblock, and behind it is an ideological conviction, shared by the top tier of our power structure for the last several decades, that laissez-faire capitalism is the answer to all our problems and that the market will always self-correct every imbalance they create.   \\n Sean Illing  \\n How close are we to crossing an ecological threshold?   \\n Bill McKibben  \\n We’ve crossed a lot of ecological thresholds. Stopping climate change is no longer on the menu of options. The question that remains is: have we crossed such epic thresholds that we won’t be able to have a planet that maintains the kind of civilizations we’re used to?   \\n We don’t know the answer to that question. There are scientists who think so. I’d say the consensus remains that we have a narrow window, that’s closing pretty fast, to take fundamentally transformative actions that might keep us below a temperature that we could figure out how to live with. But if we have any margin for error, we’re pushing it.   \\n I think people are starting to fully appreciate this fact. Thirty years ago, it was hard to describe climate change because it wasn’t something you could capture in a picture. Now, there are a thousand pictures a year. We just  watched a town called “Paradise”   literally turning to hell and inside of half an hour.   \\n The question that haunts me is not whether we will move to renewable energy or not — we will. The question is whether we can do it in time. Because on current trajectories, the world that we will run on sun and wind in 50 years will be a broken world. If we move more quickly, it will be less broken.  \\n I realize that’s not a great rallying cry, it’s not the “I have a dream” speech, but it’s our reality right now.   \\n   \\n \\n \\n \\n  \\n  \\n  \\n  \\n  \\n \\n Henry Holt  \\n  \\n  \\n  \\n Sean Illing  \\n You spend a lot of time talking about disruptions to the food supply. What can we expect on this front?  \\n Bill McKibben  \\n The first thing to worry about with food is that it will be a lot harder to grow as it gets hotter and drier. Second, even if we manage to grow food under increasingly harsh conditions, it will be harder to transport when you’re facing a series of crises — droughts that stop river traffic, enormous storms that wash out ports and roads for days, etc.   \\n It’s pretty clear that the food we’re growing and the food we will grow in the future will be less nutritious than the food we’re used to because elevated levels of CO2 in the atmosphere are causing  plants to produce food with less protein, less minerals  , which is not what we want. This is especially true in the developing world where people are already short on these things.  \\n Sean Illing  \\n I worry a lot of people have trouble connecting the dots between these food supply disruptions and the political chaos that results from them. For instance, you point out how a 2010 heat wave in Russia, which wrecked their grain harvest, led directly to the Arab Spring.  How did that happen and what are similar scenarios that could play out in the near future?  \\n Bill McKibben  \\n That happened because the Arab world is the biggest importer of grain from eastern Europe and when the price went way up for a loaf of bread, the predictable trouble ensued. Probably an even more dire example is what happened in Syria. In the early part of this century, we had the deepest drought ever recorded in the Fertile Crescent. That drove an immense number of Syrian farmers, maybe a million farm families, off their land and into the cities.   \\n And there’s been a lot of academic work to establish that that was one of the key things that triggered the civil war ( here   and  here  ). Syria was already a brutal and unstable place, unable to cope with this massive influx of people. And so many of those people were spun out into the rest of the world where they utterly discombobulated the politics of Western Europe inside of 18 months.  \\n So we can look at what happened in Syria and Europe and expect much more of that in the century ahead. And I think the low end is 200 million climate refugees and the high end approaches a billion. So take the kind of upheaval that the wave of refugees out of Syria created and multiply it by a couple of hundred and ask yourself how that’s going to impact war and peace, or development, or any of the other things we desperately care about?  \\n “The countries that have contributed the least to climate change are going to suffer the most, and that’s deeply unjust”    \\n Sean Illing  \\n What’s the timeline for these huge migration shifts? Do we have any clear way of knowing?  \\n Bill McKibben  \\n No one really knows. One thing I’ve learned in 30 years of watching this is that the scientists are always conservative. The physical damage that we’re seeing now, the loss of half the summer sea ice in the Arctic, that was supposed to happen in 2080. That’s what we thought 30 years ago.  \\n Things like immigration patterns and civil wars are hard to predict granularly, but pretty easy to predict in general terms. We know what happens when people have to move en masse. It’s always trouble. You can see it, obviously, on our own southern border.   \\n There was a  great piece in the Times   a couple of weeks ago explaining how climate of drought in Honduras and Guatemala was forcing many of the people who end up on our southern borders to get there.   \\n As with many other things, climate change is such a huge phenomenon that it forces us to revisit very basic questions. In this case, there are serious questions of international justice that we haven’t dealt with before: The countries that have contributed the least to climate change are going to suffer the most, and that’s deeply unjust.   \\n Sean Illing  \\n All of this is connected to the fact that we’re running out of places to live, that the amount of habitable territory on earth is rapidly shrinking.   \\n Bill McKibben  \\n Absolutely. Look, there have been an awful lot of stories written about people who will someday have to abandon their Miami condominium towers because the waters will be rising and that’s bad.   \\n What’s worse is the people who will have to leave very modest and poor homes in Florida. And what’s probably worse than that is people whose entire island nations are currently sinking below the waves.   \\n Sean Illing  \\n What’s your response to people who believe we’ll innovate our way out of climate problems and find ways to conquer other problems that appear?  \\n Bill McKibben  \\n It reminds me of that ad I used to see for stockbrokers: “Past performance is no guarantee of future results.” That’s the point. You have to look around the world and see what our actual situation is, and our actual situation is that we’ve pushed things too far. We’ve leveraged ourselves in all kinds of ways and the bill is coming due. This is what it means when half the Arctic is melting. This is what it means when the ocean is already 30 percent more acidic. These are not things that some technology is likely to resolve.   \\n The really excruciating thing is that we’ve invented a lot of the technologies that we need. I mean, a solar panel is a freaking miracle. You point a sheet of glass at the sun and out the back comes light and information and all the other things of modernity. We have this piece of magical hardware, and we’re not putting it to use at the scale we should be.   \\n It’s like that old joke about God and the flood victim up on his roof. God keeps trying to send him a guy in a boat, and he keeps saying “No, I’m staying here. God will provide for me.” And when he finally drowns and goes to heaven, he says, “God, why didn’t you save me?” And God says, “I sent you a boat. What did you want?” That’s where we are.  \\n Sean Illing  \\n I sometimes think our collision with the physical world is a necessary correction of sorts. By that, I mean that we’ve separated ourselves so much from our environment, and exploited it for so long. Climate change, in an obviously tragic way, forces us to reengage with the planet we’ve forgotten.   \\n Bill McKibben  \\n I take the point, but there’s something hideous about the fact that the people who will pay the highest price are the people who didn’t actually disengage from the planet. It’s also horrendous that this foolish thing we’re doing will also take out an astonishing percentage of the rest of the DNA on the planet too. So there’s some truth in what you’re saying, but the brute fact that lots of people and animals are going to be taught this lesson despite having no role in creating the problem is just awful.   \\n But you’re right: One way or the other, we’re going to have to re-examine our relationship with the world around us. The idea that we can take the world for granted will not survive this century. And either we’re going to do what we need to do or the world will do what it needs to do to us.   \\n Sean Illing  \\n I see the gap between our actual interests — protecting the environment — and the incentives guiding our civilization — neoliberal capitalism — growing. Am I too pessimistic?   \\n Bill McKibben  \\n The only way to square that circle is for human beings to do what we’ve done a few times in the past, which is to build movements big enough to demand that those in power make change. That’s what I’ve spent the last 10 years trying to do. And we’ve had success doing this.   \\n But I think we’re at the beginning of something quite remarkable right now, and much of it has nothing to do with the work that my organization has done. We’ve seen incredible leadership from  indigenous communities  , we’ve seen the ascent of this  Green New Deal  , we’ve seen kids around the world  protesting climate inaction  , we’ve seen the  Extinction Rebellion   protests in London — all sorts of antibodies are kicking in.  \\n I’m hopeful that we’re going to see much more of this in the near future.   \\n Sean Illing  \\n I appreciate that your book ends with a call to nonviolent organization; as far as I can tell, this is the best way to undermine the concentrations of wealth and power that currently block the way to meaningful solutions.   \\n Can you briefly lay out your vision here?  \\n “Now, climate change is clearly harder because no one made $1 trillion a year being a bigot”    \\n Bill McKibben  \\n The antibodies kick in when the body has a fever, but you don’t always get enough antibodies in time — sometimes the patient dies. That’s a realistic possibility here. But it’s also a realistic possibility to imagine people continuing to rise up in sufficient numbers and with sufficient savvy that we change the zeitgeists pretty quickly.   \\n That’s what activists play for at this level: a change in the zeitgeists. A change in what people perceive as normal, natural, or obvious. And when it happens, the consequences can be immediate and breathtaking. You’re a young man, but you’re probably still old enough to remember that there was a time not long ago when gay marriage seemed like a preposterous idea that would never ever happen.  \\n It’s only five or six years ago that people like Barack Obama were still against it. But great organizing caused a shift in the zeitgeist. Now, climate change is clearly harder because no one made $1 trillion a year being a bigot. So the fight is harder, but the principle’s the same.   \\n It’s a very hard task. I’ve watched the climate movement build from nothing to where it is now and that’s been very hard work and now we need another order of magnitude. I don’t know if we can do it, but I know that we will try and I know that human beings will put up a good fight, and if we go down, we’ll go down with our dignity intact. There’s something to be said for that.   \\n The thing that always worried me the most was that we’d just go over this cliff without hardly even noticing it or putting up a fight. But now I’m confident that won’t happen.   \\n  '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UtilsSoup.get_with_selector(configs[website][ScraperConfigReader.content_selector_key], soup)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
