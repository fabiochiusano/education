{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "\n",
    "# signal TOR for a new connection \n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_url(url):\n",
    "    url = url.split(\"?\")[0]\n",
    "    url = url.split(\"#\")[0]\n",
    "    url = url.split(\"&\")[0]\n",
    "    url = url.split(\"@\")[0]\n",
    "    if url[-1] == \"/\":\n",
    "        url = url[:-1]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = [\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36\",\n",
    "              \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "              \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.112 Safari/537.36\",\n",
    "              \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36\",\n",
    "              \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36\"]\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def get_random_from():\n",
    "    return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) + \"@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_counter = 0\n",
    "def make_get(url, print_ip=False):\n",
    "    global ip_counter\n",
    "    if ip_counter > 10:\n",
    "        renew_connection()\n",
    "        ip_counter = 0\n",
    "    session = get_tor_session()\n",
    "    if print_ip:\n",
    "        print(session.get(\"http://httpbin.org/ip\").text)\n",
    "    ip_counter += 1\n",
    "    headers = {\n",
    "        'User-Agent': get_random_user_agent(),\n",
    "        'From': get_random_from()\n",
    "    }\n",
    "    return session.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "\n",
    "first_url_key = \"first_url\"\n",
    "content_selector_key = \"content_selector\"\n",
    "title_selector_key = \"title_selector\"\n",
    "\n",
    "configs[\"medium\"] = {\n",
    "    first_url_key: [\"https://medium.com\"],\n",
    "    content_selector_key: [\".section-content\", \".elevate\"],\n",
    "    title_selector_key: [\"h1.graf--title\", \"h1.elevate-h1\"]\n",
    "}\n",
    "\n",
    "configs[\"tutorialspoint\"] = {\n",
    "    first_url_key: [\"https://www.tutorialspoint.com\", \"https://www.tutorialspoint.com/index.htm\"],\n",
    "    content_selector_key: [\".content > div > p\", \".tutorial-content > div > p\", \".content > div > div > p\"],\n",
    "    title_selector_key: [\".content > div > h1:first-of-type\", \".tutorial-content > div > h1:first-of-type\", \".content > div > div > h1:first-of-type\"]\n",
    "}\n",
    "\n",
    "configs[\"kdnuggets\"] = {\n",
    "    first_url_key: [\"https://www.kdnuggets.com\"],\n",
    "    content_selector_key: [\"#post- \"],\n",
    "    title_selector_key: [\"#title\"]\n",
    "}\n",
    "\n",
    "configs[\"datasciencecentral\"] = { # NOT WORKING\n",
    "    first_url_key: [\"https://www.datasciencecentral.com\"],\n",
    "    content_selector_key: [\"article .entry-content\"],\n",
    "    title_selector_key: [\"article .entry-title\"]\n",
    "}\n",
    "\n",
    "configs[\"smartdatacollective\"] = {\n",
    "    first_url_key: [\"https://www.smartdatacollective.com\"],\n",
    "    content_selector_key: [\".single-content\"],\n",
    "    title_selector_key: [\".single-title\"]\n",
    "}\n",
    "\n",
    "configs[\"machinelearningmastery\"] = { # NOT IMPLEMENTED\n",
    "    first_url_key: [\"https://www.kdnuggets.com\"],\n",
    "    content_selector_key: [],\n",
    "    title_selector_key: []\n",
    "}\n",
    "\n",
    "configs[\"wikihow\"] = {\n",
    "    first_url_key: [\"https://www.wikihow.com\"],\n",
    "    content_selector_key: [\"#bodycontents .steps .step\"],\n",
    "    title_selector_key: [\"#bodycontents > #intro > h1:first-of-type\"]\n",
    "}\n",
    "\n",
    "configs[\"splinters\"] = {\n",
    "    first_url_key: [\"https://schwitzsplinters.blogspot.com\"],\n",
    "    content_selector_key: [\".post-body\"],\n",
    "    title_selector_key: [\".post-title\"]\n",
    "}\n",
    "\n",
    "configs[\"thehistoryblog\"] = {\n",
    "    first_url_key: [\"http://www.thehistoryblog.com\"],\n",
    "    content_selector_key: [\".post > .entry\"],\n",
    "    title_selector_key: [\".post > h3:first-of-type\", \".post > h2:first-of-type\"]\n",
    "}\n",
    "\n",
    "configs[\"chemistry-blog\"] = {\n",
    "    first_url_key: [\"http://www.chemistry-blog.com\"],\n",
    "    content_selector_key: [\".post > .entry\"],\n",
    "    title_selector_key: [\".title\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first_urls(website, links):\n",
    "    for url in config[website][first_url_key]:\n",
    "        if url in links:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "from bs4.element import Tag\n",
    "\n",
    "def from_tag_to_inner_text(tag):\n",
    "    res = \"\"\n",
    "    if tag.name == \"script\":\n",
    "        return res\n",
    "    elif type(tag) == Tag:\n",
    "        for tc in tag.children:\n",
    "            res += from_tag_to_inner_text(tc) + \" \"\n",
    "    else:\n",
    "        res += str(tag)\n",
    "    return res\n",
    "\n",
    "def get_with_selector(selectors, soup):\n",
    "    res = []\n",
    "    for selector in selectors:\n",
    "        el_list = soup.select(selector) # may be more than one\n",
    "        res_html = \"\"\n",
    "        res_string = \"\"\n",
    "        for el in el_list:\n",
    "            res_html += str(el) + \" \"\n",
    "            #res_string += \" \".join([tag.text if type(tag) == Tag else str(tag) for tag in el.children if tag.name != \"script\"])\n",
    "            res_string += from_tag_to_inner_text(el) + \" \"\n",
    "        if len(el_list) > 0:\n",
    "            break\n",
    "    return res_html, res_string\n",
    "\n",
    "extensions_banned = [\".jpg\", \".png\", \".zip\"]\n",
    "def check_link_extension(link):\n",
    "    for extension in extensions_banned:\n",
    "        if link[-len(extension):] == extension:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"wikihow\"\n",
    "\n",
    "directory = \"../articles/\" + website\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "first_url = configs[website][first_url_key][0]\n",
    "prefix_url = \"/\".join(first_url.split(\"/\")[:3])\n",
    "queue = [first_url]\n",
    "queue_length = [1]\n",
    "already_considered = set()\n",
    "already_considered.add(first_url)\n",
    "titles = set()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while len(queue) > 0:\n",
    "    # get url to visit\n",
    "    url = queue.pop(0)\n",
    "    print(\"Visiting \" + url)\n",
    "    print(\"URLs in queue: {0}\".format(len(queue)))\n",
    "    \n",
    "    # visit url\n",
    "    try:\n",
    "        response = make_get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        data = {\"url\": url, \"html\": str(soup)}\n",
    "        \n",
    "        # get all outer links from url\n",
    "        links = soup.find_all(\"a\")\n",
    "        links = [tag[\"href\"] for tag in links if tag.has_attr(\"href\")]\n",
    "        \n",
    "        # from relative to absolute links\n",
    "        links = [prefix_url + link if len(link) > 0 and link[0] == \"/\" else link for link in links] # fix \"/index.html\"\n",
    "        links = [prefix_url + \"/\" + link if \"//\" not in link else link for link in links] # fix \"index.html\"\n",
    "        links = [link for link in links if website in link] # must contain the name of the website\n",
    "        \n",
    "        # if it does not contain a link to the homepage, then drop it\n",
    "        #if not check_first_urls(website, links):\n",
    "        #    print(\"Not found link to homepage: \" + str(links))\n",
    "        #    print(\"------------\")\n",
    "        #    continue\n",
    "        \n",
    "        links = [link for link in links if check_link_extension(link)] # remove links that end with banned extension\n",
    "                \n",
    "        links = [normalize_url(link) for link in links] # remove http parameters (after ?)\n",
    "        \n",
    "        links = list(set(links)) # remove duplicates\n",
    "                \n",
    "        # add links to queue if not already considered\n",
    "        for link in links:\n",
    "            if link not in already_considered:\n",
    "                already_considered.add(link)\n",
    "                queue.append(link)\n",
    "                                \n",
    "        # get titles\n",
    "        data[\"title_html\"], data[\"title\"] = get_with_selector(configs[website][title_selector_key], soup)\n",
    "\n",
    "        # get texts\n",
    "        data[\"content_html\"], data[\"content\"] = get_with_selector(configs[website][content_selector_key], soup)\n",
    "\n",
    "        # save article\n",
    "        if data[\"title\"] != \"\" and data[\"content\"] != \"\" and data[\"title\"] not in titles: # we save only if we got the necessary info\n",
    "            titles.add(data[\"title\"])\n",
    "            print(\"Extracted tutorial: \" + data[\"title\"])\n",
    "            counter += 1\n",
    "            sub_directory = directory + \"/\" + url.split(\"/\")[2]\n",
    "            if not os.path.exists(sub_directory):\n",
    "                os.makedirs(sub_directory)\n",
    "            with open(sub_directory + \"/\" + str(counter) + '.json', 'w') as outfile:\n",
    "                json.dump(data, outfile)\n",
    "\n",
    "        # update statistics...\n",
    "        queue_length.append(len(queue))\n",
    "        \n",
    "        # sleep...\n",
    "        time.sleep(0.2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"WE\")\n",
    "        print(\"------------\")\n",
    "        time.sleep(1) # time to escape by KeywordInterrupt\n",
    "        continue\n",
    "        \n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"tutorialspoint\"\n",
    "\n",
    "url = \"https://www.tutorialspoint.com/react_native/index.htm\"\n",
    "response = make_get(url, print_ip=True)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = soup.select(configs[website][content_selector_key][0])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, title = get_with_selector(configs[website][title_selector_key], soup, \"title\")\n",
    "print(title)\n",
    "\n",
    "# get texts\n",
    "c, content = get_with_selector(configs[website][content_selector_key], soup, \"content\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.select(\"#bodycontents .steps .step\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = soup.select(\".content > div > p\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = list(a.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ch[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tag[\"href\"] for tag in links if tag[\"href\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.select(\".content > div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get titles\n",
    "title = soup.select(\".content > div > h1:first-of-type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEE ALL TITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = \"wikihow/www.wikihow.com\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    json_file = mypath + \"/\" + file\n",
    "    with open(json_file, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "        print(data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
