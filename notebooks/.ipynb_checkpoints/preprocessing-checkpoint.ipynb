{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from scraper_data_reader.ipynb\n",
      "Importing Jupyter notebook from utils_os.ipynb\n",
      "Importing Jupyter notebook from scraper_config_reader.ipynb\n",
      "Importing Jupyter notebook from constants.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0503 23:04:24.951239 4676937152 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scraper_data_reader import ReaderScrapedData\n",
    "from utils_os import UtilsOS\n",
    "from constants import Constants\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True #OPTIONAL - to disable outputs from Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def __init__(self, on_field, produce_field, verbose=False):\n",
    "        self._on_field = on_field\n",
    "        self._produce_field = produce_field\n",
    "        self._verbose = verbose\n",
    "        \n",
    "    def apply(self, dataset):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocesserCleanText(Preprocesser):\n",
    "    def __init__(self, on_field, produce_field, verbose=False):\n",
    "        super().__init__(on_field, produce_field, verbose)\n",
    "        self.name = \"PreprocesserCleanText\"\n",
    "    \n",
    "    def _clean_html(self, raw_html):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        cleantext = re.sub(\"(<!--.*?-->)\", \"\", cleantext, flags=re.DOTALL)\n",
    "        return cleantext\n",
    "\n",
    "    def _remove_newlines(self, content):\n",
    "        return content.replace(\"\\n\", \" \")\n",
    "\n",
    "    def _remove_extra_white_spaces(self, content):\n",
    "        content = re.sub(' +', ' ', content)\n",
    "        content = content.strip()\n",
    "        return content\n",
    "\n",
    "    def _remove_urls(self, content):\n",
    "        content = re.sub(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', content, flags=re.MULTILINE)\n",
    "        return content\n",
    "\n",
    "    def _remove_code(self, content):\n",
    "        content = re.sub(r'(\\w+(\\.\\w+)*\\([^\\)]*\\))', '', content, flags=re.MULTILINE) # matches a.b.c(d)\n",
    "        return content\n",
    "    \n",
    "    def _remove_backslashes(self, content):\n",
    "        return content.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "\n",
    "    def _remove_alt_html(self, content):\n",
    "        content = content.split(\"&lt\")[0]\n",
    "        return content\n",
    "\n",
    "    def apply(self, dataset):\n",
    "        for sample in tqdm(dataset):\n",
    "            content = sample[self._on_field]\n",
    "            content = self._clean_html(content)\n",
    "            content = self._remove_newlines(content)\n",
    "            content = self._remove_extra_white_spaces(content)\n",
    "            content = self._remove_urls(content)\n",
    "            content = self._remove_code(content)\n",
    "            content = self._remove_alt_html(content)\n",
    "            sample[self._produce_field] = content\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocesserTokenizer(Preprocesser):\n",
    "    _token_blacklist = [\"was\", \"wasn\", \"did\", \"didn\", \"you\", \"your\", \"isn\", \"wouldn\", \"doesn\", \"don\"]\n",
    "    \n",
    "    def __init__(self, path_to_stem_dictionary, on_field, produce_field, verbose=False):\n",
    "        super().__init__(on_field, produce_field, verbose)\n",
    "        self.name = \"PreprocesserTokenizer\"\n",
    "        self._path_to_stem_dictionary = path_to_stem_dictionary\n",
    "        \n",
    "        # Initialize stemmer and punctuation\n",
    "        self._stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "        self._punctuation = set(string.punctuation)\n",
    "    \n",
    "    def _add_to_stem_dictionary(self, stemmed_word, word, stem_dictionary):\n",
    "        \"\"\"Adds a stemmed_word -> word instance to the stem_dictionary\"\"\"\n",
    "        if stemmed_word not in stem_dictionary:\n",
    "            stem_dictionary[stemmed_word] = {word: 1}\n",
    "        else:\n",
    "            d = stem_dictionary[stemmed_word]\n",
    "            if word not in d:\n",
    "                d[word] = 1\n",
    "            else:\n",
    "                d[word] += 1\n",
    "            \n",
    "    def _tokenize(self, sample, stem_dictionary):\n",
    "        # splits \"I am Fabio\" into [\"I\", \"am\", \"Fabio\"]. It's a little smarter than a .split(\" \")\n",
    "        tokenList = nltk.word_tokenize(sample)\n",
    "        \n",
    "        # Convert all text to lower case\n",
    "        textList = [word.lower() for word in tokenList if not word.isupper()]\n",
    "\n",
    "        # Remove punctuation\n",
    "        textList = [word for word in textList if word not in self._punctuation]\n",
    "        textList = [\"\".join(c for c in word if c not in self._punctuation) for word in textList ]\n",
    "\n",
    "        # Convert digits into NUM\n",
    "        textList = [re.sub(\"\\d+\", \"NUM\", word) for word in textList]  \n",
    "\n",
    "        # Stem words \n",
    "        stemmedTextList = [self._stemmer.stem(word) for word in textList]\n",
    "        for sw, w in zip(stemmedTextList, textList):\n",
    "            self._add_to_stem_dictionary(sw, w, stem_dictionary)\n",
    "        textList = stemmedTextList\n",
    "\n",
    "        # Remove blanks\n",
    "        textList = [word for word in textList if word != ' ']\n",
    "        textList = [word for word in textList if word != '']\n",
    "\n",
    "        # Remove short words\n",
    "        textList = [word for word in textList if len(word) > 2]\n",
    "\n",
    "        # token blacklist\n",
    "        textList = [word for word in textList if word not in PreprocesserTokenizer._token_blacklist]\n",
    "\n",
    "        return textList\n",
    "    \n",
    "    def apply(self, dataset):\n",
    "        stem_dictionary = {}\n",
    "        \n",
    "        for sample in tqdm(dataset):\n",
    "            sample[self._produce_field] = self._tokenize(sample[self._on_field], stem_dictionary)\n",
    "            \n",
    "        # Save stem dictionary\n",
    "        UtilsOS.write_to_json(stem_dictionary, self._path_to_stem_dictionary)\n",
    "        if self._verbose:\n",
    "            print(\"Saved stem dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocesserReadTime(Preprocesser):\n",
    "    def __init__(self, wps, on_field, produce_field, verbose=False):\n",
    "        super().__init__(on_field, produce_field, verbose)\n",
    "        self.name = \"PreprocesserReadTime\"\n",
    "        self._wps = wps\n",
    "    \n",
    "    def _get_read_time(self, text):\n",
    "        num_of_words = len(text.split(\" \"))\n",
    "        read_time = num_of_words / self._wps\n",
    "        return read_time\n",
    "    \n",
    "    def apply(self, dataset):\n",
    "        for sample in dataset:\n",
    "            sample[self._produce_field] = self._get_read_time(sample[self._on_field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocesserTFIDF(Preprocesser):\n",
    "    def __init__(self, path_to_wiki_tfidf, on_field, produce_field, verbose=False):\n",
    "        super().__init__(on_field, produce_field, verbose)\n",
    "        self.name = \"PreprocesserTFIDF\"\n",
    "        \n",
    "        # Read wikipedia idf\n",
    "        self._idf = pd.read_csv(path_to_wiki_tfidf)\n",
    "        self._idf = self._idf.set_index('term')\n",
    "        if self._verbose:\n",
    "            print(\"Number of words considered in wikipedia: {0}\".format(self._idf.shape[0]))\n",
    "        \n",
    "    def apply(self, dataset):\n",
    "        \"\"\"From tokenized text to TF-IDF vector (as Python dictionary)\"\"\"\n",
    "        for sample in tqdm(dataset):\n",
    "            tokens = sample[self._on_field]\n",
    "            \n",
    "            # Create FreqDF with word frequencies and convert it to a data frame\n",
    "            freq = FreqDist(tokens)\n",
    "            freqDF = pd.DataFrame.from_dict(freq, orient='index')\n",
    "            freqDF.columns = ['freq']\n",
    "\n",
    "            # Merge freqDF with idf data frame\n",
    "            freqit = freqDF.join(self._idf[['idf', 'logidf']])\n",
    "\n",
    "            # Replace null values with max\n",
    "            maxidf = max(freqit['idf'].dropna())\n",
    "            maxlogidf = max(freqit['logidf'].dropna())\n",
    "            freqit.loc[pd.isnull(freqit['idf']), 'idf'] = maxidf\n",
    "            freqit.loc[pd.isnull(freqit['logidf']), 'logidf'] = maxlogidf\n",
    "\n",
    "            # Create tfidf columns\n",
    "            freqit['tfidf'] = freqit['freq'] * freqit['idf']\n",
    "            freqit['logtfidf'] = freqit['freq'] * freqit['logidf']\n",
    "\n",
    "            sample[self._produce_field] = freqit.to_dict() # To dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(n_elements_in_batch, l):\n",
    "    return [l[i:i + n_elements_in_batch] for i in range(0, len(l), n_elements_in_batch)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocesserBERT(Preprocesser):\n",
    "    def _load_bert(self):\n",
    "        url = \"https://tfhub.dev/google/elmo/2\"\n",
    "        return hub.Module(url)\n",
    "    \n",
    "    def _from_texts_to_vectors(self, texts):\n",
    "        embeddings = self._bert(\n",
    "            texts,\n",
    "            signature=\"default\",\n",
    "            as_dict=True)[\"default\"]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.tables_initializer())\n",
    "            x = sess.run(embeddings)\n",
    "\n",
    "        return x # array with length len(texts), where each element is an array with lenght 1024\n",
    "    \n",
    "    def __init__(self, on_field, produce_field, verbose=False):\n",
    "        super().__init__(on_field, produce_field, verbose)\n",
    "        self.name = \"PreprocesserBERT\"\n",
    "        self._bert = self._load_bert()\n",
    "        \n",
    "    def apply(self, dataset):\n",
    "        for samples in tqdm(grouper(50, dataset)):\n",
    "            contents = [\" \".join(sample[self._on_field]) for sample in samples]\n",
    "            vectors = self._from_texts_to_vectors(contents)\n",
    "            for sample, vector in zip(samples, vectors):\n",
    "                sample[self._produce_field] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocesserMain:    \n",
    "    def __init__(self, preprocessers, path_to_preprocessed, verbose=False):\n",
    "        self._preprocessers = preprocessers\n",
    "        self._path_to_preprocessed = path_to_preprocessed\n",
    "        self._verbose = verbose\n",
    "        \n",
    "    def run_preprocessing(self, dataset):                \n",
    "        # Create target directory\n",
    "        UtilsOS.directory_maybe_create(self._path_to_preprocessed)\n",
    "        \n",
    "        # Preprocessing\n",
    "        for preprocesser in preprocessers:\n",
    "            if self._verbose:\n",
    "                print(\"Starting preprocesser {0}\".format(preprocesser.name))\n",
    "            preprocesser.apply(dataset)\n",
    "            if self._verbose:\n",
    "                print(\"Finished preprocesser {0}\".format(preprocesser.name))\n",
    "\n",
    "        # Save data\n",
    "        for i, sample in tqdm(enumerate(dataset)):\n",
    "            UtilsOS.write_to_json(sample, self._path_to_preprocessed + \"/{0}.json\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 64/4443 [00:00<00:06, 637.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4443 articles\n",
      "Starting preprocesser PreprocesserCleanText\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4443/4443 [00:05<00:00, 759.27it/s] \n",
      "100%|██████████| 4443/4443 [00:00<00:00, 34236.73it/s]\n",
      "  0%|          | 0/4443 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocesser PreprocesserCleanText\n",
      "Starting preprocesser PreprocesserCleanText\n",
      "Finished preprocesser PreprocesserCleanText\n",
      "Starting preprocesser PreprocesserTokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4443/4443 [02:00<00:00, 36.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocesser PreprocesserTokenizer\n",
      "Starting preprocesser PreprocesserReadTime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4443 [00:00<12:38,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocesser PreprocesserReadTime\n",
      "Starting preprocesser PreprocesserTFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4443/4443 [09:38<00:00,  7.33it/s]\n",
      "16it [00:00, 141.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocesser PreprocesserTFIDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4443it [00:31, 140.70it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read scraped data\n",
    "    data = ReaderScrapedData.read_data(Constants.path_to_scraper_config, Constants.path_to_articles)\n",
    "    dataset_nested = [data[website][domain] for website in data.keys() for domain in data[website].keys()]\n",
    "    dataset = [el for subl in dataset_nested for el in subl]\n",
    "    print(\"Read {0} articles\".format(len(dataset)))\n",
    "    \n",
    "    wps = 200 / 60 # = 3.33wps\n",
    "    \n",
    "    # Initialize preprocessers\n",
    "    preprocessers = []\n",
    "    preprocessers.append(PreprocesserCleanText(on_field=\"content\", produce_field=\"content\"))\n",
    "    preprocessers.append(PreprocesserCleanText(on_field=\"title\", produce_field=\"title\"))\n",
    "    preprocessers.append(PreprocesserTokenizer(Constants.path_to_stem_dictionary, on_field=\"content\", produce_field=\"content_tokenized\"))\n",
    "    preprocessers.append(PreprocesserReadTime(wps=wps, on_field=\"content\", produce_field=\"read_time\"))\n",
    "    preprocessers.append(PreprocesserTFIDF(Constants.path_to_wiki_tfidf, on_field=\"content_tokenized\", produce_field=\"tfidf\"))\n",
    "    #preprocessers.append(PreprocesserBERT(on_field=\"content_tokenized\", produce_field=\"bert_vector\"))\n",
    "    \n",
    "    # Run preprocessing\n",
    "    preprocesser = PreprocesserMain(preprocessers, Constants.path_to_preprocessed, verbose=True)\n",
    "    preprocesser.run_preprocessing(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
