{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hidden(l):\n",
    "    return [el for el in l if el[0] != \".\"]\n",
    "\n",
    "def get_relative_path_to_dirs(start_path):\n",
    "    subdirs = [x[1] for x in os.walk(start_path)][0]\n",
    "    subdirs = remove_hidden(subdirs)\n",
    "    subdirs = [start_path + \"/\" + subdir for subdir in subdirs]\n",
    "    return subdirs\n",
    "\n",
    "def get_relative_path_to_files(start_path):\n",
    "    files = [f for f in listdir(start_path) if isfile(join(start_path, f))]\n",
    "    files = remove_hidden(files)\n",
    "    files = [start_path + \"/\" + file for file in files]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../articles/medium',\n",
       " '../articles/splinters',\n",
       " '../articles/thehistoryblog',\n",
       " '../articles/tutorialspoint',\n",
       " '../articles/chemistry-blog',\n",
       " '../articles/wikihow',\n",
       " '../articles/kdnuggets',\n",
       " '../articles/smartdatacollective']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_articles_directory = \"../articles\"\n",
    "\n",
    "subdirs = get_relative_path_to_dirs(home_articles_directory)\n",
    "subdirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for subdir in subdirs:\n",
    "    subsubdirs = get_relative_path_to_dirs(subdir)\n",
    "    for subsubdir in subsubdirs:\n",
    "        onlyfiles = get_relative_path_to_files(subsubdir)\n",
    "        read_json_list = []\n",
    "        for file in onlyfiles:\n",
    "            with open(file, 'r') as infile:\n",
    "                d = json.load(infile)\n",
    "                dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset[-121][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alphabet_detector import AlphabetDetector\n",
    "ad = AlphabetDetector()\n",
    "\n",
    "def ok_title(title):\n",
    "    num_words = len(title.split(\" \"))\n",
    "    ok_num_words = num_words >= 2 and num_words <= 20\n",
    "    ok_alphabet = ad.only_alphabet_chars(title, \"LATIN\")\n",
    "    return ok_num_words and ok_alphabet\n",
    "\n",
    "def ok_content(content):\n",
    "    num_words = len(content.split(\" \"))\n",
    "    ok_num_words = num_words >= 100\n",
    "    ok_alphabet = ad.only_alphabet_chars(content, \"LATIN\")\n",
    "    return ok_num_words and ok_alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prev length: 1386\n",
      "New length: 1089\n",
      "Dropped total: 297\n",
      "\tDropped title: 6\n",
      "\tDropped content: 291\n"
     ]
    }
   ],
   "source": [
    "dataset_copy = []\n",
    "not_ok_title = 0\n",
    "not_ok_content = 0\n",
    "for i,sample in enumerate(dataset):\n",
    "    if ok_title(sample[\"title\"]) and ok_content(sample[\"content\"]):\n",
    "        dataset_copy.append(sample)\n",
    "    if not ok_title(sample[\"title\"]):\n",
    "        not_ok_title += 1\n",
    "    if not ok_content(sample[\"content\"]):\n",
    "        not_ok_content += 1\n",
    "        \n",
    "print(\"Prev length: {0}\".format(len(dataset)))\n",
    "print(\"New length: {0}\".format(len(dataset_copy)))\n",
    "print(\"Dropped total: {0}\".format(len(dataset) - len(dataset_copy)))\n",
    "print(\"\\tDropped title: {0}\".format(not_ok_title))\n",
    "print(\"\\tDropped content: {0}\".format(not_ok_content))\n",
    "\n",
    "dataset = dataset_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    cleantext = re.sub(\"(<!--.*?-->)\", \"\", cleantext, flags=re.DOTALL)\n",
    "    return cleantext\n",
    "\n",
    "def remove_newlines(content):\n",
    "    return content.replace(\"\\n\", \" \")\n",
    "\n",
    "def remove_extra_white_spaces(content):\n",
    "    content = re.sub(' +', ' ', content)\n",
    "    content = content.strip()\n",
    "    return content\n",
    "\n",
    "def remove_urls(content):\n",
    "    content = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE)\n",
    "    return content\n",
    "\n",
    "def remove_code(content):\n",
    "    content = re.sub(r'(\\w+(\\.\\w+)*\\([^\\)]*\\))', '', content, flags=re.MULTILINE) # matches a.b.c(d)\n",
    "    return content\n",
    "\n",
    "def remove_alt_html(content):\n",
    "    content = content.split(\"&lt\")[0]\n",
    "    return content\n",
    "\n",
    "def clean_content(content):\n",
    "    content = clean_html(content)\n",
    "    content = remove_newlines(content)\n",
    "    content = remove_extra_white_spaces(content)\n",
    "    content = remove_urls(content)\n",
    "    content = remove_code(content)\n",
    "    content = remove_alt_html(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset:\n",
    "    sample[\"content\"] = clean_content(sample[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = pd.read_csv(\"../resources/wiki-30k-10-IDF.csv\")\n",
    "idf = idf.set_index('term')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words considered in wikipedia: 87709\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words considered in wikipedia: {0}\".format(idf.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Set tokenizers, tagger and stemmer\n",
    "tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "sentTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def to_token_list(content):\n",
    "    textList = nltk.word_tokenize(content)\n",
    "    tokenList = []\n",
    "    for token in textList:\n",
    "        try:\n",
    "            thisToken = token\n",
    "            uselessUnicode = [u'\\u2013', u'\\u2014', u'\\u201d', u'\\u201c'] ### don't include these when they are alone\n",
    "            if thisToken not in uselessUnicode:\n",
    "                thisToken = thisToken.replace(u'\\u201d','') # delete this (unicode quote)\n",
    "                thisToken = thisToken.replace(u'\\u201c','') # delete this (unicode quote)\n",
    "                tokenList.append(thisToken)\n",
    "        except:\n",
    "            tokenList.append('**CODEC_ERROR**')\n",
    "            # #######################prints word on CODEC ERROR\n",
    "            print('**CODEC_ERROR**')\n",
    "            print(token) \n",
    "            print('****')\n",
    "    return tokenList\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "import re\n",
    "\n",
    "def add_to_stem_dictionary(stemmed_word, word, stem_dictionary):\n",
    "    if stemmed_word not in stem_dictionary:\n",
    "        stem_dictionary[stemmed_word] = {word: 1}\n",
    "    else:\n",
    "        d = stem_dictionary[stemmed_word]\n",
    "        if word not in d:\n",
    "            d[word] = 1\n",
    "        else:\n",
    "            d[word] += 1\n",
    "\n",
    "def cleanTokens(tokenList, stem_dictionary):\n",
    "    # Convert all text to lower case\n",
    "    textList = [word.lower() for word in tokenList]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    textList = [word for word in textList if word not in punctuation]\n",
    "    textList = [\"\".join(c for c in word if c not in punctuation) for word in textList ]\n",
    "    \n",
    "    # Convert digits into NUM\n",
    "    textList = [re.sub(\"\\d+\", \"NUM\", word) for word in textList]  \n",
    "    \n",
    "    # Stem words \n",
    "    stemmedTextList = [stemmer.stem(word) for word in textList]\n",
    "    for sw,w in zip(stemmedTextList, textList):\n",
    "        add_to_stem_dictionary(sw, w, stem_dictionary)\n",
    "    textList = stemmedTextList\n",
    "    \n",
    "    # Remove blanks\n",
    "    textList = [word for word in textList if word != ' ']\n",
    "    textList = [word for word in textList if word != '']\n",
    "    \n",
    "    # Remove short words\n",
    "    textList = [word for word in textList if len(word) > 2]\n",
    "    \n",
    "    return textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_sample_to_tfidf(sample, stem_dictionary):\n",
    "    # From text to tokens\n",
    "    #tl = to_token_list(sample)\n",
    "    tl = nltk.word_tokenize(sample) # splits \"I am Fabio\" into [\"I\", \"am\", \"Fabio\"]. It's a little smarter than a .split(\" \")\n",
    "    raw_text = ' '.join(tl) # Join back the tokens with a space between them\n",
    "    tokens = cleanTokens(tl, stem_dictionary)\n",
    "    \n",
    "    ## create FreqDF with word frequencies\n",
    "    freq = FreqDist(tokens)\n",
    "    \n",
    "    # convert it to a data frame\n",
    "    freqDF = pd.DataFrame.from_dict(freq, orient='index')\n",
    "    freqDF.columns = ['freq']\n",
    "    \n",
    "    ## merge freqDF with idf data frame\n",
    "    freqit = freqDF.join(idf[['idf', 'logidf']])\n",
    "    \n",
    "    # replace null values with max\n",
    "    maxidf = max(freqit['idf'].dropna())\n",
    "    maxlogidf = max(freqit['logidf'].dropna())\n",
    "    freqit.loc[pd.isnull(freqit['idf']), 'idf'] = maxidf\n",
    "    freqit.loc[pd.isnull(freqit['logidf']), 'logidf'] = maxlogidf\n",
    "    \n",
    "    ## create tfidf columns\n",
    "    freqit['tfidf'] = freqit['freq'] * freqit['idf']\n",
    "    freqit['logtfidf'] = freqit['freq'] * freqit['logidf']\n",
    "    \n",
    "    ## order by logtfidf weight\n",
    "    freqit = freqit.sort_values(by='logtfidf', ascending=False) \n",
    "    \n",
    "    return freqit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n"
     ]
    }
   ],
   "source": [
    "stem_dictionary = {}\n",
    "\n",
    "for i,sample in enumerate(dataset):\n",
    "    sample[\"tfidf\"] = from_sample_to_tfidf(sample[\"content\"], stem_dictionary).to_dict()\n",
    "    if i % 50 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign read time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wps = 200 / 60 # = 3.33 \n",
    "\n",
    "for el in dataset:\n",
    "    num_of_words = len(el[\"content\"].split(\" \"))\n",
    "    read_time = num_of_words / wps\n",
    "    el[\"read_time\"] = read_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,sample in enumerate(dataset):\n",
    "    with open(\"../preprocessed/\" + str(i) + '.json', 'w') as outfile:\n",
    "        json.dump(sample, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save stem_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../stemmer/stem_dictionary.json', 'w') as outfile:\n",
    "    json.dump(stem_dictionary, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
