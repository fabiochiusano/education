{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from preprocessed_data_reader.ipynb\n",
      "Importing Jupyter notebook from utils_os.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "import nbimporter\n",
    "from preprocessed_data_reader import ReaderPreprocessedData\n",
    "from utils_os import UtilsOS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserState:\n",
    "    def __init__(self):\n",
    "        self.last_read_articles = []\n",
    "        \n",
    "    def add_read_article(self, article):\n",
    "        self.last_read_articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    WordInfo = namedtuple('WordInfo', 'word perc_sim idf')\n",
    "    SimData = namedtuple('SimData', 'index similarity words_importance_list')\n",
    "    \n",
    "    def recommend_articles(self, UserState, how_many=-1):\n",
    "        raise NotImplementedException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender based on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderTFIDF(Recommender):\n",
    "    def _cosine_similarity_on_tfidf_vectors(self, tfidf_1, tfidf_2, on=\"tfidf\"):\n",
    "        admissible_on = [\"tfidf\", \"logtfidf\"]\n",
    "        if on not in admissible_on:\n",
    "            raise ValueError(\"on must be one of {0}\".format(admissible_on))\n",
    "\n",
    "        try:\n",
    "            a = tfidf_1.loc[list(tfidf_2.index)].dropna()\n",
    "            b = tfidf_2.loc[list(tfidf_1.index)].dropna()\n",
    "            prod = np.multiply(a[on].values, b[on].values) \n",
    "            norm_1 = np.linalg.norm(tfidf_1[on].values)\n",
    "            norm_2 = np.linalg.norm(tfidf_2[on].values)\n",
    "            cosine_similarity = np.sum(prod) / (norm_1 * norm_2) # default is norm 2\n",
    "            keys = a.index.values # same as b.index.values\n",
    "            perc_in_similarity = prod / sum(prod)\n",
    "            idf_of_word = a[\"idf\"]# values are a tuple(perc_in_similarity, idf_of_word)\n",
    "            words_importance_list = list(zip(keys, perc_in_similarity, idf_of_word)) # [(word, perc_in_similarity, idf_of_word), ...]\n",
    "            words_importance_list = [Recommender.WordInfo(*t) for t in words_importance_list] # [WordInfo, ...]\n",
    "            return cosine_similarity, words_importance_list\n",
    "        except: # e.g. the case where no index overlaps\n",
    "            return 0, []\n",
    "        \n",
    "    def _order_dataset_by_similarity(self, reference_article):\n",
    "        similarities = []\n",
    "        for i, article in enumerate(self._dataset):\n",
    "            cos_sim, words_importance_list = self._cosine_similarity_on_tfidf_vectors(reference_article[\"tfidf\"], article[\"tfidf\"], on=\"logtfidf\")\n",
    "            words_importance_list = sorted(words_importance_list, key=lambda t:t.perc_sim, reverse=True) # sort by word importance\n",
    "            similarities.append(Recommender.SimData(i, cos_sim, words_importance_list)) # similarities = [SimData, ...]\n",
    "        similarities = sorted(similarities, key=lambda t:t.similarity, reverse=True) # sort by article similarity\n",
    "        return similarities\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self._dataset = dataset\n",
    "        \n",
    "    def recommend_articles(self, user_state, how_many=-1):\n",
    "        \"\"\"user_state is of type UserState\n",
    "           returns the indices of the best articles in the dataset\"\"\"\n",
    "        last_article_read = self._dataset[user_state.last_read_articles[-1]]\n",
    "        similarities = self._order_dataset_by_similarity(last_article_read)\n",
    "        \n",
    "        similarities = [t.index for t in similarities]\n",
    "        \n",
    "        if how_many != -1:\n",
    "            return similarities[:how_many]\n",
    "        return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender based on BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0430 18:05:24.005646 4575921600 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#!python -m spacy download en_core_web_md #you will need to install this on first load\n",
    "#import spacy\n",
    "#from spacy.lang.en import English\n",
    "#from spacy import displacy\n",
    "#nlp = spacy.load('en_core_web_md')\n",
    "from IPython.display import HTML\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True #OPTIONAL - to disable outputs from Tensorflow\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderBERT(Recommender):\n",
    "    def _load_bert(self):\n",
    "        url = \"https://tfhub.dev/google/elmo/2\"\n",
    "        return hub.Module(url)\n",
    "    \n",
    "    def _from_texts_to_vectors(self, texts):\n",
    "        embeddings = self._bert(\n",
    "            texts,\n",
    "            signature=\"default\",\n",
    "            as_dict=True)[\"default\"]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.tables_initializer())\n",
    "            x = sess.run(embeddings)\n",
    "\n",
    "        return x # array with length len(texts), where each element is an array with lenght 1024\n",
    "    \n",
    "    def _get_n_most_similar_to_search(self, search_string, n):\n",
    "        \"\"\"dataset should be a list of lists long 1024\"\"\"\n",
    "        search_vect = self._from_texts_to_vectors([search_string], self._bert)\n",
    "        cosine_similarities = pd.Series(cosine_similarity(search_vect, self._dataset_bert_vectorized).flatten())\n",
    "        return cosine_similarities.nlargest(n)\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self._dataset = dataset\n",
    "        self._bert = self._load_bert()\n",
    "        \n",
    "        contents = [\" \".join(sample[\"content_tokenized\"]) for sample in dataset]\n",
    "        self._dataset_bert_vectorized = self._from_texts_to_vectors(contents)\n",
    "        \n",
    "    def recommend_articles(self, user_state, how_many=-1):\n",
    "        \"\"\"returns the indices of the best articles in the datasets\"\"\"\n",
    "        last_read_articles = [self._dataset[i] for i in user_state.last_read_articles]\n",
    "        search_vectors = self._from_texts_to_vectors([\" \".join(art[\"content_tokenized\"]) for art in last_read_articles], self._bert)\n",
    "        search_vect = np.mean(search_vectors)\n",
    "        cosine_similarities = pd.Series(cosine_similarity(search_vect, self._dataset_bert_vectorized).flatten())\n",
    "        if how_many == -1:\n",
    "            return [i for i,j in cosine_similarities.nlargest(how_many).iteritems()]\n",
    "        else:\n",
    "            return [i for i,j in cosine_similarities.nlargest(how_many).iteritems()][:how_many]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReaderPreprocessedData.read_data(\"../preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_article(dataset):\n",
    "    random_index = random.randint(0, len(dataset) - 1)\n",
    "    return dataset[random_index]\n",
    "\n",
    "counter = 0\n",
    "recommender = RecommenderBERT(dataset)\n",
    "user_state = UserState()\n",
    "top_n = 10\n",
    "\n",
    "while True:\n",
    "    if counter == 0:\n",
    "        print(\"Titles:\")\n",
    "        articles = [get_random_article(dataset) for j in range(top_n)]\n",
    "        for j,article in enumerate(articles):\n",
    "            print(\"{0} - Title: {1}\".format(j + 1, article[\"title\"]))\n",
    "            print(\"URL: {0}\".format(article[\"url\"]))\n",
    "            print(\"--------------------------------\")\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    if counter == 0:\n",
    "        time_to_read = int(input(\"Indicate available time [min]: \"))\n",
    "        time_to_read *= 60\n",
    "    chosen_index = int(input(\"Choose an article: \"))\n",
    "    user_state.add_read_article(chosen_index)\n",
    "\n",
    "    if counter == 0:\n",
    "        chosen_article = articles[chosen_index]\n",
    "    else:\n",
    "        #chosen_article = dataset[similarities[i].index]\n",
    "        chosen_article = dataset[similarities[i]]\n",
    "\n",
    "    similarities = recommender.recommend_articles(user_state, how_many=top_n)\n",
    "    #similarities = [sim for sim in similarities if dataset[sim[0]][\"read_time\"] < time_to_read]\n",
    "    similarities = [sim for sim in similarities if dataset[sim][\"read_time\"] < time_to_read]\n",
    "\n",
    "    print(\"The top {0} similar articles are:\".format(top_n))\n",
    "    for i in range(top_n):\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"{0} - Title: {1}\".format(i + 1, dataset[similarities[i]][\"title\"]))\n",
    "        print(\"URL: {0}\".format(dataset[similarities[i]][\"url\"]))\n",
    "        print(\"Time to read [min]: {0:.2f}\".format(dataset[similarities[i]][\"read_time\"] / 60))\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def to_tsne(x):\n",
    "    \"\"\"From 1024 to 50 with PCA, from 50 to 2 with TSNE\"\"\"\n",
    "    y = PCA(n_components=50).fit_transform(x)\n",
    "    y = TSNE(n_components=2).fit_transform(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def scatter_plot(y):\n",
    "    \"\"\"Scatter plot of sentences by their 2 dimensions\"\"\"\n",
    "    data = [\n",
    "        go.Scatter(\n",
    "            x=[i[0] for i in y],\n",
    "            y=[i[1] for i in y],\n",
    "            mode='markers',\n",
    "            text=[i for i in sentences],\n",
    "        marker=dict(\n",
    "            size=16,\n",
    "            color = [len(i) for i in sentences], #set color equal to a variable\n",
    "            opacity= 0.8,\n",
    "            colorscale='Viridis',\n",
    "            showscale=False\n",
    "        )\n",
    "        )\n",
    "    ]\n",
    "    layout = go.Layout()\n",
    "    layout = dict(\n",
    "                  yaxis = dict(zeroline = False),\n",
    "                  xaxis = dict(zeroline = False)\n",
    "                 )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    file = plot(fig, filename='Sentence encode.html') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bert_vectorized = self._from_texts_to_vectors(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = to_tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
