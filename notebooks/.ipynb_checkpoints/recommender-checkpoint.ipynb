{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hidden(l):\n",
    "    return [el for el in l if el[0] != \".\"]\n",
    "\n",
    "def get_relative_path_to_dirs(start_path):\n",
    "    subdirs = [x[1] for x in os.walk(start_path)][0]\n",
    "    subdirs = remove_hidden(subdirs)\n",
    "    subdirs = [start_path + \"/\" + subdir for subdir in subdirs]\n",
    "    return subdirs\n",
    "\n",
    "def get_relative_path_to_files(start_path):\n",
    "    files = [f for f in listdir(start_path) if isfile(join(start_path, f))]\n",
    "    files = remove_hidden(files)\n",
    "    files = [start_path + \"/\" + file for file in files]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../articles/medium',\n",
       " '../articles/splinters',\n",
       " '../articles/thehistoryblog',\n",
       " '../articles/tutorialspoint',\n",
       " '../articles/chemistry-blog',\n",
       " '../articles/wikihow',\n",
       " '../articles/kdnuggets',\n",
       " '../articles/smartdatacollective']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_articles_directory = \"../articles\"\n",
    "\n",
    "subdirs = get_relative_path_to_dirs(home_articles_directory)\n",
    "subdirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for subdir in subdirs:\n",
    "    subsubdirs = get_relative_path_to_dirs(subdir)\n",
    "    for subsubdir in subsubdirs:\n",
    "        onlyfiles = get_relative_path_to_files(subsubdir)\n",
    "        read_json_list = []\n",
    "        for file in onlyfiles:\n",
    "            with open(file, 'r') as infile:\n",
    "                d = json.load(infile)\n",
    "                dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = pd.read_csv(\"../resources/wiki-30k-10-IDF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>freq</th>\n",
       "      <th>idf</th>\n",
       "      <th>logidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>6.782192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kalmar</td>\n",
       "      <td>55</td>\n",
       "      <td>288.654545</td>\n",
       "      <td>5.665231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rickman</td>\n",
       "      <td>58</td>\n",
       "      <td>273.724138</td>\n",
       "      <td>5.612121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multi-point</td>\n",
       "      <td>17</td>\n",
       "      <td>933.882353</td>\n",
       "      <td>6.839350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jiankang</td>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>henstridg</td>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rebel</td>\n",
       "      <td>2188</td>\n",
       "      <td>7.255941</td>\n",
       "      <td>1.981821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gainsbourg</td>\n",
       "      <td>29</td>\n",
       "      <td>547.448276</td>\n",
       "      <td>6.305268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mclaurin</td>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blassi</td>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abdoulay</td>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>chongxi</td>\n",
       "      <td>11</td>\n",
       "      <td>1443.272727</td>\n",
       "      <td>7.274669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>benedikt</td>\n",
       "      <td>37</td>\n",
       "      <td>429.081081</td>\n",
       "      <td>6.061646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>expeditionari</td>\n",
       "      <td>476</td>\n",
       "      <td>33.352941</td>\n",
       "      <td>3.507146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sylver</td>\n",
       "      <td>13</td>\n",
       "      <td>1221.230769</td>\n",
       "      <td>7.107614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>meteorologist</td>\n",
       "      <td>145</td>\n",
       "      <td>109.489655</td>\n",
       "      <td>4.695830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jidaigeki</td>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>constantinopolitan</td>\n",
       "      <td>19</td>\n",
       "      <td>835.578947</td>\n",
       "      <td>6.728125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>baladi</td>\n",
       "      <td>11</td>\n",
       "      <td>1443.272727</td>\n",
       "      <td>7.274669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>balada</td>\n",
       "      <td>22</td>\n",
       "      <td>721.636364</td>\n",
       "      <td>6.581521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>architecton</td>\n",
       "      <td>24</td>\n",
       "      <td>661.500000</td>\n",
       "      <td>6.494510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>marquett</td>\n",
       "      <td>100</td>\n",
       "      <td>158.760000</td>\n",
       "      <td>5.067394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>jebusit</td>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>num.cohen</td>\n",
       "      <td>11</td>\n",
       "      <td>1443.272727</td>\n",
       "      <td>7.274669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stoicism</td>\n",
       "      <td>52</td>\n",
       "      <td>305.307692</td>\n",
       "      <td>5.721320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>fir</td>\n",
       "      <td>179</td>\n",
       "      <td>88.692737</td>\n",
       "      <td>4.485178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>fis</td>\n",
       "      <td>46</td>\n",
       "      <td>345.130435</td>\n",
       "      <td>5.843922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>fip</td>\n",
       "      <td>22</td>\n",
       "      <td>721.636364</td>\n",
       "      <td>6.581521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fit</td>\n",
       "      <td>4710</td>\n",
       "      <td>3.370701</td>\n",
       "      <td>1.215121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fiu</td>\n",
       "      <td>14</td>\n",
       "      <td>1134.000000</td>\n",
       "      <td>7.033506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87679</th>\n",
       "      <td>num+num</td>\n",
       "      <td>145</td>\n",
       "      <td>109.489655</td>\n",
       "      <td>4.695830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87680</th>\n",
       "      <td>everglad</td>\n",
       "      <td>61</td>\n",
       "      <td>260.262295</td>\n",
       "      <td>5.561690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87681</th>\n",
       "      <td>parthenon</td>\n",
       "      <td>55</td>\n",
       "      <td>288.654545</td>\n",
       "      <td>5.665231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87682</th>\n",
       "      <td>shakya</td>\n",
       "      <td>14</td>\n",
       "      <td>1134.000000</td>\n",
       "      <td>7.033506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87683</th>\n",
       "      <td>ridgeway</td>\n",
       "      <td>64</td>\n",
       "      <td>248.062500</td>\n",
       "      <td>5.513681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87684</th>\n",
       "      <td>ica</td>\n",
       "      <td>43</td>\n",
       "      <td>369.209302</td>\n",
       "      <td>5.911364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87685</th>\n",
       "      <td>hsa</td>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87686</th>\n",
       "      <td>dogtown</td>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87687</th>\n",
       "      <td>adoption</td>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87688</th>\n",
       "      <td>time-bound</td>\n",
       "      <td>15</td>\n",
       "      <td>1058.400000</td>\n",
       "      <td>6.964514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87689</th>\n",
       "      <td>anti-terrorist</td>\n",
       "      <td>54</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>5.683580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87690</th>\n",
       "      <td>e.a.</td>\n",
       "      <td>19</td>\n",
       "      <td>835.578947</td>\n",
       "      <td>6.728125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87691</th>\n",
       "      <td>cutlet</td>\n",
       "      <td>16</td>\n",
       "      <td>992.250000</td>\n",
       "      <td>6.899975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87692</th>\n",
       "      <td>birthmark</td>\n",
       "      <td>16</td>\n",
       "      <td>992.250000</td>\n",
       "      <td>6.899975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87693</th>\n",
       "      <td>hse</td>\n",
       "      <td>15</td>\n",
       "      <td>1058.400000</td>\n",
       "      <td>6.964514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87694</th>\n",
       "      <td>libretto</td>\n",
       "      <td>227</td>\n",
       "      <td>69.938326</td>\n",
       "      <td>4.247614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87695</th>\n",
       "      <td>libretti</td>\n",
       "      <td>38</td>\n",
       "      <td>417.789474</td>\n",
       "      <td>6.034978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87696</th>\n",
       "      <td>inglouri</td>\n",
       "      <td>15</td>\n",
       "      <td>1058.400000</td>\n",
       "      <td>6.964514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87697</th>\n",
       "      <td>haywood</td>\n",
       "      <td>101</td>\n",
       "      <td>157.188119</td>\n",
       "      <td>5.057443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87698</th>\n",
       "      <td>wesley</td>\n",
       "      <td>546</td>\n",
       "      <td>29.076923</td>\n",
       "      <td>3.369945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87699</th>\n",
       "      <td>hands-fre</td>\n",
       "      <td>24</td>\n",
       "      <td>661.500000</td>\n",
       "      <td>6.494510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87700</th>\n",
       "      <td>footwork</td>\n",
       "      <td>46</td>\n",
       "      <td>345.130435</td>\n",
       "      <td>5.843922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87701</th>\n",
       "      <td>square-integr</td>\n",
       "      <td>13</td>\n",
       "      <td>1221.230769</td>\n",
       "      <td>7.107614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87702</th>\n",
       "      <td>morain</td>\n",
       "      <td>84</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>5.241747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87703</th>\n",
       "      <td>singletari</td>\n",
       "      <td>14</td>\n",
       "      <td>1134.000000</td>\n",
       "      <td>7.033506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87704</th>\n",
       "      <td>cordial</td>\n",
       "      <td>275</td>\n",
       "      <td>57.730909</td>\n",
       "      <td>4.055793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87705</th>\n",
       "      <td>pandyan</td>\n",
       "      <td>19</td>\n",
       "      <td>835.578947</td>\n",
       "      <td>6.728125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87706</th>\n",
       "      <td>pate</td>\n",
       "      <td>45</td>\n",
       "      <td>352.800000</td>\n",
       "      <td>5.865901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87707</th>\n",
       "      <td>league-record</td>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87708</th>\n",
       "      <td>showtun</td>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87709 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     term  freq          idf    logidf\n",
       "0                     NaN    18   882.000000  6.782192\n",
       "1                  kalmar    55   288.654545  5.665231\n",
       "2                 rickman    58   273.724138  5.612121\n",
       "3             multi-point    17   933.882353  6.839350\n",
       "4                jiankang    10  1587.600000  7.369979\n",
       "5               henstridg    12  1323.000000  7.187657\n",
       "6                   rebel  2188     7.255941  1.981821\n",
       "7              gainsbourg    29   547.448276  6.305268\n",
       "8                mclaurin    12  1323.000000  7.187657\n",
       "9                  blassi    12  1323.000000  7.187657\n",
       "10               abdoulay    10  1587.600000  7.369979\n",
       "11                chongxi    11  1443.272727  7.274669\n",
       "12               benedikt    37   429.081081  6.061646\n",
       "13          expeditionari   476    33.352941  3.507146\n",
       "14                 sylver    13  1221.230769  7.107614\n",
       "15          meteorologist   145   109.489655  4.695830\n",
       "16              jidaigeki    12  1323.000000  7.187657\n",
       "17     constantinopolitan    19   835.578947  6.728125\n",
       "18                 baladi    11  1443.272727  7.274669\n",
       "19                 balada    22   721.636364  6.581521\n",
       "20            architecton    24   661.500000  6.494510\n",
       "21               marquett   100   158.760000  5.067394\n",
       "22                jebusit    12  1323.000000  7.187657\n",
       "23              num.cohen    11  1443.272727  7.274669\n",
       "24               stoicism    52   305.307692  5.721320\n",
       "25                    fir   179    88.692737  4.485178\n",
       "26                    fis    46   345.130435  5.843922\n",
       "27                    fip    22   721.636364  6.581521\n",
       "28                    fit  4710     3.370701  1.215121\n",
       "29                    fiu    14  1134.000000  7.033506\n",
       "...                   ...   ...          ...       ...\n",
       "87679             num+num   145   109.489655  4.695830\n",
       "87680            everglad    61   260.262295  5.561690\n",
       "87681           parthenon    55   288.654545  5.665231\n",
       "87682              shakya    14  1134.000000  7.033506\n",
       "87683            ridgeway    64   248.062500  5.513681\n",
       "87684                 ica    43   369.209302  5.911364\n",
       "87685                 hsa    10  1587.600000  7.369979\n",
       "87686             dogtown    10  1587.600000  7.369979\n",
       "87687            adoption    12  1323.000000  7.187657\n",
       "87688          time-bound    15  1058.400000  6.964514\n",
       "87689      anti-terrorist    54   294.000000  5.683580\n",
       "87690                e.a.    19   835.578947  6.728125\n",
       "87691              cutlet    16   992.250000  6.899975\n",
       "87692           birthmark    16   992.250000  6.899975\n",
       "87693                 hse    15  1058.400000  6.964514\n",
       "87694            libretto   227    69.938326  4.247614\n",
       "87695            libretti    38   417.789474  6.034978\n",
       "87696            inglouri    15  1058.400000  6.964514\n",
       "87697             haywood   101   157.188119  5.057443\n",
       "87698              wesley   546    29.076923  3.369945\n",
       "87699           hands-fre    24   661.500000  6.494510\n",
       "87700            footwork    46   345.130435  5.843922\n",
       "87701       square-integr    13  1221.230769  7.107614\n",
       "87702              morain    84   189.000000  5.241747\n",
       "87703          singletari    14  1134.000000  7.033506\n",
       "87704             cordial   275    57.730909  4.055793\n",
       "87705             pandyan    19   835.578947  6.728125\n",
       "87706                pate    45   352.800000  5.865901\n",
       "87707       league-record    10  1587.600000  7.369979\n",
       "87708             showtun    12  1323.000000  7.187657\n",
       "\n",
       "[87709 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "      <th>idf</th>\n",
       "      <th>logidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>18</td>\n",
       "      <td>882.000000</td>\n",
       "      <td>6.782192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kalmar</th>\n",
       "      <td>55</td>\n",
       "      <td>288.654545</td>\n",
       "      <td>5.665231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rickman</th>\n",
       "      <td>58</td>\n",
       "      <td>273.724138</td>\n",
       "      <td>5.612121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multi-point</th>\n",
       "      <td>17</td>\n",
       "      <td>933.882353</td>\n",
       "      <td>6.839350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jiankang</th>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>henstridg</th>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rebel</th>\n",
       "      <td>2188</td>\n",
       "      <td>7.255941</td>\n",
       "      <td>1.981821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gainsbourg</th>\n",
       "      <td>29</td>\n",
       "      <td>547.448276</td>\n",
       "      <td>6.305268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mclaurin</th>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blassi</th>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abdoulay</th>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chongxi</th>\n",
       "      <td>11</td>\n",
       "      <td>1443.272727</td>\n",
       "      <td>7.274669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benedikt</th>\n",
       "      <td>37</td>\n",
       "      <td>429.081081</td>\n",
       "      <td>6.061646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expeditionari</th>\n",
       "      <td>476</td>\n",
       "      <td>33.352941</td>\n",
       "      <td>3.507146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sylver</th>\n",
       "      <td>13</td>\n",
       "      <td>1221.230769</td>\n",
       "      <td>7.107614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meteorologist</th>\n",
       "      <td>145</td>\n",
       "      <td>109.489655</td>\n",
       "      <td>4.695830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jidaigeki</th>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>constantinopolitan</th>\n",
       "      <td>19</td>\n",
       "      <td>835.578947</td>\n",
       "      <td>6.728125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baladi</th>\n",
       "      <td>11</td>\n",
       "      <td>1443.272727</td>\n",
       "      <td>7.274669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balada</th>\n",
       "      <td>22</td>\n",
       "      <td>721.636364</td>\n",
       "      <td>6.581521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>architecton</th>\n",
       "      <td>24</td>\n",
       "      <td>661.500000</td>\n",
       "      <td>6.494510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marquett</th>\n",
       "      <td>100</td>\n",
       "      <td>158.760000</td>\n",
       "      <td>5.067394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jebusit</th>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num.cohen</th>\n",
       "      <td>11</td>\n",
       "      <td>1443.272727</td>\n",
       "      <td>7.274669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stoicism</th>\n",
       "      <td>52</td>\n",
       "      <td>305.307692</td>\n",
       "      <td>5.721320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fir</th>\n",
       "      <td>179</td>\n",
       "      <td>88.692737</td>\n",
       "      <td>4.485178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fis</th>\n",
       "      <td>46</td>\n",
       "      <td>345.130435</td>\n",
       "      <td>5.843922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fip</th>\n",
       "      <td>22</td>\n",
       "      <td>721.636364</td>\n",
       "      <td>6.581521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit</th>\n",
       "      <td>4710</td>\n",
       "      <td>3.370701</td>\n",
       "      <td>1.215121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fiu</th>\n",
       "      <td>14</td>\n",
       "      <td>1134.000000</td>\n",
       "      <td>7.033506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num+num</th>\n",
       "      <td>145</td>\n",
       "      <td>109.489655</td>\n",
       "      <td>4.695830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everglad</th>\n",
       "      <td>61</td>\n",
       "      <td>260.262295</td>\n",
       "      <td>5.561690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parthenon</th>\n",
       "      <td>55</td>\n",
       "      <td>288.654545</td>\n",
       "      <td>5.665231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shakya</th>\n",
       "      <td>14</td>\n",
       "      <td>1134.000000</td>\n",
       "      <td>7.033506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridgeway</th>\n",
       "      <td>64</td>\n",
       "      <td>248.062500</td>\n",
       "      <td>5.513681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ica</th>\n",
       "      <td>43</td>\n",
       "      <td>369.209302</td>\n",
       "      <td>5.911364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hsa</th>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dogtown</th>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adoption</th>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time-bound</th>\n",
       "      <td>15</td>\n",
       "      <td>1058.400000</td>\n",
       "      <td>6.964514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anti-terrorist</th>\n",
       "      <td>54</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>5.683580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e.a.</th>\n",
       "      <td>19</td>\n",
       "      <td>835.578947</td>\n",
       "      <td>6.728125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutlet</th>\n",
       "      <td>16</td>\n",
       "      <td>992.250000</td>\n",
       "      <td>6.899975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>birthmark</th>\n",
       "      <td>16</td>\n",
       "      <td>992.250000</td>\n",
       "      <td>6.899975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hse</th>\n",
       "      <td>15</td>\n",
       "      <td>1058.400000</td>\n",
       "      <td>6.964514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libretto</th>\n",
       "      <td>227</td>\n",
       "      <td>69.938326</td>\n",
       "      <td>4.247614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>libretti</th>\n",
       "      <td>38</td>\n",
       "      <td>417.789474</td>\n",
       "      <td>6.034978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inglouri</th>\n",
       "      <td>15</td>\n",
       "      <td>1058.400000</td>\n",
       "      <td>6.964514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haywood</th>\n",
       "      <td>101</td>\n",
       "      <td>157.188119</td>\n",
       "      <td>5.057443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wesley</th>\n",
       "      <td>546</td>\n",
       "      <td>29.076923</td>\n",
       "      <td>3.369945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hands-fre</th>\n",
       "      <td>24</td>\n",
       "      <td>661.500000</td>\n",
       "      <td>6.494510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>footwork</th>\n",
       "      <td>46</td>\n",
       "      <td>345.130435</td>\n",
       "      <td>5.843922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>square-integr</th>\n",
       "      <td>13</td>\n",
       "      <td>1221.230769</td>\n",
       "      <td>7.107614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morain</th>\n",
       "      <td>84</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>5.241747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>singletari</th>\n",
       "      <td>14</td>\n",
       "      <td>1134.000000</td>\n",
       "      <td>7.033506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cordial</th>\n",
       "      <td>275</td>\n",
       "      <td>57.730909</td>\n",
       "      <td>4.055793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandyan</th>\n",
       "      <td>19</td>\n",
       "      <td>835.578947</td>\n",
       "      <td>6.728125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pate</th>\n",
       "      <td>45</td>\n",
       "      <td>352.800000</td>\n",
       "      <td>5.865901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>league-record</th>\n",
       "      <td>10</td>\n",
       "      <td>1587.600000</td>\n",
       "      <td>7.369979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>showtun</th>\n",
       "      <td>12</td>\n",
       "      <td>1323.000000</td>\n",
       "      <td>7.187657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87709 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    freq          idf    logidf\n",
       "term                                           \n",
       "NaN                   18   882.000000  6.782192\n",
       "kalmar                55   288.654545  5.665231\n",
       "rickman               58   273.724138  5.612121\n",
       "multi-point           17   933.882353  6.839350\n",
       "jiankang              10  1587.600000  7.369979\n",
       "henstridg             12  1323.000000  7.187657\n",
       "rebel               2188     7.255941  1.981821\n",
       "gainsbourg            29   547.448276  6.305268\n",
       "mclaurin              12  1323.000000  7.187657\n",
       "blassi                12  1323.000000  7.187657\n",
       "abdoulay              10  1587.600000  7.369979\n",
       "chongxi               11  1443.272727  7.274669\n",
       "benedikt              37   429.081081  6.061646\n",
       "expeditionari        476    33.352941  3.507146\n",
       "sylver                13  1221.230769  7.107614\n",
       "meteorologist        145   109.489655  4.695830\n",
       "jidaigeki             12  1323.000000  7.187657\n",
       "constantinopolitan    19   835.578947  6.728125\n",
       "baladi                11  1443.272727  7.274669\n",
       "balada                22   721.636364  6.581521\n",
       "architecton           24   661.500000  6.494510\n",
       "marquett             100   158.760000  5.067394\n",
       "jebusit               12  1323.000000  7.187657\n",
       "num.cohen             11  1443.272727  7.274669\n",
       "stoicism              52   305.307692  5.721320\n",
       "fir                  179    88.692737  4.485178\n",
       "fis                   46   345.130435  5.843922\n",
       "fip                   22   721.636364  6.581521\n",
       "fit                 4710     3.370701  1.215121\n",
       "fiu                   14  1134.000000  7.033506\n",
       "...                  ...          ...       ...\n",
       "num+num              145   109.489655  4.695830\n",
       "everglad              61   260.262295  5.561690\n",
       "parthenon             55   288.654545  5.665231\n",
       "shakya                14  1134.000000  7.033506\n",
       "ridgeway              64   248.062500  5.513681\n",
       "ica                   43   369.209302  5.911364\n",
       "hsa                   10  1587.600000  7.369979\n",
       "dogtown               10  1587.600000  7.369979\n",
       "adoption              12  1323.000000  7.187657\n",
       "time-bound            15  1058.400000  6.964514\n",
       "anti-terrorist        54   294.000000  5.683580\n",
       "e.a.                  19   835.578947  6.728125\n",
       "cutlet                16   992.250000  6.899975\n",
       "birthmark             16   992.250000  6.899975\n",
       "hse                   15  1058.400000  6.964514\n",
       "libretto             227    69.938326  4.247614\n",
       "libretti              38   417.789474  6.034978\n",
       "inglouri              15  1058.400000  6.964514\n",
       "haywood              101   157.188119  5.057443\n",
       "wesley               546    29.076923  3.369945\n",
       "hands-fre             24   661.500000  6.494510\n",
       "footwork              46   345.130435  5.843922\n",
       "square-integr         13  1221.230769  7.107614\n",
       "morain                84   189.000000  5.241747\n",
       "singletari            14  1134.000000  7.033506\n",
       "cordial              275    57.730909  4.055793\n",
       "pandyan               19   835.578947  6.728125\n",
       "pate                  45   352.800000  5.865901\n",
       "league-record         10  1587.600000  7.369979\n",
       "showtun               12  1323.000000  7.187657\n",
       "\n",
       "[87709 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = idf.set_index('term')\n",
    "idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take sample content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n I keep bumping into this question.  Casey Perin gave a talk on it at UCR; Daniel Greco has a forthcoming paper on it in Phil Review.  Benj Hellie launched an extended Facebook conversation about it.  Can the radical skeptic live his skepticism?  I submit the following for your consideration.\\n\\nFirst, a bit about belief.  I\\'ve argued that to believe some proposition P is nothing more or less than to be disposed to act and react in a broadly belief-that-P-ish way -- that is, to be disposed, circumstances to being right, to say things like \"P\", to build one\\'s plans on the likelihood of P\\'s truth, to feel surprised should P prove false, etc.  Among the relevant dispositions is the disposition to consciously judge that P is the case, that is, to momentarily explicitly regard P as true, to endorse P intellectually (though not necessarily in language).  Dispositions to judge that P often pull apart from the other dispositions constitutive of belief, for example in self-deception, implicit bias, conceptual confusion, and momentary forgetting.  (See here and here.)  To believe that P is to steer one\\'s way through the world as though P were the case.  One important part of the steering, but not the only part, is being disposed to explicitly judge that P is the case.\\n\\nOkay, now skepticism. My paradigm radical skeptics are Sextus Empiricus, Montaigne (of the Apology), and Zhuangzi (of Inner Chapter 2).  When such radical skeptics say they aim to suspend all belief, I recommend that we interpret them as really endorsing two goals: (a.) suspending all judgment, and (b.) standing openly ready, with equanimity, for alternative possibilities.\\n\\nArguments that it\\'s impossible to suspend all belief tend to be, at root, arguments that it\\'s impossible to refrain from action and that action requires belief.  Perhaps it is impossible to refrain from all action.  No skeptic advises sitting all day in bed (as though that weren\\'t itself an action).  Sextus advises acting from habit; Zhuangzi seems to endorse well-trained spontaneity.  (Of course, they can\\'t insist dogmatically on this, and Zhuangzi actively undermines himself.)  If the runaway carriage is speeding toward the skeptic, the skeptic will leap aside.  On my account of belief, such a disposition is partly constitutive of believing that the carriage is heading your way.  So the skeptic will have at least part of the dispositional profile constitutive of that belief.  This much I accept.\\n\\nBut it\\'s not clear that the skeptic needs to match the entire dispositional profile constitutive of believing the carriage is coming.  In particular, it\\'s not clear that the skeptic needs to consciously judge that the carriage is coming.  Maybe most of us would in fact reach such a judgment, but spontaneous skillful action without conscious judgment is sometimes thought to be characteristic of \"flow\" states of peak performance; and Heidegger seems to have valued them and regarded them as prevalent; and perhaps certain types of meditative practice aim at them.  Suspension of judgment seems consistent with action, perhaps even highly skilled action.  Though suspension of judgment isn\\'t suspension of the entirety of the dispositional profile characteristic of belief, it\\'s suspension of an important part of the profile -- perhaps enough so that the skeptic achieves what I call a state of in-between believing, in which there\\'s enough deviation from the relevant dispositional profile that it\\'s neither quite right to say he believes nor quite right to say he fails to believe.\\n\\nThe skeptic will also, I suggest, stand openly ready, with equanimity, for alternative possibilities.  The skeptic will leap away from the carriage, but she won\\'t be as much surprised as the non-skeptic would be if the carriage suddenly turns into a rooster.  The skeptic will utter affirmations -- Zhuangzi compares our utterances to the cheeping of baby birds -- but with an openness to the opposing view.  The skeptic will be less perturbed by apparent misfortune (for maybe it\\'s really good fortune in disguise) and thus perhaps achieve a certain tranquility unavailable to dogmatists (as emphasized by both Sextus and Zhuangzi).  The skeptic stands humbly aware, before God or the universe, of his flawed, infinitesmal perspective (as expressed by Montaigne).\\n\\nJudgment is stoppered; action still flows; there\\'s a humility, openness, tranquility, lack of surprise.  None of this seems psychologically impossible to me.  In certain moods, I even find it an appealing prospect. \\n  \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset[369][\"content\"]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute tf-idf for each article (against wikipedia corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#Set tokenizers, tagger and stemmer\n",
    "tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "sentTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "tagger = PerceptronTagger()\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_token_list(content):\n",
    "    textList = nltk.word_tokenize(content)\n",
    "    tokenList = []\n",
    "    for token in textList:\n",
    "        try:\n",
    "            thisToken = token\n",
    "            uselessUnicode = [u'\\u2013', u'\\u2014', u'\\u201d', u'\\u201c'] ### don't include these when they are alone\n",
    "            if thisToken not in uselessUnicode:\n",
    "                thisToken = thisToken.replace(u'\\u201d','') # delete this (unicode quote)\n",
    "                thisToken = thisToken.replace(u'\\u201c','') # delete this (unicode quote)\n",
    "                tokenList.append(thisToken)\n",
    "        except:\n",
    "            tokenList.append('**CODEC_ERROR**')\n",
    "            # #######################prints word on CODEC ERROR\n",
    "            print('**CODEC_ERROR**')\n",
    "            print(token) \n",
    "            print('****')\n",
    "    return tokenList\n",
    "\n",
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "import re\n",
    "\n",
    "def cleanTokens(tokenList):\n",
    "    #Convert all text to lower case\n",
    "    textList=[word.lower() for word in tokenList]\n",
    "    \n",
    "    #Remove punctuation\n",
    "    textList=[word for word in textList if word not in punctuation]\n",
    "    textList=[\"\".join(c for c in word if c not in punctuation) for word in textList ]\n",
    "    \n",
    "    #convert digits into NUM\n",
    "    textList=[re.sub(\"\\d+\", \"NUM\", word) for word in textList]  \n",
    "    \n",
    "    #Stem words \n",
    "    textList=[stemmer.stem(word) for word in textList]\n",
    "    \n",
    "    #Remove blanks\n",
    "    textList=[word for word in textList if word!= ' ']\n",
    "    textList=[word for word in textList if word!= '']\n",
    "    \n",
    "    #Extract tokens\n",
    "    return textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_sample_to_tfidf(sample):\n",
    "    tl = to_token_list(sample)\n",
    "    raw_text = ' '.join(tl)\n",
    "    tokens = cleanTokens(tl)\n",
    "    \n",
    "    ## create FreqDF with word frequencies\n",
    "    freq = FreqDist(tokens)\n",
    "    \n",
    "    # convert it to a data frame\n",
    "    freqDF = pd.DataFrame.from_dict(freq, orient='index')\n",
    "    freqDF.columns = ['freq']\n",
    "    \n",
    "    ## merge freqDF with idf data frame\n",
    "    freqit = freqDF.join(idf[['idf', 'logidf']])\n",
    "    \n",
    "    # replace null values with max\n",
    "    maxidf = max(freqit['idf'].dropna())\n",
    "    maxlogidf = max(freqit['logidf'].dropna())\n",
    "    freqit.loc[pd.isnull(freqit['idf']), 'idf'] = maxidf\n",
    "    freqit.loc[pd.isnull(freqit['logidf']), 'logidf'] = maxlogidf\n",
    "    \n",
    "    ## create tfidf columns\n",
    "    freqit['tfidf'] = freqit['freq'] * freqit['idf']\n",
    "    freqit['logtfidf'] = freqit['freq'] * freqit['logidf']\n",
    "    \n",
    "    ## order by logtfidf weight\n",
    "    freqit = freqit.sort_values(by='logtfidf', ascending=False) \n",
    "    \n",
    "    return freqit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "for i,sample in enumerate(dataset):\n",
    "    sample[\"tfidf\"] = from_sample_to_tfidf(sample[\"content\"])\n",
    "    if i % 50 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity between two articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_on_articles(article_1, article_2):\n",
    "    d = {}\n",
    "    try:\n",
    "        w1 = article_1[\"tfidf\"].loc[list(article_2[\"tfidf\"].index)].dropna()\n",
    "        w2 = article_2[\"tfidf\"].loc[list(article_1[\"tfidf\"].index)].dropna()\n",
    "        norm_1 = 0\n",
    "        norm_2 = 0\n",
    "        for i,row in article_1[\"tfidf\"].iterrows():\n",
    "            norm_1 += row[\"logtfidf\"] * row[\"logtfidf\"]\n",
    "        for i,row in article_2[\"tfidf\"].iterrows():\n",
    "            norm_2 += row[\"logtfidf\"] * row[\"logtfidf\"]\n",
    "        cosine_similarity = 0\n",
    "        for i,row in w1.iterrows():\n",
    "            v1 = row[\"logtfidf\"]\n",
    "            v2 = w2.loc[i][\"logtfidf\"]\n",
    "            delta = (v1 * v2) / (math.sqrt(norm_1) * math.sqrt(norm_2))\n",
    "            cosine_similarity += delta\n",
    "            d[i] = delta\n",
    "        for k in d.keys():\n",
    "            d[k] = d[k] / cosine_similarity\n",
    "        return cosine_similarity, d\n",
    "    except: # e.g. the case where no index overlaps\n",
    "        return 0, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute similarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(dataset)\n",
    "sim_matrix = [[0 for j in range(N)] for i in range(N)]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 1674436 similarities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/education/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n",
      "/anaconda3/envs/education/lib/python3.5/site-packages/ipykernel_launcher.py:3: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['memcach', 'tutori', 'pdf', 'page', 'job', 'search', 'resourc', 'guid',\\n       'quick', 'discuss', 'version', 'next', 'previous'],\\n      dtype='object')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-4e43a0e1a147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity_on_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0msim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-fdfc3abcec50>\u001b[0m in \u001b[0;36mcosine_similarity_on_articles\u001b[0;34m(article_1, article_2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_similarity_on_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticle_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcosine_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnorm_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/education/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/education/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot index with multidimensional key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/education/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1203\u001b[0m             \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             keyarr, indexer = self._get_listlike_indexer(key, axis,\n\u001b[0;32m-> 1205\u001b[0;31m                                                          raise_missing=False)\n\u001b[0m\u001b[1;32m   1206\u001b[0m             return self.obj._reindex_with_indexers({axis: [keyarr, indexer]},\n\u001b[1;32m   1207\u001b[0m                                                    copy=True, allow_dups=True)\n",
      "\u001b[0;32m/anaconda3/envs/education/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/education/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['memcach', 'tutori', 'pdf', 'page', 'job', 'search', 'resourc', 'guid',\\n       'quick', 'discuss', 'version', 'next', 'previous'],\\n      dtype='object')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "print(\"Computing {0} similarities.\".format(N*N))\n",
    "for i in range(N-1):\n",
    "    for j in range(i+1, N):\n",
    "        sim, d = cosine_similarity_on_articles(dataset[i], dataset[j])\n",
    "        sim_matrix[i][j] = sim\n",
    "        sim_matrix[j][i] = sim\n",
    "        num_sim = i*N + j\n",
    "        if num_sim % 100 == 0:\n",
    "            print(num_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_article(dataset):\n",
    "    random_index = random.randint(0, len(dataset) - 1)\n",
    "    print(random_index)\n",
    "    return dataset[random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_recommender(dataset, top_n=5):\n",
    "    random_article = get_random_article(dataset)\n",
    "    \n",
    "    print(\"Selected the following article:\")\n",
    "    print(\"Title: {0}\".format(random_article[\"title\"]))\n",
    "    print(\"URL: {0}\".format(random_article[\"url\"]))\n",
    "    \n",
    "    print(\"Computing most similar articles...\")\n",
    "    similarities = []\n",
    "    for i,article in enumerate(dataset):\n",
    "        cos_sim, d = cosine_similarity_on_articles(random_article, article)\n",
    "        d = d.items() # convert to list of tuples (key, value)\n",
    "        d = sorted(d, key=lambda t:t[1], reverse=True)\n",
    "        similarities.append((i, cos_sim, d))\n",
    "    similarities = sorted(similarities, key=lambda t:t[1], reverse=True)\n",
    "    \n",
    "    print(\"The top {0} similar articles are:\".format(top_n))\n",
    "    for i in range(1, top_n+1):\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"Title: {0}\".format(dataset[similarities[i][0]][\"title\"]))\n",
    "        print(\"URL: {0}\".format(dataset[similarities[i][0]][\"url\"]))\n",
    "        print(\"Similarity score: {0}\".format(similarities[i][1]))\n",
    "        print(\"Index: {0}\".format(similarities[i][0]))\n",
    "        print(\"Most important words:\")\n",
    "        for j in range(min(5, len(similarities[i][2]))):\n",
    "            print(\"\\t{0}, with percentage {1}%\".format(similarities[i][2][j][0], similarities[i][2][j][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "Selected the following article:\n",
      "Title: An Evolving Manifesto — Part 1: The Intro\n",
      "URL: https://medium.com/empathic-futures-lab/an-evolving-manifesto-part-1-the-intro-e2640358e2ce\n",
      "Computing most similar articles...\n",
      "The top 10 similar articles are:\n",
      "--------------------------------\n",
      "Title: \n",
      "Two Views of the Relationship Between Philosophy and Science Fiction\n",
      "\n",
      "URL: https://schwitzsplinters.blogspot.com/2014/11/two-views-of-relationship-between.html\n",
      "Similarity score: 0.06358378035196\n",
      "Index: 351\n",
      "Most important words:\n",
      "\tintro, with percentage 0.06358376889835327%\n",
      "\tan, with percentage 1.1453606725156235e-08%\n",
      "\tthe, with percentage 0.0%\n",
      "--------------------------------\n",
      "Title: Mmm… Futuricious…\n",
      "URL: http://www.thehistoryblog.com/archives/292\n",
      "Similarity score: 0.04926920057803024\n",
      "Index: 463\n",
      "Most important words:\n",
      "\tmanifesto, with percentage 0.04926919460589389%\n",
      "\tan, with percentage 4.480160695100559e-09%\n",
      "\tnum, with percentage 1.4919756537115058e-09%\n",
      "\tthe, with percentage 0.0%\n",
      "--------------------------------\n",
      "Title: I created the exact same app in React and Vue. Here are the differences.\n",
      "URL: https://medium.com/javascript-in-plain-english/i-created-the-exact-same-app-in-react-and-vue-here-are-the-differences-e9a1ae8077fd\n",
      "Similarity score: 0.0414630513478217\n",
      "Index: 134\n",
      "Most important words:\n",
      "\tintro, with percentage 0.04146304294618127%\n",
      "\tan, with percentage 7.468909059055838e-09%\n",
      "\tnum, with percentage 9.327313712276202e-10%\n",
      "\tthe, with percentage 0.0%\n",
      "--------------------------------\n",
      "Title: Another 10 Free Must-Read Books for Machine Learning and Data Science\n",
      "URL: https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\n",
      "Similarity score: 0.02872219657930521\n",
      "Index: 1184\n",
      "Most important words:\n",
      "\tintro, with percentage 0.028722179770421224%\n",
      "\tan, with percentage 1.0347689577997407e-08%\n",
      "\tnum, with percentage 6.461194407917559e-09%\n",
      "\tthe, with percentage 0.0%\n",
      "--------------------------------\n",
      "Title: 96% of Companies Are Failing Miserably When it Comes to Marketing Data Insights\n",
      "URL: https://www.smartdatacollective.com/96-companies-are-failing-miserably-when-it-comes-marketing-data-insights\n",
      "Similarity score: 0.024257316323025364\n",
      "Index: 1250\n",
      "Most important words:\n",
      "\tintro, with percentage 0.024257303218450144%\n",
      "\tan, with percentage 8.739136295023557e-09%\n",
      "\tnum, with percentage 4.3654389230615744e-09%\n",
      "\tthe, with percentage 0.0%\n",
      "--------------------------------\n",
      "Title: \n",
      "What Experimental Philosophy Might Be\n",
      "\n",
      "URL: https://schwitzsplinters.blogspot.com/2012/09\n",
      "Similarity score: 0.016835856469948212\n",
      "Index: 364\n",
      "Most important words:\n",
      "\tmanifesto, with percentage 0.01683584530787972%\n",
      "\tan, with percentage 1.0206146460007663e-08%\n",
      "\tnum, with percentage 9.559220336587144e-10%\n",
      "\tthe, with percentage 0.0%\n",
      "--------------------------------\n",
      "Title: Houdini wishes you a Happy Halloween\n",
      "URL: http://www.thehistoryblog.com/archives/date/2010/10\n",
      "Similarity score: 0.015953184558825762\n",
      "Index: 589\n",
      "Most important words:\n",
      "\tintro, with percentage 0.015953181686471123%\n",
      "\tan, with percentage 1.4368562309031132e-09%\n",
      "\tnum, with percentage 1.4354984075027725e-09%\n",
      "\tthe, with percentage 0.0%\n",
      "--------------------------------\n",
      "Title: \n",
      "Please hand me your final product\n",
      "\n",
      "URL: http://www.chemistry-blog.com/2008/07/06/please-hand-me-your-final-product\n",
      "Similarity score: 0.012983552393569821\n",
      "Index: 1025\n",
      "Most important words:\n",
      "\tevolv, with percentage 0.012982829927141535%\n",
      "\tpart, with percentage 7.153535684926068e-07%\n",
      "\tan, with percentage 5.059414846034525e-09%\n",
      "\tnum, with percentage 2.053444948428661e-09%\n",
      "\tthe, with percentage 0.0%\n",
      "--------------------------------\n",
      "Title: Hadoop Toolbox: When to Use What\n",
      "URL: https://www.smartdatacollective.com/hadoop-toolbox-when-use-what\n",
      "Similarity score: 0.010204932825654005\n",
      "Index: 1280\n",
      "Most important words:\n",
      "\tintro, with percentage 0.00901279380137031%\n",
      "\tevolv, with percentage 0.0011903000943494478%\n",
      "\tpart, with percentage 1.836394060136029e-06%\n",
      "\tan, with percentage 1.6235117465409462e-09%\n",
      "\tnum, with percentage 9.123623631133357e-10%\n",
      "--------------------------------\n",
      "Title: \n",
      "New Paper in Draft: If Materialism Is True, the United States Is Probably Conscious\n",
      "\n",
      "URL: https://schwitzsplinters.blogspot.com/2012/06\n",
      "Similarity score: 0.00922738041092157\n",
      "Index: 295\n",
      "Most important words:\n",
      "\tevolv, with percentage 0.009227374118075614%\n",
      "\tan, with percentage 6.29284595604914e-09%\n",
      "\tthe, with percentage 0.0%\n"
     ]
    }
   ],
   "source": [
    "start_recommender(dataset, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract content headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = [\"h1\", \"h2\", \"h3\"]\n",
    "\n",
    "for d in dataset:\n",
    "    headers = []\n",
    "    soup = BeautifulSoup(d[\"content_html\"])\n",
    "    for h in hs:\n",
    "        headers += [el.text for el in soup.select(h)]\n",
    "    d[\"headers\"] = headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average number of headers per article: {0}\".format(sum([len(d[\"headers\"]) for d in dataset]) / len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    cleantext = re.sub(\"(<!--.*?-->)\", \"\", cleantext, flags=re.DOTALL)\n",
    "    return cleantext\n",
    "\n",
    "def remove_newlines(content):\n",
    "    return content.replace(\"\\n\", \" \")\n",
    "\n",
    "def remove_white_spaces(content):\n",
    "    content = re.sub(' +', ' ', content)\n",
    "    content = content.strip()\n",
    "    return content\n",
    "\n",
    "def remove_urls(content):\n",
    "    content = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE)\n",
    "    return content\n",
    "\n",
    "def remove_code(content):\n",
    "    content = re.sub(r'(\\w+(\\.\\w+)*\\([^\\)]*\\))', '', content, flags=re.MULTILINE) # matches a.b.c(d)\n",
    "    return content\n",
    "\n",
    "def remove_alt_html(content):\n",
    "    content = content.split(\"&lt\")[0]\n",
    "    return content\n",
    "\n",
    "def clean_content(content):\n",
    "    content = clean_html(content)\n",
    "    content = remove_newlines(content)\n",
    "    content = remove_white_spaces(content)\n",
    "    content = remove_urls(content)\n",
    "    content = remove_code(content)\n",
    "    content = remove_alt_html(content)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bringing VR and AR to the Enterprise: A Conversation with Handsome’s New EVP HandsomeBlockedUnblockFollowFollowingOct 24, 2018\n"
     ]
    }
   ],
   "source": [
    "print(clean_content(dataset[155][\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://medium.com/handsome-perspectives/bringing-vr-and-ar-to-the-enterprise-a-conversation-with-handsomes-new-evp-7dc5ed4dc2f3'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[155][\"url\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
