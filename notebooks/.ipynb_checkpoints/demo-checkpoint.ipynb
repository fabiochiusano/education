{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from preprocessed_data_reader.ipynb\n",
      "Importing Jupyter notebook from utils_os.ipynb\n",
      "Importing Jupyter notebook from recommender.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0430 17:48:43.794015 4478801344 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "from preprocessed_data_reader import ReaderPreprocessedData\n",
    "from recommender import UserState, RecommenderTFIDF, RecommenderBERT\n",
    "from utils_os import UtilsOS\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_article(dataset):\n",
    "    random_index = random.randint(0, len(dataset) - 1)\n",
    "    return dataset[random_index]\n",
    "\"\"\"\n",
    "def retrieve_best_unstemmed_word(stem_word, stem_dictionary):\n",
    "    d = stem_dictionary[stem_word]\n",
    "    return max(d, key=d.get)\n",
    "\"\"\"\n",
    "def interactive_demo(dataset, stem_dictionary, top_n=10, max_words=10, max_sim_explained=0.9, idf_threshold=2.5):\n",
    "    counter = 0\n",
    "    recommender = RecommenderTFIDF(dataset)\n",
    "    user_state = UserState()\n",
    "    while True:\n",
    "        if counter == 0:\n",
    "            print(\"Titles:\")\n",
    "            articles = [get_random_article(dataset) for j in range(top_n)]\n",
    "            for j,article in enumerate(articles):\n",
    "                print(\"{0} - Title: {1}\".format(j, article[\"title\"]))\n",
    "                print(\"URL: {1}\".format(j, article[\"url\"]))\n",
    "                print(\"--------------------------------\")\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        if counter == 0:\n",
    "            time_to_read = int(input(\"Indicate available time [min]: \"))\n",
    "            time_to_read *= 60\n",
    "        chosen_index = int(input(\"Choose an article: \"))\n",
    "        user_state.add_read_article(chosen_index)\n",
    "        \n",
    "        if counter == 0:\n",
    "            chosen_article = articles[chosen_index]\n",
    "        else:\n",
    "            #chosen_article = dataset[similarities[i].index]\n",
    "            chosen_article = dataset[similarities[i]]\n",
    "\n",
    "        similarities = recommender.recommend_articles(user_state, how_many=top_n)\n",
    "        #similarities = [sim for sim in similarities if dataset[sim[0]][\"read_time\"] < time_to_read]\n",
    "        similarities = [sim for sim in similarities if dataset[sim][\"read_time\"] < time_to_read]\n",
    "\n",
    "        print(\"The top {0} similar articles are:\".format(top_n))\n",
    "        for i in range(1, top_n+1): # i == 0 corresponds to the same article\n",
    "            print(\"--------------------------------\")\n",
    "            #print(\"{0} - Title: {1}\".format(i - 1, dataset[similarities[i].index][\"title\"]))\n",
    "            print(\"{0} - Title: {1}\".format(i - 1, dataset[similarities[i]][\"title\"]))\n",
    "            #print(\"URL: {0}\".format(dataset[similarities[i].index][\"url\"]))\n",
    "            print(\"URL: {0}\".format(dataset[similarities[i]][\"url\"]))\n",
    "            #print(\"Similarity score: {0:.2f}%\".format(similarities[i].similarity * 100))\n",
    "            print(\"Time to read [min]: {0:.2f}\".format(dataset[similarities[i]][\"read_time\"] / 60))\n",
    "            #print(\"Time to read [min]: {0:.2f}\".format(dataset[similarities[i].index][\"read_time\"] / 60))\n",
    "            \"\"\"\n",
    "            print(\"Most important words:\")\n",
    "            sim_tot = 0\n",
    "            for j in range(min(max_words, len(similarities[i].words_importance_list))):\n",
    "                sw = similarities[i].words_importance_list[j].word\n",
    "                w = retrieve_best_unstemmed_word(sw, stem_dictionary)\n",
    "                score = similarities[i].words_importance_list[j].perc_sim\n",
    "                idf_score = similarities[i].words_importance_list[j].idf\n",
    "                if idf_score >= idf_threshold:\n",
    "                    print(\"\\t{0}, with percentage {1:.2f}%\".format(w.capitalize(), score * 100))\n",
    "                sim_tot += score\n",
    "                if sim_tot >= max_sim_explained:\n",
    "                    break\n",
    "            \"\"\"\n",
    "                    \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = ReaderPreprocessedData.read_data(\"../preprocessed\")\n",
    "    print(\"Read {0} articles\".format(len(dataset)))\n",
    "    \n",
    "    stem_dictionary = UtilsOS.read_json('json/stem_dictionary.json')\n",
    "    \n",
    "    interactive_demo(dataset, stem_dictionary, top_n=10, max_words=5, max_sim_explained=0.9, idf_threshold=2.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
