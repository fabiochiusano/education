{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from utils_os.ipynb\n",
      "Importing Jupyter notebook from scraper_data_reader.ipynb\n",
      "Importing Jupyter notebook from scraper_config_reader.ipynb\n",
      "Importing Jupyter notebook from constants.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "from utils_os import UtilsOS\n",
    "from scraper_data_reader import ReaderScrapedData\n",
    "from constants import Constants\n",
    "\n",
    "import pycountry\n",
    "import itertools\n",
    "from alphabet_detector import AlphabetDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLCleaner:\n",
    "    def _contains_lang_code(self, url):\n",
    "        \"\"\"Returns true if the url contains a language code\"\"\"\n",
    "        return any(t in self._lang_codes for t in url)\n",
    "\n",
    "    def _contains_website_name(self, url, website):\n",
    "        \"\"\"Returns true if the url contains the website name\"\"\"\n",
    "        return any(website in t for t in url)\n",
    "\n",
    "    def _contains_banned_extension(self, url):\n",
    "        \"\"\"Returns true if the url does not end with a banned extension\"\"\"\n",
    "        return any(extension in url[-1] for extension in self._extensions_banned if len(url) > 0)\n",
    "\n",
    "    def _contains_banned_word(self, url):\n",
    "        \"\"\"Returns true if the url does not contain a banned word\"\"\"\n",
    "        return any(word in url for word in self._banned_words)\n",
    "    \n",
    "    def _contains_foreign_alphabet(self, url):\n",
    "        \"\"\"Returns true if the url contains a character that is not LATIN or bad encoded strings (signaled by a lot of % chars)\"\"\"\n",
    "        has_not_latin = not self._ad.only_alphabet_chars(\"\".join(url), \"LATIN\")\n",
    "        has_bad_encoding = \"\".join(url).count(\"%\") >= 3\n",
    "        return has_not_latin or has_bad_encoding \n",
    "    \n",
    "    def _website_part_is_ok(self, url):\n",
    "        \"\"\"Returns true if the website part is something like www.xxxxxxxx.aaa where www and aaa must be long at least 3 characters\"\"\"\n",
    "        website_part = url[2]\n",
    "        ok_start = len(website_part.split(\".\")[0]) >= 3\n",
    "        ok_end = len(website_part.split(\".\")[-1]) >= 3\n",
    "        return ok_start and ok_end\n",
    "    \n",
    "    def _does_end_with_number(self, url):\n",
    "        return url[-1].isdigit()\n",
    "    \n",
    "    def _ends_with_something_not_allowed(self, url):\n",
    "        return any(end in url[-1] for end in self._not_allowed_endings if len(url) > 0)\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._banned_words = set([\"video\", \"page\", \"live\", \"about\", \"search\", \"contact-us\",\n",
    "                        \"terms\", \"author\", \"login\", \"submit\", \"sgs\",\n",
    "                        \"signin\", \"policy\", \"comments\", \"register\"])\n",
    "        self._extensions_banned = set([\".jpg\", \".png\", \".zip\", \".xml\", \".jpeg\"])\n",
    "        \n",
    "        self._lang_codes = [lang.alpha_3.lower()[:2] for lang in pycountry.languages]\n",
    "        self._lang_codes = [t[0] + t[1] + t[2]\n",
    "                           for t in zip(self._lang_codes, [\"-\" for i in range(len(self._lang_codes))], self._lang_codes)]\n",
    "        self._lang_codes = list(set(self._lang_codes))\n",
    "        self._lang_codes.remove(\"en-en\")\n",
    "        \n",
    "        self._not_allowed_endings = set([\"mailto\"])\n",
    "        \n",
    "        self._ad = AlphabetDetector()\n",
    "\n",
    "    def filter_urls(self, urls, website):\n",
    "        urls = [url.split(\"/\") for url in urls]\n",
    "        \n",
    "        urls = [url for url in urls if not self._contains_lang_code(url)]\n",
    "        urls = [url for url in urls if self._contains_website_name(url, website)]\n",
    "        urls = [url for url in urls if not self._contains_banned_extension(url)]\n",
    "        urls = [url for url in urls if not self._contains_banned_word(url)]\n",
    "        urls = [url for url in urls if not self._contains_foreign_alphabet(url)]\n",
    "        urls = [url for url in urls if not self._does_end_with_number(url)]\n",
    "        urls = [url for url in urls if self._website_part_is_ok(url)]\n",
    "        urls = [url for url in urls if not self._ends_with_something_not_allowed(url)]\n",
    "        \n",
    "        return [\"/\".join(url) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.bigdatanews.datasciencecentral.com/']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_cleaner = URLCleaner()\n",
    "url_cleaner.filter_urls([\"https://www.bigdatanews.datasciencecentral.com/\"], \"datasciencecentral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs without article: 20\n",
      "\tFiltered: 2 --> 10.00% (better close to 100%)\n",
      "URLs with article: 32\n",
      "\tFiltered: 2 --> 6.25% (better close to 0%, but I may be removing wrong articles so it's ok)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # get urls that are not articles\n",
    "    url_not_article = UtilsOS.read_json(Constants.path_to_url_not_article)\n",
    "\n",
    "    # get urls that are articles\n",
    "    data = ReaderScrapedData.read_data(Constants.path_to_scraper_config, Constants.path_to_articles)\n",
    "    dataset_nested = [data[website][domain] for website in data.keys() for domain in data[website].keys()]\n",
    "    dataset = [el for subl in dataset_nested for el in subl]\n",
    "    url_article = [sample[\"url\"] for sample in dataset]\n",
    "    \n",
    "    url_cleaner = URLCleaner()\n",
    "\n",
    "    num_filtered = len(url_not_article) - len(url_cleaner.filter_urls(url_not_article, \"\"))\n",
    "    print(\"URLs without article: {0}\".format(len(url_not_article)))\n",
    "    print(\"\\tFiltered: {0} --> {1:.2f}% (better close to 100%)\".format(num_filtered, num_filtered / len(url_not_article) * 100))\n",
    "\n",
    "    num_filtered = len(url_article) - len(url_cleaner.filter_urls(url_article, \"\"))\n",
    "    print(\"URLs with article: {0}\".format(len(url_article)))\n",
    "    print(\"\\tFiltered: {0} --> {1:.2f}% (better close to 0%, but I may be removing wrong articles so it's ok)\".format(num_filtered, num_filtered / len(url_article) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
