{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from utils_os.ipynb\n",
      "Importing Jupyter notebook from scraper_data_reader.ipynb\n",
      "Importing Jupyter notebook from scraper_config_reader.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "from utils_os import UtilsOS\n",
    "from scraper_data_reader import ReaderScrapedData\n",
    "\n",
    "import pycountry\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URLCleaner:\n",
    "    def _contains_lang_code(self, url):\n",
    "        \"\"\"Returns true if the url contains a language code\"\"\"\n",
    "        return any(t in self._lang_codes for t in url)\n",
    "\n",
    "    def _contains_website_name(self, url, website):\n",
    "        \"\"\"Returns true if the url contains the website name\"\"\"\n",
    "        return any(website in t for t in url)\n",
    "\n",
    "    def _contains_banned_extension(self, url):\n",
    "        \"\"\"Returns true if the url does not end with a banned extension\"\"\"\n",
    "        return any(extension in url[-1] for extension in self._extensions_banned if len(url) > 0)\n",
    "\n",
    "    def _contains_banned_word(self, url):\n",
    "        \"\"\"Returns true if the url does not end with a banned extension\"\"\"\n",
    "        return any(word in url for word in self._banned_words)\n",
    "    \n",
    "    def _does_end_with_number(self, url):\n",
    "        return url[-1].isdigit()\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._banned_words = set([\"category\", \"video\", \"page\", \"live\", \"about\", \"search\", \"contact-us\",\n",
    "                        \"terms\", \"author\", \"login\", \"submit\", \"sgs\", \"tagged\", \"tag\", \"feeds\",\n",
    "                        \"topic\", \"topics\", \"signin\", \"policy\", \"comments\", \"feed\"])\n",
    "        self._extensions_banned = set([\".jpg\", \".png\", \".zip\", \".xml\"])\n",
    "        \n",
    "        self._lang_codes = [lang.alpha_3.lower()[:2] for lang in pycountry.languages]\n",
    "        self._lang_codes = [t[0] + t[1] + t[2]\n",
    "                           for t in zip(self._lang_codes, [\"-\" for i in range(len(self._lang_codes))], self._lang_codes)]\n",
    "        self._lang_codes = list(set(self._lang_codes))\n",
    "        self._lang_codes.remove(\"en-en\")\n",
    "\n",
    "    def filter_urls(self, urls, website):\n",
    "        urls = [url.split(\"/\") for url in urls]\n",
    "        \n",
    "        urls = [url for url in urls if not self._contains_lang_code(url)]\n",
    "        urls = [url for url in urls if self._contains_website_name(url, website)]\n",
    "        urls = [url for url in urls if not self._contains_banned_extension(url)]\n",
    "        urls = [url for url in urls if not self._contains_banned_word(url)]\n",
    "        urls = [url for url in urls if not self._does_end_with_number(url)]\n",
    "        \n",
    "        return [\"/\".join(url) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs without article: 322\n",
      "\tFiltered: 131 --> 40.68%\n",
      "URLs with article: 2971\n",
      "\tFiltered: 74 --> 2.49%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # get urls that are not articles\n",
    "    url_not_article = UtilsOS.read_json(\"url_not_article.json\")\n",
    "\n",
    "    # get urls that are articles\n",
    "    data = ReaderScrapedData.read_data(\"scraper_configs.json\")\n",
    "    dataset_nested = [data[website][domain] for website in data.keys() for domain in data[website].keys()]\n",
    "    dataset = [el for subl in dataset_nested for el in subl]\n",
    "    url_article = [sample[\"url\"] for sample in dataset]\n",
    "    \n",
    "    url_cleaner = URLCleaner()\n",
    "\n",
    "    num_filtered = len(url_not_article) - len(url_cleaner.filter_urls(url_not_article, \"\"))\n",
    "    print(\"URLs without article: {0}\".format(len(url_not_article)))\n",
    "    print(\"\\tFiltered: {0} --> {1:.2f}%\".format(num_filtered, num_filtered / len(url_not_article) * 100))\n",
    "\n",
    "\n",
    "    num_filtered = len(url_article) - len(url_cleaner.filter_urls(url_article, \"\"))\n",
    "    print(\"URLs with article: {0}\".format(len(url_article)))\n",
    "    print(\"\\tFiltered: {0} --> {1:.2f}%\".format(num_filtered, num_filtered / len(url_article) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
