{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from scraper_config_reader.ipynb\n",
      "Importing Jupyter notebook from scraper_requests.ipynb\n",
      "Importing Jupyter notebook from scraper_data_reader.ipynb\n",
      "Importing Jupyter notebook from utils_os.ipynb\n",
      "Importing Jupyter notebook from model_article_url_discriminator.ipynb\n",
      "Importing Jupyter notebook from constants.ipynb\n",
      "Importing Jupyter notebook from utils_soup.ipynb\n"
     ]
    }
   ],
   "source": [
    "import nbimporter\n",
    "\n",
    "from scraper_config_reader import ScraperConfigReader\n",
    "from scraper_requests import ScraperRequests\n",
    "from scraper_data_reader import ReaderScrapedData\n",
    "from model_article_url_discriminator import URLCleaner\n",
    "from utils_soup import UtilsSoup\n",
    "from utils_os import UtilsOS\n",
    "from constants import Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from alphabet_detector import AlphabetDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    \"\"\"\"\"\"\n",
    "    _extensions_banned = [\".jpg\", \".png\", \".zip\", \".xml\"]\n",
    "    \n",
    "    def _manage_article_directory(self, directory, start_from_zero):\n",
    "        exists_directory = UtilsOS.directory_exists(directory)\n",
    "        if start_from_zero and exists_directory:\n",
    "            UtilsOS.directory_remove(directory)\n",
    "        UtilsOS.directory_maybe_create(directory)\n",
    "    \n",
    "    def _check_link_extension(self, link):\n",
    "        for extension in self._extensions_banned:\n",
    "            if link[-len(extension):] == extension:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _normalize_url(self, url):\n",
    "        url = url.split(\"?\")[0]\n",
    "        url = url.split(\"#\")[0]\n",
    "        url = url.split(\"&\")[0]\n",
    "        url = url.split(\"@\")[0]\n",
    "        if url[-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "        return url\n",
    "            \n",
    "    def _clean_links(self, website, links, prefix_url):\n",
    "        links = [prefix_url + link if len(link) > 0 and link[0] == \"/\" else link for link in links] # fix \"/index.html\"\n",
    "        links = [prefix_url + \"/\" + link if \"//\" not in link else link for link in links] # fix \"index.html\"\n",
    "        links = [self._normalize_url(link) for link in links] # remove http parameters (after ?)\n",
    "        links = list(set(links)) # remove duplicates\n",
    "        return links\n",
    "    \n",
    "    def _ok_title(self, title, ad):\n",
    "        num_words = len(title.split(\" \"))\n",
    "        ok_num_words = num_words >= 2 and num_words <= 20\n",
    "        ok_alphabet = ad.only_alphabet_chars(title, \"LATIN\")\n",
    "        return ok_num_words and ok_alphabet\n",
    "\n",
    "    def _ok_content(self, content, ad):\n",
    "        num_words = len(content.split(\" \"))\n",
    "        ok_num_words = num_words >= 100\n",
    "        ok_alphabet = ad.only_alphabet_chars(content, \"LATIN\")\n",
    "        return ok_num_words and ok_alphabet\n",
    "    \n",
    "    def _get_random_url_by_domain(self, data):\n",
    "        # return one url for each domain\n",
    "        res = [] # res = []\n",
    "        domain_urls = [data[dom] for dom in data.keys()]\n",
    "        for urls in domain_urls:\n",
    "            res.append(random.choice(urls)[\"url\"])\n",
    "        return res\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def scrape_website_incremental(self, website, config, path_to_articles, path_to_url_not_article,\n",
    "                                   max_scraped=-1, max_tries=-1, start_from_zero=False):\n",
    "        \"\"\"Incremental scraping of a website, according to a site configuration\"\"\"\n",
    "        directory = path_to_articles + \"/\" + website\n",
    "\n",
    "        # eventually clean \"article\" directory\n",
    "        self._manage_article_directory(directory, start_from_zero)\n",
    "\n",
    "        # create request handler, url cleaner and alphabet detector\n",
    "        scraper_requests = ScraperRequests()\n",
    "        url_cleaner = URLCleaner()\n",
    "        ad = AlphabetDetector()\n",
    "\n",
    "        # parse already scraped data\n",
    "        scraped_data = ReaderScrapedData.read_data_of_website(website, path_to_articles)\n",
    "        titles = set(ReaderScrapedData.get_titles(scraped_data))\n",
    "        urls_by_domain = self._get_random_url_by_domain(scraped_data) # get one url for each domain of the selected website\n",
    "        urls = ReaderScrapedData.get_urls(scraped_data)\n",
    "        if not UtilsOS.file_exists(path_to_url_not_article):\n",
    "            UtilsOS.write_to_json([], path_to_url_not_article)\n",
    "        url_not_article = UtilsOS.read_json(path_to_url_not_article) # list of strings\n",
    "            \n",
    "\n",
    "        # decides from which url we start scraping\n",
    "        first_url = config[ScraperConfigReader.first_url_key][0]\n",
    "        if len(urls_by_domain) > 0:\n",
    "            queue = urls_by_domain\n",
    "        else:\n",
    "            queue = [first_url]\n",
    "\n",
    "        # extract prefix url\n",
    "        prefix_url = \"/\".join(first_url.split(\"/\")[:3])\n",
    "\n",
    "        # create the url black list\n",
    "        already_considered = set(urls)\n",
    "        already_considered.add(first_url)\n",
    "\n",
    "        counter = 0\n",
    "        counter_added = 0\n",
    "        counter_delta_incremental = len(urls)\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            # get url to visit\n",
    "            url = queue.pop(0)\n",
    "            \n",
    "            # if we already know that this link does not correspond to an article, we don't visit it\n",
    "            if url in url_not_article and not len(queue) == 0:\n",
    "                continue\n",
    "            \n",
    "            print(\"Visiting \" + url)\n",
    "            print(\"URLs in queue: {0}\".format(len(queue)))\n",
    "\n",
    "            # visit url\n",
    "            try:\n",
    "                # make request\n",
    "                response = scraper_requests.make_get(url)\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # get all outer links from url, clean them and add to queue\n",
    "                links = soup.find_all(\"a\")\n",
    "                print(len(links))\n",
    "                links = [tag[\"href\"] for tag in links if tag.has_attr(\"href\")]\n",
    "                links = self._clean_links(website, links, prefix_url)\n",
    "                links = url_cleaner.filter_urls(links, website)\n",
    "                print(len(links))\n",
    "                for link in links:\n",
    "                    if link not in already_considered:\n",
    "                        already_considered.add(link)\n",
    "                        queue.append(link)\n",
    "\n",
    "                # fill data\n",
    "                data = {\"url\": url, \"html\": str(soup)}\n",
    "                data[\"title_html\"], data[\"title\"] = UtilsSoup.get_with_selector(config[ScraperConfigReader.title_selector_key], soup)\n",
    "                data[\"content_html\"], data[\"content\"] = UtilsSoup.get_with_selector(config[ScraperConfigReader.content_selector_key], soup)\n",
    "                data[\"timestamp_scraper\"] = datetime.today().timestamp()\n",
    "                data[\"website\"] = website\n",
    "                \n",
    "                ok_title = self._ok_title(data[\"title\"], ad)\n",
    "                ok_content = self._ok_content(data[\"content\"], ad)\n",
    "\n",
    "                # eventually save article\n",
    "                if ok_title and ok_content: # we save only if we got the necessary info\n",
    "                    if data[\"title\"] not in titles:\n",
    "                        titles.add(data[\"title\"])\n",
    "                        counter_added += 1\n",
    "                        print(\"{0} - Extracted article: \".format(counter_delta_incremental + counter_added) + data[\"title\"])\n",
    "\n",
    "                        # eventually create domain directory\n",
    "                        sub_directory = directory + \"/\" + url.split(\"/\")[2]\n",
    "                        UtilsOS.directory_maybe_create(sub_directory)\n",
    "\n",
    "                        # write to file\n",
    "                        UtilsOS.write_to_json(data, sub_directory + \"/\" + str(counter_delta_incremental + counter_added) + '.json')\n",
    "\n",
    "                        # eventually end scraping\n",
    "                        if counter_added == max_scraped:\n",
    "                            break\n",
    "                    else:\n",
    "                        print(\"Article already extracted: {0}\".format(data[\"title\"]))\n",
    "                else:\n",
    "                    url_not_article.append(data[\"url\"])\n",
    "                    UtilsOS.write_to_json(url_not_article, path_to_url_not_article)\n",
    "                counter += 1\n",
    "                if counter == max_tries:\n",
    "                    break\n",
    "                \n",
    "                # sleep...\n",
    "                time.sleep(0.2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"------------\")\n",
    "                time.sleep(1) # time to escape by KeywordInterrupt\n",
    "                continue\n",
    "                \n",
    "            # shuffle queue\n",
    "            random.shuffle(queue)\n",
    "\n",
    "            print(\"------------\")\n",
    "            \n",
    "    def incremental_scraping_from_configs(self, configs, path_to_articles, path_to_url_not_article, max_scraped=10, max_tries=30):\n",
    "        items = list(configs.items())\n",
    "        random.shuffle(items)\n",
    "        while True:\n",
    "            for website, conf in items:\n",
    "                self.scrape_website_incremental(website, conf, path_to_articles, path_to_url_not_article, max_scraped=max_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = ScraperConfigReader.get_configs(Constants.path_to_scraper_config, need_javascript=False)\n",
    "    scraper = Scraper()\n",
    "    scraper.incremental_scraping_from_configs(configs, Constants.path_to_articles, Constants.path_to_url_not_article,\n",
    "                                              max_tries=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = ScraperConfigReader.get_configs(Constants.path_to_scraper_config, need_javascript=False)\n",
    "    scraper = Scraper()\n",
    "    website = \"techcrunch\"\n",
    "    scraper.scrape_website_incremental(website, configs[website], Constants.path_to_articles, Constants.path_to_url_not_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.vox.com/2019/5/3/18307660/climate-change-green-new-deal-bill-mckibben-falter4\"\n",
    "\n",
    "configs = ScraperConfigReader.get_configs(Constants.path_to_scraper_config, need_javascript=False)\n",
    "website = \"vox\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "title = UtilsSoup.get_with_selector(configs[website][ScraperConfigReader.title_selector_key], soup)[1]\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UtilsSoup.get_with_selector(configs[website][ScraperConfigReader.content_selector_key], soup)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
