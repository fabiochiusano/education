{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "\n",
    "from scraper_config_reader import ScraperConfigReader\n",
    "from scraper_requests import ScraperRequests\n",
    "from scraper_data_reader import ReaderScrapedData\n",
    "from utils_soup import UtilsSoup\n",
    "from utils_os import UtilsOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    \"\"\"\"\"\"\n",
    "    _extensions_banned = [\".jpg\", \".png\", \".zip\"]\n",
    "    \n",
    "    def _manage_article_directory(self, directory, start_from_zero):\n",
    "        exists_directory = UtilsOS.directory_exists(directory)\n",
    "        if start_from_zero and exists_directory:\n",
    "            UtilsOS.directory_remove(directory)\n",
    "        UtilsOS.directory_maybe_create(directory)\n",
    "    \n",
    "    def _check_link_extension(self, link):\n",
    "        for extension in self._extensions_banned:\n",
    "            if link[-len(extension):] == extension:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _normalize_url(self, url):\n",
    "        url = url.split(\"?\")[0]\n",
    "        url = url.split(\"#\")[0]\n",
    "        url = url.split(\"&\")[0]\n",
    "        url = url.split(\"@\")[0]\n",
    "        if url[-1] == \"/\":\n",
    "            url = url[:-1]\n",
    "        return url\n",
    "            \n",
    "    def _clean_links(self, website, links, prefix_url):\n",
    "        links = [prefix_url + link if len(link) > 0 and link[0] == \"/\" else link for link in links] # fix \"/index.html\"\n",
    "        links = [prefix_url + \"/\" + link if \"//\" not in link else link for link in links] # fix \"index.html\"\n",
    "        links = [link for link in links if website in link] # must contain the name of the website\n",
    "        links = [link for link in links if self._check_link_extension(link)] # remove links that end with banned extension\n",
    "        links = [self._normalize_url(link) for link in links] # remove http parameters (after ?)\n",
    "        links = list(set(links)) # remove duplicates\n",
    "        return links\n",
    "    \n",
    "    def _get_random_url_by_domain(self, data):\n",
    "        # return one url for each domain\n",
    "        res = [] # res = []\n",
    "        domain_urls = [data[dom] for dom in data.keys()]\n",
    "        for urls in domain_urls:\n",
    "            res.append(random.choice(urls)[\"url\"])\n",
    "        return res\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def scrape_website_incremental(self, website, config, max_scraped=-1, start_from_zero=False):\n",
    "        \"\"\"Incremental scraping of a website, according to a site configuration\"\"\"\n",
    "        directory = \"../articles/\" + website\n",
    "\n",
    "        # eventually clean \"article\" directory\n",
    "        self._manage_article_directory(directory, start_from_zero)\n",
    "\n",
    "        # create request handler and scraped data reader\n",
    "        scraper_requests = ScraperRequests()\n",
    "\n",
    "        # parse already scraped data\n",
    "        scraped_data = ReaderScrapedData.read_data_of_website(website)\n",
    "        titles = set(ReaderScrapedData.get_titles(scraped_data))\n",
    "        urls_by_domain = self._get_random_url_by_domain(scraped_data) # get one url for each domain of the selected website\n",
    "        urls = ReaderScrapedData.get_urls(scraped_data)\n",
    "\n",
    "        # decides from which url we start scraping\n",
    "        first_url = config[ScraperConfigReader.first_url_key][0]\n",
    "        if len(urls_by_domain) > 0:\n",
    "            queue = urls_by_domain\n",
    "        else:\n",
    "            queue = [first_url]\n",
    "\n",
    "        # extract prefix url\n",
    "        prefix_url = \"/\".join(first_url.split(\"/\")[:3])\n",
    "\n",
    "        # create the url black list\n",
    "        already_considered = set(urls)\n",
    "        already_considered.add(first_url)\n",
    "\n",
    "        counter = 0\n",
    "        counter_delta_incremental = len(urls)\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            # get url to visit\n",
    "            url = queue.pop(0)\n",
    "            print(\"Visiting \" + url)\n",
    "            print(\"URLs in queue: {0}\".format(len(queue)))\n",
    "\n",
    "            # visit url\n",
    "            try:\n",
    "                # make request\n",
    "                response = scraper_requests.make_get(url)\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # get all outer links from url\n",
    "                links = soup.find_all(\"a\")\n",
    "                links = [tag[\"href\"] for tag in links if tag.has_attr(\"href\")]\n",
    "\n",
    "                # clean links\n",
    "                links = self._clean_links(website, links, prefix_url)\n",
    "\n",
    "                # add links to queue if not already considered\n",
    "                for link in links:\n",
    "                    if link not in already_considered:\n",
    "                        already_considered.add(link)\n",
    "                        queue.append(link)\n",
    "\n",
    "                # fill data\n",
    "                data = {\"url\": url, \"html\": str(soup)}\n",
    "                data[\"title_html\"], data[\"title\"] = UtilsSoup.get_with_selector(config[ScraperConfigReader.title_selector_key], soup)\n",
    "                data[\"content_html\"], data[\"content\"] = UtilsSoup.get_with_selector(config[ScraperConfigReader.content_selector_key], soup)\n",
    "\n",
    "                # eventually save article\n",
    "                if data[\"title\"] != \"\" and data[\"content\"] != \"\" and data[\"title\"] not in titles: # we save only if we got the necessary info\n",
    "                    titles.add(data[\"title\"])\n",
    "                    counter += 1\n",
    "                    print(\"{0} - Extracted article: \".format(counter_delta_incremental + counter) + data[\"title\"])\n",
    "\n",
    "                    # eventually create domain directory\n",
    "                    sub_directory = directory + \"/\" + url.split(\"/\")[2]\n",
    "                    UtilsOS.directory_maybe_create(directory)\n",
    "\n",
    "                    # write to file\n",
    "                    UtilsOS.write_to_json(sub_directory + \"/\" + str(counter_delta_incremental + counter) + '.json', data)\n",
    "\n",
    "                    # eventually end scraping\n",
    "                    if counter == max_scraped:\n",
    "                        break\n",
    "\n",
    "                # sleep...\n",
    "                time.sleep(0.2)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"------------\")\n",
    "                time.sleep(1) # time to escape by KeywordInterrupt\n",
    "                continue\n",
    "\n",
    "            print(\"------------\")\n",
    "            \n",
    "    def incremental_scraping_from_configs(self, configs, max_scraped=10):\n",
    "        items = list(configs.items())\n",
    "        random.shuffle(items)\n",
    "        while True:\n",
    "            for website,conf in items:\n",
    "                self.scrape_website_incremental(website, conf, max_scraped=max_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = ScraperConfigReader.get_configs(\"scraper_configs.json\")\n",
    "    scraper = Scraper()\n",
    "    #scraper.scrape_website_incremental(\"tutorialspoint\", configs[\"tutorialspoint\"], max_scraped=5, start_from_zero=False)\n",
    "    scraper.incremental_scraping_from_configs(configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
