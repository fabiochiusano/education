{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from scraper_data_reader.ipynb\n",
      "Importing Jupyter notebook from utils_os.ipynb\n",
      "Importing Jupyter notebook from scraper_config_reader.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from alphabet_detector import AlphabetDetector\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from scraper_data_reader import ReaderScrapedData\n",
    "from utils_os import UtilsOS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocesser:\n",
    "    def _ok_title(self, title, ad):\n",
    "        num_words = len(title.split(\" \"))\n",
    "        ok_num_words = num_words >= 2 and num_words <= 20\n",
    "        ok_alphabet = ad.only_alphabet_chars(title, \"LATIN\")\n",
    "        return ok_num_words and ok_alphabet\n",
    "\n",
    "    def _ok_content(self, content, ad):\n",
    "        num_words = len(content.split(\" \"))\n",
    "        ok_num_words = num_words >= 100\n",
    "        ok_alphabet = ad.only_alphabet_chars(content, \"LATIN\")\n",
    "        return ok_num_words and ok_alphabet\n",
    "\n",
    "    def _filter_articles(self, dataset, ad):\n",
    "        dataset_copy = []\n",
    "        not_ok_title = 0\n",
    "        not_ok_content = 0\n",
    "        for i,sample in enumerate(dataset):\n",
    "            title_ok, content_ok = self._ok_title(sample[\"title\"], ad), self._ok_content(sample[\"content\"], ad)\n",
    "            if title_ok and content_ok:\n",
    "                dataset_copy.append(sample)\n",
    "            if not title_ok:\n",
    "                not_ok_title += 1\n",
    "            if not content_ok:\n",
    "                not_ok_content += 1\n",
    "\n",
    "        if self._verbose:\n",
    "            print(\"Prev length: {0}\".format(len(dataset)))\n",
    "            print(\"New length: {0}\".format(len(dataset_copy)))\n",
    "            print(\"Dropped total: {0}\".format(len(dataset) - len(dataset_copy)))\n",
    "            print(\"\\tDropped title: {0}\".format(not_ok_title))\n",
    "            print(\"\\tDropped content: {0}\".format(not_ok_content))\n",
    "\n",
    "        return dataset_copy\n",
    "    \n",
    "    def _clean_html(self, raw_html):\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, '', raw_html)\n",
    "        cleantext = re.sub(\"(<!--.*?-->)\", \"\", cleantext, flags=re.DOTALL)\n",
    "        return cleantext\n",
    "\n",
    "    def _remove_newlines(self, content):\n",
    "        return content.replace(\"\\n\", \" \")\n",
    "\n",
    "    def _remove_extra_white_spaces(self, content):\n",
    "        content = re.sub(' +', ' ', content)\n",
    "        content = content.strip()\n",
    "        return content\n",
    "\n",
    "    def _remove_urls(self, content):\n",
    "        content = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE)\n",
    "        content = re.sub(r'http?:\\/\\/.*[\\r\\n]*', '', content, flags=re.MULTILINE)\n",
    "        return content\n",
    "\n",
    "    def _remove_code(self, content):\n",
    "        content = re.sub(r'(\\w+(\\.\\w+)*\\([^\\)]*\\))', '', content, flags=re.MULTILINE) # matches a.b.c(d)\n",
    "        return content\n",
    "\n",
    "    def _remove_alt_html(self, content):\n",
    "        content = content.split(\"&lt\")[0]\n",
    "        return content\n",
    "\n",
    "    def _clean_text(self, content):\n",
    "        content = self._clean_html(content)\n",
    "        content = self._remove_newlines(content)\n",
    "        content = self._remove_extra_white_spaces(content)\n",
    "        content = self._remove_urls(content)\n",
    "        content = self._remove_code(content)\n",
    "        content = self._remove_alt_html(content)\n",
    "        return content\n",
    "    \n",
    "    def _add_to_stem_dictionary(self, stemmed_word, word, stem_dictionary):\n",
    "        \"\"\"Adds a stemmed_word -> word instance to the stem_dictionary\"\"\"\n",
    "        if stemmed_word not in stem_dictionary:\n",
    "            stem_dictionary[stemmed_word] = {word: 1}\n",
    "        else:\n",
    "            d = stem_dictionary[stemmed_word]\n",
    "            if word not in d:\n",
    "                d[word] = 1\n",
    "            else:\n",
    "                d[word] += 1\n",
    "\n",
    "    def _clean_tokens(self, tokenList, stem_dictionary, token_blacklist, stemmer, punctuation):\n",
    "        # Convert all text to lower case\n",
    "        textList = [word.lower() for word in tokenList if not word.isupper()]\n",
    "\n",
    "        # Remove punctuation\n",
    "        textList = [word for word in textList if word not in punctuation]\n",
    "        textList = [\"\".join(c for c in word if c not in punctuation) for word in textList ]\n",
    "\n",
    "        # Convert digits into NUM\n",
    "        textList = [re.sub(\"\\d+\", \"NUM\", word) for word in textList]  \n",
    "\n",
    "        # Stem words \n",
    "        stemmedTextList = [stemmer.stem(word) for word in textList]\n",
    "        for sw,w in zip(stemmedTextList, textList):\n",
    "            self._add_to_stem_dictionary(sw, w, stem_dictionary)\n",
    "        textList = stemmedTextList\n",
    "\n",
    "        # Remove blanks\n",
    "        textList = [word for word in textList if word != ' ']\n",
    "        textList = [word for word in textList if word != '']\n",
    "\n",
    "        # Remove short words\n",
    "        textList = [word for word in textList if len(word) > 2]\n",
    "\n",
    "        # token blacklist\n",
    "        textList = [word for word in textList if word not in token_blacklist]\n",
    "\n",
    "        return textList\n",
    "    \n",
    "    def _from_sample_to_tfidf(self, sample, stem_dictionary, token_blacklist, idf,\n",
    "                        stemmer, ad, punctuation):\n",
    "        \"\"\"From text string to TF-IDF vector (as Python dictionary)\"\"\"\n",
    "        # Tokenize\n",
    "        tl = nltk.word_tokenize(sample) # splits \"I am Fabio\" into [\"I\", \"am\", \"Fabio\"]. It's a little smarter than a .split(\" \")\n",
    "        raw_text = ' '.join(tl) # Join back the tokens with a space between them\n",
    "        tokens = self._clean_tokens(tl, stem_dictionary, token_blacklist, stemmer, punctuation)\n",
    "\n",
    "        # Create FreqDF with word frequencies and convert it to a data frame\n",
    "        freq = FreqDist(tokens)\n",
    "        freqDF = pd.DataFrame.from_dict(freq, orient='index')\n",
    "        freqDF.columns = ['freq']\n",
    "\n",
    "        # Merge freqDF with idf data frame\n",
    "        freqit = freqDF.join(idf[['idf', 'logidf']])\n",
    "\n",
    "        # Replace null values with max\n",
    "        maxidf = max(freqit['idf'].dropna())\n",
    "        maxlogidf = max(freqit['logidf'].dropna())\n",
    "        freqit.loc[pd.isnull(freqit['idf']), 'idf'] = maxidf\n",
    "        freqit.loc[pd.isnull(freqit['logidf']), 'logidf'] = maxlogidf\n",
    "\n",
    "        # Create tfidf columns\n",
    "        freqit['tfidf'] = freqit['freq'] * freqit['idf']\n",
    "        freqit['logtfidf'] = freqit['freq'] * freqit['logidf']\n",
    "\n",
    "        # Order by logtfidf weight\n",
    "        #freqit = freqit.sort_values(by='logtfidf', ascending=False) \n",
    "\n",
    "        return freqit.to_dict() # To dictionary\n",
    "    \n",
    "    def _get_read_time(self, text, wps):\n",
    "        num_of_words = len(text.split(\" \"))\n",
    "        read_time = num_of_words / wps\n",
    "        return read_time\n",
    "\n",
    "    def __init__(self, verbose=False):\n",
    "        self._verbose = verbose\n",
    "        \n",
    "    def run_preprocessing(self, dataset):\n",
    "        stem_dictionary = {}\n",
    "        token_blacklist = [\"was\", \"wasn\", \"did\", \"didn\", \"you\", \"your\", \"isn\", \"wouldn\", \"doesn\"]\n",
    "        wps = 200 / 60 # = 3.33\n",
    "\n",
    "        # Read wikipedia idf\n",
    "        idf = pd.read_csv(\"../resources/wiki-30k-10-IDF.csv\")\n",
    "        idf = idf.set_index('term')\n",
    "        print(\"Number of words considered in wikipedia: {0}\".format(idf.shape[0]))\n",
    "\n",
    "        # Initialize stemmer, alphabet detector and punctuation\n",
    "        stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "        ad = AlphabetDetector()\n",
    "        punctuation = set(string.punctuation)\n",
    "\n",
    "        # Preprocessing\n",
    "        dataset = self._filter_articles(dataset, ad)\n",
    "        for i,sample in enumerate(dataset):\n",
    "            sample[\"title\"], sample[\"content\"] = self._clean_text(sample[\"title\"]), self._clean_text(sample[\"content\"])\n",
    "            sample[\"tfidf\"] = self._from_sample_to_tfidf(sample[\"content\"],\n",
    "                                                   stem_dictionary, token_blacklist, idf,\n",
    "                                                   stemmer, ad, punctuation)\n",
    "            sample[\"read_time\"] = self._get_read_time(sample[\"content\"], wps)\n",
    "\n",
    "            # Save data\n",
    "            UtilsOS.write_to_json(sample, \"../preprocessed/\" + str(i) + '.json')\n",
    "\n",
    "            if self._verbose and i % 50 == 0:\n",
    "                print(\".. Processed articles: \" + str(i) + \"/\" + str(len(dataset)))\n",
    "\n",
    "        # Save stem dictionary\n",
    "        UtilsOS.write_to_json(stem_dictionary, '../stemmer/stem_dictionary.json')\n",
    "        if self._verbose:\n",
    "            print(\"Saved stem dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 3191 articles\n",
      "Number of words considered in wikipedia: 87709\n",
      "Prev length: 3191\n",
      "New length: 2617\n",
      "Dropped total: 574\n",
      "\tDropped title: 161\n",
      "\tDropped content: 428\n",
      ".. Processed articles: 0/2617\n",
      ".. Processed articles: 50/2617\n",
      ".. Processed articles: 100/2617\n",
      ".. Processed articles: 150/2617\n",
      ".. Processed articles: 200/2617\n",
      ".. Processed articles: 250/2617\n",
      ".. Processed articles: 300/2617\n",
      ".. Processed articles: 350/2617\n",
      ".. Processed articles: 400/2617\n",
      ".. Processed articles: 450/2617\n",
      ".. Processed articles: 500/2617\n",
      ".. Processed articles: 550/2617\n",
      ".. Processed articles: 600/2617\n",
      ".. Processed articles: 650/2617\n",
      ".. Processed articles: 700/2617\n",
      ".. Processed articles: 750/2617\n",
      ".. Processed articles: 800/2617\n",
      ".. Processed articles: 850/2617\n",
      ".. Processed articles: 900/2617\n",
      ".. Processed articles: 950/2617\n",
      ".. Processed articles: 1000/2617\n",
      ".. Processed articles: 1050/2617\n",
      ".. Processed articles: 1100/2617\n",
      ".. Processed articles: 1150/2617\n",
      ".. Processed articles: 1200/2617\n",
      ".. Processed articles: 1250/2617\n",
      ".. Processed articles: 1300/2617\n",
      ".. Processed articles: 1350/2617\n",
      ".. Processed articles: 1400/2617\n",
      ".. Processed articles: 1450/2617\n",
      ".. Processed articles: 1500/2617\n",
      ".. Processed articles: 1550/2617\n",
      ".. Processed articles: 1600/2617\n",
      ".. Processed articles: 1650/2617\n",
      ".. Processed articles: 1700/2617\n",
      ".. Processed articles: 1750/2617\n",
      ".. Processed articles: 1800/2617\n",
      ".. Processed articles: 1850/2617\n",
      ".. Processed articles: 1900/2617\n",
      ".. Processed articles: 1950/2617\n",
      ".. Processed articles: 2000/2617\n",
      ".. Processed articles: 2050/2617\n",
      ".. Processed articles: 2100/2617\n",
      ".. Processed articles: 2150/2617\n",
      ".. Processed articles: 2200/2617\n",
      ".. Processed articles: 2250/2617\n",
      ".. Processed articles: 2300/2617\n",
      ".. Processed articles: 2350/2617\n",
      ".. Processed articles: 2400/2617\n",
      ".. Processed articles: 2450/2617\n",
      ".. Processed articles: 2500/2617\n",
      ".. Processed articles: 2550/2617\n",
      ".. Processed articles: 2600/2617\n",
      "Saved stem dictionary\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read scraped data\n",
    "    data = ReaderScrapedData.read_data(\"scraper_configs.json\")\n",
    "    dataset_nested = [data[website][domain] for website in data.keys() for domain in data[website].keys()]\n",
    "    dataset = [el for subl in dataset_nested for el in subl]\n",
    "    print(\"Read {0} articles\".format(len(dataset)))\n",
    "    \n",
    "    # Run preprocessing\n",
    "    preprocesser = Preprocesser(verbose=True)\n",
    "    preprocesser.run_preprocessing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ReaderScrapedData.read_data(\"scraper_configs.json\")\n",
    "dataset_nested = [data[website][domain] for website in data.keys() for domain in data[website].keys()]\n",
    "dataset = [el for subl in dataset_nested for el in subl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz#egg=en_core_web_md==2.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz (120.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 120.9MB 3.9MB/s \n",
      "\u001b[?25hInstalling collected packages: en-core-web-md\n",
      "  Running setup.py install for en-core-web-md ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-md-2.0.0\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/envs/education/lib/python3.5/site-packages/en_core_web_md -->\n",
      "    /anaconda3/envs/education/lib/python3.5/site-packages/spacy/data/en_core_web_md\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_md')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#!python -m spacy download en_core_web_md #you will need to install this on first load\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "from IPython.display import HTML\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True #OPTIONAL - to disable outputs from Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'init_scope'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c442a90366c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/elmo/2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/education/lib/python3.5/site-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    167\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m           tags=self._tags)\n\u001b[0m\u001b[1;32m    170\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/education/lib/python3.5/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[0;34m(self, name, trainable, tags)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_variables_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/education/lib/python3.5/site-packages/tensorflow_hub/native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# when creating the Module state. This use case has showed up in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;31m# TPU training code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'init_scope'"
     ]
    }
   ],
   "source": [
    "url = \"https://tfhub.dev/google/elmo/2\"\n",
    "embed = hub.Module(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset[0][\"content\"]\n",
    "doc = nlp(text)\n",
    "\n",
    "sentences = []\n",
    "for i in doc.sents:\n",
    "    if len(i) > 1:\n",
    "        sentences.append(i.string.strip())\n",
    "    \n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed(\n",
    "    sentences,\n",
    "    signature=\"default\",\n",
    "    as_dict=True)[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  sess.run(tf.tables_initializer())\n",
    "  x = sess.run(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
