{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tor_session():\n",
    "    session = requests.session()\n",
    "    # Tor uses the 9050 port as the default socks port\n",
    "    session.proxies = {'http':  'socks5://127.0.0.1:9050',\n",
    "                       'https': 'socks5://127.0.0.1:9050'}\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stem import Signal\n",
    "from stem.control import Controller\n",
    "\n",
    "# signal TOR for a new connection \n",
    "def renew_connection():\n",
    "    with Controller.from_port(port = 9051) as controller:\n",
    "        controller.authenticate(password=\"password\")\n",
    "        controller.signal(Signal.NEWNYM)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_url(url):\n",
    "    url = url.split(\"?\")[0]\n",
    "    url = url.split(\"#\")[0]\n",
    "    url = url.split(\"&\")[0]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_counter = 0\n",
    "def make_get(url, print_ip=False):\n",
    "    global ip_counter\n",
    "    url = normalize_url(url)\n",
    "    if ip_counter > 10:\n",
    "        renew_connection()\n",
    "        ip_counter = 0\n",
    "    session = get_tor_session()\n",
    "    #if print_ip:\n",
    "    #    print(session.get(\"http://httpbin.org/ip\").text)\n",
    "    ip_counter += 1\n",
    "    return session.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "\n",
    "first_url_key = \"first_url\"\n",
    "content_selector_key = \"content_selector\"\n",
    "title_selector_key = \"title_selector\"\n",
    "\n",
    "configs[\"medium\"] = {\n",
    "    first_url_key: [\"https://medium.com\", \"https://medium.com/\"],\n",
    "    content_selector_key: [\".sectionLayout--insetColumn\"],\n",
    "    title_selector_key: [\"h1.graf--title\"]\n",
    "}\n",
    "\n",
    "configs[\"tutorialspoint\"] = {\n",
    "    first_url_key: [\"https://www.tutorialspoint.com\", \"https://www.tutorialspoint.com/\", \"https://www.tutorialspoint.com/index.htm\"],\n",
    "    content_selector_key: [\".content > div\", \".tutorial-content > div\", \".content > div > div\"],\n",
    "    title_selector_key: [\".content > div > h1:first-of-type\", \".tutorial-content > div > h1:first-of-type\", \".content > div > div > h1:first-of-type\"]\n",
    "}\n",
    "\n",
    "configs[\"kdnuggets\"] = {\n",
    "    first_url_key: [\"https://www.kdnuggets.com\", \"https://www.kdnuggets.com/\"],\n",
    "    content_selector_key: [\"#post- \"],\n",
    "    title_selector_key: [\"#title\"]\n",
    "}\n",
    "\n",
    "configs[\"datasciencecentral\"] = {\n",
    "    first_url_key: [\"https://www.datasciencecentral.com\"],\n",
    "    content_selector_key: [\"article .entry-content\"],\n",
    "    title_selector_key: [\"article .entry-title\"]\n",
    "}\n",
    "\n",
    "configs[\"smartdatacollective\"] = {\n",
    "    first_url_key: [\"https://www.smartdatacollective.com\", \"https://www.smartdatacollective.com/\"],\n",
    "    content_selector_key: [\".single-content\"],\n",
    "    title_selector_key: [\".single-title\"]\n",
    "}\n",
    "\n",
    "configs[\"machinelearningmastery\"] = {\n",
    "    first_url_key: [\"https://www.kdnuggets.com\"],\n",
    "    content_selector_key: [],\n",
    "    title_selector_key: []\n",
    "}\n",
    "\n",
    "configs[\"wikihow\"] = {\n",
    "    first_url_key: [\"https://www.wikihow.com\", \"https://www.wikihow.com/\"],\n",
    "    content_selector_key: [\"#bodycontents .steps .step\"],\n",
    "    title_selector_key: [\"#bodycontents > #intro > h1:first-of-type\"]\n",
    "}\n",
    "\n",
    "configs[\"splinters\"] = {\n",
    "    first_url_key: [\"https://schwitzsplinters.blogspot.com\", \"https://schwitzsplinters.blogspot.com/\"],\n",
    "    content_selector_key: [\".post-body\"],\n",
    "    title_selector_key: [\".post-title\"]\n",
    "}\n",
    "\n",
    "configs[\"thehistoryblog\"] = {\n",
    "    first_url_key: [\"http://www.thehistoryblog.com\", \"http://www.thehistoryblog.com/\"],\n",
    "    content_selector_key: [\".post > .entry\"],\n",
    "    title_selector_key: [\".post > h3:first-of-type\", \".post > h2:first-of-type\"]\n",
    "}\n",
    "\n",
    "configs[\"chemistry-blog\"] = {\n",
    "    first_url_key: [\"http://www.chemistry-blog.com\", \"http://www.chemistry-blog.com/\"],\n",
    "    content_selector_key: [\".post > .entry\"],\n",
    "    title_selector_key: [\".title\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_first_urls(website, links):\n",
    "    for url in config[website][first_url_key]:\n",
    "        if url in links:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "from bs4.element import Tag\n",
    "def get_with_selector(selectors, soup, mode):\n",
    "    res = []\n",
    "    for selector in selectors:\n",
    "        res = soup.select(selector)\n",
    "        if len(res) > 0:\n",
    "            if mode == \"title\":\n",
    "                return str(res[0]), res[0].text\n",
    "            else:\n",
    "                return str(res[0]), \" \".join([tag.text if type(tag) == Tag else str(tag) for tag in res[0].children])\n",
    "    return None, \"\"\n",
    "\n",
    "extensions_banned = [\".jpg\", \".png\", \".zip\"]\n",
    "def check_link_extension(link):\n",
    "    for extension in extensions_banned:\n",
    "        if link[-len(extension):] == extension:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"chemistry-blog\"\n",
    "\n",
    "directory = website\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "first_url = configs[website][first_url_key][0]\n",
    "prefix_url = \"/\".join(first_url.split(\"/\")[:3])\n",
    "queue = [first_url]\n",
    "already_considered = set()\n",
    "already_considered.add(first_url)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "while len(queue) > 0:\n",
    "    # get url to visit\n",
    "    url = queue.pop(0)\n",
    "    print(\"Visiting \" + url)\n",
    "    print(\"URLs in queue: {0}\".format(len(queue)))\n",
    "    \n",
    "    # visit url\n",
    "    try:\n",
    "        response = make_get(url, print_ip=True)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        data = {\"url\": url, \"html\": str(soup)}\n",
    "        \n",
    "        # get all outer links from url\n",
    "        links = soup.find_all(\"a\")\n",
    "        links = [tag[\"href\"] for tag in links if tag.has_attr(\"href\")]\n",
    "        \n",
    "        # from relative to absolute links\n",
    "        links = [prefix_url + link if len(link) > 0 and link[0] == \"/\" else link for link in links] # fix \"/index.html\"\n",
    "        links = [prefix_url + \"/\" + link if \"//\" not in link else link for link in links] # fix \"index.html\"\n",
    "        links = [link for link in links if website in link] # must contain the name of the website\n",
    "        #links = [link if \"//\" in link else (prefix_url + \"/\" if  + link for link in links]\n",
    "        #links = [link if link[-2] != \"/\" else link[:-1] for link in links] # remove ending double slash\n",
    "        \n",
    "        # if it does not contain a link to the homepage, then drop it\n",
    "        #if not check_first_urls(website, links):\n",
    "        #    print(\"Not found link to homepage: \" + str(links))\n",
    "        #    print(\"------------\")\n",
    "        #    continue\n",
    "                \n",
    "        links = [link for link in links if check_link_extension(link)] # remove links that end with banned extension\n",
    "                \n",
    "        links = [normalize_url(link) for link in links] # remove http parameters (after ?)\n",
    "        \n",
    "        links = list(set(links)) # remove duplicates\n",
    "                \n",
    "        # add links to queue if not already considered\n",
    "        for link in links:\n",
    "            if link not in already_considered:\n",
    "                already_considered.add(link)\n",
    "                queue.append(link)\n",
    "                                \n",
    "        # get titles\n",
    "        data[\"title_html\"], data[\"title\"] = get_with_selector(configs[website][title_selector_key], soup, \"title\")\n",
    "\n",
    "        # get texts\n",
    "        data[\"content_html\"], data[\"content\"] = get_with_selector(configs[website][content_selector_key], soup, \"content\")\n",
    "\n",
    "        # save article\n",
    "        if data[\"title\"] != \"\" and data[\"content\"] != \"\": # we save only if we got the necessary info\n",
    "            print(\"Extracted tutorial: \" + data[\"title\"])\n",
    "            counter += 1\n",
    "            sub_directory = directory + \"/\" + url.split(\"/\")[2]\n",
    "            if not os.path.exists(sub_directory):\n",
    "                os.makedirs(sub_directory)\n",
    "            with open(sub_directory + \"/\" + str(counter) + '.json', 'w') as outfile:\n",
    "                json.dump(data, outfile)\n",
    "\n",
    "        # sleep...\n",
    "        time.sleep(0.2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"------------\")\n",
    "        time.sleep(1) # time to escape by KeywordInterrupt\n",
    "        continue\n",
    "        \n",
    "    print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"tutorialspoint\"\n",
    "\n",
    "url = \"https://www.tutorialspoint.com/blockchain_online_training/index.asp\"\n",
    "response = make_get(url, print_ip=True)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = soup.select(configs[website][title_selector_key][1])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, title = get_with_selector(configs[website][title_selector_key], soup, \"title\")\n",
    "print(title)\n",
    "\n",
    "# get texts\n",
    "c, content = get_with_selector(configs[website][content_selector_key], soup, \"content\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\".content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tag[\"href\"] for tag in links if tag[\"href\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.select(\".content > div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get titles\n",
    "title = soup.select(\".content > div > h1:first-of-type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEE ALL TITLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = \"wikihow/www.wikihow.com\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    json_file = mypath + \"/\" + file\n",
    "    with open(json_file, 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "        print(data[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
