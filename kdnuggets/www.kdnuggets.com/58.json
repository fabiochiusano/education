{"url": "https://www.kdnuggets.com/sequence.html", "title": "Sequence Modeling with Neural Networks \u2013 Part I", "title_html": "<h1 id=\"title\">Sequence Modeling with Neural Networks \u2013 Part I</h1>", "content": "\n  comments \n By Zied Haj-Yahia, Senior Data Scientist at Capgemini Consulting \n \u00a0 \n Context \n \u00a0\nIn the previous course\u00a0Introduction to Deep Learning, we saw how to use Neural Networks to model a dataset of many examples. The good news is that the basic architecture of Neural Networks is quite generic whatever the application: a stacking of several perceptrons to compose complex hierarchical models and their optimization using gradient descent and backpropagation. \n Inspite of this, you have probably heard about Multilayer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), LSTM, Auto-Encoders, etc. These deep learning models are different from each other. Each model is known to be particulary performant in some specific tasks, even though, fundamentally, they all share the same basic architecture. \n What makes the difference between them is their ability to be more suited for some data structures: text processing could be different from image processing, which in turn could be different from signal processing. \n In the context of this post, we will focus on modeling\u00a0sequences\u00a0as a well-known data structure and will study its\u00a0specific learning framework. \n Applications of sequence modeling are plentiful in day-to-day business practice. Some of them emerged to meet today\u2019s challenges in terms of quality of service and customer engagement. Here some examples: \n \nSpeech Recognition to listen to the voice of customers.\nMachine Language Translation from diverse source languages to more common languages.\nTopic Extraction to find the main subject of customer\u2019s translated query.\nSpeech Generation to have conversational ability and engage with customers in a human like manner.\nText Summarization of customer feedback to work on key challenges and pain points.\n \n In the auto industry, self-parking is also a sequence modeling task. In fact, parking could be seen as a sequence of mouvements where the next movement depends on the previous ones. \n Other applications cover text classification, translating videos to natural language, image caption generation, hand writing recognition/generation, anomaly detection, and many more in the future\u2026which none of us can think (or aware) at the moment. \n However, before we go any further in the applications of Sequence Modeling, let us understand what we are dealing with when we talk about sequences. \n \u00a0 \n Introduction to Sequence Modeling \n \u00a0\nSequences are a data structure where each example could be seen as a series of data points. This sentence: \u201cI am currently reading an article about sequence modeling with Neural Networks\u201d is an example that consists of multiple words and words depend on each other. The same applies to medical records. One single medical record consists in many measurments across time. It is the same for speech waveforms. \n So why we need a different learning framework to model sequences and what are the special features that we are looking for in this framework? \n For illustration purposes and with no loss of generality, let us focus on text as a sequence of words to motivate this need for a different learning framework. \n In fact, machine learning algorithms typically require the text input to be represented as a\u00a0fixed-lengthvector. Many operations needed to train the model (network) can be expressed through algebraic operations on the matrix of input feature values and the matrix of weights (think about a n-by-p design matrix, where n is the number of samples observed, and p is the number of variables measured in all samples). \n Perhaps the most common fixed-length vector representation for texts is the\u00a0bag-of-words\u00a0or bag-of-n-grams due to its simplicity, efficiency and often surprising accuracy. However, the bag-of-words (BOW) representation has many disadvantages: \n \nFirst, the word order is lost, and thus different sentences can have exactly the same representation, as long as the same words are used. Example: \u201cThe food was good, not bad at all.\u201d vs \u201cThe food was bad, not good at all.\u201d. Even though bag-of-n-grams considers the word order in short context, it suffers from data sparsity and high dimensionality.\nIn addition, Bag-of-words and bag-of-n-grams have very little knowledge about the semantics of the words or more formally the distances between the words. This means that words \u201cpowerful\u201d, \u201cstrong\u201d and \u201cParis\u201d are equally distant despite the fact that semantically, \u201cpowerful\u201d should be closer to \u201cstrong\u201d than \u201cParis\u201d.\nHumans don\u2019t start their thinking from scratch every second. As you read this article,\u00a0you understand each word based on your understanding of previous words. Traditional neural networks can\u2019t do this, and it seems like a major shortcoming. Bag-of-words and bag-of-n-grams as text representations do not allow to keep track of long-term dependencies inside the same sentence or paragraph.\nAnother disadvantage of modeling sequences with traditional Neural Networks (e.g.\u00a0Feedforward Neural Networks) is the fact of not sharing parameters across time. Let us take for example these two sentences : \u201cOn Monday, it was snowing\u201d and \u201cIt was snowing on Monday\u201d. These sentences mean the same thing, though the details are in different parts of the sequence. Actually, when we feed these two sentences into a Feedforward Neural Network for a prediction task, the model will assign different weights to \u201cOn Monday\u201d at each moment in time.\u00a0Things we learn about the sequence won\u2019t transfer if they appear at different points in the sequence.\u00a0Sharing parameters gives the network the ability to look for a given feature everywhere in the sequence, rather than in just a certain area.\n \n Thus, to model sequences, we need a specific learning framework able to: \n \ndeal with variable-length sequences\nmaintain sequence order\nkeep track of long-term dependencies rather than cutting input data too short\nshare parameters across the sequence (so not re-learn things across the sequence)\n \n Recurrent neural networks (RNNs) could address this issue. They are networks with loops in them, allowing information to persist. \n So, let us find out more about RNNs! \n \u00a0 \n Recurrent Neural Networks \n \u00a0 \n How a Recurrent Neural Network works? \n \u00a0\nA Recurrent Neural Network is architected in the same way as a \u201ctraditional\u201d Neural Network. We have some inputs, we have some hidden layers and we have some outputs. \n  \n The only difference is that each hidden unit is doing a slightly different function. So, let\u2019s explore how this hidden unit works. \n A recurrent hidden unit computes a function of an input and its own previous output, also known as the cell state. For textual data, an input could be a vector representing a word\u00a0x(i)\u00a0in a sentence of\u00a0n\u00a0words (also known as word embedding). \n  \n W\u00a0and\u00a0U\u00a0are weight matrices and\u00a0tanh\u00a0is the hyperbolic tangent function. \n Similarly, at the next step, it computes a function of the new input and its previous cell state:\u00a0s2\u00a0=\u00a0tanh(Wx1+\u00a0Us1 . This behavior is similar to a hidden unit in a feed-forward Network. The difference, proper to sequences, is that we are adding an additional term to incorporate its own previous state. \n A common way of viewing recurrent neural networks is by unfolding them across time. We can notice that we are using the same weight matrices W and U throughout the sequence. This solves our problem of parameter sharing. We don\u2019t have new parameters for every point of the sequence. Thus, once we learn something, it can apply at any point in the sequence. \n  \n The fact of not having new parameters for every point of the sequence also helps us\u00a0deal with variable-length sequences. In case of a sequence that has a length of 4, we could unroll this RNN to four timesteps. In other cases, we can unroll it to ten timesteps since the length of the sequence is not prespecified in the algorithm. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word. \n NB: \n \nSn, the cell state at time\u00a0n, can contain information from all of the past timesteps: each cell state is a function of the previous self state which in turn is a function of the previous cell state.\u00a0This solves our issue of long-term dependencies.\nThe above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence, we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence.\n \n Now that we understand how a single hidden unit works, we need to figure out how to train an entire Recurrent Neural Network made up of many hidden units and even many layers of many hidden units. \n \u00a0 \n How do we train a Recurrent Neural Network? \n \u00a0\nLet\u2019s consider the following task: for a set of speeches in English, we need the model to automatically convert the spoken language into text i.e.\u00a0at each timestep, the model produces a prediction of a transcript (an output) based on the part of speech at this timestep (the new input) and the previous transcript (the previous cell state). \n Naturally, because we have an output at every timestep, we can have a loss at every timestep. This loss reflects how much the predicted transcripts are close to the \u201cofficial\u201d transcripts. \n  \n The total loss is just the sum of the losses at every timestep. \n  \n Since the loss is a function of the network weights, our task it to find the set of weights\u00a0theta\u00a0that achieve the lowest loss. For that, as explained in the first article \u201cIntroduction to Deep Learning\u201d, we we can apply\u00a0the gradient descent algorithm with backpropagation (chain rule) at every timestep, thus taking into account the additional time dimension. \n W\u00a0and\u00a0U\u00a0are our two weight matrices. Let us try it out for\u00a0W. \n Knowing that the total loss is the sum of the losses at every timestep, the total gradient is just the sum of the gradients at every timestep: \n  \n And now, we can focus on a single timestep to calculate the derivative of the loss with respect to\u00a0W. \n  \n Easy to handle: we just use backpropagation. \n  \n We remember that\u00a0s2\u00a0=\u00a0tanh(Wx1\u00a0+\u00a0Us1)\u00a0so s2 also depends on s1 and s1 also depends on W.\u00a0This actually means that we can not just leave the derivative of\u00a0s2\u00a0with respect to\u00a0W\u00a0as a constant. We have to expand it out farther. \n So how does\u00a0s2\u00a0depend on\u00a0W? \n It depends directly on W because it feeds right in (c.f. above formula of\u00a0s2). We also know that\u00a0s2\u00a0depends on\u00a0s1\u00a0which depends on W. And we can also see that\u00a0s2\u00a0depends on\u00a0s0\u00a0which also depends on W. \n  \n Thus, the derivative of the loss with respect to\u00a0W\u00a0could be written as follows: \n  \n We can see that the last two terms are basically summing the contributions of\u00a0W\u00a0in previous timesteps to the error at timestep\u00a0t. This is key to understand how we model long-term dependencies. From one iteration to another, the gradient descent algorithm allows to shift network parameters such that they include contributions to the error from past timesteps. \n For any timestep\u00a0t, the derivative of the loss with respect to\u00a0W\u00a0could be written as follows: \n  \n So to train the model i.e.\u00a0to estimate the weights of the network, we apply this same process of backpropagation through time for every weight (parameter) and then we use it in the process of gradient descent. \n \u00a0 \n Why are Recurrent Neural Networks hard to train? \n \u00a0\nIn practice RNNs are a bit difficult to train. To understand why, let\u2019s take a closer look at the gradient we calculated above: \n  \n We can see that as the gap between timesteps gets bigger, the product of the gradients gets longer and longer. But, what are each of these terms? \n  \n Each term is basically a product of two terms: transposed\u00a0W\u00a0and a second one that depends on f\u2019, the derivative of the activation function. \n \nInitial weights\u00a0W\u00a0are usually sampled from standard normal distribution and then mostly < 1.\nIt turns out (I won\u2019t prove it here but\u00a0this paper\u00a0goes into detail) that the second term is a Jacobian matrix because we are taking the derivative of a vector function with respect to a vector and its 2-norm, which you can think of it as an absolute value,\u00a0has an upper bound of 1. This makes intuitive sense because our tanh (or sigmoid) activation function maps all values into a range between -1 and 1, and the derivative f\u2019 is bounded by 1 (1/4 in the case of sigmoid).\n \n Thus, with small values in the matrix and multiple matrix multiplications, the\u00a0gradient values are shrinking exponentially fast, eventually vanishing completely after a few time steps. Gradient contributions from \u201cfar away\u201d steps become zero, and the state at those steps doesn\u2019t contribute to what you are learning: you end up not learning long-range dependencies. \n Vanishing gradients aren\u2019t exclusive to RNNs. They also happen in deep Feedforward Neural Networks. It\u2019s just that RNNs tend to be very deep (as deep as the sentence length in our case), which makes the problem a lot more common. \n Fortunately, there are a few ways to combat the vanishing gradient problem.\u00a0Proper initialization of the\u00a0Wmatrix\u00a0can reduce the effect of vanishing gradients. So can regularization. A more preferred solution is to use\u00a0ReLU\u00a0instead of tanh or sigmoid activation functions. The ReLU derivative is a constant of either 0 or 1, so it isn\u2019t as likely to suffer from vanishing gradients. \n An even more popular solution is to use Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) architectures. LSTMs were first proposed in 1997 and are perhaps the most widely used models in NLP today. GRUs, first proposed in 2014, are simplified versions of LSTMs. Both of these RNN architectures were explicitly designed to deal with vanishing gradients and efficiently learn long-range dependencies. We\u2019ll cover them in the next part of this article. \n It will come soon! \n \u00a0 \n Resources I used when writing this article: \n \u00a0 \n \nhttp://introtodeeplearning.com\nOn the difficulty of training recurrent neural networks\nDistributed Representations of Sentences and Documents\nUnderstanding LSTM Networks\nBackpropagation Through Time and Vanishing Gradients\n \n \u00a0\nBio: Zied Haj-Yahia is Senior Data Scientist at Capgemini Consulting. He specializes in building predictive models utilizing both traditional statistical methods and modern machine learning techniques. He also runs some workshops for university students (ESSEC, HEC, Ecole polytechnique) interested in Data Science and its applications. He is the co-founder of Global International Trading (GIT), a central purchasing office based in Paris. \n Original. Reposted with permission. \n Related: \n \nIntroduction to Deep Learning\n7 Steps to Understanding Deep Learning\nUltimate Guide to Getting Started with TensorFlow\n \n  \n  \n  \n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n  \n", "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Sequence Modeling with Neural Networks \u2013 Part I</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/10/sequence-modeling-neural-networks-part-1.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Sequence Modeling with Neural Networks \u2013 Part I Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2018/10/upcoming-meetings-ai-analytics-big-data-science-machine-learning.html\" rel=\"prev\" title=\"Upcoming Meetings in AI, Analytics, Big Data, Data Science, Deep Learning, Machine Learning: October and Beyond\"/>\n<link href=\"https://www.kdnuggets.com/2018/10/linear-regression-wild.html\" rel=\"next\" title=\"Linear Regression in the\u00a0Wild\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2018/10/sequence-modeling-neural-networks-part-1.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=85969\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2018/10/sequence-modeling-neural-networks-part-1.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-85969 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 3-Oct, 2018  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/10/index.html\">Oct</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/10/tutorials.html\">Tutorials, Overviews</a> \u00bb Sequence Modeling with Neural Networks \u2013 Part I (\u00a0<a href=\"/2018/n38.html\">18:n38</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\">Sequence Modeling with Neural Networks \u2013 Part I</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/10/upcoming-meetings-ai-analytics-big-data-science-machine-learning.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/10/linear-regression-wild.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/neural-networks\" rel=\"tag\">Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/nlp\" rel=\"tag\">NLP</a>, <a href=\"https://www.kdnuggets.com/tag/recurrent-neural-networks\" rel=\"tag\">Recurrent Neural Networks</a>, <a href=\"https://www.kdnuggets.com/tag/sequences\" rel=\"tag\">Sequences</a></div>\n<br/>\n<p class=\"excerpt\">\n     In the context of this post, we will focus on modeling sequences as a well-known data structure and will study its specific learning framework.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"https://www.linkedin.com/in/zied-hajyahia/\" rel=\"noopener noreferrer\" target=\"_blank\">Zied Haj-Yahia</a>, Senior Data Scientist at Capgemini Consulting</b></p>\n<p>\u00a0</p>\n<h3>Context</h3>\n<p>\u00a0<br>\nIn the previous course\u00a0<a href=\"https://ziedhy.github.io/2018/08/Introduction_Deep_Learning.html\" rel=\"noopener noreferrer\" target=\"_blank\">Introduction to Deep Learning</a>, we saw how to use Neural Networks to model a dataset of many examples. The good news is that the basic architecture of Neural Networks is quite generic whatever the application: a stacking of several perceptrons to compose complex hierarchical models and their optimization using gradient descent and backpropagation.</br></p>\n<p>Inspite of this, you have probably heard about Multilayer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), LSTM, Auto-Encoders, etc. These deep learning models are different from each other. Each model is known to be particulary performant in some specific tasks, even though, fundamentally, they all share the same basic architecture.</p>\n<p>What makes the difference between them is their ability to be more suited for some data structures: text processing could be different from image processing, which in turn could be different from signal processing.</p>\n<p>In the context of this post, we will focus on modeling\u00a0<strong>sequences</strong>\u00a0as a well-known data structure and will study its\u00a0<strong>specific learning framework</strong>.</p>\n<p>Applications of sequence modeling are plentiful in day-to-day business practice. Some of them emerged to meet today\u2019s challenges in terms of quality of service and customer engagement. Here some examples:</p>\n<ul>\n<li>Speech Recognition to listen to the voice of customers.\n<li>Machine Language Translation from diverse source languages to more common languages.\n<li>Topic Extraction to find the main subject of customer\u2019s translated query.\n<li>Speech Generation to have conversational ability and engage with customers in a human like manner.\n<li>Text Summarization of customer feedback to work on key challenges and pain points.\n</li></li></li></li></li></ul>\n<p>In the auto industry, self-parking is also a sequence modeling task. In fact, parking could be seen as a sequence of mouvements where the next movement depends on the previous ones.</p>\n<p>Other applications cover text classification, translating videos to natural language, image caption generation, hand writing recognition/generation, anomaly detection, and many more in the future\u2026which none of us can think (or aware) at the moment.</p>\n<p>However, before we go any further in the applications of Sequence Modeling, let us understand what we are dealing with when we talk about sequences.</p>\n<p>\u00a0</p>\n<h3>Introduction to Sequence Modeling</h3>\n<p>\u00a0<br>\nSequences are a data structure where each example could be seen as a series of data points. This sentence: \u201cI am currently reading an article about sequence modeling with Neural Networks\u201d is an example that consists of multiple words and words depend on each other. The same applies to medical records. One single medical record consists in many measurments across time. It is the same for speech waveforms.</br></p>\n<p><strong><em>So why we need a different learning framework to model sequences and what are the special features that we are looking for in this framework?</em></strong></p>\n<p>For illustration purposes and with no loss of generality, let us focus on text as a sequence of words to motivate this need for a different learning framework.</p>\n<p>In fact, machine learning algorithms typically require the text input to be represented as a\u00a0<strong>fixed-length</strong>vector. Many operations needed to train the model (network) can be expressed through algebraic operations on the matrix of input feature values and the matrix of weights (think about a n-by-p design matrix, where n is the number of samples observed, and p is the number of variables measured in all samples).</p>\n<p>Perhaps the most common fixed-length vector representation for texts is the\u00a0<strong>bag-of-words</strong>\u00a0or bag-of-n-grams due to its simplicity, efficiency and often surprising accuracy. However, the bag-of-words (BOW) representation has many disadvantages:</p>\n<ul>\n<li>First, the word order is lost, and thus different sentences can have exactly the same representation, as long as the same words are used. Example: \u201cThe food was good, not bad at all.\u201d vs \u201cThe food was bad, not good at all.\u201d. Even though bag-of-n-grams considers the word order in short context, it suffers from data sparsity and high dimensionality.\n<li>In addition, Bag-of-words and bag-of-n-grams have very little knowledge about the semantics of the words or more formally the distances between the words. This means that words \u201cpowerful\u201d, \u201cstrong\u201d and \u201cParis\u201d are equally distant despite the fact that semantically, \u201cpowerful\u201d should be closer to \u201cstrong\u201d than \u201cParis\u201d.\n<li>Humans don\u2019t start their thinking from scratch every second. As you read this article,\u00a0<strong>you understand each word based on your understanding of previous words</strong>. Traditional neural networks can\u2019t do this, and it seems like a major shortcoming. Bag-of-words and bag-of-n-grams as text representations do not allow to keep track of long-term dependencies inside the same sentence or paragraph.\n<li>Another disadvantage of modeling sequences with traditional Neural Networks (e.g.\u00a0Feedforward Neural Networks) is the fact of not sharing parameters across time. Let us take for example these two sentences : \u201cOn Monday, it was snowing\u201d and \u201cIt was snowing on Monday\u201d. These sentences mean the same thing, though the details are in different parts of the sequence. Actually, when we feed these two sentences into a Feedforward Neural Network for a prediction task, the model will assign different weights to \u201cOn Monday\u201d at each moment in time.\u00a0<strong>Things we learn about the sequence won\u2019t transfer if they appear at different points in the sequence.</strong>\u00a0Sharing parameters gives the network the ability to look for a given feature everywhere in the sequence, rather than in just a certain area.\n</li></li></li></li></ul>\n<p>Thus, to model sequences, we need a specific learning framework able to:</p>\n<ul>\n<li>deal with variable-length sequences\n<li>maintain sequence order\n<li>keep track of long-term dependencies rather than cutting input data too short\n<li>share parameters across the sequence (so not re-learn things across the sequence)\n</li></li></li></li></ul>\n<p>Recurrent neural networks (RNNs) could address this issue. They are networks with loops in them, allowing information to persist.</p>\n<p>So, let us find out more about RNNs!</p>\n<p>\u00a0</p>\n<h3>Recurrent Neural Networks</h3>\n<p>\u00a0</p>\n<h3>How a Recurrent Neural Network works?</h3>\n<p>\u00a0<br>\nA Recurrent Neural Network is architected in the same way as a \u201ctraditional\u201d Neural Network. We have some inputs, we have some hidden layers and we have some outputs.</br></p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/fNwbPK/zied_sequence_1.png\" width=\"80%\"/></p>\n<p>The only difference is that each hidden unit is doing a slightly different function. So, let\u2019s explore how this hidden unit works.</p>\n<p>A recurrent hidden unit computes a function of an input and its own previous output, also known as the cell state. For textual data, an input could be a vector representing a word\u00a0<em>x(i)</em>\u00a0in a sentence of\u00a0<em>n</em>\u00a0words (also known as word embedding).</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/jNukJe/zied_sequence_2.png\" width=\"99%\"/></p>\n<p><em>W</em>\u00a0and\u00a0<em>U</em>\u00a0are weight matrices and\u00a0<em>tanh</em>\u00a0is the hyperbolic tangent function.</p>\n<p>Similarly, at the next step, it computes a function of the new input and its previous cell state:\u00a0<strong><em>s2</em>\u00a0=\u00a0<em>tanh</em>(<em>Wx1</em>+\u00a0<em>Us1</em> </strong>. This behavior is similar to a hidden unit in a feed-forward Network. The difference, proper to sequences, is that we are adding an additional term to incorporate its own previous state.</p>\n<p>A common way of viewing recurrent neural networks is by unfolding them across time. We can notice that <strong>we are using the same weight matrices <em>W</em> and <em>U</em> throughout the sequence. This solves our problem of parameter sharing</strong>. We don\u2019t have new parameters for every point of the sequence. Thus, once we learn something, it can apply at any point in the sequence.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/dxsuBz/zied_sequence_3.png\" width=\"75%\"/></p>\n<p>The fact of not having new parameters for every point of the sequence also helps us\u00a0<strong>deal with variable-length sequences</strong>. In case of a sequence that has a length of 4, we could unroll this RNN to four timesteps. In other cases, we can unroll it to ten timesteps since the length of the sequence is not prespecified in the algorithm. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word.</p>\n<p><b>NB:</b></p>\n<ul>\n<li><em>Sn</em>, the cell state at time\u00a0<em>n</em>, can contain information from all of the past timesteps: each cell state is a function of the previous self state which in turn is a function of the previous cell state.\u00a0<strong>This solves our issue of long-term dependencies</strong>.\n<li>The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence, we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence.\n</li></li></ul>\n<p>Now that we understand how a single hidden unit works, we need to figure out how to train an entire Recurrent Neural Network made up of many hidden units and even many layers of many hidden units.</p>\n<p>\u00a0</p>\n<h3>How do we train a Recurrent Neural Network?</h3>\n<p>\u00a0<br>\nLet\u2019s consider the following task: for a set of speeches in English, we need the model to automatically convert the spoken language into text i.e.\u00a0at each timestep, the model produces a prediction of a transcript (an output) based on the part of speech at this timestep (the new input) and the previous transcript (the previous cell state).</br></p>\n<p>Naturally, because we have an output at every timestep, we can have a loss at every timestep. This loss reflects how much the predicted transcripts are close to the \u201cofficial\u201d transcripts.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/buBnWz/zied_sequence_4.png\"/></p>\n<p>The total loss is just the sum of the losses at every timestep.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/drgXye/zied_sequence_5.png\"/></p>\n<p>Since the loss is a function of the network weights, our task it to find the set of weights\u00a0<em>theta</em>\u00a0that achieve the lowest loss. For that, as explained in the first article \u201cIntroduction to Deep Learning\u201d, we we can apply\u00a0<strong>the gradient descent algorithm with backpropagation (chain rule) at every timestep</strong>, thus taking into account the additional time dimension.</p>\n<p><em>W</em>\u00a0and\u00a0<em>U</em>\u00a0are our two weight matrices. Let us try it out for\u00a0<em>W</em>.</p>\n<p>Knowing that the total loss is the sum of the losses at every timestep, the total gradient is just the sum of the gradients at every timestep:</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/fvvEBz/zied_sequence_6.png\"/></p>\n<p>And now, we can focus on a single timestep to calculate the derivative of the loss with respect to\u00a0<em>W</em>.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/f3PVjK/zied_sequence_7.png\"/></p>\n<p>Easy to handle: we just use backpropagation.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/kqfQJe/zied_sequence_8.png\"/></p>\n<p>We remember that\u00a0<strong><em>s2\u00a0=\u00a0tanh(Wx1\u00a0+\u00a0Us1)</em></strong>\u00a0so s2 also depends on s1 and s1 also depends on W.\u00a0<strong>This actually means that we can not just leave the derivative of\u00a0<em>s2</em>\u00a0with respect to\u00a0<em>W</em>\u00a0as a constant. We have to expand it out farther.</strong></p>\n<p>So how does\u00a0<em>s2</em>\u00a0depend on\u00a0<em>W</em>?</p>\n<p>It depends directly on W because it feeds right in (c.f. above formula of\u00a0<em>s2</em>). We also know that\u00a0<em>s2</em>\u00a0depends on\u00a0<em>s1</em>\u00a0which depends on W. And we can also see that\u00a0<em>s2</em>\u00a0depends on\u00a0<em>s0</em>\u00a0which also depends on W.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/duni4K/zied_sequence_9.png\" width=\"75%\"/></p>\n<p>Thus, the derivative of the loss with respect to\u00a0<em>W</em>\u00a0could be written as follows:</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/mtxi4K/zied_sequence_10.png\"/></p>\n<p>We can see that the last two terms are basically summing the contributions of\u00a0<em>W</em>\u00a0in previous timesteps to the error at timestep\u00a0<em>t</em>. This is key to understand how we model long-term dependencies. From one iteration to another, the gradient descent algorithm allows to shift network parameters such that they include contributions to the error from past timesteps.</p>\n<p>For any timestep\u00a0<em>t</em>, the derivative of the loss with respect to\u00a0<em>W</em>\u00a0could be written as follows:</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/cdEKde/zied_sequence_11.png\"/></p>\n<p>So to train the model i.e.\u00a0to estimate the weights of the network, we apply this same process of backpropagation through time for every weight (parameter) and then we use it in the process of gradient descent.</p>\n<p>\u00a0</p>\n<h3>Why are Recurrent Neural Networks hard to train?</h3>\n<p>\u00a0<br/>\nIn practice RNNs are a bit difficult to train. To understand why, let\u2019s take a closer look at the gradient we calculated above:</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/ix6nWz/zied_sequence_12.png\" width=\"99%\"/></p>\n<p>We can see that as the gap between timesteps gets bigger, the product of the gradients gets longer and longer. But, what are each of these terms?</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/c3wnWz/zied_sequence_13.png\"/></p>\n<p>Each term is basically a product of two terms: transposed\u00a0<em>W</em>\u00a0and a second one that depends on f\u2019, the derivative of the activation function.</p>\n<ul>\n<li>Initial weights\u00a0<em>W</em>\u00a0are usually sampled from standard normal distribution and then mostly &lt; 1.\n<li>It turns out (I won\u2019t prove it here but\u00a0<a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">this paper</a>\u00a0goes into detail) that the second term is a Jacobian matrix because we are taking the derivative of a vector function with respect to a vector and its 2-norm, which you can think of it as an absolute value,\u00a0<strong>has an upper bound of 1</strong>. This makes intuitive sense because our tanh (or sigmoid) activation function maps all values into a range between -1 and 1, and the derivative f\u2019 is bounded by 1 (1/4 in the case of sigmoid).\n</li></li></ul>\n<p>Thus, with small values in the matrix and multiple matrix multiplications, the\u00a0<strong>gradient values are shrinking exponentially fast, eventually vanishing completely after a few time steps</strong>. Gradient contributions from \u201cfar away\u201d steps become zero, and the state at those steps doesn\u2019t contribute to what you are learning: you end up not learning long-range dependencies.</p>\n<p>Vanishing gradients aren\u2019t exclusive to RNNs. They also happen in deep Feedforward Neural Networks. It\u2019s just that RNNs tend to be very deep (as deep as the sentence length in our case), which makes the problem a lot more common.</p>\n<p>Fortunately, there are a few ways to combat the vanishing gradient problem.\u00a0<strong>Proper initialization of the\u00a0<em>W</em>matrix</strong>\u00a0can reduce the effect of vanishing gradients. So can regularization. A more preferred solution is to use\u00a0<strong><em>ReLU</em></strong>\u00a0instead of tanh or sigmoid activation functions. The ReLU derivative is a constant of either 0 or 1, so it isn\u2019t as likely to suffer from vanishing gradients.</p>\n<p>An even more popular solution is to use Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) architectures. LSTMs were first proposed in 1997 and are perhaps the most widely used models in NLP today. GRUs, first proposed in 2014, are simplified versions of LSTMs. Both of these RNN architectures were explicitly designed to deal with vanishing gradients and efficiently learn long-range dependencies. We\u2019ll cover them in the next part of this article.</p>\n<p>It will come soon!</p>\n<p>\u00a0</p>\n<h3>Resources I used when writing this article:</h3>\n<p>\u00a0</p>\n<ul>\n<li><a href=\"http://introtodeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">http://introtodeeplearning.com</a>\n<li><a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">On the difficulty of training recurrent neural networks</a>\n<li><a href=\"https://cs.stanford.edu/~quocle/paragraph_vector.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Distributed Representations of Sentences and Documents</a>\n<li><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noopener noreferrer\" target=\"_blank\">Understanding LSTM Networks</a>\n<li><a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" rel=\"noopener noreferrer\" target=\"_blank\">Backpropagation Through Time and Vanishing Gradients</a>\n</li></li></li></li></li></ul>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"https://www.linkedin.com/in/zied-hajyahia/\" rel=\"noopener noreferrer\" target=\"_blank\">Zied Haj-Yahia</a></b> is Senior Data Scientist at Capgemini Consulting. He specializes in building predictive models utilizing both traditional statistical methods and modern machine learning techniques. He also runs some workshops for university students (ESSEC, HEC, Ecole polytechnique) interested in Data Science and its applications. He is the co-founder of Global International Trading (GIT), a central purchasing office based in Paris.</p>\n<p><a href=\"https://ziedhy.github.io/2018/09/Sequence_Modeling_Part_1.html\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/09/introduction-deep-learning.html\">Introduction to Deep Learning</a>\n<li><a href=\"/2016/01/seven-steps-deep-learning.html\">7 Steps to Understanding Deep Learning</a>\n<li><a href=\"/2018/09/ultimate-guide-tensorflow.html\">Ultimate Guide to Getting Started with TensorFlow</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>\n<div class=\"page-link\"></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2018/10/upcoming-meetings-ai-analytics-big-data-science-machine-learning.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2018/10/linear-regression-wild.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-1-another-10');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-2-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/03/data-science-job-applications.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-3-tell-you');\"><b>What no one will tell you about data science job applications</b></a>\n<li> <a href=\"/2019/03/typical-data-scientist-2019.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-4-typical');\"><b>Who is a typical Data Scientist in 2019?</b></a>\n<li> <a href=\"/2019/02/asking-great-questions-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-5-great-questions');\"><b>Asking Great Questions as a Data Scientist</b></a>\n<li> <a href=\"/2019/03/pareto-principle-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-6-pareto');\"><b>The Pareto Principle for Data Scientists</b></a>\n<li> <a href=\"/2019/03/women-ai-big-data-science-machine-learning.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-7-19-inspiring-women');\"><b>19 Inspiring Women in AI, Big Data, Data Science, Machine Learning</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-1-ann-genetic');\"><b>Artificial Neural Networks Optimization using Genetic Algorithm with Python</b></a>\n<li> <a href=\"/2019/02/artificial-neural-network-implementation-using-numpy-and-image-classification.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-2-ann-numpy-images');\"><b>Artificial Neural Network Implementation using NumPy and Image Classification</b></a>\n<li> <a href=\"/2019/02/setup-python-environment-machine-learning.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-3-py-ml-setup');\"><b>How to Setup a Python Environment for Machine Learning</b></a>\n<li> <a href=\"/2019/03/typical-data-scientist-2019.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-4-typical');\"><b>Who is a typical Data Scientist in 2019?</b></a>\n<li> <a href=\"/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-5-azure-cert');\"><b>8 Reasons Why You Should Get a Microsoft Azure Certification</b></a>\n<li> <a href=\"/2019/03/pareto-principle-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-6-pareto');\"><b>The Pareto Principle for Data Scientists</b></a>\n<li> <a href=\"/2019/02/running-r-and-python-in-jupyter.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-7-r-python-jupyter');\"><b>Running R and Python in Jupyter</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/03/datathon-data-science-hackathon-april.html\">Datathon 2019: The International Data Science Hackathon...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/random-forest-python.html\">Explaining Random Forest (with Python Implementation)</a><li> <a href=\"https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html\">A Beginner\u2019s Guide to Linear Regression in Python wit...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/interpolation-autoencoders-adversarial-regularizer.html\">Interpolation in Autoencoders via an Adversarial Regula...</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/03-29-cisco-machine-learning-engineer-support-bot-b.html\">Cisco: Machine Learning Engineer/Support Bot Designer [...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/delaware-gain-skills-need-data-driven-career.html\">Gain the Skills You Need to Level-Up in Your Data-Drive...</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/03/delaware-gain-skills-need-data-driven-career.html\">Gain the Skills You Need to Level-Up in Your Data-Driven Career</a><li> <a href=\"https://www.kdnuggets.com/2019/03/d3js-graph-gallery-data-visualization.html\">D3.js Graph Gallery for Data Visualization</a><li> <a href=\"https://www.kdnuggets.com/2019/03/7-gotchas-data-engineers-google-bigquery.html\">7 \u201cGotchas\u201d for Data Engineers New to Google BigQuery</a><li> <a href=\"https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html\">The Deep Learning Toolset\u200a\u2014\u200aAn Overview</a><li> <a href=\"https://www.kdnuggets.com/2019/03/top-tweets-mar20-26.html\">Top tweets, Mar 20-26: 10 More Free Must-Read Books for Mac...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/ieg-network-google-intel-facebook.html\">Network with Google, Intel, Facebook, LinkedIn &amp; more</a><li> <a href=\"https://www.kdnuggets.com/2019/03/activestate-python-programmer.html\">[PDF] Python: The Programmer\u2019s Lingua Franca</a><li> <a href=\"https://www.kdnuggets.com/2019/03/explainable-ai.html\">Explainable AI or Halting Faulty Models ahead of Disaster</a><li> <a href=\"https://www.kdnuggets.com/2019/03/how-choose-right-chart-type.html\">How to Choose the Right Chart Type</a><li> <a href=\"https://www.kdnuggets.com/2019/03/data-pipelines-luigi-airflow-everything-need-know.html\">Data Pipelines, Luigi, Airflow: Everything you need to know</a><li> <a href=\"https://www.kdnuggets.com/2019/n12.html\">KDnuggets 19:n12, Mar 27: My Best Tips for Agile Data Scien...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/top-news-week-0318-0324.html\">Top Stories, Mar 18-24: Another 10 Free Must-Read Books for Ma...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/databricks-solve-big-problems-data-science-ebook.html\">How to solve 4 big problems in data science \u2013 eBook.</a><li> <a href=\"https://www.kdnuggets.com/2019/03/four-levels-analytics-maturity.html\">The Four Levels of Analytics Maturity</a><li> <a href=\"https://www.kdnuggets.com/2019/03/pedestrian-detection-aerial-images-retinanet.html\">Pedestrian Detection in Aerial Images Using RetinaNet</a><li> <a href=\"https://www.kdnuggets.com/2019/03/data-science-decision-makers.html\">Data Science for Decision Makers: A Discussion with Dr Stelios...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/coursera-earn-ibm-data-science-certificate.html\">Earn an IBM Data Science Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/03/databricks-scaling-big-data-ai-spark-ai-summit-2019.html\">Scaling Big Data and AI \u2013 Spark + AI Summit 2019</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/03-25-pear-therapeutics-data-scientist.html\">Pear Therapeutics: Data Scientist (Analytics) [San Francisco, ...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/ai-black-box-explanation-problem.html\">The AI Black Box Explanation Problem</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2018/index.html\">2018</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/10/index.html\">Oct</a> \u00bb <a href=\"https://www.kdnuggets.com/2018/10/tutorials.html\">Tutorials, Overviews</a> \u00bb Sequence Modeling with Neural Networks \u2013 Part I (\u00a0<a href=\"/2018/n38.html\">18:n38</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1553995273\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"bottom-right\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"bottom-right\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.692 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-03-30 21:21:13 -->\n<!-- Compression = gzip -->", "content_html": "<div class=\"post\" id=\"post-\">\n<div align=\"right\"><img alt=\"c\" height=\"12\" src=\"/images/comment.gif\" width=\"16\"/> <a href=\"#comments\">comments</a></div>\n<p><b>By <a href=\"https://www.linkedin.com/in/zied-hajyahia/\" rel=\"noopener noreferrer\" target=\"_blank\">Zied Haj-Yahia</a>, Senior Data Scientist at Capgemini Consulting</b></p>\n<p>\u00a0</p>\n<h3>Context</h3>\n<p>\u00a0<br>\nIn the previous course\u00a0<a href=\"https://ziedhy.github.io/2018/08/Introduction_Deep_Learning.html\" rel=\"noopener noreferrer\" target=\"_blank\">Introduction to Deep Learning</a>, we saw how to use Neural Networks to model a dataset of many examples. The good news is that the basic architecture of Neural Networks is quite generic whatever the application: a stacking of several perceptrons to compose complex hierarchical models and their optimization using gradient descent and backpropagation.</br></p>\n<p>Inspite of this, you have probably heard about Multilayer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), LSTM, Auto-Encoders, etc. These deep learning models are different from each other. Each model is known to be particulary performant in some specific tasks, even though, fundamentally, they all share the same basic architecture.</p>\n<p>What makes the difference between them is their ability to be more suited for some data structures: text processing could be different from image processing, which in turn could be different from signal processing.</p>\n<p>In the context of this post, we will focus on modeling\u00a0<strong>sequences</strong>\u00a0as a well-known data structure and will study its\u00a0<strong>specific learning framework</strong>.</p>\n<p>Applications of sequence modeling are plentiful in day-to-day business practice. Some of them emerged to meet today\u2019s challenges in terms of quality of service and customer engagement. Here some examples:</p>\n<ul>\n<li>Speech Recognition to listen to the voice of customers.\n<li>Machine Language Translation from diverse source languages to more common languages.\n<li>Topic Extraction to find the main subject of customer\u2019s translated query.\n<li>Speech Generation to have conversational ability and engage with customers in a human like manner.\n<li>Text Summarization of customer feedback to work on key challenges and pain points.\n</li></li></li></li></li></ul>\n<p>In the auto industry, self-parking is also a sequence modeling task. In fact, parking could be seen as a sequence of mouvements where the next movement depends on the previous ones.</p>\n<p>Other applications cover text classification, translating videos to natural language, image caption generation, hand writing recognition/generation, anomaly detection, and many more in the future\u2026which none of us can think (or aware) at the moment.</p>\n<p>However, before we go any further in the applications of Sequence Modeling, let us understand what we are dealing with when we talk about sequences.</p>\n<p>\u00a0</p>\n<h3>Introduction to Sequence Modeling</h3>\n<p>\u00a0<br>\nSequences are a data structure where each example could be seen as a series of data points. This sentence: \u201cI am currently reading an article about sequence modeling with Neural Networks\u201d is an example that consists of multiple words and words depend on each other. The same applies to medical records. One single medical record consists in many measurments across time. It is the same for speech waveforms.</br></p>\n<p><strong><em>So why we need a different learning framework to model sequences and what are the special features that we are looking for in this framework?</em></strong></p>\n<p>For illustration purposes and with no loss of generality, let us focus on text as a sequence of words to motivate this need for a different learning framework.</p>\n<p>In fact, machine learning algorithms typically require the text input to be represented as a\u00a0<strong>fixed-length</strong>vector. Many operations needed to train the model (network) can be expressed through algebraic operations on the matrix of input feature values and the matrix of weights (think about a n-by-p design matrix, where n is the number of samples observed, and p is the number of variables measured in all samples).</p>\n<p>Perhaps the most common fixed-length vector representation for texts is the\u00a0<strong>bag-of-words</strong>\u00a0or bag-of-n-grams due to its simplicity, efficiency and often surprising accuracy. However, the bag-of-words (BOW) representation has many disadvantages:</p>\n<ul>\n<li>First, the word order is lost, and thus different sentences can have exactly the same representation, as long as the same words are used. Example: \u201cThe food was good, not bad at all.\u201d vs \u201cThe food was bad, not good at all.\u201d. Even though bag-of-n-grams considers the word order in short context, it suffers from data sparsity and high dimensionality.\n<li>In addition, Bag-of-words and bag-of-n-grams have very little knowledge about the semantics of the words or more formally the distances between the words. This means that words \u201cpowerful\u201d, \u201cstrong\u201d and \u201cParis\u201d are equally distant despite the fact that semantically, \u201cpowerful\u201d should be closer to \u201cstrong\u201d than \u201cParis\u201d.\n<li>Humans don\u2019t start their thinking from scratch every second. As you read this article,\u00a0<strong>you understand each word based on your understanding of previous words</strong>. Traditional neural networks can\u2019t do this, and it seems like a major shortcoming. Bag-of-words and bag-of-n-grams as text representations do not allow to keep track of long-term dependencies inside the same sentence or paragraph.\n<li>Another disadvantage of modeling sequences with traditional Neural Networks (e.g.\u00a0Feedforward Neural Networks) is the fact of not sharing parameters across time. Let us take for example these two sentences : \u201cOn Monday, it was snowing\u201d and \u201cIt was snowing on Monday\u201d. These sentences mean the same thing, though the details are in different parts of the sequence. Actually, when we feed these two sentences into a Feedforward Neural Network for a prediction task, the model will assign different weights to \u201cOn Monday\u201d at each moment in time.\u00a0<strong>Things we learn about the sequence won\u2019t transfer if they appear at different points in the sequence.</strong>\u00a0Sharing parameters gives the network the ability to look for a given feature everywhere in the sequence, rather than in just a certain area.\n</li></li></li></li></ul>\n<p>Thus, to model sequences, we need a specific learning framework able to:</p>\n<ul>\n<li>deal with variable-length sequences\n<li>maintain sequence order\n<li>keep track of long-term dependencies rather than cutting input data too short\n<li>share parameters across the sequence (so not re-learn things across the sequence)\n</li></li></li></li></ul>\n<p>Recurrent neural networks (RNNs) could address this issue. They are networks with loops in them, allowing information to persist.</p>\n<p>So, let us find out more about RNNs!</p>\n<p>\u00a0</p>\n<h3>Recurrent Neural Networks</h3>\n<p>\u00a0</p>\n<h3>How a Recurrent Neural Network works?</h3>\n<p>\u00a0<br>\nA Recurrent Neural Network is architected in the same way as a \u201ctraditional\u201d Neural Network. We have some inputs, we have some hidden layers and we have some outputs.</br></p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/fNwbPK/zied_sequence_1.png\" width=\"80%\"/></p>\n<p>The only difference is that each hidden unit is doing a slightly different function. So, let\u2019s explore how this hidden unit works.</p>\n<p>A recurrent hidden unit computes a function of an input and its own previous output, also known as the cell state. For textual data, an input could be a vector representing a word\u00a0<em>x(i)</em>\u00a0in a sentence of\u00a0<em>n</em>\u00a0words (also known as word embedding).</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/jNukJe/zied_sequence_2.png\" width=\"99%\"/></p>\n<p><em>W</em>\u00a0and\u00a0<em>U</em>\u00a0are weight matrices and\u00a0<em>tanh</em>\u00a0is the hyperbolic tangent function.</p>\n<p>Similarly, at the next step, it computes a function of the new input and its previous cell state:\u00a0<strong><em>s2</em>\u00a0=\u00a0<em>tanh</em>(<em>Wx1</em>+\u00a0<em>Us1</em> </strong>. This behavior is similar to a hidden unit in a feed-forward Network. The difference, proper to sequences, is that we are adding an additional term to incorporate its own previous state.</p>\n<p>A common way of viewing recurrent neural networks is by unfolding them across time. We can notice that <strong>we are using the same weight matrices <em>W</em> and <em>U</em> throughout the sequence. This solves our problem of parameter sharing</strong>. We don\u2019t have new parameters for every point of the sequence. Thus, once we learn something, it can apply at any point in the sequence.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/dxsuBz/zied_sequence_3.png\" width=\"75%\"/></p>\n<p>The fact of not having new parameters for every point of the sequence also helps us\u00a0<strong>deal with variable-length sequences</strong>. In case of a sequence that has a length of 4, we could unroll this RNN to four timesteps. In other cases, we can unroll it to ten timesteps since the length of the sequence is not prespecified in the algorithm. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word.</p>\n<p><b>NB:</b></p>\n<ul>\n<li><em>Sn</em>, the cell state at time\u00a0<em>n</em>, can contain information from all of the past timesteps: each cell state is a function of the previous self state which in turn is a function of the previous cell state.\u00a0<strong>This solves our issue of long-term dependencies</strong>.\n<li>The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence, we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence.\n</li></li></ul>\n<p>Now that we understand how a single hidden unit works, we need to figure out how to train an entire Recurrent Neural Network made up of many hidden units and even many layers of many hidden units.</p>\n<p>\u00a0</p>\n<h3>How do we train a Recurrent Neural Network?</h3>\n<p>\u00a0<br>\nLet\u2019s consider the following task: for a set of speeches in English, we need the model to automatically convert the spoken language into text i.e.\u00a0at each timestep, the model produces a prediction of a transcript (an output) based on the part of speech at this timestep (the new input) and the previous transcript (the previous cell state).</br></p>\n<p>Naturally, because we have an output at every timestep, we can have a loss at every timestep. This loss reflects how much the predicted transcripts are close to the \u201cofficial\u201d transcripts.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/buBnWz/zied_sequence_4.png\"/></p>\n<p>The total loss is just the sum of the losses at every timestep.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/drgXye/zied_sequence_5.png\"/></p>\n<p>Since the loss is a function of the network weights, our task it to find the set of weights\u00a0<em>theta</em>\u00a0that achieve the lowest loss. For that, as explained in the first article \u201cIntroduction to Deep Learning\u201d, we we can apply\u00a0<strong>the gradient descent algorithm with backpropagation (chain rule) at every timestep</strong>, thus taking into account the additional time dimension.</p>\n<p><em>W</em>\u00a0and\u00a0<em>U</em>\u00a0are our two weight matrices. Let us try it out for\u00a0<em>W</em>.</p>\n<p>Knowing that the total loss is the sum of the losses at every timestep, the total gradient is just the sum of the gradients at every timestep:</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/fvvEBz/zied_sequence_6.png\"/></p>\n<p>And now, we can focus on a single timestep to calculate the derivative of the loss with respect to\u00a0<em>W</em>.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/f3PVjK/zied_sequence_7.png\"/></p>\n<p>Easy to handle: we just use backpropagation.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/kqfQJe/zied_sequence_8.png\"/></p>\n<p>We remember that\u00a0<strong><em>s2\u00a0=\u00a0tanh(Wx1\u00a0+\u00a0Us1)</em></strong>\u00a0so s2 also depends on s1 and s1 also depends on W.\u00a0<strong>This actually means that we can not just leave the derivative of\u00a0<em>s2</em>\u00a0with respect to\u00a0<em>W</em>\u00a0as a constant. We have to expand it out farther.</strong></p>\n<p>So how does\u00a0<em>s2</em>\u00a0depend on\u00a0<em>W</em>?</p>\n<p>It depends directly on W because it feeds right in (c.f. above formula of\u00a0<em>s2</em>). We also know that\u00a0<em>s2</em>\u00a0depends on\u00a0<em>s1</em>\u00a0which depends on W. And we can also see that\u00a0<em>s2</em>\u00a0depends on\u00a0<em>s0</em>\u00a0which also depends on W.</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/duni4K/zied_sequence_9.png\" width=\"75%\"/></p>\n<p>Thus, the derivative of the loss with respect to\u00a0<em>W</em>\u00a0could be written as follows:</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/mtxi4K/zied_sequence_10.png\"/></p>\n<p>We can see that the last two terms are basically summing the contributions of\u00a0<em>W</em>\u00a0in previous timesteps to the error at timestep\u00a0<em>t</em>. This is key to understand how we model long-term dependencies. From one iteration to another, the gradient descent algorithm allows to shift network parameters such that they include contributions to the error from past timesteps.</p>\n<p>For any timestep\u00a0<em>t</em>, the derivative of the loss with respect to\u00a0<em>W</em>\u00a0could be written as follows:</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/cdEKde/zied_sequence_11.png\"/></p>\n<p>So to train the model i.e.\u00a0to estimate the weights of the network, we apply this same process of backpropagation through time for every weight (parameter) and then we use it in the process of gradient descent.</p>\n<p>\u00a0</p>\n<h3>Why are Recurrent Neural Networks hard to train?</h3>\n<p>\u00a0<br/>\nIn practice RNNs are a bit difficult to train. To understand why, let\u2019s take a closer look at the gradient we calculated above:</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/ix6nWz/zied_sequence_12.png\" width=\"99%\"/></p>\n<p>We can see that as the gap between timesteps gets bigger, the product of the gradients gets longer and longer. But, what are each of these terms?</p>\n<p><img alt=\"Image\" class=\"aligncenter\" src=\"https://image.ibb.co/c3wnWz/zied_sequence_13.png\"/></p>\n<p>Each term is basically a product of two terms: transposed\u00a0<em>W</em>\u00a0and a second one that depends on f\u2019, the derivative of the activation function.</p>\n<ul>\n<li>Initial weights\u00a0<em>W</em>\u00a0are usually sampled from standard normal distribution and then mostly &lt; 1.\n<li>It turns out (I won\u2019t prove it here but\u00a0<a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">this paper</a>\u00a0goes into detail) that the second term is a Jacobian matrix because we are taking the derivative of a vector function with respect to a vector and its 2-norm, which you can think of it as an absolute value,\u00a0<strong>has an upper bound of 1</strong>. This makes intuitive sense because our tanh (or sigmoid) activation function maps all values into a range between -1 and 1, and the derivative f\u2019 is bounded by 1 (1/4 in the case of sigmoid).\n</li></li></ul>\n<p>Thus, with small values in the matrix and multiple matrix multiplications, the\u00a0<strong>gradient values are shrinking exponentially fast, eventually vanishing completely after a few time steps</strong>. Gradient contributions from \u201cfar away\u201d steps become zero, and the state at those steps doesn\u2019t contribute to what you are learning: you end up not learning long-range dependencies.</p>\n<p>Vanishing gradients aren\u2019t exclusive to RNNs. They also happen in deep Feedforward Neural Networks. It\u2019s just that RNNs tend to be very deep (as deep as the sentence length in our case), which makes the problem a lot more common.</p>\n<p>Fortunately, there are a few ways to combat the vanishing gradient problem.\u00a0<strong>Proper initialization of the\u00a0<em>W</em>matrix</strong>\u00a0can reduce the effect of vanishing gradients. So can regularization. A more preferred solution is to use\u00a0<strong><em>ReLU</em></strong>\u00a0instead of tanh or sigmoid activation functions. The ReLU derivative is a constant of either 0 or 1, so it isn\u2019t as likely to suffer from vanishing gradients.</p>\n<p>An even more popular solution is to use Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) architectures. LSTMs were first proposed in 1997 and are perhaps the most widely used models in NLP today. GRUs, first proposed in 2014, are simplified versions of LSTMs. Both of these RNN architectures were explicitly designed to deal with vanishing gradients and efficiently learn long-range dependencies. We\u2019ll cover them in the next part of this article.</p>\n<p>It will come soon!</p>\n<p>\u00a0</p>\n<h3>Resources I used when writing this article:</h3>\n<p>\u00a0</p>\n<ul>\n<li><a href=\"http://introtodeeplearning.com/\" rel=\"noopener noreferrer\" target=\"_blank\">http://introtodeeplearning.com</a>\n<li><a href=\"http://proceedings.mlr.press/v28/pascanu13.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">On the difficulty of training recurrent neural networks</a>\n<li><a href=\"https://cs.stanford.edu/~quocle/paragraph_vector.pdf\" rel=\"noopener noreferrer\" target=\"_blank\">Distributed Representations of Sentences and Documents</a>\n<li><a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\" rel=\"noopener noreferrer\" target=\"_blank\">Understanding LSTM Networks</a>\n<li><a href=\"http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\" rel=\"noopener noreferrer\" target=\"_blank\">Backpropagation Through Time and Vanishing Gradients</a>\n</li></li></li></li></li></ul>\n<p>\u00a0<br/>\n<b>Bio: <a href=\"https://www.linkedin.com/in/zied-hajyahia/\" rel=\"noopener noreferrer\" target=\"_blank\">Zied Haj-Yahia</a></b> is Senior Data Scientist at Capgemini Consulting. He specializes in building predictive models utilizing both traditional statistical methods and modern machine learning techniques. He also runs some workshops for university students (ESSEC, HEC, Ecole polytechnique) interested in Data Science and its applications. He is the co-founder of Global International Trading (GIT), a central purchasing office based in Paris.</p>\n<p><a href=\"https://ziedhy.github.io/2018/09/Sequence_Modeling_Part_1.html\" rel=\"noopener noreferrer\" target=\"_blank\">Original</a>. Reposted with permission.</p>\n<p><b>Related:</b></p>\n<ul class=\"three_ul\">\n<li><a href=\"/2018/09/introduction-deep-learning.html\">Introduction to Deep Learning</a>\n<li><a href=\"/2016/01/seven-steps-deep-learning.html\">7 Steps to Understanding Deep Learning</a>\n<li><a href=\"/2018/09/ultimate-guide-tensorflow.html\">Ultimate Guide to Getting Started with TensorFlow</a>\n</li></li></li></ul>\n<p><a name=\"comments\"></a></p>\n<div id=\"disqus_thread\"></div>\n<p> <script type=\"text/javascript\">\n var disqus_shortname = 'kdnuggets';\n (function() { var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true; dsq.src = 'https://kdnuggets.disqus.com/embed.js';\n (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq); })();\n </script></p>\n</div>"}