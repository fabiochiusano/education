{"url": "https://www.kdnuggets.com/bayesian.html", "title": "Bayesian Basics, Explained", "title_html": "<h1 id=\"title\"><img align=\"right\" alt=\"Silver Blog\" src=\"/images/top-kdnuggets-blog-2016-dec-silver.png\" width=\"80\"/>Bayesian Basics, Explained</h1>", "content": "\n By Kevin Gray, Cannon Gray. \n \nEditor's note: The following is an interview with Columbia University Professor Andrew Gelman conducted by Marketing scientist Kevin Gray, in which Gelman spells out the ABCs of Bayesian statistics.\n \n Kevin Gray: Most marketing researchers have heard of Bayesian statistics but know little about it. \u00a0Can you briefly explain in layperson's terms what it is and how it differs from the 'ordinary' statistics most of us learned in college? \n Andrew Gelman: Bayesian statistics uses the mathematical rules of probability to combines data with \u201cprior information\u201d to give inferences which (if the model being used is correct) are more precise than would be obtained by either source of information alone. \n Classical statistical methods avoid prior distributions. \u00a0In classical statistics, you might include in your model a predictor (for example), or you might exclude it, or you might pool it as part of some larger set of predictors in order to get a more stable estimate. \u00a0These are pretty much your only choices. \u00a0In Bayesian inference you can\u2014OK, you must\u2014assign a prior distribution representing the set of values the coefficient can be. \u00a0You can reproduce the classical methods using Bayesian inference: \u00a0In a regression prediction context, setting the prior of a coefficient to uniform or \u201cnoninformative\u201d is mathematically equivalent to including the corresponding predictor in a least squares or maximum likelihood estimate; setting the prior to a spike at zero is the same as excluding the predictor, and you can reproduce a pooling of predictors thorough a joint deterministic prior on their coefficients. \u00a0But in \u00a0Bayesian inference you can do much more: \u00a0by setting what is called an \u201cinformative prior,\u201d you can partially constrain a coefficient, setting a compromise between noisy least-squares estimation or completely setting it to zero. \u00a0It turns out this is a powerful tool in many problems\u2014especially because in problems with structure, we can fit so-called hierarchical models which allow us to estimate aspects of the prior distribution from data. \n  \n KG: Could you give us a quick overview of its history and how it has developed over the years? \n AG: \u00a0The theory of Bayesian inference originates with its namesake, Thomas Bayes, an 18th-century English cleric, but it really took off in the late 18th century with the work of the French mathematician and physicist Pierre-Simon Laplace. Bayesian methods were used for a long time after that to solve specific problems in science, but it was in the mid-20th century that they became proposed as a general statistical tool. Some key figures include John Maynard Keynes and Frank Ramsey who in the 1920s developed an axiomatic theory of probability; Harold Jeffreys and Edwin Jaynes, who from the 1930s through the 1970s developed Bayesian methods for a variety of problems in the physical sciences; Jimmie Savage and Dennis Lindley, mathematicians who in research from the 1950s through the 1970s connected and contrasted Bayesian methods with classical statistics; and, not least, Alan Turing, who used Bayesian probability methods to crack the Enigma code in the second world war, and his colleague I. J. Good, who explored and wrote prolifically about these ideas over the succeeding decades. \n Within statistics, Bayesian and related methods have become gradually more popular over the past several decades, often developed in different applied fields, such as animal breeding in the 1950s, educational measurement in the 1960s and 1970s, spatial statistics in the 1980s, and marketing and political science in the 1990s. \u00a0Eventually a sort of critical mass developed in which Bayesian models and methods that had been developed in different applied fields became recognized as more broadly useful. \n Another factor that has fostered the spread of Bayesian methods is progress in computing speed and improved computing algorithms. \u00a0\u00a0Except in simple problems, Bayesian inference requires difficult mathematical calculations\u2014high-dimensional integrals\u2014which are often most practically computed using stochastic simulation, that is, computation using random numbers. \u00a0This is the so-called Monte Carlo method, which was developed systematically by the mathematician Stanislaw Ulam and others when trying out designs for the hydrogen bomb in the 1940s and then rapidly picked up in the worlds of physics and chemistry. \u00a0The potential for these methods to solve otherwise intractable statistics problems became apparent in the 1980s, and since then each decade has seen big jumps in the sophistication of algorithms, the capacity of computers to run these algorithms in real time, and the complexity of the statistical models that practitioners are now fitting to data. \n Now, don\u2019t get me wrong\u2014computational and algorithmic advances have become hugely important in non-Bayesian statistical and machine learning methods as well. Bayesian inference has moved, along with statistics more generally, away from simple formulas toward simulation-based algorithms. \n KG: What are its key strengths in comparison with Frequentist methods? Are there things that only Bayesian statistics can provide? What are its main drawbacks? \n AG: I wouldn\u2019t say there\u2019s anything that only Bayesian statistics can provide. \u00a0When Bayesian methods work best, it\u2019s by providing a clear set of paths connecting data, mathematical/statistical models, and the substantive theory of the variation and comparison of interest. \u00a0From this perspective, the greatest benefits of the Bayesian approach come not from default implementations, valuable as they can be in practice, but in the active process of model building, checking, and improvement. \u00a0\u00a0In classical statistics, improvements in methods often seem distressingly indirect: you try a new test that\u2019s supposed to capture some subtle aspect of your data, or you restrict your parameters or smooth your weights, in some attempt to balance bias and variance. \u00a0Under a Bayesian approach, all the tuning parameters are supposed to be interpretable in real-world terms, which implies\u2014or should imply\u2014that improvements in a Bayesian model come from, or supply, improvements in understanding of the underlying problem under studied. \u00a0The drawback of this Bayesian approach is that it can require a bit of a commitment to construction of a model that might be complicated, and you can end up putting effort into modeling aspects of data that maybe aren\u2019t so relevant for your particular inquiry. \n KG: Are there misunderstandings about Bayesian methods that you often encounter? \n AG: Yes, but that\u2019s a whole subject in itself\u2014I\u2019ve written papers on the topic! \u00a0The only thing I\u2019ll say here is that Bayesian methods are often characterized as \u201csubjective\u201d because the user must choose a \u201cprior distribution,\u201d that is, a mathematical expression of prior information. \u00a0The prior distribution requires information and user input, that\u2019s for sure, but I don\u2019t see this as being any more \u201csubjective\u201d than other aspects of a statistical procedure, such as the choice of model for the data (for example, logistic regression) or the choice of which variables to include in a prediction, the choice of which coefficients should vary over time or across situations, the choice of statistical test, and so forth. Indeed, Bayesian methods can in many ways be more \u201cobjective\u201d than conventional approaches in that Bayesian inference, with its smoothing and partial pooling, is well adapted to including diverse sources of information and thus can reduce the number of data coding or data exclusion choice points in an analysis. \n KG: Do you think Bayesian methods will one day mostly replace Frequentist statistics? \n AG: There\u2019s room for lots of methods. \u00a0What\u2019s important in any case is what problems they can solve. \u00a0We use the methods we already know and then learn something new when we need to go further. Bayesian methods offer a clarity that comes from the explicit specification of a so-called \u201cgenerative model\u201d: \u00a0a probability model of the data-collection process and a probability model of the underlying parameters. \u00a0But construction of these models can take work, and it makes sense to me that for problems where you have a simpler model that does the job, you just go with that. \n Looking at the comparison from the other direction, when it comes to big problems with streaming data, Bayesian methods are useful but the Bayesian computation can in practice only be approximate. \u00a0And once you enter the zone of approximation, you can\u2019t cleanly specify where the modeling approximation ends and the computing approximation begins. \u00a0At that point, you need to evaluate any method, Bayesian or otherwise, by looking at what it does to the data, and the best available method for any particular problem might well be set up in a non-Bayesian way. \n", "html": "<!DOCTYPE html>\n\n<html lang=\"en-US\" xmlns=\"http://www.w3.org/1999/xhtml\">\n<head profile=\"http://gmpg.org/xfn/11\">\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n<title>  Bayesian Basics, Explained</title>\n<link href=\"/wp-content/themes/kdn17/images/favicon.ico\" rel=\"shortcut icon\"/>\n<link href=\"/wp-content/themes/kdn17/style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<script src=\"/wp-content/themes/kdn17/js/jquery-1.9.1.min.js\" type=\"text/javascript\"></script>\n<script src=\"/aps/kda_all.js\" type=\"text/javascript\"></script>\n<link href=\"/feed/\" rel=\"alternate\" title=\"KDnuggets: Analytics, Big Data, Data Mining and Data Science Feed\" type=\"application/rss+xml\"/>\n<link href=\"//s.w.org\" rel=\"dns-prefetch\"/>\n<link href=\"https://www.kdnuggets.com/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/comments/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/2016/12/bayesian-basics-explained.html/feed\" rel=\"alternate\" title=\"KDnuggets \u00bb Bayesian Basics, Explained Comments Feed\" type=\"application/rss+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-json/\" rel=\"https://api.w.org/\"/>\n<link href=\"https://www.kdnuggets.com/xmlrpc.php?rsd\" rel=\"EditURI\" title=\"RSD\" type=\"application/rsd+xml\"/>\n<link href=\"https://www.kdnuggets.com/wp-includes/wlwmanifest.xml\" rel=\"wlwmanifest\" type=\"application/wlwmanifest+xml\"/>\n<link href=\"https://www.kdnuggets.com/2016/12/4-cognitive-bias-key-points-data-scientists-need-know.html\" rel=\"prev\" title=\"4 Cognitive Bias Key Points Data Scientists Need to Know\"/>\n<link href=\"https://www.kdnuggets.com/2016/12/poll-analytics-data-mining-data-science-applied-2016.html\" rel=\"next\" title=\"Where Analytics, Data Mining, Data Science were applied in 2016\"/>\n<meta content=\"WordPress 4.9.10\" name=\"generator\">\n<link href=\"https://www.kdnuggets.com/2016/12/bayesian-basics-explained.html\" rel=\"canonical\"/>\n<link href=\"https://www.kdnuggets.com/?p=59102\" rel=\"shortlink\"/>\n<link href=\"https://www.kdnuggets.com/2016/12/bayesian-basics-explained.html\" rel=\"canonical\"/>\n<!-- BEGIN ExactMetrics v5.3.7 Universal Analytics - https://exactmetrics.com/ -->\n<script>\n(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n\t(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n\tm=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n  ga('create', 'UA-361129-1', 'auto');\n  ga('send', 'pageview');\n</script>\n<!-- END ExactMetrics Universal Analytics -->\n</meta></head>\n<body class=\"post-template-default single single-post postid-59102 single-format-standard\">\n<div class=\"main_wrapper\"><!-- publ: 9-Dec, 2016  -->\n<div id=\"wrapper\">\n<div id=\"header\">\n<div id=\"header_log\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<h1>KDnuggets</h1>\n<div class=\"text-container\">\n            \u00a0\u00a0<a href=\"/news/subscribe.html\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a> \u00a0|\n <a href=\"https://twitter.com/kdnuggets\" target=\"_blank\"><img alt=\"Twitter\" height=\"48\" src=\"/images/tw_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n <a href=\"https://www.facebook.com/kdnuggets\" target=\"_blank\"><img alt=\"Facebook\" height=\"48\" src=\"/images/fb_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \u00a0\u00a0\n<a href=\"https://www.linkedin.com/groups/54257/\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"48\" src=\"/images/in_c48.png\" style=\"vertical-align: bottom\" width=\"48\"/></a> \n\u00a0|\u00a0 <a href=\"/contact.html\"><b>Contact</b></a>\n</div>\n</div>\n<div class=\"search\">\n<form action=\"/\" id=\"searchform\" method=\"get\">\n<input id=\"s\" name=\"s\" placeholder=\"search KDnuggets\" type=\"text\" value=\"\"/>\n<input type=\"submit\" value=\"Search\"/></form>\n</div>\n<div href=\"#\" id=\"pull\">\n<img class=\"menu\" src=\"/images/menu-30.png\">\n<div class=\"logo\">\n<a href=\"/\"></a>\n</div>\n<img class=\"search-icon\" src=\"/images/search-icon.png\">\n</img></img></div>\n<div id=\"pull-menu\">\n<div class=\"navigation\"><ul class=\"menu\" id=\"menu-menu\"><li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-1070\" id=\"menu-item-1070\"><a href=\"/software/index.html\" title=\"Data Science Software\">SOFTWARE</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13756\" id=\"menu-item-13756\"><a href=\"/news/index.html\" title=\"News\">News/Blog</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-46286\" id=\"menu-item-46286\"><a href=\"/news/top-stories.html\">Top stories</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-42152\" id=\"menu-item-42152\"><a href=\"https://www.kdnuggets.com/opinions/index.html\" title=\"Opinions\">Opinions</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-46415\" id=\"menu-item-46415\"><a href=\"https://www.kdnuggets.com/tutorials/index.html\">Tutorials</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13364\" id=\"menu-item-13364\"><a href=\"/jobs/index.html\" title=\"Jobs in Analytics, Data Science\">JOBS</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-63505\" id=\"menu-item-63505\"><a href=\"https://www.kdnuggets.com/companies/index.html\">Companies</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13366\" id=\"menu-item-13366\"><a href=\"/courses/index.html\">Courses</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-1499\" id=\"menu-item-1499\"><a href=\"https://www.kdnuggets.com/datasets/index.html\">Datasets</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-14286\" id=\"menu-item-14286\"><a href=\"https://www.kdnuggets.com/education/index.html\" title=\"Education in Analytics, Big Data, Data Science\">EDUCATION</a></li>\n<li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-51558\" id=\"menu-item-51558\"><a href=\"https://www.kdnuggets.com/education/analytics-data-mining-certificates.html\" title=\"Certificates in Analytics, Big Data, Data Science\">Certificates</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-14752\" id=\"menu-item-14752\"><a href=\"/meetings/index.html\">Meetings</a></li>\n<li class=\"menu-item menu-item-type-custom menu-item-object-custom menu-item-13721\" id=\"menu-item-13721\"><a href=\"/webcasts/index.html\" title=\"Webcasts and Webinars\">Webinars</a></li>\n</ul></div></div>\n</div> <!--#header end-->\n<div id=\"spacer\">\n         \u00a0\n      </div>\n<div id=\"content_wrapper\">\n<div id=\"ad_wrapper\">\n<script type=\"text/javascript\">\n\tjQuery(function() {\n   \t    var pull        = $('#pull');\n            menu        = $('#header .navigation ul');\n            menuImage = $('#header img.menu');\n            mobileMenu        = $('#pull-menu-mobile');\n            search = $('img.search-icon');\n            searchBar = $('div.search');\n            searchClick = false;\n            search.on('click', function() {\n                  searchBar.slideToggle();\n                  searchClick = true;\n            });  \n     \t    $(menuImage).on('click', function(e) {\n\t        //e.preventDefault();\n                if (!searchClick) {\n                  menu.slideToggle();\n                }\n                searchClick = false;\n\t    });\n           /* pullMobile.on('click', function(e) {\n              e.preventDefault();\n                if (!searchClick) {\n                  mobileMenu.slideToggle();\n                }\n                searchClick = false;\n\t    });*/\n            \n\t});\n\tkpath = '/'; kda_top(); kda_sid_init(); kda_sid_n=3;\n\t</script>\n</div> <div class=\"breadcumb\">\n<br/>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2016/index.html\">2016</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/12/index.html\">Dec</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/12/opinions-interviews.html\">Opinions, Interviews</a> \u00bb Bayesian Basics, Explained (\u00a0<a href=\"/2016/n44.html\">16:n44</a>\u00a0)    </div>\n<div class=\"single\" id=\"content\">\n<div id=\"post-header\">\n<h1 id=\"title\"><img align=\"right\" alt=\"Silver Blog\" src=\"/images/top-kdnuggets-blog-2016-dec-silver.png\" width=\"80\"/>Bayesian Basics, Explained</h1>\n<div class=\"pagi\">\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2016/12/4-cognitive-bias-key-points-data-scientists-need-know.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"> <strong>Previous post</strong></img></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2016/12/poll-analytics-data-mining-data-science-applied-2016.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/>\u00a0<br/>\u00a0\n    <span class=\"http-likes\" style=\"float: left; font-size:14px\">http likes 1272</span> <div class=\"addthis_native_toolbox\"></div>\n</div>\n<div class=\"tag-data\">Tags: <a href=\"https://www.kdnuggets.com/tag/andrew-gelman\" rel=\"tag\">Andrew Gelman</a>, <a href=\"https://www.kdnuggets.com/tag/bayesian\" rel=\"tag\">Bayesian</a>, <a href=\"https://www.kdnuggets.com/tag/explained\" rel=\"tag\">Explained</a></div>\n<br/>\n<p class=\"excerpt\">\n     This interview between Professor Andrew Gelman of Columbia University and marketing scientist Kevin Gray covers the basics of Bayesian statistics and how it differs from the ordinary statistics most of us learned in college.\n  </p>\n</div>\n<div id=\"post-header-ad\">\n<script type=\"text/javascript\">kda_sid_write(1); kda_sid_n=2;</script>\n</div>\n<hr class=\"grey-line\"/><br/>\n<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/kevin-gray\" rel=\"author\" title=\"Posts by Kevin Gray\">Kevin Gray</a>, Cannon Gray.</b></div>\n<blockquote><p>\n<strong>Editor's note</strong>: The following is an interview with Columbia University Professor Andrew Gelman conducted by Marketing scientist Kevin Gray, in which Gelman spells out the ABCs of Bayesian statistics.\n</p></blockquote>\n<p><strong>Kevin Gray: Most marketing researchers have heard of Bayesian statistics but know little about it. \u00a0Can you briefly explain in layperson's terms what it is and how it differs from the 'ordinary' statistics most of us learned in college?</strong></p>\n<p><strong>Andrew Gelman</strong>: Bayesian statistics uses the mathematical rules of probability to combines data with \u201cprior information\u201d to give inferences which (if the model being used is correct) are more precise than would be obtained by either source of information alone.</p>\n<p>Classical statistical methods avoid prior distributions. \u00a0In classical statistics, you might include in your model a predictor (for example), or you might exclude it, or you might pool it as part of some larger set of predictors in order to get a more stable estimate. \u00a0These are pretty much your only choices. \u00a0In Bayesian inference you can\u2014OK, you must\u2014assign a prior distribution representing the set of values the coefficient can be. \u00a0You can reproduce the classical methods using Bayesian inference: \u00a0In a regression prediction context, setting the prior of a coefficient to uniform or \u201cnoninformative\u201d is mathematically equivalent to including the corresponding predictor in a least squares or maximum likelihood estimate; setting the prior to a spike at zero is the same as excluding the predictor, and you can reproduce a pooling of predictors thorough a joint deterministic prior on their coefficients. \u00a0But in \u00a0Bayesian inference you can do much more: \u00a0by setting what is called an \u201cinformative prior,\u201d you can partially constrain a coefficient, setting a compromise between noisy least-squares estimation or completely setting it to zero. \u00a0It turns out this is a powerful tool in many problems\u2014especially because in problems with structure, we can fit so-called hierarchical models which allow us to estimate aspects of the prior distribution from data.</p>\n<p><img alt=\"Bayesian neon\" src=\"/wp-content/uploads/bayesian-neon.jpeg\" width=\"99%\"/></p>\n<p><strong>KG: Could you give us a quick overview of its history and how it has developed over the years?</strong></p>\n<p><strong>AG</strong>: \u00a0The theory of Bayesian inference originates with its namesake, Thomas Bayes, an 18th-century English cleric, but it really took off in the late 18th century with the work of the French mathematician and physicist Pierre-Simon Laplace. Bayesian methods were used for a long time after that to solve specific problems in science, but it was in the mid-20th century that they became proposed as a general statistical tool. Some key figures include John Maynard Keynes and Frank Ramsey who in the 1920s developed an axiomatic theory of probability; Harold Jeffreys and Edwin Jaynes, who from the 1930s through the 1970s developed Bayesian methods for a variety of problems in the physical sciences; Jimmie Savage and Dennis Lindley, mathematicians who in research from the 1950s through the 1970s connected and contrasted Bayesian methods with classical statistics; and, not least, Alan Turing, who used Bayesian probability methods to crack the Enigma code in the second world war, and his colleague I. J. Good, who explored and wrote prolifically about these ideas over the succeeding decades.</p>\n<p>Within statistics, Bayesian and related methods have become gradually more popular over the past several decades, often developed in different applied fields, such as animal breeding in the 1950s, educational measurement in the 1960s and 1970s, spatial statistics in the 1980s, and marketing and political science in the 1990s. \u00a0Eventually a sort of critical mass developed in which Bayesian models and methods that had been developed in different applied fields became recognized as more broadly useful.</p>\n<p>Another factor that has fostered the spread of Bayesian methods is progress in computing speed and improved computing algorithms. \u00a0\u00a0Except in simple problems, Bayesian inference requires difficult mathematical calculations\u2014high-dimensional integrals\u2014which are often most practically computed using stochastic simulation, that is, computation using random numbers. \u00a0This is the so-called Monte Carlo method, which was developed systematically by the mathematician Stanislaw Ulam and others when trying out designs for the hydrogen bomb in the 1940s and then rapidly picked up in the worlds of physics and chemistry. \u00a0The potential for these methods to solve otherwise intractable statistics problems became apparent in the 1980s, and since then each decade has seen big jumps in the sophistication of algorithms, the capacity of computers to run these algorithms in real time, and the complexity of the statistical models that practitioners are now fitting to data.</p>\n<p>Now, don\u2019t get me wrong\u2014computational and algorithmic advances have become hugely important in non-Bayesian statistical and machine learning methods as well. Bayesian inference has moved, along with statistics more generally, away from simple formulas toward simulation-based algorithms.</p>\n<p><strong>KG: What are its key strengths in comparison with Frequentist methods? Are there things that only Bayesian statistics can provide? What are its main drawbacks?</strong></p>\n<p><strong>AG</strong>: I wouldn\u2019t say there\u2019s anything that only Bayesian statistics can provide. \u00a0When Bayesian methods work best, it\u2019s by providing a clear set of paths connecting data, mathematical/statistical models, and the substantive theory of the variation and comparison of interest. \u00a0From this perspective, the greatest benefits of the Bayesian approach come not from default implementations, valuable as they can be in practice, but in the active process of model building, checking, and improvement. \u00a0\u00a0In classical statistics, improvements in methods often seem distressingly indirect: you try a new test that\u2019s supposed to capture some subtle aspect of your data, or you restrict your parameters or smooth your weights, in some attempt to balance bias and variance. \u00a0Under a Bayesian approach, all the tuning parameters are supposed to be interpretable in real-world terms, which implies\u2014or should imply\u2014that improvements in a Bayesian model come from, or supply, improvements in understanding of the underlying problem under studied. \u00a0The drawback of this Bayesian approach is that it can require a bit of a commitment to construction of a model that might be complicated, and you can end up putting effort into modeling aspects of data that maybe aren\u2019t so relevant for your particular inquiry.</p>\n<p><strong>KG: Are there misunderstandings about Bayesian methods that you often encounter?</strong></p>\n<p><strong>AG</strong>: Yes, but that\u2019s a whole subject in itself\u2014I\u2019ve written papers on the topic! \u00a0The only thing I\u2019ll say here is that Bayesian methods are often characterized as \u201csubjective\u201d because the user must choose a \u201cprior distribution,\u201d that is, a mathematical expression of prior information. \u00a0The prior distribution requires information and user input, that\u2019s for sure, but I don\u2019t see this as being any more \u201csubjective\u201d than other aspects of a statistical procedure, such as the choice of model for the data (for example, logistic regression) or the choice of which variables to include in a prediction, the choice of which coefficients should vary over time or across situations, the choice of statistical test, and so forth. Indeed, Bayesian methods can in many ways be more \u201cobjective\u201d than conventional approaches in that Bayesian inference, with its smoothing and partial pooling, is well adapted to including diverse sources of information and thus can reduce the number of data coding or data exclusion choice points in an analysis.</p>\n<p><strong>KG: Do you think Bayesian methods will one day mostly replace Frequentist statistics?</strong></p>\n<p><strong>AG</strong>: There\u2019s room for lots of methods. \u00a0What\u2019s important in any case is what problems they can solve. \u00a0We use the methods we already know and then learn something new when we need to go further. Bayesian methods offer a clarity that comes from the explicit specification of a so-called \u201cgenerative model\u201d: \u00a0a probability model of the data-collection process and a probability model of the underlying parameters. \u00a0But construction of these models can take work, and it makes sense to me that for problems where you have a simpler model that does the job, you just go with that.</p>\n<p>Looking at the comparison from the other direction, when it comes to big problems with streaming data, Bayesian methods are useful but the Bayesian computation can in practice only be approximate. \u00a0And once you enter the zone of approximation, you can\u2019t cleanly specify where the modeling approximation ends and the computing approximation begins. \u00a0At that point, you need to evaluate any method, Bayesian or otherwise, by looking at what it does to the data, and the best available method for any particular problem might well be set up in a non-Bayesian way.</p>\n</div>\n<div class=\"page-link\"><p>Pages: 1 <a href=\"https://www.kdnuggets.com/2016/12/bayesian-basics-explained.html/2\">2</a></p></div>\n<div class=\"pagi\">\n<hr class=\"grey-line\"/>\n<div class=\"pagi-left\">\n<a href=\"https://www.kdnuggets.com/2016/12/4-cognitive-bias-key-points-data-scientists-need-know.html\" rel=\"prev\"><img height=\"10\" src=\"/images/prv.gif\" width=\"8\"/> <strong>Previous post</strong></a></div>\n<div class=\"pagi-right\">\n<a href=\"https://www.kdnuggets.com/2016/12/poll-analytics-data-mining-data-science-applied-2016.html\" rel=\"next\"><strong>Next post</strong> <img height=\"10\" src=\"/images/nxt.gif\" width=\"8\"/></a></div>\n<br/><br/>\n<div>\n<hr class=\"grey-line\"/><br/>\n<h2>Top Stories Past 30 Days</h2>\n<table align=\"center\" cellpadding=\"3\" cellspacing=\"10\" class=\"latn\" width=\"100%\">\n<tr>\n<td valign=\"top\" width=\"50%\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Popular</b></th></tr>\n<tr><td>\n<ol class=\"three_ol\"><li> <a href=\"/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-1-another-10');\"><b>Another 10 Free Must-Read Books for Machine Learning and Data Science</b></a>\n<li> <a href=\"/2018/05/simplilearn-9-must-have-skills-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-2-simplilearn');\"><b>9 Must-have skills you need to become a Data Scientist, updated</b></a>\n<li> <a href=\"/2019/03/data-science-job-applications.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-3-tell-you');\"><b>What no one will tell you about data science job applications</b></a>\n<li> <a href=\"/2019/03/typical-data-scientist-2019.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-4-typical');\"><b>Who is a typical Data Scientist in 2019?</b></a>\n<li> <a href=\"/2019/02/asking-great-questions-data-scientist.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-5-great-questions');\"><b>Asking Great Questions as a Data Scientist</b></a>\n<li> <a href=\"/2019/03/pareto-principle-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-6-pareto');\"><b>The Pareto Principle for Data Scientists</b></a>\n<li> <a href=\"/2019/03/women-ai-big-data-science-machine-learning.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-mp-7-19-inspiring-women');\"><b>19 Inspiring Women in AI, Big Data, Data Science, Machine Learning</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td> <td valign=\"top\">\n<table cellpadding=\"3\" cellspacing=\"2\">\n<tr><th><b>Most Shared</b></th></tr>\n<tr><td><ol class=\"three_ol\">\n<li> <a href=\"/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-1-ann-genetic');\"><b>Artificial Neural Networks Optimization using Genetic Algorithm with Python</b></a>\n<li> <a href=\"/2019/02/artificial-neural-network-implementation-using-numpy-and-image-classification.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-2-ann-numpy-images');\"><b>Artificial Neural Network Implementation using NumPy and Image Classification</b></a>\n<li> <a href=\"/2019/02/setup-python-environment-machine-learning.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-3-py-ml-setup');\"><b>How to Setup a Python Environment for Machine Learning</b></a>\n<li> <a href=\"/2019/03/typical-data-scientist-2019.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-4-typical');\"><b>Who is a typical Data Scientist in 2019?</b></a>\n<li> <a href=\"/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-5-azure-cert');\"><b>8 Reasons Why You Should Get a Microsoft Azure Certification</b></a>\n<li> <a href=\"/2019/03/pareto-principle-data-scientists.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-6-pareto');\"><b>The Pareto Principle for Data Scientists</b></a>\n<li> <a href=\"/2019/02/running-r-and-python-in-jupyter.html\" onclick=\"ga('send','pageview','/x/pbc/2019/03-26-ms-7-r-python-jupyter');\"><b>Running R and Python in Jupyter</b></a>\n</li></li></li></li></li></li></li></ol>\n</td></tr>\n</table>\n</td>\n</tr>\n</table>\n</div>\n</div>\n<!--#content end--></div>\n<div id=\"sidebar\">\n<div class=\"latn\">\n<h3><b><a href=\"/news/index.html\">Latest News</a></b></h3>\n<ul style=\"font-size:14px; margin-top:5px\">\n<li> <a href=\"https://www.kdnuggets.com/2019/03/datathon-data-science-hackathon-april.html\">Datathon 2019: The International Data Science Hackathon...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/random-forest-python.html\">Explaining Random Forest (with Python Implementation)</a><li> <a href=\"https://www.kdnuggets.com/2019/03/beginners-guide-linear-regression-python-scikit-learn.html\">A Beginner\u2019s Guide to Linear Regression in Python wit...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/interpolation-autoencoders-adversarial-regularizer.html\">Interpolation in Autoencoders via an Adversarial Regula...</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/03-29-cisco-machine-learning-engineer-support-bot-b.html\">Cisco: Machine Learning Engineer/Support Bot Designer [...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/delaware-gain-skills-need-data-driven-career.html\">Gain the Skills You Need to Level-Up in Your Data-Drive...</a></li></li></li></li></li></li></ul>\n</div>\n<div>\n<script type=\"text/javascript\">kda_sid_write(kda_sid_n);</script>\n</div>\n<br/><script src=\"/aps/sbm.js\" type=\"text/javascript\"></script>\n<div class=\"latn\" style=\"margin-top: 15px;\">\n<h3><b>More Recent Stories</b></h3>\n<ul class=\"next-posts\">\n<li> <a href=\"https://www.kdnuggets.com/2019/03/delaware-gain-skills-need-data-driven-career.html\">Gain the Skills You Need to Level-Up in Your Data-Driven Career</a><li> <a href=\"https://www.kdnuggets.com/2019/03/d3js-graph-gallery-data-visualization.html\">D3.js Graph Gallery for Data Visualization</a><li> <a href=\"https://www.kdnuggets.com/2019/03/7-gotchas-data-engineers-google-bigquery.html\">7 \u201cGotchas\u201d for Data Engineers New to Google BigQuery</a><li> <a href=\"https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html\">The Deep Learning Toolset\u200a\u2014\u200aAn Overview</a><li> <a href=\"https://www.kdnuggets.com/2019/03/top-tweets-mar20-26.html\">Top tweets, Mar 20-26: 10 More Free Must-Read Books for Mac...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/ieg-network-google-intel-facebook.html\">Network with Google, Intel, Facebook, LinkedIn &amp; more</a><li> <a href=\"https://www.kdnuggets.com/2019/03/activestate-python-programmer.html\">[PDF] Python: The Programmer\u2019s Lingua Franca</a><li> <a href=\"https://www.kdnuggets.com/2019/03/explainable-ai.html\">Explainable AI or Halting Faulty Models ahead of Disaster</a><li> <a href=\"https://www.kdnuggets.com/2019/03/how-choose-right-chart-type.html\">How to Choose the Right Chart Type</a><li> <a href=\"https://www.kdnuggets.com/2019/03/data-pipelines-luigi-airflow-everything-need-know.html\">Data Pipelines, Luigi, Airflow: Everything you need to know</a><li> <a href=\"https://www.kdnuggets.com/2019/n12.html\">KDnuggets 19:n12, Mar 27: My Best Tips for Agile Data Scien...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/top-news-week-0318-0324.html\">Top Stories, Mar 18-24: Another 10 Free Must-Read Books for Ma...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/databricks-solve-big-problems-data-science-ebook.html\">How to solve 4 big problems in data science \u2013 eBook.</a><li> <a href=\"https://www.kdnuggets.com/2019/03/four-levels-analytics-maturity.html\">The Four Levels of Analytics Maturity</a><li> <a href=\"https://www.kdnuggets.com/2019/03/pedestrian-detection-aerial-images-retinanet.html\">Pedestrian Detection in Aerial Images Using RetinaNet</a><li> <a href=\"https://www.kdnuggets.com/2019/03/data-science-decision-makers.html\">Data Science for Decision Makers: A Discussion with Dr Stelios...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/coursera-earn-ibm-data-science-certificate.html\">Earn an IBM Data Science Certificate</a><li> <a href=\"https://www.kdnuggets.com/2019/03/databricks-scaling-big-data-ai-spark-ai-summit-2019.html\">Scaling Big Data and AI \u2013 Spark + AI Summit 2019</a><li> <a href=\"https://www.kdnuggets.com/jobs/19/03-25-pear-therapeutics-data-scientist.html\">Pear Therapeutics: Data Scientist (Analytics) [San Francisco, ...</a><li> <a href=\"https://www.kdnuggets.com/2019/03/ai-black-box-explanation-problem.html\">The AI Black Box Explanation Problem</a></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></li></ul>\n</div>\n</div><div class=\"breadcrumbs_bottom\">\n<div class=\"breadcumb\">\n<br>\n<a href=\"/\">KDnuggets Home</a> \u00bb <a href=\"/news/index.html\">News</a> \u00bb <a href=\"/2016/index.html\">2016</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/12/index.html\">Dec</a> \u00bb <a href=\"https://www.kdnuggets.com/2016/12/opinions-interviews.html\">Opinions, Interviews</a> \u00bb Bayesian Basics, Explained (\u00a0<a href=\"/2016/n44.html\">16:n44</a>\u00a0)    </br></div>\n</div>\n<!--#content_wrapper end--></div>\n<br>\n<div id=\"footer\">\n<br/>\u00a9 2019 KDnuggets. <a href=\"/about/index.html\">About KDnuggets</a>. \u00a0<a href=\"/news/privacy-policy.html\">Privacy policy</a>. <a href=\"/terms-of-service.html\">Terms of Service</a><br/>\u00a0\n<div class=\"kd_bottom\">\n<div class=\"footer-container\">\n<div class=\"footer-news\">\n<a href=\"/news/subscribe.html\" onclick=\"_gaq.push(['_trackPageview','/x/bot/sub']);\" target=\"_blank\"><b>Subscribe to KDnuggets News</b></a>\n</div>\n<div class=\"footer-sm\">\n<a href=\"https://twitter.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/twt']);\" target=\"_blank\"><img height=\"32\" src=\"/images/tw_c48.png\" width=\"32\"/></a>\n<a href=\"https://facebook.com/kdnuggets\" onclick=\"_gaq.push(['_trackPageview','/x/bot/fb']);\" target=\"_blank\"><img alt=\"Facebook\" height=\"32\" src=\"/images/fb_c48.png\" width=\"32\"/></a>\n<a href=\"https://www.linkedin.com/groups/54257\" onclick=\"_gaq.push(['_trackPageview','/x/bot/in']);\" target=\"_blank\"><img alt=\"LinkedIn\" height=\"32\" src=\"/images/in_c48.png\" width=\"32\"/></a>\n</div>\n</div>\n<div class=\"close-footer\">X</div>\n</div>\n<script type=\"text/javascript\">\n  jQuery('.close-footer').click(\n      function(){       \n         jQuery('.kd_bottom').hide();\n      }\n   );\n</script> </div>\n<div class=\"clear\"><!--blank--></div>\n</br></div>\n<div style=\"display: none;\"><div id=\"boxzilla-box-82996-content\"><script type=\"text/javascript\">(function() {\n\tif (!window.mc4wp) {\n\t\twindow.mc4wp = {\n\t\t\tlisteners: [],\n\t\t\tforms    : {\n\t\t\t\ton: function (event, callback) {\n\t\t\t\t\twindow.mc4wp.listeners.push({\n\t\t\t\t\t\tevent   : event,\n\t\t\t\t\t\tcallback: callback\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n})();\n</script><!-- MailChimp for WordPress v4.1.14 - https://wordpress.org/plugins/mailchimp-for-wp/ --><form class=\"mc4wp-form mc4wp-form-77281\" data-id=\"77281\" data-name=\"Subscribe to KDnuggets News\" id=\"mc4wp-form-1\" method=\"post\"><div class=\"mc4wp-form-fields\"><div class=\"header-container\">\n<img align=\"left\" src=\"/wp-content/uploads/envelope.png\"><a href=\"/news/subscribe.html\">Get KDnuggets, a leading newsletter on AI, \r\n  Data Science, and Machine Learning</a>\n</img></div>\n<div class=\"form-fields\">\n<div class=\"field-container\"><label>Email:</label><input maxlength=\"60\" name=\"EMAIL\" placeholder=\"Your email\" required=\"\" size=\"30\" type=\"email\"/></div>\n<div class=\"field-container submit-container\"><div class=\"form-button\" onclick=\"document.getElementById('mc4wp-form-1').submit()\">Sign Up</div></div>\n</div>\n<label style=\"display: none !important;\">Leave this field empty if you're human: <input autocomplete=\"off\" name=\"_mc4wp_honeypot\" tabindex=\"-1\" type=\"text\" value=\"\"/></label><input name=\"_mc4wp_timestamp\" type=\"hidden\" value=\"1554055417\"/><input name=\"_mc4wp_form_id\" type=\"hidden\" value=\"77281\"/><input name=\"_mc4wp_form_element_id\" type=\"hidden\" value=\"mc4wp-form-1\"/></div><div class=\"mc4wp-response\"></div></form><!-- / MailChimp for WordPress Plugin -->\n</div></div><script type=\"text/javascript\">(function() {function addEventListener(element,event,handler) {\n\tif(element.addEventListener) {\n\t\telement.addEventListener(event,handler, false);\n\t} else if(element.attachEvent){\n\t\telement.attachEvent('on'+event,handler);\n\t}\n}function maybePrefixUrlField() {\n\tif(this.value.trim() !== '' && this.value.indexOf('http') !== 0) {\n\t\tthis.value = \"http://\" + this.value;\n\t}\n}\n\nvar urlFields = document.querySelectorAll('.mc4wp-form input[type=\"url\"]');\nif( urlFields && urlFields.length > 0 ) {\n\tfor( var j=0; j < urlFields.length; j++ ) {\n\t\taddEventListener(urlFields[j],'blur',maybePrefixUrlField);\n\t}\n}/* test if browser supports date fields */\nvar testInput = document.createElement('input');\ntestInput.setAttribute('type', 'date');\nif( testInput.type !== 'date') {\n\n\t/* add placeholder & pattern to all date fields */\n\tvar dateFields = document.querySelectorAll('.mc4wp-form input[type=\"date\"]');\n\tfor(var i=0; i<dateFields.length; i++) {\n\t\tif(!dateFields[i].placeholder) {\n\t\t\tdateFields[i].placeholder = 'YYYY-MM-DD';\n\t\t}\n\t\tif(!dateFields[i].pattern) {\n\t\t\tdateFields[i].pattern = '[0-9]{4}-(0[1-9]|1[012])-(0[1-9]|1[0-9]|2[0-9]|3[01])';\n\t\t}\n\t}\n}\n\n})();</script><script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_options = {\"testMode\":\"\",\"boxes\":[{\"id\":82996,\"icon\":\"&times;\",\"content\":\"\",\"css\":{\"background_color\":\"#eeee22\",\"width\":600,\"border_width\":2,\"border_style\":\"double\",\"position\":\"bottom-left\"},\"trigger\":{\"method\":\"time_on_page\",\"value\":\"3\"},\"animation\":\"fade\",\"cookie\":{\"triggered\":0,\"dismissed\":336},\"rehide\":true,\"position\":\"bottom-left\",\"screenWidthCondition\":{\"condition\":\"larger\",\"value\":500},\"closable\":true,\"post\":{\"id\":82996,\"title\":\"Subscribe to KDnuggets\",\"slug\":\"subscribe-to-kdnuggets\"}}]};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla/assets/js/script.min.js?ver=3.2.5\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar boxzilla_stats_config = {\"ajaxurl\":\"https:\\/\\/www.kdnuggets.com\\/wp-admin\\/admin-ajax.php?action=boxzilla_stats_track\"};\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/boxzilla-stats/assets/js/tracking.min.js?ver=1.0.4\" type=\"text/javascript\"></script>\n<script src=\"https://www.kdnuggets.com/wp-includes/js/wp-embed.min.js?ver=4.9.10\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\">\n/* <![CDATA[ */\nvar mc4wp_forms_config = [];\n/* ]]> */\n</script>\n<script src=\"https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/forms-api.min.js?ver=4.1.14\" type=\"text/javascript\"></script>\n<!--[if lte IE 9]>\n<script type='text/javascript' src='https://www.kdnuggets.com/wp-content/plugins/mailchimp-for-wp/assets/js/third-party/placeholders.min.js?ver=4.1.14'></script>\n<![endif]-->\n<!--/.main_wrapper--></div>\n<script src=\"https://s7.addthis.com/js/300/addthis_widget.js#pubid=gpsaddthis\" type=\"text/javascript\"></script>\n</body>\n</html>\n<!-- Dynamic page generated in 0.693 seconds. -->\n<!-- Cached page generated by WP-Super-Cache on 2019-03-31 14:03:37 -->\n<!-- Compression = gzip -->", "content_html": "<div class=\"post\" id=\"post-\">\n<div class=\"author-link\"><b>By <a href=\"https://www.kdnuggets.com/author/kevin-gray\" rel=\"author\" title=\"Posts by Kevin Gray\">Kevin Gray</a>, Cannon Gray.</b></div>\n<blockquote><p>\n<strong>Editor's note</strong>: The following is an interview with Columbia University Professor Andrew Gelman conducted by Marketing scientist Kevin Gray, in which Gelman spells out the ABCs of Bayesian statistics.\n</p></blockquote>\n<p><strong>Kevin Gray: Most marketing researchers have heard of Bayesian statistics but know little about it. \u00a0Can you briefly explain in layperson's terms what it is and how it differs from the 'ordinary' statistics most of us learned in college?</strong></p>\n<p><strong>Andrew Gelman</strong>: Bayesian statistics uses the mathematical rules of probability to combines data with \u201cprior information\u201d to give inferences which (if the model being used is correct) are more precise than would be obtained by either source of information alone.</p>\n<p>Classical statistical methods avoid prior distributions. \u00a0In classical statistics, you might include in your model a predictor (for example), or you might exclude it, or you might pool it as part of some larger set of predictors in order to get a more stable estimate. \u00a0These are pretty much your only choices. \u00a0In Bayesian inference you can\u2014OK, you must\u2014assign a prior distribution representing the set of values the coefficient can be. \u00a0You can reproduce the classical methods using Bayesian inference: \u00a0In a regression prediction context, setting the prior of a coefficient to uniform or \u201cnoninformative\u201d is mathematically equivalent to including the corresponding predictor in a least squares or maximum likelihood estimate; setting the prior to a spike at zero is the same as excluding the predictor, and you can reproduce a pooling of predictors thorough a joint deterministic prior on their coefficients. \u00a0But in \u00a0Bayesian inference you can do much more: \u00a0by setting what is called an \u201cinformative prior,\u201d you can partially constrain a coefficient, setting a compromise between noisy least-squares estimation or completely setting it to zero. \u00a0It turns out this is a powerful tool in many problems\u2014especially because in problems with structure, we can fit so-called hierarchical models which allow us to estimate aspects of the prior distribution from data.</p>\n<p><img alt=\"Bayesian neon\" src=\"/wp-content/uploads/bayesian-neon.jpeg\" width=\"99%\"/></p>\n<p><strong>KG: Could you give us a quick overview of its history and how it has developed over the years?</strong></p>\n<p><strong>AG</strong>: \u00a0The theory of Bayesian inference originates with its namesake, Thomas Bayes, an 18th-century English cleric, but it really took off in the late 18th century with the work of the French mathematician and physicist Pierre-Simon Laplace. Bayesian methods were used for a long time after that to solve specific problems in science, but it was in the mid-20th century that they became proposed as a general statistical tool. Some key figures include John Maynard Keynes and Frank Ramsey who in the 1920s developed an axiomatic theory of probability; Harold Jeffreys and Edwin Jaynes, who from the 1930s through the 1970s developed Bayesian methods for a variety of problems in the physical sciences; Jimmie Savage and Dennis Lindley, mathematicians who in research from the 1950s through the 1970s connected and contrasted Bayesian methods with classical statistics; and, not least, Alan Turing, who used Bayesian probability methods to crack the Enigma code in the second world war, and his colleague I. J. Good, who explored and wrote prolifically about these ideas over the succeeding decades.</p>\n<p>Within statistics, Bayesian and related methods have become gradually more popular over the past several decades, often developed in different applied fields, such as animal breeding in the 1950s, educational measurement in the 1960s and 1970s, spatial statistics in the 1980s, and marketing and political science in the 1990s. \u00a0Eventually a sort of critical mass developed in which Bayesian models and methods that had been developed in different applied fields became recognized as more broadly useful.</p>\n<p>Another factor that has fostered the spread of Bayesian methods is progress in computing speed and improved computing algorithms. \u00a0\u00a0Except in simple problems, Bayesian inference requires difficult mathematical calculations\u2014high-dimensional integrals\u2014which are often most practically computed using stochastic simulation, that is, computation using random numbers. \u00a0This is the so-called Monte Carlo method, which was developed systematically by the mathematician Stanislaw Ulam and others when trying out designs for the hydrogen bomb in the 1940s and then rapidly picked up in the worlds of physics and chemistry. \u00a0The potential for these methods to solve otherwise intractable statistics problems became apparent in the 1980s, and since then each decade has seen big jumps in the sophistication of algorithms, the capacity of computers to run these algorithms in real time, and the complexity of the statistical models that practitioners are now fitting to data.</p>\n<p>Now, don\u2019t get me wrong\u2014computational and algorithmic advances have become hugely important in non-Bayesian statistical and machine learning methods as well. Bayesian inference has moved, along with statistics more generally, away from simple formulas toward simulation-based algorithms.</p>\n<p><strong>KG: What are its key strengths in comparison with Frequentist methods? Are there things that only Bayesian statistics can provide? What are its main drawbacks?</strong></p>\n<p><strong>AG</strong>: I wouldn\u2019t say there\u2019s anything that only Bayesian statistics can provide. \u00a0When Bayesian methods work best, it\u2019s by providing a clear set of paths connecting data, mathematical/statistical models, and the substantive theory of the variation and comparison of interest. \u00a0From this perspective, the greatest benefits of the Bayesian approach come not from default implementations, valuable as they can be in practice, but in the active process of model building, checking, and improvement. \u00a0\u00a0In classical statistics, improvements in methods often seem distressingly indirect: you try a new test that\u2019s supposed to capture some subtle aspect of your data, or you restrict your parameters or smooth your weights, in some attempt to balance bias and variance. \u00a0Under a Bayesian approach, all the tuning parameters are supposed to be interpretable in real-world terms, which implies\u2014or should imply\u2014that improvements in a Bayesian model come from, or supply, improvements in understanding of the underlying problem under studied. \u00a0The drawback of this Bayesian approach is that it can require a bit of a commitment to construction of a model that might be complicated, and you can end up putting effort into modeling aspects of data that maybe aren\u2019t so relevant for your particular inquiry.</p>\n<p><strong>KG: Are there misunderstandings about Bayesian methods that you often encounter?</strong></p>\n<p><strong>AG</strong>: Yes, but that\u2019s a whole subject in itself\u2014I\u2019ve written papers on the topic! \u00a0The only thing I\u2019ll say here is that Bayesian methods are often characterized as \u201csubjective\u201d because the user must choose a \u201cprior distribution,\u201d that is, a mathematical expression of prior information. \u00a0The prior distribution requires information and user input, that\u2019s for sure, but I don\u2019t see this as being any more \u201csubjective\u201d than other aspects of a statistical procedure, such as the choice of model for the data (for example, logistic regression) or the choice of which variables to include in a prediction, the choice of which coefficients should vary over time or across situations, the choice of statistical test, and so forth. Indeed, Bayesian methods can in many ways be more \u201cobjective\u201d than conventional approaches in that Bayesian inference, with its smoothing and partial pooling, is well adapted to including diverse sources of information and thus can reduce the number of data coding or data exclusion choice points in an analysis.</p>\n<p><strong>KG: Do you think Bayesian methods will one day mostly replace Frequentist statistics?</strong></p>\n<p><strong>AG</strong>: There\u2019s room for lots of methods. \u00a0What\u2019s important in any case is what problems they can solve. \u00a0We use the methods we already know and then learn something new when we need to go further. Bayesian methods offer a clarity that comes from the explicit specification of a so-called \u201cgenerative model\u201d: \u00a0a probability model of the data-collection process and a probability model of the underlying parameters. \u00a0But construction of these models can take work, and it makes sense to me that for problems where you have a simpler model that does the job, you just go with that.</p>\n<p>Looking at the comparison from the other direction, when it comes to big problems with streaming data, Bayesian methods are useful but the Bayesian computation can in practice only be approximate. \u00a0And once you enter the zone of approximation, you can\u2019t cleanly specify where the modeling approximation ends and the computing approximation begins. \u00a0At that point, you need to evaluate any method, Bayesian or otherwise, by looking at what it does to the data, and the best available method for any particular problem might well be set up in a non-Bayesian way.</p>\n</div>"}